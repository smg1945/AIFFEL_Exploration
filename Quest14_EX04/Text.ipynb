{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from konlpy.tag import Mecab\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import Bidirectional, Attention\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 및 테스트 데이터 불러오기\n",
    "train_data = pd.read_table('C:/Users/ZAKAR/Documents/GitHub/AIFFEL/Exploration/Quest14_EX04/data/ratings_train.txt')\n",
    "test_data = pd.read_table('C:/Users/ZAKAR/Documents/GitHub/AIFFEL/Exploration/Quest14_EX04/data/ratings_test.txt')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab('C:/mecab/mecab-ko-dic')\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def load_data(train_data, test_data, num_words=None):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for (sentence, label) in train_data:\n",
    "        tokens = tokenizer.morphs(sentence)\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "        X_train.append(tokens)\n",
    "        y_train.append(label)\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "    for (sentence, label) in test_data:\n",
    "        tokens = tokenizer.morphs(sentence)\n",
    "        tokens = [token for token in tokens if token not in stopwords]\n",
    "        X_test.append(tokens)\n",
    "        y_test.append(label)\n",
    "\n",
    "    words = np.concatenate(X_train).tolist()\n",
    "    counter = Counter(words)\n",
    "    if num_words is not None:\n",
    "        counter = counter.most_common(num_words - 4)\n",
    "    else:\n",
    "        counter = counter.items()\n",
    "    word_to_index = {word: index + 4 for index, (word, _) in enumerate(counter)}\n",
    "    word_to_index['<PAD>'] = 0\n",
    "    word_to_index['<BOS>'] = 1\n",
    "    word_to_index['<UNK>'] = 2\n",
    "    word_to_index['<UNUSED>'] = 3\n",
    "\n",
    "    X_train = [[word_to_index.get(word, 2) for word in sentence] for sentence in X_train]\n",
    "    X_test = [[word_to_index.get(word, 2) for word in sentence] for sentence in X_test]\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, word_to_index\n",
    "\n",
    "train_data = [(\"안녕하세요\", 1), (\"반갑습니다\", 1), (\"잘가요\", 0), (\"안녕히가세요\", 0)]\n",
    "test_data = [(\"안녕\", 1), (\"잘있어요\", 0)]\n",
    "num_words = 10000\n",
    "X_train, y_train, X_test, y_test, word_to_index = load_data(train_data, test_data)\n",
    "\n",
    "index_to_word = {index:word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    return [word_to_index['<BOS>']]+[word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "\n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    return ' '.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1:])\n",
    "\n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_to_index)\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(input_dim=vocab_size, output_dim=4, input_length=max_length))\n",
    "model1.add(LSTM(units=128))\n",
    "model1.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(input_dim=vocab_size, output_dim=4, input_length=max_length))\n",
    "model2.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model2.add(GlobalMaxPooling1D())\n",
    "model2.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(Embedding(input_dim=vocab_size, output_dim=4, input_length=max_length))\n",
    "model3.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model3.add(Attention())\n",
    "model3.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "model1.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "model2.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))\n",
    "model3.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model1.history.history['loss'], label='Training Loss')\n",
    "plt.plot(model1.history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model 1 - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model1.history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(model1.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model 1 - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model 2\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model2.history.history['loss'], label='Training Loss')\n",
    "plt.plot(model2.history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model 2 - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model2.history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(model2.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model 2 - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Model 3\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model3.history.history['loss'], label='Training Loss')\n",
    "plt.plot(model3.history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model 3 - Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model3.history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(model3.history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model 3 - Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model1.get_layer('embedding_layer').get_weights()[0]\n",
    "\n",
    "print(\"Embedding Weights Shape:\", embedding_weights.shape)\n",
    "\n",
    "avg_embedding = np.mean(embedding_weights)\n",
    "std_embedding = np.std(embedding_weights)\n",
    "print(\"Average Embedding Value:\", avg_embedding)\n",
    "print(\"Standard Deviation of Embedding:\", std_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_file_path = '/data/word2vec_ko.model'\n",
    "word2vec_model = Word2Vec.load(word2vec_file_path)\n",
    "\n",
    "vector = word2vec_model.wv['끝']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
