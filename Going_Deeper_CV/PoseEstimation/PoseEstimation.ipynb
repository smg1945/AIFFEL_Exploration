{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 프로젝트는 다양한 모델들을 학습하여 Pose Estimation 결과를 비교해보는 과제이다. 간단한 Simplebaseline 모델부터 Stacked Hourglass 등의 모델까지 이미 구현된 모델의 코드로 학습 진행 경과 및 결과를 시각화하여 비교해보도록 하자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과제목표\n",
    "\n",
    "1. tfrecord를 활용한 데이터셋 구성과 전처리를 통해 프로젝트 베이스라인 구성을 확인하였다.\n",
    "    - MPII 데이터셋을 기반으로 1epoch에 30분 이내에 학습가능한 베이스라인을 구축하였다.\n",
    "2. simplebaseline 모델을 정상적으로 구현하였다.\n",
    "    - simplebaseline 모델을 구현하여 실습코드의 모델을 대체하여 정상적으로 학습이 진행되었다.\n",
    "3. Hourglass 모델과 simplebaseline 모델을 비교분석한 결과를 체계적으로 정리하였다.\n",
    "    - 두 모델의 pose estimation 테스트결과 이미지 및 학습진행상황 등을 체계적으로 비교분석하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>데이터셋 준비 및 전처리<h2/>\n",
    "<h5>모델의 훈련에 필요한 데이터는 MPII Human Pose Dataset을 사용하였다.<h5/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, json, os, math\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "import ray\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PROJECT_PATH = 'C:/Users/ZAKAR/Documents/GitHub/AIFFEL/Exploration/Going_Deeper_CV/PoseEstimation'\n",
    "IMAGE_PATH = os.path.join(PROJECT_PATH, 'data', 'images')\n",
    "MODEL_PATH = os.path.join(PROJECT_PATH, 'models')\n",
    "TFRECORD_PATH = os.path.join(PROJECT_PATH, 'tfrecords_mpii')\n",
    "TRAIN_JSON = os.path.join(PROJECT_PATH, 'data', 'train.json')\n",
    "VALID_JSON = os.path.join(PROJECT_PATH, 'data', 'validation.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에 담겨 있는 사람들의 pose keypoint 정보들을 가지고 있는 JSON 파일은 Pose Estimation을 위한 label로 사용할 수 있다. JSON 파일을 파싱하는 함수는 tfrecords_mpii.py에서 호출하여 사용할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TFRecord 파일 만들기<h3/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "큰 크기의 데이터셋을 다루게되면 학습 과정에서 GPU의 연산 속도보다 HDD I/O가 느리기 때문에 병목 현상이 발생하고 효율성이 떨어지는 문제가 생긴다. 이러한 문제를 해결하여 학습 속도를 향상시키려면 어떻게 해야 할까?\n",
    "<br><br>\n",
    "'데이터 직렬화'는 데이터에서 의미 있는 정보만을 추출해 나열함으로써 데이터의 크기를 줄이고 처리 속도를 높일 수 있는 방법이다. Tensorflow에서는 TFRecord 형태로 데이터셋을 변환하여 데이터 직렬화를 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 대규모의 데이터를 다룰 때에는 데이터를 더 효율적으로 관리하고, 학습 속도를 높이며, 메모리 사용량을 줄이기 위하여 annotation을 shard로 나누는 방식을 사용한다. 이를 sharding이라고 하며 하나의 큰 데이터를 적절한 크기와 개수의 파일들로 나누어 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-26 17:29:57,620\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': './data/images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': './data/images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=22832)\u001b[0m Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=7324)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=25572)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=27132)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\u001b[32m [repeated 52x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=14324)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
      "Successfully wrote 25204 annotations to TF Records.\n"
     ]
    }
   ],
   "source": [
    "from mpii_gtuV0hd.tfrecords_mpii import main\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=22500)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(build_single_tfrecord pid=27176)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "from mpii_gtuV0hd.preprocess import Preprocessor\n",
    "\n",
    "\n",
    "preprocessed_data = Preprocessor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>모델 설계 및 학습<h2/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미 구현된 Hourglass 모델과 Simplebaseline 모델을 불러와서 준비한 데이터로 학습을 시켜보자."
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyAAAAFbCAIAAACiaH0UAAAgAElEQVR4Aexdd1yUx9Y+2+hVQAQJig0LGsESGxbsHUURscRYYmLitUWxJMaSxNxEvSYxiS2JveSqsbfYC/ZeMHZAUYpSpCzLlu+XPR9zJ+8uK23ZXTjzB5x3dsqZZ9ozHTRkCAFCgBAgBAgBQoAQIARKFQEo1dAoMEKAECAECAFCgBAgBAgBDREsKgSEACFQbhFQl9iUW2goYYQAIWBkBIhgGRlgCp4QIAQIAUKAECAEKh4CRLAqXp5TigmBioGAQqF4oTXPnz9PSEh4pjVPtSZea+K0JjbfPPmniY2Nffz4cXZ2dsVAi1JJCBACpYwAEaxSBpSCIwQIATNB4Pvvv69UqZKdnZ21tbVEIhGJRFB0c/DgQY1Go1KpzCRRpAYhQAhYCgJEsCwlp0hPQoAQKCwCarVao9F4e3vb2Ng0adKkefPmLVq0aN26dXBwcPv27Tt2DOmsNd26devZs2fv3r379u0bGhoaFhYWHh4+cODA4cOHd+jQwc7Oztvb++bNm0SwCos7uSMECAEOASJYHBgkEgKEQHlB4OTJk2KxeMSIEQqtycvLU+YblUqp3fuuQR6mN8Vff/21RCIhgqUXHLIkBAiBwiBABKswKJEbQoAQsBgEkDYFBwdbW1tHR0crlUqeXeVpDXItZp+Xl6dQKPCvQqGIj49v3bo1APj6+t64cYNmsCwm70lRQsCcECCCZU65QboQAoRAiRFQq9UvXrxwcHAIDg7OyclRq9UqlYpd16DSGvxk9ihoNBqFQqHRaFauXOno6AgAPj4+RLBKnCEUACFQQREgglVBM56STQiUSwRwN/rMmTMBYN26dRrN3+uA/FIg/8ns0VKtViuVyuzs7P79+8tkMhcXl7feeosIVrksJ5QoQqAMECCCVQYgUxSEACFQRgioVCq5XF6lShUvL6+0tLQiEay8vDyNRnPkyJHKlSt37ty5ffv2Xl5e169fpyXCMso8ioYQKF8IEMEqX/lJqSEEKjACuNK3evVqiUTy6aef4gQVP2Ul4FuCGSylUqlSqaKiogBgxYoVI0eOdHFxoVOEFbhAUdIJgRIhQASrRPCRZ0KAEDAfBJRKpUajady4sb29fUxMTJEIllL599HCu3fv1qtXz8fH5969e2PHjnVyciKCZT75S5oQApaFABEsy8ov0pYQIAT0I6BSqZRK5alTp6ytrcPDw+VyObor5AwWkrOlS5cCQFRUlFqtHjx4sKurKxEs/XCTLSFACLwJASJYb0KIficECAFLQAB3UIWGhgLAwYMHBct/LAU832JucPrq1atX3bt3d3JyOnDggEajCQ8PJ4LFcCOBECAEiooAEayiIkbuCQFCwOwQwAOA8fHxXl5eQUFBqampTEWeURW0BwtvZ9ixY4eDg8OAAQNevnyp0WgiIiKIYDEYSSAECIGiIkAEq6iIkXtCgBAwOwTy8vLUavX8+fMBYPny5exeKwGjEnyyTVoqlSo7O/uDDz4AgPXr12PyIiMjiWCZXU6TQoSA5SBABMty8oo0JQQIgQIQQEYVFBTk6ur69OlTJFLo9o0zWLi2eOHCBU9Pz8DAwEePHqFHIlgFgE3WhAAhUCgEiGAVCiZyRAgQAmaLAL57s/q31TY2NhMnTsS7Rtn+qjcSLHS/YMECAPjyyy9VKhWuGBLBMtscJ8UIAYtAgAiWRWQTKUkIEAIFIpCbm6vRaFq1aiUSia5cuaK7DsjIlu5PeHjw0aNHQUFBvr6+Fy5c0Gg0OTk5Go2GCFaBiNMPhAAhUAgEiGAVAiRyQggQAuaKAL4teOXKFVdX127dur1+/VqXRRkgWDh9tWHDBrFYPGbMmNzcXHzymQiWuWY46UUIWAwCRLAsJqtIUUKAENBFAKevRowYAQDbtm1j+9YZqTKwRIjs6tWrV4MGDbKxsdm2bZtGo0GORQRLF2qyIQQIgSIhQASrSHCRY0KAEDAjBNRqtUqlSktLq1Onjr+///Pnz9n0VWEIFm5vP3bsmL29fbdu3dLT0zUajVKpRHtaIjSjnCZVCAELRIAIlgVmGqlMCBACWgQUCoVKpVq0aJFMJvv222/1kqqCZrDQPi8vb+bMmQDw3XffIbtSqVREsKh8EQKEQMkRIIJVcgwpBEKAEDABAmq1Go/7tWvXzsrK6u7duzh9xSaxUKeCCBZub799+3bVqlUDAgLQO17pTgTLBNlJURIC5Q4BIljlLkspQYRAxUAAydDu3butra1xf3qRCBaC9NNPPwHAlClT8C54DIEIVsUoQZRKQsC4CBDBMi6+FDohQAgYCQHc3t6vXz8AOHbsGGNXhZnBwu3tz549a9++vZub259//qnRaJBXqdVqIlhGyjIKlhCoUAgQwapQ2U2JJQTKCQI4ffXw4cPq1asHBwcnJydrNBqkTYUhWLhba9++fWKxOCwsDDe2oyURrHJSRCgZhICpESCCZeocoPgJAUKg6Ajg9NW0adMA4Ndff8UACrnJHXlYZmbmqFGjAOC3337TaDS4nYuWCIueFeSDECAE9CNABEs/LmRLCBACZosAMqT09PTmzZt7e3vfv3/fwKyV7iZ33N5+7do1qVTasmXLxMREPD+I6aUZLLPNd1KMELAsBIhgWVZ+kbaEACHw/7NNa9euBYBZs2Yh3+KJVEEyv4yIjw/OnTsXLxflZ79oDxYVMkKAECg5AkSwSo4hhUAIEAJlhwCbYQoNDRWJRBcvXmS0iSdJemU2U/X48WN/f38/P7+rV6/i9nbePRGssstOiokQKL8IEMEqv3lLKSMEyiMCuMB39uxZDw+PAQMGpKWl4fog+2tguZC52bx5MwC8++67yK5UKhURrPJYWChNhIApESCCZUr0KW5CgBAoKgJyuVyj0YwZMwYAtm/fzjiTQOAJEy9rNJqXL1/27NnTzs5u165duL1dsKRIM1hFzRRyTwgQAroIEMHSxYRsCAFCwEwRwOmrR48e1a9fPzAwMDY2lq0PFp5gnT59GgA6dOigyjdEsMw0v0ktQsCSESCCZcm5R7oTAhUMAbxMYfHixQCwcOFCJFWCCSpdS+YA56smTZokkUgWL16Mj+0gu2Ju2B4veuy5ghUuSi4hUMoIEMEqZUApOEKAEDASAmq1WqVSZWdnd+3a1dnZ+cqVK+z6dYyRJ0m6Mto8ePDAzc2tfv36cXFxbOKKCUjOaInQSDlIwRICFQoBIlgVKrspsYSABSOA01e4P33ChAlyuRwpF0uSLqnSnc3CxwcnTJgguFyU90sEi0FKAiFACBQbASJYxYaOPBIChECZIpCXl6dUKseMGWNtbX3w4EHB9QrIpVAhwYwU+0xOTg4KCvLw8IiOjlapVLijS5eEEcEq03ylyAiBcooAEaxymrGULEKgfCGA01fR0dE+Pj7dunVLSkrC7e1s5skwwcJf9+3bBwC9evVitzMgSIyBoTMiWOWr7FBqCAHTIEAEyzS4U6yEACFQJASQ9MycOZO9HoiXVxWeYGVlZQ0YMMDGxmbTpk24k535JYJVpLwgx4QAIVAYBIhgFQYlckMIEAKmRAAfw3nx4sU777xTs2bNu3fvqtVqpVLJE6M3zmDdvHnT2to6MDAwMzNTQM74cOgUoSlzmuImBMoRAkSwylFmUlIIgXKKAE5frVy5EgBmz56N1AoZFZuFMkCw8IWc2bNnSyQSfHwwLy9PQKpYOESwymkhomQRAmWNABGsskac4iMECIEiIYC3gWZmZg4ePLhSpUqnT59mtzPwJMkwwXr+/Lm3t7evr++DBw/Ywzg8qeJl2oNVpAwix4QAIaAXASJYemEhS0KAEDAXBJDuHD582MnJKTIyMiMjAylX4WewVCrVhg0b+McHMW08qeJlIljmkvekByFgyQgQwbLk3CPdCYHyjgC76Wr69OkAsGXLFo1Gk5ubi+kuzAyWRqNJT09v1aqVo6Pj0aNHBbcz6IZDS4TlvUxR+giBMkKACFYZAU3REAKEQDEQwM1SV69erVWrVqtWrfB2Br33VxlYIjx58qRIJAoODlYoFBggasLPWvEyzWAVI6fICyFACAgQIIIlAIQ+CQFCwIwQQC6F16/j44NsB1UhlwgVCsWoUaOkUumKFSsEd5PypIqXiWCZUQkgVQgBi0WACJbFZh0pTgiUdwSQSyUkJISEhPj5+V2/ft3A/VV6Z7DUavWTJ0+cnZ1r1KiRlpaGm7d4LoUQ8kuNtERY3osVpY8QKCMEiGCVEdAUDSFACBQVAZy+2r59OwB8/PHH+FROkWaw1Gr1N998AwBTp07FyxoEXApVEljSDFZRc4rcEwKEgC4CRLB0MSEbQoAQMD0CSHpycnI++ugjBwf7vXv3sgU+fgqKyXpnsDIzM2vWrOnu7n7z5k1kZgIuhekUWBLBMn32kwaEgOUjQATL8vOQUkAIlEcE8DbRc+fOu7u79+zZKzMzU6lUMpKEKeaJkV6CtW3bNgAIDQ3Ve7MDI2d8OLREWB5LE6WJEDABAkSwTAA6RUkIEAJvRADZz7///fcC37JlyzUaDb73LCBDjCQJCBbe5tC1a1dra2uc/UKXAu+ohsCSZrDemDvkgBAgBN6IABGsN0JEDggBQqCsEcDdV3fu3KlSpcrbb78dFxeHO6iQRTFSxRMjAcFSq9VXrlyxtbVt0KBBbm4uvmao6x0TxodDM1hlndkUHyFQThEgglVOM5aSRQhYMgLIhzZt2gQAn332GXsbR5chMbKlS7A+/vhjAFi0aJHuT4gN80sEy5ILC+lOCJgpAkSwzDRjSC1CoMIigOwqOTm5Z8+e3t7e0dHRbH2wkARLo9EkJye7u7t7eHgkJyczIqXrHUEmglVhCxslnBAwHgJEsIyHLYVMCBACxUEACdaZM2ekUungwYPztIaRJAEZYvZsmgod4N2ko0ePZvaoisC7Xkvag1WcbCM/hAAh8E8EiGD9Ew/6IgQIAZMigIRJoVBERUVJpdLNmzfj9BUjUgKGxOwZkVKr1UqlslGjRtbW1hcvXmT2mCyBd72WRLBMWgQockKgnCBABKucZCQlgxAoHwjg9NXt27etra1bt26N+9MLulyUZ0tIpJBvHThwQCaThYSE4F0PAhLGPnmBl4lglY+yRKkgBEyLABEs0+JPsRMChIAeBDZs2AAAX331Fd62wBMptVrNbsNiT9+gA5VKhccPw8PDAWDjxo0868JoBEHptSSCpSdLyIoQIASKiAARrCICRs4JAULAaAjgNFJ8fHy1atX8/f0fPnwoWOArTMwxMTFubm4+Pj7Z2dlEsAqDGLkhBAgBYyBABMsYqFKYhAAhUHwEfv/9dwBo3rz5smXLFi9e/O233y5YsGDevHmff/75rFmzoqKipkyZMnHixPHjx48bN27s2LGjR48eNWrUu+++O3To0OHDh7dq1QoAvvzyS9SAn7IS8C1+WZCXaQar+JlHPgkBQiAfASJY+UjQf0KAEDApAkhx0tLSIiIiAEAmk4m1RiQSMcGwLJFIxGKxSCQCgHv37mFqiGCZNFcpckKg4iJABKvi5j2lnBAwKwSQYCUkJAQGBjo5OU2ZMmWR1nz33XdLly79+eefV6xY8euvv65evXrdunUbN278/ffft27d+scff+zatWvv3r379+8/cODAiRMnGjVqBABPnjzB1BHBMqtcJmUIgYqDABGsipPXlFJCwKwRQIKVlZU1fPhwT0/PmJiYoqqLJxDbtGlDBKuo0JF7QoAQKHUEiGCVOqQUICFACBQHAUawhg0b5unpefPmTXwWEI8N6j05KDhFqFAo1Go1EazioE9+CAFCoLQRIIJV2ohSeIQAIVAsBAQE69atW2984Fmw/Ieb04lgFQt+8kQIEAKljAARrFIGlIIjBAiB4iFABKt4uJEvQoAQME8EiGCZZ76QVoRAhUOACFaFy3JKMCFQrhEgglWus5cSRwhYDgJEsCwnr0hTQoAQeDMCRLDejBG5IAQIgTJAgAhWGYBMURAChECZIUAEq8ygpogIAULAEAJEsAyhQ78RAoSApSFABMvScoz0JQTKKQJmSLAqVap08+ZNjUaDN2yVU+ApWYQAIWAUBIhgGQXWch8o3j9EfwmBUkRAqVSqVKrXr18PHTrU09PTtNc0qNXqIUOGODo6Xrt2TaVS5eXllWJKKShCoOQI4ICk3Pc1Fp1AIlgWnX2mUZ4qtmlwrxixKhSKkSNHmpxgaTSaYcOGiUSiixcvVgzgKZWEACFQyggQwSplQMt9cMiu0tLSUlJSXpIhBEoPgZSUlLS0tNjY2O7du3t5eeHanFKp1Gg0/IWiBckajaZULhrFibTk5OSQkBCRSHTgwIH09PSkpKQUMoSAGSCQnJyclJSUqDVyubzc9zgWnUAiWBadfSZQXq1Wy+XyUaNGDRw4MExrBgwYgEJYWNgArWH2gp+YPRMEDtgnLzBZb/j4K4uXCbpRsJ+YYMAN/sT+Mi9MEPgtyF6gM34K/Arc8EGxtPOWAvf8J++Ml/W6KcgB7xhlXRz0JoFZ6nUviE6vm4iIiD59+jg7O1erVu369esmucldoVBoNJq1a9dWqlQJADp27BgeHh4aGtpPa0K1hpd1fzLshvfbr18/dKwbiF5nvKWuFxYUE3j3vMz75e1RZlrxPxUpTAwfvQgC4T+ZzAReMV3vBSlWkHfmng+Wd1xUPXn3fJgF2fNx8bLAL8NWYC/wwpwNHDiwX79+S5Ysyc7OxuGHCXoCirIQCBDBKgRI5CQfAdw38MsvvwAZQsCYCNSoUePq1atlT7DUarVSqZTL5UOGDAEAkUhkzFRS2IRA8RFYvHgxHb/I75rM9D8RLDPNGPNUC5dggoKCZDLZgAED1pAhBEoPgdWrV69du3bFihVBQUE+Pj43btwoe4KVl5enVqtPnTrl5eXVsWPHtWvX/v7775u1ZovW6MqbN28W/MRstmzZwrvHT3TMy7wbXua98/a8zIfD3KMDgRq8kuwnXmByycPEoHg9BSrpjYt3z9LCq63XkgXFe9drqTddLHwmCMLRDYp3UJgweTfoVxAms+TtUR8+yRs3bty6devQoUOtrKwaNmwYExPDKoh59hekFREsKgOFRQAH95cuXXJycgKAu3fvFtYnuSMEioLA2LFjK1eubJJThDiEmD59OgBs27atKFqTW0KgLBCYOHEiAMyfP1+hUPD7EcsiboqjiAgQwSoiYBXYOe5NGThwoFQqrVOnTm5uroIMIVB6CGCJSktLGzJkiElOESqVSrVa/eTJk4CAgIYNGz58+DAvL08ul5deEikkQqCYCGRnZ+fm5v7xxx/u7u7Ozs6HDh2i6Svz742JYJl/HpmFhmq1WqVSvXz5slq1agCwadMmunrRLDKmHClh8otGcfrq559/BoBFixZhmS9HAFNSLBgBbG+nR/09tzp69Oi0tDQqn+afnUSwzD+PzEJDvGjxm2++sba2fuutt9LS0sxCLVKiHCFgWoKlUqnUavXLly979uzp4uJy7tw5du9DOcKYkmKRCODc6vXr1xs1agQA69ato+kri8hIIlgWkU0mVlKtVuPgvkWLFgDw2Wef4e1EJlaLoi9fCJiWYOH29r179zo6Or7//vvZ2dm0waV8lS8LTg02vz/88INILOrQoUNsbCydH7SI7CSCZRHZZGIl8XaGQ4cOubm5iUQi2t5u4vwop9GbkGDh9JVKpfr444/Z9nbs1cop2JQsi0EAC2diYmJYWBgAfPfddzR9ZSmZRwTLUnLKlHri9vawsDCxWNy5c2e83c6UClHc5REBExIsPJB17dq1ypUrd+3aNT09nWYIymMRs8g04dzqli1b7OzsateuffbsWVq8tpSMJIJlKTllMj1xoSQuLq527doAcPDgQewITaYQRVxOETAVwVKr1TiEWLhwIQAsXbqUZgjKaRGzyGRhCxwVFQUAU6ZMUSgUdMDIUjKSCJal5JTJ9MzNzVWr1VFRUVKptHLlyi9evKDHGUyWGeU6YlMRLNxBHBcX17x58wYNGvz1119EsMp1QbOkxOH01bFjxypVqmRjY7Nx40aavrKg/COCZUGZZQJV2fb2d955BwC+/vprGjyZIBsqRpQmJFgajWbjxo0SiSQqKopOv1eM4mYBqWRF8ZtvvgGAAQMGZGRk0OK1BeRcvopEsPKRoP/6EMDx065duypXrgwA+DgDLRHqg4rsSoqASQgWRvr69euhQ4fKZLJ9+/bRDEFJM5L8lxICeFj7r7/+wuPb+PggLmeXUgwUjHERIIJlXHwtPXSszIMGDRKJRJGRka9fv6b1QUvPU7PV3yQEC/uwEydOyGSyYcOG4X1vNIQw20JSoRTDwrl+/XqRSNSqVSt6fNDicp8IlsVlWdkpjNX71q1bdevWBYAjR46WXdwUU8VDoOwJFlvv/vzzzwHghx9+0Gg0NENQ8YqeOaYYq0NaWtrIkSMBYMaMGTS3ao75ZFAnIlgG4anYP+bm5mo0milTpohEooYNGz579oymryp2iTBu6sueYLElmJo1awYHByclJdEGF+PmMYVeaASwcB47dkwsFvv5+R09+vf4lu5mKzR+ZuGQCJZZZIMZKoH7K+VyeZs2bQBgxYoVZqgkqVSeEChjgqVWq7EPW7VqFQDMnTtXo9HgoKI8oUppsUQEsC6o1erZs2cDwLvvvovUH+0tMUUVU2ciWBUz39+calwo+fHHH52cnMRi8a1bt2hw/2bUyEUJEChjgoXrg8nJyZ07d65ateqlS5fodoYS5B55LU0EsHBeunTJw8NDJBLh3Wy0eF2aEJdJWESwygRmC4wEr78aOHAgAHz44Yd4PJjGTxaYkxajchkTLJy+OnjwIACMHj0a11+ohFtMcSnXiuLzOOvWrQOAkJCQhIQEGt9aYoYTwbLEXDO6zrjSf/bsWX9/fwA4duwY7b4yOugVPoKyJFgYV05OzgcffCCVSjdt2kTb2yt8ATQXAHD6KjY2tnXr1mKx+NNPP6XFa3PJmyLqQQSriIBVDOe4E2XmzJkA0LJlS3y8nQb3FSPzyzSV+AwI/sVRe1ZW1vDhwz09PW/fvl1UWo8DA9w1+OTJE0wJBs5SxeLSaDRXr151dnbu0qULe+yZOSOBEDAVAkiwDh06BACBgYG3bt1i+wVNpRLFWzwEiGAVD7fy7At7oLS0tI4dOwLA6tWry3NqKW1mhoBSqRw+fHiVKlXu3LmDm6Kwd1FqTV5eXq7WyOXyHK3B3ggTUXiChQsuS5YsEYvFc+fOVavVcrnczJAgdSoiAmwed/z48SKRaOTIkTi3SuNbSywNRLAsMdeMqzNupfzll19cXV0dHByuXr3KXmwwbsQUesVD4N69e9euXbty5cpVrbl27dqOHTubNWtmb28/Z86cFStWLFmyZOHChV9//fX8+fM///zzmTNnRkVFTZ48+ZNPPpkwYcLixYtTUlLYRFchCRZysri4uICAgHr16j158oRKeMUremaaYiycly9flkgkVapU2blzJ3uvzEw1JrUKRoAIVsHYVNRfcJJg8ODBADBz5szs7GzWgVVUSCjdxkLAwcEBdIxYLLaystKx/ttCLBZLJBJbW1uRSAQAHTp0ePr0KSufRSJYO3bsAIAJEybQ7itj5S6FWywE8vLyfvzxRwDo2bMnnWwtFoTm4okIlrnkhJnogV3UgQMH/Pz8pFLpwYMH6fSKmWRNOVNDrVZHR0dLJJIOHTrMmzdv/vz5X3755YIFC77Wmm+++WbJkiXff//90qVLf/755+XLl69aterXX39dvXr1mjVrfv/99+DgYKlUOmzYsKLOYOHhwYyMjNDQUHd39wsXLtAGl3JWtCw3ObgO+Pjx45o1a9rY2MybN0+tVtPdbJaboUSwLDfvjKI5rg9OmTIFALp160bb242CMgWqRaBt27bW1tanT58uKh7Xr19v2LAhAAwePDg5OblIM1i4BBMdHQ0A/fr1oxmCooJP7o2HAO5/3b17NwC88847iYmJtHhtPLTLIGQiWGUAssVEgYP7+Pj49u3bA8DatWtZ12UxaSBFLQEBtVqdlJRkY2PTokWLzMys3NxchdbgBnaU8/LydIWcnBylUjljxgxcIiwqwcIZAoVCERUVZWNjs3LlSpVKRTMEllBkyr+OWDjT09N79eollUrHjh1Li9eWnutEsCw9B0tTf1wfXL58ub29vY+Pz5UrV2j1pDTxpbC0COAcEpIkfIKJnZDSe6UCsnw2lI+LiwsODkaCFRERUaQZLIzo/v37Pj4+DRs2fPXqlVKpRH0ocwgB0yKAhfPcuXMA4Ofnd+HCBZVKhYNe0ypGsRcbASJYxYauvHlUaY1cLh8xYgQAzJ8/Py8vj/V85S21lB7TIYCnoipXruzt7Z2SksKTKl5mvIoJOABYsWKFtbV1QECAo6Nj//79i7EH69dffwWAGTNmaDQaup3BdAWBYhYiIJfLJ02aBAB9+/alp52F6FjgNxEsC8w046iMdOrgwYN+fn52dna7d++mGm4cpCt0qDgRtXXrVolE8sknnzDyhKAYJlhqtTozM3PAgAE2NjYzZszw9/fv2bPny5cvMRBWXAu6aBRHC69evWratKmvr298fDwOKip0flDizQMBLJyPHz+uXLmys7PzunXrVCoVjijMQ0HSojgIEMEqDmrl0g/ORX/66acAMHDgwOfPn9P5wXKZ0aZNFK7HNWnSxNraOiYmBhkVmyg1QLDw+MWhQ4fc3d379u27fft2b2/v3r17F5VgHT16FAAiIyNpg4tpSwLFLkAgLy9vzZo1ANCsWTPca8jqhcAlfVoKAkSwLCWnCqMn663+IRTGJ7Kre/futWzZEgBWrVpFp6sKgxu5KRICOH11584dKyur3r17Y6njSRUv85NbuBlFpVJ98sknALBu3bpr165VqlSpb9++hSRY2FdlZ2dHRka6u7ufPn2aNrgUKe/IsfEQwMKZmpoaEBBgZ2c3ZcqU0n188B/9AQSohgcAACAASURBVPdhvBRRyIgAESzzLQlqtVqhUOC5KvY8iLwAoz2HlYcGHxXB5Y9CjoFwLvr7778HgICAgBs3btD1weZbMixWMyxmgwYNEolEu3fvxsLJkype5gmWQqFQq9UxMTE1a9asXr36kydPLl++7OrqWlSCdevWLXt7+5YtW7L1RIvFkhQvPwhgsT9y5AgA1KxZ8/Hjx0ql0kDTrVQqi9IvKP6/Y8jLwyMdtDJeZkWHCFaZQW2yiAxUVNQJX7pNTU0dNmwYAHzxxRe4OPhGjyZLEkVsmQgolcq0tDQ3N7eAgIDU1FRMBE+qeJknWDjX9d133wHAtGnTNBrNqVOn3NzcCk+wcEHwyy+/tLa2Xrp0KY5eLBNF0rocIpCdnR0aGioWi8PCwmjxutxkMBEsc8xKZDYvX75cunTppEmTpkyZMmnSpIkTJ/5La8aPH//RRx+NGzfuQ635QGtGjx797rvvDh06NDIyMiIiYpDWDB48eOLEiYJOSzfBuL199+7d7u7unp6ehw4dovG9LkpkU0IEkCQtXLhQIpEsWbKEhcaXT15mBAsHAC9evAgJCXFzc8PyeezYMdyMVcglQo1Gk5CQ4OPj4+fnl5mZicowHUggBEyLwP37962srNzd3Y8fP25ghikvL0+lUh07dmyK1kyePHnSpEkTJkz417/+NX78+I8//ljQNYwZM2bEiBHDhw8fMmRIZGTkoEGDwsPDBw8ePGjQILzmkPbRGzXfiWAZFd5iBo49yoULF2Qymd4X2XQtJRKJtbW1jY2NnZ2dg4ODs7MzumnUqBF2VAZUwX3Hc+fOBYBRo0ZlZGSwO4cM+KKfCIEiIaBUKhUKRYMGDZycnOLi4phfnlTxMiNY2Af88ccftra2gwYNysjI0Gg0J0+eLDzBwqC2bt0KAO+//z7NEDDwSTAHBHJzc2fOnIlvaxre/Ip1Ydy4cbpdgF4bkUgklUqtra1tbW3t7OycnZ0dHR3R5fr166kiGDv3iWAZG+HihI/dzNKlSwFg+vTpR48e3bNnz6FDhw4fPnzkyJGjR4+eOHHi5MmTp0+fPnPmzNmzZ8+dO3f+/PkLFy5cvnz5itZcv369WbNmUqk0NDTUMMHCxf4rV674+/sDwLJly2j6qjh5Rn4MIoCD8v3799vY2HzwwQf83Z48qeJlRrDwdob3338ft7djPKdOnSoSwcrOzm7btq2Hh8ejR4/oclGDeUU/ljUCr169cnFxcXR0/Pnnnw0QLBx4P3nyJDAwsGHDhps3bz58+PD+/fv//PNP7BeOHTt24sSJU6dOndGa6Ojo81rDuoabN29u3LhRKpUGBAT89ddfBuIqawjKaXxEsMwuY7EWPXz40NfXl9+qUiRF79+/7+7uDgDdu3c3QLDYRe0///wzALRr1+7x48dU64oENTkuDAI48u7SpQsAXL58mffCkypexnKL69fR0dGVKlVq0aJFQkICkrPCEyyM6/z58yKRqEuXLjR+4MEn2eQIqFSq9evXA8Bbb72Vnp5uYPEaj3r88MMPIpHo66+/Lp7mX3/9NQD4+/vfuHGDmvriYVh4X0SwCo9VGbnEDVi7du0CALyJMScnhx0PZAIeFWR/mT1WwvHjx+M8cI8ePQwQLLa7ZeDAgQDw2WefUfdTRtlckaLBFefY2Fg3N7f27dtnZmbyqedJFS+zGSyNRrNgwQIAwE4F714vEsFSKBQffvihnZ3dnj172KCC14FkQsBUCMjl8po1a1pbW0+YMMFA84uVSKFQ9O/f39nZ+cKFC2q1Wi6X52nPBmJHwHoBpVLJZCYolcp79+75+vqKRKJatWrdvHmTCJaxM50IlrERLlr4yK5SUlJCQkK8vb3Pnj2LVY51PEwQhIv2+DcxMbFatWq4f8swwcLR0q5du+zs7GrWrHnkyBEDNVwQI30SAoVEAF9T/uijjwBg8+bNuCqNi4Z4GRU7Pc7fMILPMKtUqkePHgUEBNSoUQOnvnJycvAUYSGXCDUazcOHDytVqtSwYUMMv5BqkzNCoAwQuHbtGgC4ubnFxsbiiFdvpDhy3r59u5OT05gxY3CYIegO+E8mM0GlUq1cuRIArKys/Pz8iGDpxbl0LYlglS6eJQ0NCRa+9xkREYGjE6wh+BOrLYKY0B47qh9//FEsFoeHh4vFYgNLhMwLbm//4IMPcnNzDUxQC2KkT0KgMAjgyPvly5c1atTw8/NLSEgojC/ezdq1awFg3LhxeIQKr3Qv/AyWUqnE+x3mzp1L4wceWJJNjoBSqYyIiACAXr16GZ5PwvYft7fv2rULHQu6A/6TyUxISkp655133N3du3fv7unpeefOHcMxmhyccqAAESwzykSsQgqFYvbs2TKZbNOmTaw/YJWECQK90R7pUVBQEAAcOnTQysqqW7duBS0R4kTC1atXq1WrZm1tvXLlSjpUIkCVPkuOAI68ly9fLpVKp0+fnvgiMTEx8dmzZ3FxcY8fP3748OG9e/diYmLu3Llz69at69evX7169bLWXLhw4dKlS0eOHOnUqZO9vf327duxOuB2rsITrIyMjHr16rm6umZkZND4oeQZSiGUIgJJSUkymcze3v7AgQMFNdSsF7hw4YK7u3uHDh0SExPZVYXYa6BKfO/AZObgyJEjYrG4S5cuixcvdnJyunv3LhGsUsxKvUERwdILi2kssSY8ePDA0dGxefPm2dnZ7LgTX1tYheG1xHkCtVp9+PBhGxub9u3bP3uWYIBgsQCXL18OAD169MD7hPjjXXz4JBMCxUCA3efZqlUrAKhatWqtmrVq1KhRvXr1atWqvaU1VatW9fb2rqI1np6eHh4e7lpTSWvc3d0lEkmPHj3kcjkW8iIRLLVafeDAAQDo06cPdSfFyEHyYlQEcMt53bp1DS9eY5n/5ptvAODHH39kr8SyZhyV5D+ZjP1FTk5OZGSkjY3Nb7/9tmjRIiJYRs1WFjgRLAaFuQi//fYbu04dV0P43b6s2gjUZT0Zblf//fffU1NTpVJpQTNYSKTi4uLwYNfMmTNp+koAKX2WHAGcMTp//nyVKlXEYnG1atVq1KhRq1Ytf3//+vXrN2zYsHHjxk2aNGnWrNk777zTqlWr4ODg9u3bd+rUqWvXrt27d+/Xr1/r1q0B4MMPP2SvsxWJYKlUql69etna2l69erWgulPyZFIIhEAxEMjKyvL29pbJZEuXLjUwfYWVKC4urkWLFrVr17516xYbKgiKNP/JZDZul8lkQUFBr1+/njt3bqVKleiahmJkWVG9EMEqKmLGco/VICkpqW7dur6+vnhdAptP4muL3hksXO+LiYnx9vZ+6623kpOT09PTJRKJYYK1d+9eiUTSoEEDtpveWMkrRLhs1zMJ5QYBvFw0KyvL39+/SZMmjx49Sk1NffXqVZrWpHMmIyPj9evXGVqTqTWvX7/Ozs7euXMnbsDSaDS4sVdAsFJSUhAuHGOoVKo2bdoAwJMnT1Qq1Z07d6ytrdu0aaNRa9hW+nIDb0VOSCFaFHN3gnOrjo6OWVlZelt1TAAW+PXr14vF4qlTp+K5EHTP+gV0yX/yslKp/PLLLwFg9uzZGo1mzpw5RLDKpnAQwSobnAsby4kTJ/A6dcF8EqstTBCEiJXws88+E4lE8+bNU6lUqampBREsrJxyuXz69OkAMHToUHza2UAlF0RHn4RAkRBo2LBhy5YtBRc0FCaEQ4cOGSZYaWlpLBwckOCkF14WHxUVJZFI1q1bx9yQQAiYHAFsad9++20AGDBggIHpK3SpVCpHjhxpbW19+PBhvmsQdAf8Jy8nJSVVq1bNy8sLZ63mzZtHBKtsygARrLLB+Q2xYC3KyMjo0qWLi4sLXpfAb8hltYUJfIi4NyUtLQ1rLHYtL1++LGiJEPuhS5cu2draOjo6rlq1iq+0fMhlI+NKaJ8+fRwcHFy0xlVrKlWq5JZvcF+OR76pXLmyp6cn27jjxZmq/zQ+Pj6418c331TTmupa4+fnV4MzNf9pamlNnTp1/DlTlzP169dv8E8ToDUNGzZs1KgR/n37nyYw3wQFBTXRmqZNmzb7p3nnnXeaa02LfNPyn6Z169ZtOBMcHNxWa9rlmw7/NB0506lTp86dO3fq1KkLZ7pqTbdu3bp27dqtW7funOmhNT21plevXr1798a/vXv37pNv+vbt269fv1Ct6f9PM2DAgNDQUBsbm5CQEP7dQFa0+CLNy1hK9+/fr5dgHT9+3MfHx93dPTQ0NDw8fIDWhIWFDRw40M3NDbcVhoWFubq6ymSyjh07Dsw34VqDX+Hh4Uwotjxw4MA3hsm70euYd8BkXiVdPZkz9hMK/F9BXCxA3q+uG+aM/4kFy1vyMh8mykwxFqCuG/yJD4eXdd1jmMOHD8/OzmZFyBKFx48fSyQSKyurBw8eGNAfR8779++XSCSDBg3CG09Y18DXF34niUDetGkTAAwaNAj3y8+fP58IlgHMS/EnIlilCGZJg7p//z4AdOnSRaVS4QXWLERWkZjAfmLr8WvWrLGysoqIiMCVFAMEC/2uXr0aADp16pSens52TfLBlo2MjcXZs2c9PT31PqdFluUDgS5duqSkpOgO1vkizcuGCdaxY8f8/PwAQCQS6eKj11LXGdlYHAKYs9OmTcPiUTZtVOnGgsPpsWPHAkCzZs10awQfHSbziy++AIA1a9YgQ8IQBCxK8MmqUnZ2dqdOnWxtbQ8ePIjtPBEsHmGjykSwjApvEQJXKpVz584ViUTLly/H+SRWi/iaw6oNHzTOAHXs2BEAjh07hj8VRLAw2Li4OFxJmTFjBts+zIdZZjKOyXBvfuPGjY8fP37z5k08qH/58uVL/zQXOXMh3+B7W/j3XL45y5lorcH3ufAvPuN4WmtOceakjjmhNcfzzbF/mqP55sg/zdGjRxcvXvyF1nz55ZdffPHFfK2ZpzVz586dw5nP881nnJmVb2ZyZsaMGdPzTVRU1DStmZpvPtGaKVOmTM43k/LNxHzzL60ZrzUfa81HWjNOaz7Umg/yzdixY9/XmjFaM1prRmnNyHzz3nvvjcg3w/PNMM4MHTo0IiLC1ta2dGew9u7dW6VKFWdn5y5duvTu3Rsn2Hr06NG7d29XV1d8N9fGxkYsFjdr1qx58+bNmjVryhmcOwwKCgoMDGzSpEnjxo2Dg4PZfMmgQYNwHmWQ1hiWw8PDBc503fNu9DrmHTCZV8NwmOgF3fB/BXGxAFkUvMBk5oz3zoLlLXmZeWeqMoEFqOsGf+LD4WXmfuDAgYMHD27QoIFEIgEAvMOpzNqo0o1IrVanpqZWrlwZAPBGq4LCx8Hn7du3fX19mzVrhksTuN0WvQi6A/6TyZcuXQKAwMBA1o8QwSoI8FK3J4JV6pAWM8CUlBRvb29/f//U1FR25wILi9UWJrCfsBKeOnXKzc2tefPmr169wopkmGDhyktgYGBMTAybA2NhlpmA47OnT5/iU9N79+4ts6gporJBANc4AgICWrRowQonHzVfpHm5oBksZOTTpk0DANxuyEJDL8HBwXhlvEwmw5cMmAMSLBQBbOUGDRoEAJ07d7bc9UEc3y5btkwikXh7e2NhLihTMNWrVq1i58qRXbGxN19fGH/C0JibMWPGiMXin376idkTwSoI8FK3J4JV6pAWM8CdO/4+LTVlyhQ2CcxqCF9zBDWK7Z3CG36XL1+OvtRqtV6Chb9mZmZ++OGHADB48GAWQjH1Lpk3bF/w5cTq1avjNd98wksWPPk2PQKYxaVFsJBCPXnyJCgoqFKlSteuXeNXt5HMtW7dWiqVtmvXTiKRnDp1yvQQkAYlQwAPS6amptauXRsALHoYhgypbt267ExfQdhgM5iRkdGvXz8PD48LFy6wG0dZCynoDvhPdJOUlOTg4ODr65uYmMh8EcEqCPNStyeCVeqQFifA7OzsBg0auLm53bp1i01fsfpggGBhf/P48WN/f39XV9fY2Fjm2ADBYq9frV271oQEi+3Nb9asGQBs2LChONiRH/NGoHQJFq6G//TTTwAwceJEfJeQ1RRGsHBrUb169dhNcuYNEmlnCAHM1i+++MLGxqZq1arJycmGXJvxb1hQ9+7da2dnJxKJDCcEp69wqWHkyJFs4I0tPKaSZ1Ss5Wc/aTSaJUuWAMDHH3/Mj0OIYJVZGSGCVWZQG4ro9u3bABAWFsaPUVi3wdccQY3Cpufbb78FgOnTp2Nnhm70EiysZj/9+Hf/FBISgouDfESGtCzt37DzW7BggY2Njbe395MnT/i2o7Rjo/BMg0ApEiykU69fv+7Vq5dUKj106BC7XgTLPD7Lg5sLAWD9+vWmSTPFWnoIYBarVCochv388884qiy9GMouJORMbdu2BYCePXvip97osU1WKpXjx4+XyWQ7d+5Uq9VYlfhGUtAdCD4zMzMbNmzo6Oh4/fp13hcRLL2YG8OSCJYxUC1amCqVatiwYTY2NliLsNYJqgr7ZAJWGKVSmZub26FDB5lMdvXqVVaL9C4RYqV9+vRpzZo1pVLpjBkz+EpbNKVL7BqvhczNze3atSsAfP3114wdljhsCsCMEChFgvX69WuNRrNnzx4nJ6ewsDD81E0q7sGytbVNTU3V/ZVsLAsBHEPu2LGjcuXKEonE8KUG5pw0bLrj4+Pd3d0B4Pz58wa0RRKJN+ngQW+1Ws0IGRsS890BNv7sJ41Gg/dId+rUifULGCMRLAPIl+5PRLBKF8/ihJaYmCgWi5s3b56ZmclOiBRUc3h7bHq2bdvm6OjYp08fvHERK1hBBEutVh89ehQAGjVqFB8fj4t0xVG6xH5w+mrDhg1VqlSxs7O7ceOGoBUocQwUgFkgUIoEKycnR6lUTpw4USQSffPNNwkJCffv33/48OGDBw/ua82dO3cePnzYpEkTkUg0duxY1iGZBRCkRLEQwIYCX/SKiIjIyMiw0IYCm+tp06bJZLLAwECcji0IEpVKpVarly5dKhaLlyxZghs52NQdY1F8d6BLsEJDQ0Ui0Y4dOwTOiGAVBHup2xPBKnVIixzgjz/+KBaLFy1aJFhlZ7WIrzl8VcEFkfDwcAD4448/+Ih1CRaGlpGRERYWJpFIhgwZYsLdVxi1QqHo3bs3bu3H2Qg+yXxySLZcBEqRYGk0mgsXLuAGYScnJ3d3d2dnZ0dHRwcHB1tbW2tra5lMJpVKcQNWfHy85YJGmiMCyDPu379fu9bf29sNz/qYOWh5eXnZ2dl16tQBgG3bthnQFpvB58+ft2jRwt/f//79+zh9xZpHXmCyoJuIiYmxt7evUaMGYshHRwSLR8OoMhEso8L75sAVCoWvr2+VKlWePXvGtrfzVQWDYLyKCTg6v379uq+vb926ddn2duZesAcL6+Hdu3cBoEqVKvv37+fnnN+saKm6wMHc7t27q1evLpVKo6OjMcmlGgkFZhYI8ATr5cuXrJBjScZP7AOYjHrjeJ2/yV2j0eCTagEBAW3atGndujVeYd+uXbsOHTqEhITgJfVdunQJDQ213JP8ZpFt5qEETl+NHz9eIpG8/fbbL168sNCGApvr5cuXOzo6Ojk5JSUlGQAYS/6OHTsAYNKkSWynLONSvMBk1mugzSeffIJbL1iXwWIkgsWgMLZABMvYCL8h/JMnTwLA6NGjBbVIUCvYJxOQo8yaNQsAFi1ahDe/s8qmO4OFk0ZfffUVALRs2VKpNW9Qzmg/Y7s5YsQIAOjduzfezsAmwI0WLQVsdATwJVosXfg3OztbqVTy1zQUUgmeYI0dO1aj0cTHx7ds2bJ69erPnj0rZCDkzHIRQML9+vXrFi1a4MVmlpsWbK7x+MWsWbOwAdSbHCz2mZmZAwcOdHZ2PnHiBBsJ8807+mXdAf+pUqkyMjJ8fHxcXFz0nhwigqUXeWNYEsEyBqpFCLNdu3b29vZXrlzBesUqDBMwLPaJAo6HEhISmjZt6uzsfPPmTf4ULg5lBDNYGo3m+fPnrq6utra2X331Fe4xL4KipecU95nduHGjYcOGYrF43759FjoqLT1Iyn9IjRo1atWqFW6gEaQWizT2pnl5eQqFAos3T7Def/99jUbz22+/AcCsWbOUSmV2dra8YIPTZoKI6NOyEEAWsmzZskqVKrm6uj58+JA1g5aVEJygPXHihIeHBwDcvXvXgP7Ioq5cuSIWi/v164d3gKH7whAsNlUmFovfe+891q3wMRLB4tEwqkwEy6jwGgpcrVbHxcXhdQlsmZy1IEzAINgnCtj0rFy5UiaTjRs3LjMz840ES61W7927FwDq1q2blpbGYjSkonF+w8Hc9OnT8T2Tp0+fYudqnNgo1LJDQKlUnj9/ftOmTdu2bfv999+3bNmyefPmDRs2bNq0ycvLy9fXd968eYsWLfrqq6++/PLL+fPn45NBs2fP/vTTT/FBoKioqPnz59++fZvdV4JLhHiRT1hYmIODA7udoewSRjGZAgFsKPr06QMAn376aVZWloWOxLC5xoR07doVm2sDiObk5EyfPt3GxmbNmjX8Qe9CEiyFQhEcHCwWi8+cOYP9BfOIkRLBMgB+6f5EBKt08SxaaJMnT5ZKpRs3bhTwJ7aazoLjHeAqTHZ2dr9+/aRS6dGjR5Fd8bVIsESo0WgyMzODg4OtrKxGjBhhwu3tOH117949/nJRXnOWZBIsCAHM1piYmEaNGpXw/WAnJ6dLly6xIooEKyoqKi4uzsXFJTQ09NWrV/yw3oJQIlULjwDueTh69Kifn59IJLp48WLh/ZqVS2yunz17htfQnz592oB62BLev3/f0dGxRYsWSUlJ/EiYtZO8wGTWCxw9ehS9I7FjfQeLlwgWg8LYAhEsYyOsP3y1Wp2Wlubk5FSnTp20tDRWSVhlYAL6Z59sQHPw4EF3d/eQkBDcL8nud2Du+SVCjUZz584dAPDy8rp8+bIJ+yes859//rlIJKpdu3ZMTAzbYaAfKbK1BAQwW1evXi2RSPr27fvtt98uWLDgG61ZtGjR4sWLv/3220WLFn3//fc//PDDjz/++NNPPy1btmz58uUrVqxYuXLlL7/88uuvv44ePdrKyqp+/fpYI3ACY9++fQAwYcKEFStWAMCvv/7KuJclAEM6FhMBLFGRkZEA0KNHD7z0nLWTxQzUFN4wIbNmzbKzs6tbt25SUpKBVOBc/vr16wFg/vz5gqLOPPICk9mTskOGDAGALVu2sPVB3o1GoyGCVWYFgQhWmUH9v4iwuK9atUosFs+ePZuf9+aJFF8reHussfh+H/Y3+KvAPU+wFArF1KlTRSJRu3btTEhocDCXmJjYvXt3AFi4cCGOU/8HDUkWiAD2CllZWZGRkc7OzsWebHj33XcBoFatWvhuGhKsPXv2AECnTp1CQkLq1auHl7fhRhMLhIpULhQCOAKMjY0NCgoCgF27dvGNZKGCMBtHSqUyLy8Pt7evWbPGQNHFBvzly5fNmjXz9fW9ffs2NpgsKayF5wVeViqVsbGxXl5eVapUwYfVdRdDiGAxPMtAIIJVBiALo8AOKTAw0MHB4fHjx4w88ZWBt+TtkZHExMTUrFmzRo0ad+/exdB03fMEKyUlxdnZ2c7O7ueff2YDHaFaxv9Garhy5UpnZ+fq1atfvnzZhMoYP7kVJQa8j+3AgQMuLi5DhgxJT0+Xy+W5WoPb0HXl3NxcRb5B+fTp09bW1iKRqEaNGjjKR4K1a9cuqVTq4uIiEolmzZpFBaYilCpsKObNm2dnZ+fv7//o0SNs5Swu7QqFQqVSbdy40c3NzdnZWe+ZPkGizp49CwDvvfceewaKOeC5FFryzT7WF1wc+Pzzz/GT7ztYODSDxaAwtkAEy9gI6wlfpVJdvnxZLBaHh4cLKgCrMExA/+wTm55///vfAPD555/jpyAQ/ESC1b17d7VavWHDBgDw8/NLTU3lF/X1KGc0K2wiU1NTBw0ahC/Jy+VyUyljtFRWxIBxUI6nFrZs2cL2pwuKJSvDuhjJ5fIpU6bg5i0/P7/ExESVSoW8befOnfb29gBQr169S5cuYa+jGwLZlBsEcIr99evXPXv2BIAffviBtXIWl0bUvEePHnidMt7NxngSnxy0zMzMHD58uLOz88GDB3F9kHfMZF5gslKpzMjIaNCggb29/Z07d7D2CeogxkgEi0feqDIRLKPCqydw7I369OkjEolOnjyJvQ6rJKwTYgIGgZ84c56UlBQSEuLk5HTq1Cl2eFDXPRKsnj17qlQqf39/qVSKfI6NbPQoZ0wrbGu2bdvm4eFRqVKlP//8k++JjRkzhW1EBHDz3927d2vXrt2iRQu8oYqtg/DFkpcFCj19+tTLy8vR0bFevXo+Pj6JiYn4yKZSqdyxY4eDgwMAjBs3jqavBLiVy8/c3Fy1Wr1161YPDw8nJ6cLFy6wVs6y0qtUKlUq1aVLl2rUqAEAuHTOmnpBWtD+0aNHVlZWbdu2lcvlgqsNGWESCOgRa9ymTZtsbW3DwsLkcjlzplvviGAJwDfeJxEs42GrP2SVShUbG2tjY9OsWTM2oGG1jlUGJmAo+IncaPPmzQ4ODkOGDMHnZfTuZGSnCLt3737+/HkAcHJyunbtmqm2t+P0lUqlmjBhAl6smpqaaqHT/vrztULashz8/vvvAeCbb75BDqRbnvWOpBEzpVK5fPlyAIiIiOjevbu3tze+qom/7tu3TyaTVa5cGefGLHcyo0IWkCInmu0QHT16NAB8+OGH6enpjCsUOTiTesDL2EaPHi2TyUJCQlJSUgwnRKFQLFy4UCqVLl26lE1fsarE+2WWrJvAnV69evUCgMOHD/PVjblhYBDBYlAYWyCCZWyE/xE+DvenTZsmFotXrVqF9YSvAExmAvpnBEUul7/33nvsNStkV3x1Yu5fvnwpk8m6du3av39/kUjUsWNHE04A4HLPkSNHqlatKpPJNm/ebEJl/pEl9FECBLD44ZRqtWrVrl+/FgY+qgAAIABJREFUrnvuSbczEESYnZ1dr149T0/Pw4cPd+7c2dbWtl+/fmFhYX379u3fv3/Lli0BoGfPnhkZGTglIPBOn+UJAZy2OX78uJ+fHwDs2bPHQqevsMWOi4vDffqYEFYXBFmGrf2LFy88PT0bNGiQkpKCeyd0ewH0yMJBBzhsjo6O9vDwaNSoEaOkuv0LeieCJcDfeJ9EsIyHrZ6Q8VRIrVq1PD092XMffC1iMhMwFLx4Xa1Wnz171t3dvXnz5oL7OXXdI8Hy8/PDd3D/+9//svkGPZoZ2Qp74s8++wwA+vXrh4euGTs0cuQUvLEQwIWJ3bt3S6XSsWPH4rZ1HEVggcSmHwsevpyDNvzfY8eOAUD37t3T09M7deokuEbL3t7e2dn5888/12g0dD+7sTLSbMLFLI6KihKJRK1bt8Zd4ZbYUGBCFi1aZG9vX6tWrcJsbz906BAATJ48me2d0G3VMaN4gqVSqXBlA8+Vr1q1CuFifpnAMpkIFoPC2AIRLGMj/L/wsVNZvXq1lZXV1KlT+UqiKwtqBeNGyFG+++47XQcsEJzQQoIlEokAoGbNmjk5OaZqp7DHvXTpUr169QBg+fLlNH31v2JhyRLy/vHjx1tbW+/atUutVrODC1g+scxjEgUlFi1VKlVYWJi1tfXWrVtzcnLatGnj5eV15syZs2fPXrhw4fLly1euXDl37hx7Ct2S0SLd34AAlpZHjx6FhIQAwIoVK/jy8wbP5vQzNtdyuRzX7P7zn/+8cWyQkZERHBzs6el55coV1toLqgxr4XkBp3WfPn2K4/bY2FhEgvllAkOICBaDwtgCESxjI/y/8HGcgUseMTEx7Ae+AjCZCegMZ84fPHjw9ttvV69e/cqVK4KZc4F73IMlk8kAQCaTTZs2zVSchm2q+O677wCgTZs2Dx48YJYMBBMKCB39LSoCyJujo6NdXFz69Onz+vVruVyek5OTrTVZWVmZmZkZ6RlpWpOamvrq1asUrUlOTk5KSkrUmlOnTuFRfLlcnpqa2qpVq1q1auktDEVVr4K7t8TzubiRYOXKlba2tgEBAfjEKjswobdUmKdlXl6eSqXatWtX5cqVHRwcrl27JmiuddW+e/cuzu6zGSm9Gz/QI0+w2FSZVCqdNGkSY3JY/nUDoXuwdME3ng0RLONh+4+QcSh29epVJyenbt264SkPdMFqAl8ZBJZIzlatWiWRSD7++GNsiVg14z2yMHEGCwDs7OzwesZ/KFRWH9gNP378uEuXLgDw1VdfmYrq6U0xNmd5ZIqOQG5urkqlWrBgAQDUqVOnR48eHTt2DAkJad++fdu2bYODg1u3bt2yZcsWWtO8efOmTZs2adIkKCiosdY00ppq1aqJxWK84Co1NbVNmzY1atTA5wry8vJwVZG2XuktuuXMEls8pVI5duxYAPjss890j9FZRJJxWlej0YwZM0YkEr3//vv4+CDfXAsSolAopkyZgvO4bH1Qb6uOHllQOFKVy+Vt27YViUTnzp1jIbMehAnsJ5rBYlAYWyCCZWyE/z98HFiEhoYCwIEDB1gNEdQiVhmYgEMftVqdkpISGhpqZWWF+yUFAzvePYbJCFbr1q3fOH4yHgqo57p166ysrJo2bYr7oAXKGy92CtlICKjVfwd8//6DwMBAAMA7QsVisVQqlclkVlZWNjY2dnZ2Dg4Ojo6Ozs7Orq6ulSpVcnd3r1y5sqenp5eXV9WqVX18fCQSibW19YkTJ/DxqODgYC8vrxs3brx48SIhIeE5mWIh8OLFi2fPnuHryEYqAKUeLA4ad+/e7erq6uDgcODAAbMaiRU+vTikPH36dJ06dbC1f2Pzm5CQYGdn17BhQ3Y7A0an26ozexSQg+7YscPOzq5nz55sezvfrQgCoRmswmdlyV0SwSo5hoUKQa1Wp6amenl5NWjQ4OXLl7wfvgIwmQlsQLN//34bG5tevXplZGTo1ljePSNYEolEJBLt37+fj64sZdQqISGhf//+ADBjxgx+ArwsNdEbl1qtPn78eJ06derXb5BvAho0CMiX+f/Mngn4K/tkgsC+gTZAPkwm814MyLoB6g0Tgy0oHN4Lc8ME3Sj4n5jC/wukfv36DRs2rF27tkQiqVu37oEDBx48eHDv3r2HWvPo0aPHnInlTLzWPM03PXv2tLKywiuCUlNTu3TpIpPJ/Pz8ateuXatWrdpkiohArVq1/Pz8atas6eXlZUEchd0f++mnnwLA4MGD8aUXU20b1dtcFNISx9LTp0+XSCRt2rSJj4/HBrkg72q1es2aNQDw9ddfs9sZ0LFuq87sMUy8tWTo0KEAsH37dt49k5nAFKAZLAaFsQUiWMZG+O/wcZwxf/58sVi8aNEiQavBVwAmCwSVSoVXXa9YsYJRLl515h4tcQ+WRCLx8fEx4W4MXNncuXOnlZVVlSpVduzYITjGzyfBJHLHjh0Fx9bos6gINGnS5NGjR8XLvvDwcCsrq/Pnz2s0mufPn3t6ehY1dnKvF4GWLVs+fPhQdyRWvGwyti+c9bl69WrTpk0BYNmyZXpbOWOrUfLwsR1OTk7u2rUrAPzyyy9vbH5zcnLc3d29vb1xIwfvXrdVRw1xAQRBu379erVq1WrXro3H0lnnwvwygaWOCBaDwtgCESxjI6xhS/JNmjRxcXHBVq/wS4S4mnb16tUqVao0btz4/v37epseQS1iF4127tzZ8PjJeOlHlbKzsydNmoSXi+bm5mKjYLxIixRyfHy8SCSysrL66aefoqOjT58+fYZM4RA4ffr0+fPnjx07FhISUqtWLXwTEwcS2ENg7uPWQ10ZbdRqdVhYGJvB+uWXXwBg0qRJ8fHxMTEx9+/ff6A1vGBAxp/uaw3zKHAv+Hzw4IFh9wWFw9sXJkzeDe+XKcA7YDL7lReYzJxhEv7666/4+Phdu3a9/fbbAPD9999bCrtig64lS5aIxeL27ds/ePDAQtcHcaHzp59+kslk9erVu3379htz4fDhwwAwZswYTDJWDWzHeJlvxtEep69mzZolEokWLlyIfQrrWZhfJrC2kQgWg8LYAhEsYyP8/9NXO3futLa2xruC+KqCsoFagT8tWbIEAD799FPdF0AxAYJaxAhW9+7dBdEZPcH5ESA1PH78uLW1taurK14uinNa+U5M/H/cuHH4RCPD38QKWWD0U6ZMqVq1KvJ+zHEeTL5Y8jJLKBKsK1euZGRkNG3a9K233hIsoDOXJBQGgfXr11tZWfn6+p49e9ZSOApOuqSmpkZGRgLAF198YSmaC3KElXC8C3rOnDmYNL5GCLwolcru3bs7OTmdPn2a9QXMPQsQffH2OIxJSUlp0qSJjY0NnrjUO/slCIT2YAmywKifRLCMCu/fgeM4o3fv3gCASyGCEs9/MhkFrJ/x8fFt27Z1d3c/fvw4C1CgN/OI9oxgdevWzSQEC9sCtVo9f/58fAUF565YGyHQv+w/5XK5u7s7e+DFfBQreyiKESODa+rUqSUkWNbW1pcuXTp+/Dje2M5fKMpKNRNYJ4Q68/Z8OefteVnXu244vPvCyIUJk3cjCJOBL7AXfOrqKQgT2wp8imrixIm6ZJdFZG4CDrq2b98ulUq9vb2PHDlioQQLE7Jv376qVasCwKFDh96YkLi4OIlE0rZtW713X/FlALObFQPc6bVq1SqRSDRq1Cg80EAEy9zKNhEs4+YItnr37t3z9vbu0KEDPrLGVwO+leRlrFrofevWrQAwbNgw5ChoKdBbtyriY8+mIlio5MWLF93c3EQi0X/+85+CqKEgIWXwieRg9erVAODm5sa/fFcGsZePKBjB+uSTT0pCsPr3729ra3v69OmhQ4fa2dlFR0e/cUmlfABYuqnArv3EiRP4wsy2bdv0biQo3UhLJTTWcM2aNQtXynBEygpYqcRSNoGg5lFRUQBQmNsZNBrNxIkTxWLxunXrGH9igPDdAerPMGH3r+JFpgcOHGQOmBsWDhMYCLREyKAwtkAEy7gI4zgDZ4w3bdqEkQlKPP/JZDagef369fDhwwFg48aNyFFYFeJVZx5ZFKYlWKjGqlWrAKBz585m9TYOkr9GjRoBwL/+9S8eRpILiQArhCUkWGFhYXZ2dps2bbKxsaldu7al0IJColRmzpBg/fvf/waAXr16CfY7l5kaxYgIZ9rOnz9fu3ZtAPjtt98stAxgQmJiYvAq6fXr179x+ionJ8fBwcHb2xt3biF6fEvOy4yBsZHqn3/+6ebm1q5duxcvXjA2xiom88sEljtEsBgUxhaIYBkRYbwFTqlUNmrUyMfHhz1HJSjx/CeT2V3n0dHRdnZ27dq1S0pKwhrLqhCvOvOIliZZIsRxFV7EoFKpYmJikMRMnTpVpVLxd6vympexjOjduHHDxsZGLBbfunWrjBUoH9GxQlhCgjVgwAArK6uwsDAAwGlO7KjKB0plkwocMMTFxeHJtUWLFr2xay8bxQoTC2b3999/DwC9e/dOTEws9SlM1i7hU7Ds9lq8W5eV5MJoa8ANTl8tXrwYAFq3bn3v3r03JmTt2rV4eQ1PnviWnJeZG3ZqCpeDf/nlF6YV757JTGDOiGAxKIwtEMEyIsJY5fCJmHnz5uEokw01WMR8BWAyNppqtXrevHkAsGDBAjZ9pbdFYB4x2FIkWKhJdHT0f7Vmi9ZszjebtGbz5s2rV69+//33x4wZ8+6770ZERERGRuJIzsbGpmbNmjNmzMCLbfQqz6AoAwGT06NHDwBo2bJlGcRYvqOYNm2aj4/P48ePi5fMgQMHAoBEItGO4/9+YASvMC1eaBXTF7Yzv/32GwDUqFEDt0uz1sacMWHUEOvj3Llz2fRMIdWOjo7evn3771qzZcuW/GZp86ZNmzZs2LB169Yffvhh9OjRY8aMGT58+ODBgwcOHNi3b98ePXr06tWrY8eO48aNS0hIYNylkJHqOsNmLSsra8iQIfy2zoKaO7SvWrWqk5MTHhBhYfItOS8zJZGS3r17t169elWrVv3rr78Yk+PdM5kJLAoiWAwKYwtEsIyFMDvu17lzZ4lEgq8HYr0SlHj+k8lYi+7fv1+nTp1atWrduHGDTV/prbTMI6antAgWjpZUKhXORUmlUonW4IXdUq2xsrLiL++2yzeOjo4uLi4ODg4AMHToUJz516u8sfJAX7iIjKurKwAcPXpUnxOyKxAB/qnB7OzsrKysCRMmeHt7X7t2LSMjIzU1NT09PS0tLT3f4EOE+MUeJUxOTk5NTcWSEB4eDgAikWj06NEWNO9SIEBl/gPCmJeX99FHH/19ycXESaiCyStaYZBAFrhlyxYAqFev3pkzZ4q0PqhUKj08PJCgY7skkUhY04SNEj4qYGtra2dnZ29vj68LODk52draAsDHH3+M28NLCBcmZOvWrba2tlWqVDl16tQbC/O1a9ekUml4eDiyTAYX35LzMhuZI59euHAhe1CIcS/ePZOZwKIggsWgMLZABMtYCGOVO378uJub2+DBg/H6dazGghLPfzIZCdb69esB4MMPP+TZld62gHnE9JQWwcJU7N2718XFpXfv3v/973/37Nmze/fu/fv3Hzp06MiRI8ePHz958mR0dPS5c+cuXrx4+fLl69ev3759+86dOw8ePLhx4wZuIBs7dmxRx6bGyBhsy+bMmWNlZVW5cmXLekvEGIAUKcwXL17079+/R48enTt37tixY+fOnYODg52cnADAy8vL19e3qtZ45xsvL68q/zTozMvLa+rUqRh1REQEALi4uOA4Xm/ZLpKSFc0xNhSHDx/28PCwtrbGjZ7YB5s5FJjXeXl5eIUytnKCA0CGk3DmzBmJRBIaGrpz584dO3bs2bNn//79Bw8ePHLkyIkTJ06dOnXmzJnz589fvHjxypUr165du3Xr1p07d+5qTWhoqEgkmjZtGm5dKEnBw/2ySqVy8uTJADBlyhQMraAw0b5fv34SiQRv2+eTybfkvIwsCvHJysrq2LGjTCbD2UpG0Xj3TGYCi4UIFoPC2AIRLGMhjG3ciBEjAODgwb9PebD6Jijx/CfK6DIpKalr16729va7d+9m7IR3zKsusC8tgoWpwCcUT548ycdoWMYkXLhwAY814TV6Jm/3lUqlQqGoUaMG3sTIGibDaaFfsfTu2LEDADw9PatVq+br61utWjV806ZOnTq1a9euU6eOv79/3bp16+Wb+lrz9/M6AQH4tHODBg1sbW1FItG///3vJ1rTs2dPsVjcoUOHN474KRf0IoAvYeN8RmRkZEZGhqAp0OvLHCyRGp48edLV1dXe3n7z5s1sd1Eh1evSpYu1tTW+s1RILxjp0aNHcU/91KlTS06wMMxz587Vr1/fysoKOS4OTfVqhc+mOTs7N27cWHdnKp99vIx1EJvQzZs3y2SyQYMG4aVxrB3j3TOZCUwZIlgMCmMLRLCMgjBWub/++qtOnTpBQUHPnz9ny+RYTxjZEnxiZcAKgzf89u7dOzs7mx0q1K0tmACBfakQLExFbGysn59f69atX758ibtE2V5RtnsUY0clcYyFm/S//fZbfMdj1KhRjCMaBfFCBIrJ2bhxo6Ojo5WVFR4aKIQ/cvI3AtnZ2V26dHFzczt16tTz588TniW8eP4iMTExSWtSUlJevXrFFgdfv36dlZWVk5Mjl8tzcuSKv02eUqm8efNm1apVRSKRjY2Ni9Zg8di2bRsr4QR34RHAhuLevXutWrWSSqW4vd1A1174kMvAJeY4UsPw8PCcnBxBI2ZYh0ePHllZWbVp0wapORJNvmnC8AVNEz428Mknn2DB++STT0qLYC1atAgAQkNDnz9/biAhmGW4sxZfBBIkk/fLy9iD4BGiESNGiMVivLqZn/Pj3TOZCSwiIlgMCmMLRLCMgjDezoAzxqwWMVIlKPH8J5Plcvn48eNlMtmPP/6o0Wjkcjl6Zw4EegvsS4Vg4WgJJ/Dx1DGLlI+OpYuRRbR58uRJixYtcNvWe++9ZyYEq1OnTgAwYsQIzCOWIhIMI3Djxg28AgAvAsUCgAeykGdjD6dUKvPy/uZSaLTUSpGXl4fd2OTJk6VSKQD4+/sHBgYGBQU1bty4TZs2mZmZfCkyrAn9yhDA3nrz5s0A0LRpU7ZTkzkwWwE1v3//fosWLWQy2VdffVX49gGLytSpU0Ui0caNG7EosvJjoGlC6vnw4cOmTZvKZDIAmDx5cgkJFg7bEhISevToIRaL33jhH95l+NZbb9nb26ekvNTNIIH+LF0Mn7Nnz/r6+gYGBuLJElQAw9Hrl7dEZ0SwdGE3kg0RrNIHFkdRr1+/bt26taur661bt7CIs6oiKPH8JxvH37x5093dPTAwMD4+HgNE77xjXnWBfckJFvaamZmZb7/9tqen55MnT/goBDLTBO2x9dy2bRsANGnSxNPTMzIykjUQzHFZCjjOw4tPAQC7orJUoKC4MMfZXKB5CkqlMjIy0s7Obt++fQUl5I32mZmZ/v7+OHMgOHXIbvnH8mOSv2/U39wcYC3LyMjAfQgTJ07ESQ7Wzpibwrw+fBPRrFmzmJgYdjEN70yvjLM49vb21apVY3tbWaoNNE04YsQrIdq2bevs7PzRRx/hQIt51xujAUvkN2vWrAGAgICAixcvGkgIpvqPP/6QyWQTJkzguRGLQqA/UwxPTWk0mjlz5gDAt99+iwNaDBO96/XLW6IzIlgMbWMLRLBKH2GsxmvXrhWLxZMnT87OzmZTOxiZoMTznygrlcqlS5cCAO4FVigUbB6Yd8yrLrAvOcHCVCxbtkwkEs2ZM0fQDPHRsSaAr/AZGRl9+vRxd3efM2eOj49PRESEaQkWJqd///5isbhRo0aZmZk8eiQbRiAxMREAmjdvnpqampmZmZWVla01WfkmOzs7MzMTzxW+1pqMjIz09PTU1NRXr17hscHvvvsOVwYB4PLly4ZjpF/fiAB2z4cPH7aysnJ0dNyyZUtR9zC9MQojOUBOkJaWho8Pvv/++/z7SIYjRb8bNmyQyWRz5szBxkfQHLEWiQm4jKhSqbKysrp27erg4PDVV1/5+fnpfRzWsAL8r6hMdnb2mDFj2I1WPOPhHbMDkm+//bZMJsNz5QIHBnoKzO7Y2NimTZu6urpevXqVHVRngejFgbdEl0SwGGLGFohglTLCWOiVSuXQoUNlMtmhQ4dwvMUvneAFd/mrKH8vqbBVldzcXKVS+fz588aNG3t5eZ07dw7HQ6ySMEGgt8C+5AQLNyt069bN1taWPaHIIuWj41sxZn/27FmRSNSnT5+zZ886OzvjNQ3IclggZSZgpiQnJ+OO+61btzLCWmY6FBRRVlZWbGxsQkLCs2fPnmrNM61hMhOYg6dPn/Ju+E/enpf1uinIgcBxfHz8nDlzxGKxv7//Bx98MHLkyFFaM1Jr3tOaEfnm3XffHTZsWGRkZHh4+IABA/r169enT5/u3bv36dPnrbfesrW1HTJkiEwmmzZt2vbt27dt27bVpOa///3v77//vmnTpj179uAVl3xhLijLzMRerVbn5ubiNsf+/funpqaaT6k2DBFSkKNHj+KZiT/++INNzxj2yPYhBQQEODo64ryXAVLC5yY2Prt373ZwcIiIiDh48KCXl1cJCRaSnhMnTnh5ebm6uu7cuVOlUhXUyuFCxIMHD5ycnDp37pyRkcG6AF7guwNexqXMX375BQDGjx/PtqzxaWTNL48Jb4nwEsF6YzErLQdEsEoLyf8PB2vXmTNnvL29Bw0aVFBlMxzr3r17AWDw4ME8u8KKpFtbMCiBfQkJFm5WOHfuXNWqVcPDw9PT0/lN+nztRZklB5vO3NzcqVOnymSy5cuXx8fH29nZDRs2rHRnsAq/sob9EE6t29nZeXh4JCcnm7wrQgWeP38+fvz4Dh069OjRo2vXrl20pqvWMJkJzEGXLl14N/wnb8/Let0U5EDguFOnTmKxWCKR4O1BDg4O9vb2Dg4OTk5Ozlrj4uLi6upaSWvc3Nw8PDwqV67s6elZpUoVb2/vqlWr+vj4eHt7i8XigICA+fPn4yVkuFZoJn+DgoJiY2PZAj0rz2YrYNceExODJ9fwIuIy2FaI7QzfqRcPIrlcPnv2bAAIDw8v/Momxn7u3DlbW9uIiAg2V8S3fgIZ1UNyk5ubi7eFbdq0KSYmxtXV9YMPPsAmungpwrZl7ty5ADB8+PDMzEwDDQtm2ZgxYyQSyeHDh4uBm0ql6t+/v52d3Z49e/iLe1hQgrQX1GUQwWKIGVsgglXKCCM1wY3h8+fPv379+gWtOX/+/Nn/Y++646LK2XVmBgSpUqSIdFERBZRdCyhVXRVXxYINXSuCDeyuig1FRVdXrNgVbIgNXeu6ioq9rLoiCgpIEel9GJiZc3/Oe8099wx1OJT9vuQPyMmkvHmSkzznzZtE4h48eHD//v17EndX4u5IXHR09O3bt6Ojo69fvz5gwAAVFRW6zh+/OdjDkJsRXk+CBYOOj48PQujixYvAovBwViPB+vDhg6Ghobm5eXp6emxsrKqqKrsES4bRkM/n9+rVi8PhLF++nAFdkzwCmOfOnWsmDKNGMXR0dA4fPnz16tVr167duHHjzz///Evibt++fffu3fsSB8ehPX78+MmTJ3Ao2osXL+BotOjo6LZt21pZWU2YMEFOTq5Xr15Dhw4dInFDJU7aP2TIEPwT9tADGX54lM6HnpaeZNiwYXZ2doqKimpqamDRItvnUJP3Hzs7u9jYWOAQtRGmoqICNh9U+he0JnTdCehXGNaBtSmo0jjw8sbGxpqYmKioqISEhOBPoErj0wOBo4wdOxYhdOfOHTwO0Ec/hh+Sw7D8/PlzU1PTDh06pKWlvX37VkNDw9fXV2aCBcK8fv26ffv2LVu2hN1IVXUh4O7Jyclt2rTR0dG5devW8+fPnz59+uTJE/rUEBMTIz013Llz56+//rp7925ISIiamtrAgQNhBzSQOQyC9LAMP9EBATQIwaJ3qgb1E4LFJrzwysXFxf3www8IIVVVVS0tLVVVVRUVFSUlJUVFRXl5eTk5OS6Xy+Fwqp/SevbsmZubC8a/9DdH+m2BCjDC60Ow4L1NSUnp2LGjra1teno6VqRhsOjFMd5woVB46NAhhBAczfD8+XN1dXV2CRZFUe/evfvtt9+2b9/+m8Rt2bIlODh406bgjRK3YcOGoKCg9RIXGBgYFBQ0adIkOLvZ3d191apVGzduXL9+/cuXL3GNGtMD7Co/Px9OYbW3tw8KClq7du267279+vXgpXuwf926dVC179H/75EeTo8PSXB87KHHYaSFn9avX7927VoDAwMTE5P8/HyZUcrNzbWwsNDQ0NCRuLi4ONyroS81/l+KomB9rXv37h8+fAAaIXMFGzMh9J/s7OyRI0dyOJz58+dj+57GFEO2sqChYedjnz59MjIyqtH6MIoA8wkNDQ0bGxs+n49/ZQxHeETCHkAMjoRYtWoVRVGPHz/W0tKqD8GCPMG8vXfv3gkJCdVwXGB4c+fO5fF4cLJuq1atVFVVlZWVW7ZsqaCgUMupgcPhhISE0NVXuI74hQJYMCbYg+EiBAtD0dAeQrDYRBg+X86ePaumptamTRs3N7f+/fsPGDAArr4aOnSoh4fHiBEjRo4c6enpCRf2jR8/fuLEib/88svkyZOnTp3q6+sLJ/wOHz4cTmcA+fBLgj0MuRnh9SFYUAuYe3bv3g05S+ePX2yGJysry87OTlNTMzo6WiwWv3r1ikWCBWUVFRV5eXlVz1Dxr8BlFRQU9PT0DAwMVFVVFRQU4NeoqCgGjI3zCENzTEyMsrKytrb2uXPnGqdcmUsZOHBg27Ztk5OTy8vLBQIBVnLgUxhAKQLh3868kjiwOwSzwtTU1Pbt2wPsGzZsqP0KL0NxwsojnCiRkJAA2/VBqVmV7kFm0BouIfSfe/fu8Xg8Q0PDmzdvgqFnjSVCwtOnT4dVFl9PAAAgAElEQVSEhGyXuN+/u23f3ebNm/H3SWBg4Jo1a1avXr1y5cqAgIDlErdq1aoVK1a8efOGYTZQY+mYAXz9+rVPnz6wk6725u3AUebNm4cQOnToENQFCqWPTgw/cBGKotLS0nr27KmjowPj0sOHD+tDsKD0L1++/PTTT1wuNyAgoHoTCOhdcOVijx49YN2/0qlh9OjRY8aMGT9+/IQJE3755ZdJkyZNmTJl2rRpPj4+nTt3lpOTO3fuHN7NQK8shpeBCSMORVGEYNWmr7IShxAsVmD830zgrUtMTGzXrt3o0aOzsrLgarbCwkLYe1VWViYQCGAGog8QdCFevXqFEBo+fLhYLAbDRvqbI/22QFpGuMwEC5RVOTk59vb2+vr6//zzD7aQwESKLg/4sQwURd2+fRsh9PPPP8OY8ubNG9YJFpA2BweHkydPnjhxIiIi4syZM5GRkecl7sKFC1FRUZcvX/5D4uD2jOvXr//xxx+XL1++cePG3r171dXVW7RoAbWjg984frFYzOfz161bB0AVFRXBmZyCZumKiorc3NwMDQ3rSrCAdZWVlVVUVKSkpHTo0IHD4XC5XHzkB3TaxvwLRARewOPHjyOEbG1t37171/zVVwxKWlJSAqY/Hh4efD4fq7qr78AQTVNTE39+yOw5c+ZM9ZSCIQnIDzp+GCLMzc2fPXtWjdaHkYNQKBQIBMbGxjo6OnBDM45AH/0Yfqzbg+aeOnVqUVERRVFPnjyRmWDBIElR1I0bNzgcTocOHZ49e1a9nT6wwylTpsAVqPjKTjiSt7S0FKYG+DKpamoAy5MLFy5g5OmVlR6WYcRmxCEEC3ebRvAQgsUmyPBiZGVlde7c+Zdffqkqa9zjwQNDDHyaC4XChw8fAsFqEg0W2MkePXpUQUHB398f39aHZYZK0R/xa0xRFJ/PnzhxorKy8uHDhyHmP//8wyLBgu/RkJAQLpe7b9++qhCuPnzlypVw3GVVO6WrT17PX6GTfPjwwcjIqFWrVqDwb87qE4FA4ObmZmxsXFT4bWaSzfH5fCsrKy6XO2bMGDi4RLZ8ZE4FtyThj4T8/Hw44hLW15oz/iA5UBNc/fj4eAMDAyUlJTiiszbm7TDC3L59W01NbejQoWFhYcePHz9x4sTJkydPnz6Nv1IiIyPPnTt34cKFixcvRkVFXbp06fLly1euXLl8+fKtW7cWLVqkrKysqakJF6UzpMLi0T2wsQ4YBjCDX375BSHk7u6OiQI9fqV+UDqeOHFCUVFx4cKFjPZiDEe4lSFcLBYXFBSMHz9eUVExIiIC8ocj8eq0RAgUqry8HHhqaWkpqNMmTZokbUTBqAVUf/bs2QihV69eMX6lP2KZIU9sACcUCufOnYsQIgSLDlcz9xOCxWYDwdyZkZHRqVOnCRMmwPlVFRUVmDzBywOP+EViPD569KipCBa80iKRyMvLS0FBAba6QKVAWgwW/ZFOsF6/fq2iomJnZ5eTkwODIIsECwpNT0+3sLDo1KlTRkYGKH5g3QpUg6ADgg9BbMwLj+Xl5WVlZWlpaUZGRvDhXn8bLIZeAZqyqr+wZAYD9MmTJxFCPXv2TEtLKyv7dpkMxra5ecrLy4cNG6anp3fgwIEzkd/ONYDTDU6fPn3y5MnjEhceHn706NFjx44dPXr08OHDByXuwIED+/bt2ytxGzduNDY2RgjBzXF4CmyEyuL1FCgrKyvr3bt3sIWta9eunz9/hg+D2otUp0avqjPUxvCIIXlhYWFmZmZ2dnZRURGYOf7ww48JCQlAPmpEEub4oUOHIoQePHhQY/xKI4ASpVWrVjdu3MCrb5XGhEB6xy4uLs7Nzb1y5YqysnLr1q1BB1ZSUlIb5EF4JyenFi1awGtLT8UYjvBP0FIURf31119aWlouLi6ZmZkwoNWVYEnrOG/fvq2rq9u2bVu4sLn62whAfl9fX4QQ1ttBwzHGf/oj7jxQQT8/P0KwqulszfAnQrDYbBR4db9+/dqpU6eJEyfC5x0EMpbS8BDA0OuC9WVTESw4++rZs2f6+voDBw7My8vDZhb0IYwhM65LeXn5+vXrEULLli2D2+soimKRYEFTXbx4Ea6sx3e20OWpRk5ojt27dysqKoJhVv0Jlmy9Jy8vr1u3bgoKCitWrJAth8ZMVVJSYmBgIPNaEj2hqalpdnY2/V1o6IrAFEVR1IcPH1atWjV8+HA7O7u2bduCVDo6OqtXr8bH+tdGH8PoYA0nP5b89evXGzduHDduXK9evWxtbbt169anT5927dopKCjMnDmLoqjacBT4dvr06ZOmpmaPHj1KSkqw2RzDkE76saKigs/nl5eX371718TEBCGkpqZWI8HCS2aFhYW///779OnT+/TpY2dnp6OjAzksXbr0/fv30BmqRx7IzcuXLzU0NNzd3fHRzRh8eqNI+8vLy1esWIEQgntswDq+TgQL08R79+6tWrVq9OjRAwYMgOuidXR0NmzYANv6MJ/DgmEPEKyZM2cihEBxTpcTR6MPZdiPB1hCsOhA/Sv8hGCx2Uz/AQQLVuixGSm2aMbDLnhAUYT94ElLSzMxMdHT0/v48SMYTLBIsGCUycnJsbGx0dfXf/nyJdjTQDgerbAH2pX+KBQKy8rKHB0dEUKOjo4cDqc+S4SQM5/PLygo4PP5cLI5/e/3Q87/9396evoziXv8+DHsZoLZ/datW+fOnYuJiZGeNtjsmvXISyAQ7Nq1C/Zpbt68eavEbd++fe/evbt3796zZw/oqPbu3RsaGrrvu9svcQck7uDBg4cOHdq/f39MTAzMNPUQpw5J8cwUEBBgaWmJEGrbtu3QoUOnTp3q7+8/ceJEsLs3NzffsmULmOZUP9ND2QKBACxmsMlcmcTx/7+jdwbsLykpKS4uLigoyMnJyc/Px19fjFrhXr1q1ap27drBvjMnJyc4hKJbt25AEM3MzI4dOwZpcWUZWcEjUAQ4BYpxfx/9HQE/zgr/BHIGBgYihOTk5GpDsADJ0NBQFxcXSNWrV6/BgwePHDnS2dlZW1sbIdSpU6dt27YB46kGeRAebgSKiooCEzq8lwIrqvFwBMrs8vJy4IWxsbHm5ubt2rUDm8u6EiwQ7MWLFxMmTNDS0kIIaWpqdu7c2cHBoWfPnurq6qCK3rVrF+RcaQ8nBKvSbvkfH0gIFptN/K8mWKCv/vTpk5WVFZzOUFdoIiMjEUKjR48G0woYU9jVYD179gyOYAV2hddZ8EyAPSA8fgR9wM2bN5WUlFxcXObNm8flcmUmWDADiUSiBQsW9OrVq3v37j9IXDea69q1q63EgcfCwqK1xMHUAhNkixYt9PX19fT0fHx8YLcdntuqAh+vfjLmFZhdGH8xA2B4ysvLqyqIvv4FmoNqZr6qhKwmvEaxGaJKP5aVldVSJKFQmJeXN3LkSLgTMyws7N27d/iWpPz8/Ddv3oSHh9vZ2SGEJk+enJKSUs2yFyCWmprq4eHRu3fvXr169ZS4Ht9d9+7df5Q46Azw187OjtYpvl1u3bVrV2dnZ0dHx7Nnz+L+yUBMJBIVFBTAgU/W1tb79+9/9epVdnY22DOlpqb++eefK1asaNOmDY/HCwwMzM3NBVMERj7wCG3K5/PNzMzatm2bkZGBtSN0D/bjvgHiAdofP37s2rWrsbFxp06dFBQUbt68WQ1WkARshnR0dIKDgx89epSRkQGr5B8/fnzw4MG+ffs6deqEEPLy8oqLiwMdm7T8IHxOTo6xsXGXLl1kMOA7fPgwQmjevHnQn2FcqqUGC6A4cOAAcHFPT8/z58+/fv06LS3t69evcXFxd+/e3bJlC1wRMXny5NTU1EphIQRLumX/G0IIwWKzlf/VBAs+E3///XeEkLe399OnT6Ojo2/dunXz5s2rV69eunTpwoUL586dg6tFwsLCsLVNqMRt377dzs5ORUXl6tWrMFaySLBgmCstLfXx8VFTUzt79ixwODw/SXugXSEcr1aMHz8eIXT+/Plly5ZhXb0MPQAms0uXLikoKBgZGbm7uw8YMGDgwIHu7u6DBw8eMmTIsGHDPDw8hg8fDkdyjB49evz48ZMnT4Yd1zNnzvT391+4cOHMmTNtbW0RQlOmTIHtTnhuq1Sq6n+tNEldA6vSqdQ1H6gOnQ5WwwBkyLzGJIDViBEjEEJz585NSkqCJAKB4OrVq/fv38c5JCcnT506FSE0bdq07Ozsqkya4D6rRYsWIYTatWsHtKpXr1729va9e/d2dHR0cnJycXFxc3Pr27dvv379oEvAES3QJYYNG+bp6enu7g6KkAMHDlQ6GcPHw6hRo0DyhIQEEFUoFMbExMARYkCGnjx5MmDAAITQxg0b8Wo+rhf2API7duxo0aLFihUrgP1U+srg9wXSwiN0iSNHjnC53NmzZ8O9e9UYuQPyYK01YMAA+kL8y5cv6ddQJiQkwMa6KVOmFBQUgIkCFhs8MIwsX75cXl5+1qxZcBTnX3/9dePGjatXr16+fPnChQtnz56FcSk8PPzIkSNgBYjHJbiu/urVq7hPUhRVe4IFt+toaWmdOnUK7rSgKKqgoODLly9Y1A8fPkAX8vLy+vr1K17exREIwcJQ/Fd5CMFis7n/vQQLVEG5ubkeHh6wHqGrq6upqYlPw2MclEo3rKH7u3fvXlZWBrmxS7DEYnFKSkrLli27d+9eVFQECwTwSQrbMGFexCEwxuFv1oqKioSEhLZt2xoZGeXk5AQEBMh85TA+eHratGlA1/Lz83NycvLy8gokrqioqLCwsKioCM7mgDVCWD4qKysDw/zS0lKBQBAXF+fs7Mzlcn18fCgxVekEgzsooHr58uWTJ09GREScOHECaO7BgwfBlnzXrl0hISG///77b7/9tnnz5uDg4A0bNqxbt45+mtHy5csXLFiwYcMGIBx0OgWzqVAozMnJyc3NzcvLe/bsGZwiff369StXrkRFRTEms2PHjh0+fHj//v179+7ds2fPzp07f//9961bt65YsQIYDGaEkHlWVtbJkycPHz585MiRgwcPhoaG7t27F4u9devWzZs3b9q0KSgoCIu9SuLgEKalEjd9+nQwAMKZY4gYng0bNsDXAmitoId8/vzZyspq2LBhFRUVoDWkKKqwsBBWoDDpYWSOObq5ubm2tvbz588zJO7rdwfm59kSl/vd4S5RKHFwVfbFixctLCx4PN7x48elCRawH7C2mT17NmzjhR7+8eNHY2Pj6dOni0QiWI2iKCoxMbF///5qampwmhq8dHQcsKW8vb29goLC33//Db9Ci2CtFT0Q1x0bFeXm5o4cOVJOTu7SpUsLFy6EswakhcdHukRGRnI4HFdXV9DowBdXTk6Oq6uro6OjSCQSCATQ9woLC318fDgczs6dO3FyLD/euNelSxdYm9PW1tbQ0GjVqpWamho+pbNFixby8vJcLpc+FtH97u7uhYWFUGWACAgWaI4BBFwo9ojF4uzsbENDQw0NjZiYGAgXCoWFhYXz5893cnKiH/ABFUEIrV+/HnDDMOKjIogNFsb2v8RDCBabDd0MCdZPP/0Em+wYi0eMRzBWePbsmaWlpYaGxtChQ8dJ3IQJEyZNmjR16tRp06bNmDFj1qxZc+bM8ff3nz9//sKFCxcvXrxs2bIVK1YEBgb6+/vjm8VgCKMTLC8vL4FAUFJSwii3lo9gVLFv3z6EENxqIj2RVNOQ0C4BAQE8Hm/lypVisXj58uWwnaeWAtCjAVV68+aNkZGRg4NDTk5ONUVX/1NERISCggKHw5k6dWp5eTmwLnpZdH9ZWVlxcTEcNkifPOrkB+t+Z2fn+Ph4sCXCRfD5/IqKit27d9vY2BhKHFw+iG8g4EhcjcUpKytzOJzdu3cLBALoV+Xl5SUlJSKRyN/fX01NrcYcqokgLy+PEAoJCaFnjqtA9xQWFqqpqVlaWtLvFaEoKikpSVNT08HBAcg3Zk5JSUm2trYWFhZv374Fmyp6bnw+XyAQnD9/Xl5e3tfXt/pmrf7XRYsWwSkhR48ela5FWVlZUlISbMXNysqic44PHz7QTzfAkj98+FBbW9vV1ZW+rxYLD4Q+OjpaTU1t1KhRjINXgAoAd8d++iMwoT///FNFRaV///4lJSWgwbp586a08LBRVyAQdO7cWVNT8+3bt0DC4AXMzs42MzMzNDSEQMz80tLS7O3ttbW17969W1ZWVlpaioWHDbZv3rxp27athoaGh4cHY1yaPn26j4/PzJkzZ8+e7efnB+PSkiVLli1btnz58uDg4MmTJ8P7hQulEyxfX1946bDZFi4altFHjx6NEDp69CiQMKC/hYWFQ4YMUVNTA7YKalpQa/30008aGhrR0dEVFRX0ikApsIuwmRi5v3v3rlKKXH3vJb/WCQFCsOoEVw2RmxXBys7OlpOT69+/fw1C037Oz8/v1atXly5dwFQlPz8fzsHD9tr4PEz6iamgd/n48SNCaNy4cXjcxwSrVatW48aNo5Ujizc/P9/AwKBNmzYvX75MlrikpKTExMRPEhf/3b1//z4uLu7dd/f27dt//vnnzZs3T548sbKyUlBQgENoli5dihB6+PChLKJI0ixevBghtGfPHsAEWBcYDGFGS18gA40a/W9OTg4sWeIlwhqFCQkJAZB37twJ1uX79+8/dOjQ4cOHjx49Gh4efuLEidOnT585c+bs2bPnz5+Puhj1xx9/XLly5dq1a9evX4+Ojh4zZgxM0rCbT7pEMIuZMWPG/PnzgUAHBASsWbMmMDBw48aNwcHBW7du3b59OwgQGhq6f/9+0EgdOXIkMjIyKCjIzMwMIQTqGXr+JSUlxsbGurq627Ztg3Mcjhw5cvToUTiQ6dSpUyD2uXPnoi7+7zmxcEjszZs3b9y48ejRox07dsARGzDh0TOX9sNBBnBDHOzVAK3np0+f9PX1nZ2dMYuCuZCiqG3btiGEQIklnSFFUbAeFx0djTWRdPN27MceoGUwecMU+/btWzD5Qghh+3RGWWvWrOFwOCdOnADNB9gtCYXCuLg4OTk5Dw8PoVAIh4tCvcAWUF5eHq4NZeQGj2DOdenSJcwkwIP3EmIFMHRR0L5g/5IlSxBCO3bsoCgKTrG6d+9epQVRFHXnzh0ul7tgwQK8JAerq5mZmVZWVu3bt4cdMFA0nOAFZlJwiY10tvn5+ebm5tbW1omJiQz1cGlpKWwxwUMTrhec0nzr1i18bRecX4UJVuvWrSdPnixdHA7JzMxECDk5OeEmAJQKCgpGjx5tYGDw999/g1Uc1inC7aILFy7EmdA9zYRgrV27VltbOzk5mRAseus0hJ8QLDZRbWYEK0deXl5DQ8PR0RGMauke8Ds5OYHtiLPEOTo6qqqqOjk5paWl1RWX5ORkmPuxPhwGsr///tvU1FRdXd3Nzc1J4pydnenlgh/kkZYTBHN2doZpSUFBwdLS0kLizM3NzczMTCXOxMTE2NjYyMjI0NCwbdu2Bt8dmJDr6enp6+vzeDwvLy/YLLZkyRIOh2Npaenk5ITrzpDB8buDOFhmJycnV1dXVVVVW1tbOEWpUqxgWaesrKykpKSwsDAvLy8nJ+fr169paWkpKSlZWVmRkZFKSkpaWloaGhqamprOTt8Mn3FzQHEYK0dHRxcXF21tbR6PN2fOnK1bt27YsAEuCly5cuWKFSuWLl26ePHiefPm+fv7z507d9asWT4+PjO8Z0yfPn3KlCmTJk2aOHHi+PHjLSwsOByOpqZm9+7dXV1d6fk7Ozv37NmTy+X26NHj6NGjcMxVWFjYkSNHDhw4gNfytm3bhhfyAgMDV69eHRAQsGzZssWLFy9fvnz48OHq6uocDqdjx47Q4tCmTk5O3bp14/F4tra2q1evXrdu3cqVK5cvXw5iz58/39/ff86cOTNnzgSxp06disWeMGHCuHHjfvnlFycnJ7hQskOHDm5ubhir7w31f/3cxcVFT0+vdevW2IAJt1FmZqa+vr6bmxsOwZ6HDx+amZnp6ur269cPNzdA1KdPn969e7do0WLYsGE4fp08QFl+++03Lpero6OjpKRkamrar18/uvDOzs4uLi4aGhoGBgagvqIX8fnzZy6XO2LECHog+C9dutSyZUtjY2OABecJb5yTk5OioiJwSum0MMsKBILS0tKioqK8vLzs7OzMzMyMjIzPnz9nZmbeunVLX1+/U6dOsbGxYrEY1lJtbW3p7QslwqsEDFv6OM3i4mIgWNIyvHr1ysrKSlNTs2/fvvTXDQ9TCgoKvXv3rtNtmAD4nTt3KiVY0Nbq6urwZkm/+87OztbW1gihvXv3MgTm8/mjR4/W19cHFR391y9fvjg4OKipqQEUUBcAx9XVFc4Hwed40dcQcSZ46Rav3uJorBzTIBaLN23aJCcnB/XCJ1BgAYiHRQQIwWIRTKpZEazY2FhYT6lmwaXSn1xdXT99+gSKKPpiAfbjpQQYCyD806dPlRKsFy9e4EvoKi2u9oE8Hq9t27YWFhbt27fv2LGjpaWllZWVtcTB+UB2dnY//vhj9+7de/bsaW9v7+DgANbH/fv3h0NrgoODgfYtXry4xvu2axRMWVnZ3Ny8Y8eOIFL79u3bSZy5xJmZmQHtM5Y4OvPT19dv27atlpaWurr60KFDraysaiyrMSNwOJyWEqeoqNhC4mp5QzkWEm60xY/seqoxtWEU1LlzZz6fX1RUtFniQP3266+/qqiomJubw73gW7ZsCQwMPHz4cGFhYWpqKpwpwMiH/qijo9NZ4qysrDp16mRpadlR4jp8d+3/v7OwsGjXrh18DLRv375Vq1YmJiZDhgzR1dWlZ8vwu7m5CQSCnJycIxJ3+PDh8PDwTZs2cTgcOzu7EydOHDx48MiRI3v37r18+TJFUc+fP6+xF6mrq0NHhe8TEMxM4kxNTeH7BH+lgLa4TZs2bdu2hX2v8+fPhzd98uTJDGmlH3V0dLKzs8vKys6cOQMa1rCwsJ07d7Zp00ZPT+/o0aMHDx48fPjwvn37IiMjcyQOrD+ls8IhTk5OmZmZePCpZjiCn4A6/PXXX5USrJiYmNqMS0pKSs+ePRMKhS9fvoyOjr57925MTMz169ddXFy0tLQOHz4cExNzR+Lu37//5csXsVgMZvtYbGkPLCzSiRR9EqKHg58tggX58Pl82KgB+kgYD+kCED+LCBCCxSKYzYVgwYu0dOlSDoezcOFCuLDvRLUuPDz8zJkzISEhBgYGvXr1wtpj/G7TX3v8aQXYQZzExMRKCdarV69MTEwMDAyCg4MjIyNhGataWZg/njp1Kjw8fPDgwS1atNi/f39WVtbXr1+zs7OzsrLAphgGaDDNBvPivLy8fInLy8vLysoqLCyEu6uBYInFYtBgjRs3rjbgYIGOHz9+6tSp0NBQGxsbDoejq6trYGCgr69vYGAA5vNGRkbGxsYmJiZw7g6dC3bq1MnKyqpLly7W1tZdu3a1sbFRUFDo0KHDmjVr2rRp06FDh/379586dQpuL8Elgic8PDwyMnLOnDk8Hq9fv347d+7cv3//vn37Dhw4AMtzcIo6RD516lRERERkZCSsEl68ePHSpUtRUVHR0dFBQUGtW7fmcDg//PDD7t27z5w5g4sLCws7f/68qakpj8fz9fVdKXGrV68ODAwMCgrauHHj5s2bf/vtt99//33Hjh27d++GU68OHDgA0+eRI0eOHTt2/Pjxfv36wYzi7e194cIFaO5jx45dvnwZlmiDgoKOHj166NAhSIIvbDl16hQsbn67U/Lc+Qvnv13VcunSJbzEee3atSFDhiCEuFyur6/v+fPnq+lLoaGhGhoa3bp1oygqNTVVepJjhNjY2MTHx5eUlLi4uLRp0yYkJAT31ePHj588eXLz5s2tW7dGCKmrq6uqqqpJnLq6eqtWrTQ0NLQkrnXr1jo6OqAuhV5haGgIXcLU1NTCwgJO6Rw4cCCYYfXv3z88PBw3OrTF9u3bVVRURo4cWVFR8fz5c7B743K54KH/BUMuR0dHgUDw4cMHa2trCwuL0NDQiIgIQAYkDw0NhTP0jb476KKmpqa4l3bo0AF/sXTu3Nna2hqOk+jWrdsPP/ygq/eNC2IjdCAQ8+fPP3/+PO4/uKNGRETo6elZWFjk5+dnZmYaGRnBBZQgOUIIPwILb9++/dOnT8Visaenp4KCQmBg4Pnz58PCwiBDqEJISAicj4pPqa1+OIIRCahDVQTr0aNHpqamenp627dvP3v2LKMiJ06cOHXqFFiSffz4sbi42M3NTUlJSVlZWUVFRUlJicfjwXeIioqKsrKykpKSvr4+3MPj5+fH4XCWLFkSFRWFu2hYWFhkZKSTkxPevMyoAp6H6OHgx4NwPTVYkE9KSoq2tnbnzp3BLBJnjgUgHhYRIASLRTCbC8GiKCo3N9fMzExTU7OwsLD2NSwvLx8wYICNjQ0+EAi/fvTXvjYEC1tiPXjwgMfjgeFI7SWRjgm7xM+fPy/9U21Cdu3ahRDatGkTDLtw3Kj0KkaNWYGVK3wCHj16NDc39/Pnz+nfHWNnWdZ3l52djZlfTk5OQUFBenr6mDFj2rRpM2PGDGyDBZlLywCt8PjxYxUVlarMO6RTSYdER0cbGhoihI4cOcJoUIClT58+SkpKcL62dPLahMybN4/D4YwYMeLr1684PugSBg8erKqq+unTJxxeVw9kPm3atNochmRkZGRpaVlaWlpSUhIpcWfOnImKitqzZ0+rVq2srKxOS1xkZOTp06dv3LhRVlaWkZFhbW3t4eHBWDcBc/gePXrIy8s/fPgwNTU1ISHh06dPiYmJSRIHRoEpEpeampqWlpaeng5/v3z5kpGRkZ6enp+f//TpUwsLCxcXFzCdBmMv/IrBayUSiUxMTECDlZmZ+ft3t2vXroCAAA6H06VLlz179oAl3JYtW06fPi0Wi9++fauvrz9mzBh6bvimP3d3d4TQrblrNpQAACAASURBVFu38vLyQJ7vex+/ZmVlZWZmwrdKbm4u/krJk7j8/PzCwsLly77tCNkX+u3qT5FINGPGDA6HAxfdgM4etyM89uzZU1dXFzRYBw8ehBrs3LkzKCgI1m23b9++detW2HB6+PDhrKys4uLiAQMGWFpaJiYmAg44T1BZdejQoWvXrqwQLGjcP/74Q0lJycvLC1v94xKxx9vbW1VVFTY9bNmyBUzp58yZ4+3tbW5urqSkNGbMmDlz5syePXvmzJkLFix48uQJRVHTp3sjhO7evUuvCCDTHHYRwqXXAQEBuHvg+hIP6wgQgsUmpM1hiRBG2IMHD3K53JUrV9apeiKRaPDgwdbW1qwQLBiOV69ejRDavXs3ts2qk0g48okTJ2BYB+rGWCnA1rjYAwRCJBKVlZWJxWIwD9+48dtxQY8ePVJSUnJ1da2K0OBCpT0wOsNNdnCyjnSc2oSUlZV5e3tzOBwdHR0DA4Nbt25VY3AKbfrkyRNtbW1dXd0+ffo4ODj06NEDDjgFfViXLl2srKw6duzYvn17sEszNjZu27atvr6+rq6ulpaWtra2pqYml8sdMmQInqiwqFAvR0dHLpfr7u4+ZsyYkSNHDh8+3MPD4+eff3Z3dx80aFD//v379u0LNklw0iYI0K3bt/Mzra2tO3fuDBehwD3cGF7IfPDgwVwu18bGpkePHnZ2dgyxO3To0K5dO1hXNTQ0hLWk1q1ba2lpaWpqwl9FRUW4Y6AarPCsNmjQoFatWuFTo3BNU1JS9PT0XFxc6BQTRH348GHLli3nzp2LI4MH5HdwcFBQUKjPptFPnz79+OOPampqSkpKbm5u8JbRKREMINbW1gYGBrm5uVAXLGd8fDyXy/Xw8MCHwMGJWRRF3bhxg8PhLF++nPGWgQk53D8YGxvLqFctH9ev+3b/FZjsJCQk9OnTR15eHggECIzzgceRI0dyOBxsn4Tlz8nJ6dSpk4WFBQgJryogHxcXZ2RkNHbsWNx8jDw7duzIFsGCEkEbBDonehPgcimK2rp1K4/Hu3TpEgCONwTA6bX6+vovXryoqKiArY5wPI1AIHB3d9fV1YVT4zE+UCicpN8kuwihjrm5ufb29kZGRmCbiMWj15r4WUSAECwWwWwWGizY1QKmJHQtQvX1hNevoqLC3d2dFYIFr258fHy7du1++OEHOJSvPu9zeHg4/m6G0QoGbpAcD+LYA/XFZ1YBwQoKCqIoCoxI8ME21SPD+BVmLLja7NKlS1hRJ10uHrUZPwEIhYWFoLuCsy1gyzpOwigU6hseHg73csAii5zEgY2UoqIirF+oqampq6traGgAFdPV1QV7L2NjYzMzMysrqzZt2sDGOkZbAIeALYSM5TPpR7xuJS8vTxdAXV1dRUWla9eu8CkPYuMpv2fPngihFi1ayMnJ0VOpqqqqS5ympiassoHYhoaGJiYmZmZm7dq1s7S07CJxw4cPB70jQ346YgAjqDxDQkLoP1EU9fnzZz09PWdnZ5jg4VdQ4MEVRgcPHsQyw6/Q6HCOFByDDlY+OGd6E+NGpAcCFHFxcb169QI84ZMDQwRZwaO/vz+PxwPjKlwERVEJCQlcLnf48OHS/BI+Y06ePMlQS8BOOlhahSsXpcWjywl+iPPNT4kpilq7Zi3+Rjp06BCXy126dCk2gaJLCI0SFRUlJycH7xr917y8PEywsBiQJCIiAiEEV3MC2pAQH+/OFsGChn737l2HDh26dOkCCrOq+lJCQgKco0avBdyyCkbuwKLgV8jkyZMn6urq06ZNg0dcTSi3yTVYYPIPXYiOM6OC5JEtBAjBYgvJb/nAS9WElz3D+xwTE6OkpGRvb1/VwCFdZ0jIIsGCDEEdPWfOHLiYWbrc2oewQrA2bdqUnJzctm1bMH+ufek4Jp1gwRTImCNxzKo8gExRUZGPjw9CqGXLlnAHLVCcqlLBxFlctSspKcEX3tE9sH0dDg6AwwWqElgsFsNNeUXVuqpEAAFKSkr4fL50x8OZM5LXUmx89x9MVNWghH/i8/lqamqmpqZY5wTIJycn6+rqwt572LcPgLx79w6M1kHTQ0dJmmBBVrisGj0AyPv37x0cHBBCZmZmjx49kuZJkG1sbKySkpKzszO+1QfC4+PjORwObGOE3gJovHr1ytDQsGfPnnCqJx18FgkWLGjCvUP4VHTpisOBBUZGRrq6uow7eXJyciwtLdu1awcVx+Tp69evgwYNatGixR9//MGgtjgOWwQLcNu+fTucpiatMKPXSCQSwSmjoAfFTLSoqMjT01NPTw84K70X/frrrwihsLAwRkWaA8Hi8/nTpk1TVVWNjo6W7nv0ihM/WwgQgsUWkt/ygaGtCQkWzApeXl4IoadPn9a+bjCCs0WwYBTLzMzs37+/oaEhHoZqL490TFYIVkhIyLp16zgcjvTWa+kSKw2hEyxYO4ChEw++kIr+CKuZEAIThlgsLigogM/Zn376qdle81wpAv+iwNDQUITQ8OHDgWeA5MnJyfr6+i4uLvigUYqicnJyJk6ciBfCGDSOTrBAK1wNwcI/0fsAvJhxcXFg/IfX7nFkjCqMISAMHIqLSUB8fDyYM+IlQoqiMjMzwaKr0l33LBKsiNMRr1+/RghNmjQJ93ksNvZAjcBEYdiwYXAqCvwKS4Tt27eH2R1qKhaLQf0G6isG8uwSLMg8Pz/fzc1NQUEBRkg6k8a1wJ4bN24ghNzc3PDR+RRFAcHS19fHIxvU+syZM1wud9SoUXCjDp3pQtFNq8GKi4tTUFCwtrZmqDlxZYmHdQQIwWIT0qYlWKC0//r1q7a2tpGRUW0MgXHlYYBgi2DBhHTv3j18tns1twtjGar31JNgwTfrqlWr7Ozs5OTkar94ypAKqgbnlE6cOHHbtm1rJW7NmjUBAQErVqxYtmwZnO20cOHCBQsWzJs3z8/PD1vC+vj4TJ8+3dvbe8KECebm5gihwMDA+qv3GEKSR0BAKBR6enoihMaPHw/aFIqiysrKLl++DCZEEC0lJQVOfJ09ezaEMHgPNHqPHj0QQtu2bTty5EhoaOiePXt27dq1Y8eOkJCQbdu2wfVEmzZtgvPJ4ISwlStXwiU/cLb4tGnT4HQG2KvBIBO41UDbB4cwBQYGYj1WYWFhSEgIfekwNTUVqJifnx984DEkZ4VgrVmzBiF0+PBhWEKFhciqhMd0EAy/xo4dCyfpQyeHY9VwTQsKClatWoUQGjp0KNic0UkJZAUEiBUNFqBx9uxZhBDeLMJADMsGHpFIBJcMjh07Fp9MVl5efvDgwWXLlqWnp+NoFy9e1NPTU1dXh3ucGPg0LcECJKH5goODGdo1RpXJI4sIEILFIphNrMGCdzgoKOjblp9937b81N6xS7DAMsnf35/L5UZGRrLywVRPgrVt2zYej9ezZ095efnx48fDlFl7fHBMUM4tWbJEWeJatmyppKQEf8EDJ0jBr0oSR/eDpZSSkpKqqqqysrKtrW1tPqNx6cRTVwQEAkH//v0RQqampocOHcrOzi4uLobrUPh8fmZm5oEDB9q1a8flcseOHQu0mzHHY3VRt27d5OXlYX8+T+K4XC545OTkFBQUFCUOOoB0l1BSUlJXV1dWVh43bhxsMpAuiF679PT0jh07IoRcXV3v3r1bVFQE2zXgIr/8/PyTJ09aWlpyOJxp06Zh/Sg9B2CTFEXV0wZr9apv+1TmzZtnb29vbW0Nt1hWr/gBHZWzszMcORsREZGfn8/n80GbC5c+RUdHgz6vb9++YO8vDQi7Gqzy8nKRSDRlyhQejwfLkTXWAnjhwIEDEUI2NjZ//PEH3PcFOAuFwrKystTU1KVLl8LrHx4eXil9aXKClZeXZ2ZmZmRklJeXR9etMjoMeWQXAUKw2MQTBoimWiIUCr/dfmVra6usrIw/tmpZPXYJFkVRb9++VVdXd3JyEggEMKrWUpKqotWTYG3duhUOAedyuXAJV1UF1RguEom+fPkSHx+Pd+nDXn36X9i3n5yc/Pm7gz389J38KSkpmZmZ0pNKjQKQCLVHADr2mjVr4BBtOTm53r17jxw50tPTs1evXi1atEAIwe09lWqA4NNfLBbz+fwTJ07s2rXr0KFDcL0P/VaiixcvPn36FO5uoncDuh+6xMePH/Py8mopf2lp6dixY1u1aoUQMjY2HjNmjJ+f39y5cwcPHgyBbdq0gas5wQxIOltWNFirV63mcDi2trYIobVr11ZKIKSLBuSDg4P19PTg/vhhw4bNnj17/vz5o0aNguNC1NXV58yZA0fJVPoisEuwKIqKiYlp3bq1p6cnrF1WWiijLlCRX3/9VUNDAyFkYWHh6+u7devWPXv2rFmzZtCgQYqKinJycvb29rBvplLS1oQEC6pz+fJlhND06dNZ+dxlQEQeq0KAEKyqkJElvAkJFrzVcHnwnDlzQNFS+zqwRbDwRuuQ7SFcLnfdunVsrX9hgiV9TAOeBekeqDvsIhSLxVu3boVLiAcOHEi3C6k9RA0RE2BviJxJnhgBeCsLCgqCgoIcHBzatGkDJ1bo6en17t17y5YtsIYFUyBO1XCe2jc6xHz37t2kSZPat2+vqakJR5tqa2vb2NisW7cOro6pdEYH+VkhWIFrA2HnY4cOHR4/flxLgoVtUjMzM5csWdKtWzctLa1WrVqpq6tra2ubm5t7e3uD8Xg1LIctgjVlyhQYHGC5Ewz2q8GN0fog4fv37+GyKXV1dSUlJUVFRRUVldatW9vb2+ObN6uqS9MSLD6fP3DgwFatWn348IGVz10GPuSxKgQIwaoKGVnC4e1qEg0WfMKOGDECIYRPoKl9HdglWCkpKd26dWvfvn16ejpb73NYWBiHw7lw4ULtK0WPGRISoqamhhCKioqih8vmxysy9ffIJgBJVScE6CcyCIVCuBcSE51adlHIpKq/te8JdZKcTgKys7NBDYa/EGqUnE6wXr16BfFBVKgIMBi4wxhuz8S2/98iiEVisRiOaUAIgY0a44iK6qtDj1xQUPD58+ekpCS6Do9eQems2CJYcKlzYmLiDz/8YGZmBiNk9UUzhKFHTkpKevXq1bNnz2JjY/H1iFUpESGfpiVYL168aNGihYODQ+3JMaP65FE2BAjBkg23ylM1FcGCcp8/f66trW1vb4/H38qlrCyULYI1btw4iqIuXLiAr82R2dqJIWZERASPxwsJCUlISPjnn39iY2P/+eefN2/evH79+tWrV39L3MuXL1+8ePH06dNnEvf4m3t07979169f+/v7w/W6oK7AkyujFPL4n4oATNUVFRUwETIem3OtQWVbUVGBOy2cL1r9jA41ohOs+Ph42aoZHBwMa3xnzpyBA9vqlA+GGmt3pGtUVYZsEaxffvmFoqhDhw7hVc7aoMeQCsSmMy3Q0lVUVOCqMZLgxyYkWEKh0M/Pj8fjnT17FuOJBSOeBkWAECw24W0qggVv79y5c+FCDBmqxBbBmjRpklAo9PDwUFFRuXPnDnwTyyAPI4lIJPrtt98QQnJycnADsby8PJy0CUbHtby5edeuXYzxkVEQefyPRwCrmv51NcWSY6ZVYxWAYA0bNozH4129ejUxMfGjxCUkJMTHx3/48CE+Pv79dxcXF/fu3btYiXv79i18vbx+/RoOPff09CwsLAQZaiy30ggyyI8JQT13EXp7ewsEgmHDhqmqql67dk0GmkivESgCsQqQ/lNV/iYkWF++fIH7MauyMqxKZhJefwQIwao/hv+XQ5MQLPicLSgoMDc319XVrfGcnv8Tl+Zji2BNnTr1/fv3CKG+ffvi7Ve0cmT0CgSCX3/91cHBoU+fPr2/O0dHxz59+jhJnLPEubi4uLq6uklcP4kbMGDAQIn7+eefHR0dpS9OkVEgkowg8G9AAAhW7969EUKqqqoaGhpwWbWysnLLli3hW0VOTo7H48F90tKn9uMQuGaKLYV0LcFji2AtWLDgzp078vLykydPxirMWsrASrQmJFgHDhyATaBsfe6yAsh/SSaEYLHZ0E1CsMCeHa6C2b17t2wamvoTrE+fPiGExo4du379ekVFxdDQUHxRGpsQk7wIAgSBWiMAo4G/v7+jo+OAAQP69+//008/wSfHoEGDBkvczz//PFTiPDw8RkjcyJEjR40aNVrixo0bN2LECF9f35cvX2K6U+vy6xsRlyizBuvPP/9ECPn4+CxdulReXv7EiRP1VF/JVqUmIViwwahLly4aGhr5+fn10T7KVmuSihAsNvtAkxAsoVDI5/NdXV0VFBRkPoCALYLVu3dvY2Pjjh07wn0ptV/LqLEZysvL8X0vMnuggWosi0QgCPzHIMDiO9j4mNSfYN24cYPL5f7444/m5uYODg4sbrupExpNRbCio6PhHFdyN06d2outyIRgsYXkt3wan2DBF+qVK1c0NDQmT54Mhz7LMKTWn2AlJiaCJSyPx5s5cyY5bYXNjkXyIgjUA4GKigo4W7X6v4JqXZN8nLBCsOTl5ZWUlBBCcP80fbtAPUCtW9ImIVhisdjT01NRUfHRo0cyTAp1qyGJXRkChGBVhoqsYU1FsCZNmoQQgiNqZJOdQbA+f/4MinTG/m28Rx32csMjxIElQoSQoaFhXFwcBMomDElFECAIsIsANjBnN9uGzo1BsLKysiCEPv7g0yXwcASDj0AgEIlE169fl5eX53A4NjY2TbLKCRA1PsGiKOr9+/fq6uo2NjZwBlhDNxbJXxoBQrCkMZE9pJEJFpwxExcXZ2RkZG1tLZt5O9SWTrBsbGzS0tLqikJGRgZCiMPhODo6snW4aF1lIPEJAgSB/yQE6ASrW7ducF9hnSoYHR2toKCAEJoxY0YTLpM1CcGCW7QPHjxIP42sTuiRyPVEgBCsegL4/5I3MsEC8/aAgACEUERExP8TpY4PQLCEQuHQoUOtra0/ffpUXl5eKHEFBQX5+fk5OTlZEvf169cvX76kpaXBrS/JycmfPn1KTEy8d++evLy8pqZmREQE2a5SR/hJdIIAQaASBDDB6ty5c9euXdPT0wUCQXFxcWFhYb7E5ebmZmdnZ2ZmZmRkfPnyJT09PSUlBY5jTUhI+Pz5c3h4uIKCgoGBwYULF5rEvB1q1fgEKz8/39zcXElJCa6ArARcEtTwCBCCxSbGjUmwYOjJzs62tbU1MDD4+PFjfTaJgOTFxcX9+vWDQxYGDx7cv39/Nzc3FxcXZ2dnR0dHBweHXr169ejR48cff7Szs+vatauNjU3nzp07SZyJiQlCqEOHDk34mchmW5K8CAIEgaZGAC8IGhkZIYT69+8/aNCgfv369e3b19XV1cnJqU+fPg4ODvb29j179qSPS126dLG0tLS2tjYwMEAIDRkyBFYVm6pCjUmwYCI4ceIEQsjb25usDzZVo1MURQgWm+A3JsEC9dWxY8cUFRXXrl1bz/NpQPLU1NQFCxb069evd+/ePXr0sLe3d/juHCXOycnJ2dnZxcWlr8ThXd+DBg36+eefx44du337dqK+YrNLkbwIAv/FCABXyM7Onjp1at++fe3t7Xv06NGzZ08Ymnr37t2nTx9HR0cnJycXiXN1de3Xr99PEjdw4MBBgwZ5eHiMHTsW7goEltMkcDYywaIoqnfv3jweLzExkZi3N0mLQ6GEYLEJfqMRLDhKmM/nw+WDT548wXsY61Of4uLi1NTU9PT0rKysbInL+e5yJS4vLw808wUSV1RUVCxxJSUlpaWlxcXFgEB9ZCBpCQIEAYIAHYHi4mJY/sPjUnZ2NoxMMC7l5uYyxqWioiIYlGBckuFiHLoA9fc3GsECS/8nT54oKioOGjSo/pKTHOqDACFY9UGPmbbRCBbsNL59+7aWltaoUaMKCgpYIVjM+pBnggBBgCBAEKg3Ao1GsKCgqVOncjicu3fvEvVVvZuuXhkQglUv+BiJG5NgURS1YMEChNDFixfZYlf0O7bwiQy199THCIyBJHkkCBAECAIYgdqPQpXGbHKe0WgESyQSpaSkGBgYGBoa8vl8DCDxNAkChGCxCXvjECy4vD0hIaFDhw42NjZwZhVZm2OzIUleBAGCAEGAPQQah2BBKcHBwXJycqGhoU1OK9nD79+aEyFYbLZc4xAsuMB1y5YtHA5n165d5IwTNpuQ5EUQIAgQBNhGoBEIFuy45PP5tra2CKG8vDy2K0HyqzMChGDVGbJqEjQCwQIFeF5eXt++fVVUVF68eMHW+mA19SI/EQQIAgQBgoDMCDQCwYKpISoqqkWLFj4+PkR9JXNjsZiQECwWwWyMuwjhdIazZ88qKSn5+fmBNou8S2y2IsmLIEAQIAiwikAjECwowt3dHSH09u1bVsUnmcmIACFYMgJXabKG1mCBFXl5efmUKVMQQjdv3iSnelbaECSQIEAQIAg0HwQammDB/qSXL19qaGg4OjqWlpY2n7r/N0tCCBabrd/QBAve0sePH+vq6g4YMCAnJ4esD7LZfiQvggBBgCDQAAg0NMGC/BcsWMDhcK5evUrWNBqgDWXJkhAsWVCrKk1DEyyhUEhR1Lp16xBC+/btoygK3quq5CHhBAGCAEGAINDkCDQowYJ9ThkZGZaWlhoaGtnZ2U1eXyIAIEAIFps9oUEJFrCr5OTkbt262djYJCYmkvVBNhuP5EUQIAgQBBoGgQYlWHDu9O7du3k83pYtW4j6qmHaUJZcCcGSBbWq0jQowQLz9iNHjnA4nDVr1pDFwapagYQTBAgCBIFmhUDDESx896uLiwtCCD68CcdqJq1PCBabDdFwBAvUVyUlJcOHD1dSUrp//z5RX7HZciQvggBBgCDQYAg0HMGCnG/evKmiojJ27FhyenuDtaEsGROCJQtqVaVpOIIF6qubN2/KyclNnDgRHslnSlUNQcIJAgQBgkDzQaDhCJZAIKAoysvLCyH08OHD5lNlIglFUYRgsdkNGohg4Tv+Fi5ciBA6duwYMW9ns9lIXgQBggBBoCERaCCCBSsb79+/NzExsbKyysrKoiiKfHg3ZEvWLW9CsOqGV/WxG4hgwVv0zz//mJiYuLq6wiYRKKt6ecivBAGCAEGAINDkCDQQwYKljMDAQIRQeHh4k1eTCMBAgBAsBiD1emxQgrVz506EUFBQEFFf1auRSGKCAEGAINC4CDQEwYKVjbzcvB49eigpKX369Imorxq3VWsujRCsmjGqfYyGIFiQZ2ZmpouLi76+/rNnz4h5e+1bhMQkCBAECAJNjkBDECzI88SJE1wud/HixWCMRdYHm7yt6QIQgkVHo77+hiBY8BZFRUUhhObPnw/sirxF9W0qkp4gQBAgCDQWAg1EsEQi0YgRI+Tk5ODDm8wLjdWetS2HEKzaIlWbeKwTLNhzC5cPKioqRkVFicViWHevjTwkDkGAIEAQIAg0OQKsEyy4bTAmJkZLS2vMmDEFBQVkfbDJW1laAEKwpDGRPYR1ggVv0ePHj9XV1T08PAQCAd5RKLuUJCVBgCBAECAINCICDUSw5s2bhxC6ePFiI1aFFFUHBAjBqgNYNUZlnWAJBAKRSBQUFMThcLZu3UpRFFFf1dgKJAJBgCBAEGhWCLBOsEQi0efPnzt16tShQ4eUlBRysUezam4sDCFYGAoWPKwTLIqiPn78aG1t3aVLl48fP4rFYiiCBVlJFgQBggBBgCDQKAiwTrAoitq+fTtC6LfffgPTK2KA1SgtWbdCCMGqG17Vx2adYInF4pMnTyKE/P39KYqCfSLVy0B+JQgQBAgCBIFmhQCLBOv8+fMURRUWFg4YMKBVq1YvX74k6qtm1dZ0YQjBoqNRXz+LBMvDw4OiqMzMzCFDhmhqav75559isRje0vpKSdITBAgCBAGCQCMiwCLBioyMFIvF586dU1ZWnjt3Lp/PJ4a5jdiSdSuKEKy64VV9bBYJ1rBhw8Ri8fXr1xFCEydOJN8o1SNPfiUIEAQIAs0WARYJ1tmzZymK+uWXXxBC169fJ1NDs210chchy03DIsEaNWoURVGzZs1SUlI6efKkSCQi5u0stxbJjiBAECAINAoCLBKsmzdvpqent23b1tXV9evXr4RgNUoDylgI0WDJCFylyVgkWF5eXnl5ebq6ug4ODjk5OSKRiNgwVoo5CSQIEAQIAs0cAVYI1ty5cxFCd+7c2bVrF758kKwPNuemJwSLzdZhi2BxOBxPT8/IyEiEUEBAAEVRZWVlbApK8iIIEAQIAgSBxkKAFYI1e/ZsHo8XGhrq7u5uamr67t07sVgsFAobqxKknDojQAhWnSGrJgErBOvRo0cKCgr29vZDhgzR1dWNjY0ViUTkLaoGdvITQYAgQBBozgiwQrBmzZolJyc3atQoJSWlFStWEN1Vc25xkI0QLDbbiC2Cpays3KpVKwUFhTFjxpDDRdlsIZIXQYAgQBBodARYIViwRKilpaWurn7t2jW4l7bRq0IKrAMChGDVAawao1ZDsOCMUPjmEEkc+CEcTKwg5OHDhyoqKgghfX3969evi0QieDlrLJ1EIAgQBAgCBIFmiEA1BAtPBNKzA8wUeHbw9/dHEjdt2rT8/HyI3wwrS0TCCBCChaFgwQMEKyMjo1OnTl5eXmVlZRUVFQKBoELiysvLaZ5y4XcHgfBUUVHx4MEDVVVVhFDfvn2FQiFhVyw0DMmCIEAQIAg0HQJ0gvX06dPy7w4Gf/xXMuB/mybwdECfHfz8/IBgHTlyhKIoMjU0XXvWtmRCsGqLVG3iYQ2WpaXltGnTapNEOs6LFy/k5eU1NDT27NlD1gel8SEhBAGCAEHg34UAkCFfX1+EUFxcnGzCL1myhMvlurq6Jicnk9MZZMOwkVMRgsUm4JhgOTg4dOnSZfXq1StXrly+fPmyZcuWLl26aNGihQsXzp8/39/ff86cObNmzfL19Z0xY8b06dOnTJkyefLkiRI3YMAADodjZ2dXVFRUUVFBTmdgs4VIXgQBxdeUEQAAIABJREFUggBBoNERAII1b948RUVFHx+ftWvXrlixYpnELVmyZOHChQsWLJg3b56fnx9MDT4+PjNmzJg2bdqUKVMmTZo0YcIELy+vzp07I4R27NhBrK8avQFlLJAQLBmBqzQZnAV66NAhUOTK/FdOTm7KlCnk8sFKQSaBBAGCAEHg34UATA0//vijzJMCJDQzM7t37x65Nu3f0vqEYLHZUqDBSkhIuHTp0pXv7up3d13ibkjcnxL3119/3b59+86dO9HR0Xfv3r1//35MTMyDBw/u37+flJREbBjZbBuSF0GAIEAQaCIEYGq4f//+92nh23+YGa5du4anhps3b/7555+3bt2iTw337t27f//+gwcP7t279+bNm9LSUoqiyMpGE7Vk3YolBKtueJHYBAGCAEGAIEAQIAgQBGpEgBCsGiGqcwQ4Xff7HkEZ/8MXT53LJgkIAgQBggBBoFkiAEdGyzglSJKRO9OaZcNWKRQhWFVCQ34gCBAECAIEAYIAQYAgIBsChGDJhhtJRRAgCBAECAIEAYIAQaBKBAjBqhIa8gNBgCBAECAIEAQIAgQB2RAgBEs23EgqggBBgCBAECAIEAQIAlUiQAhWldCQHwgCBAGCAEGAIEAQIAjIhgAhWLLhRlIRBAgCBAGCAEGAIEAQqBIBQrCqhIb8QBAgCBAECAIEAYIAQUA2BAjBkg03koogQBAgCBAECAIEAYJAlQgQglUlNOQHggBBgCBAECAIEAQIArIhQAiWbLiRVAQBggBBgCBAECAIEASqRIAQrCqhIT8QBAgCBAGCAEGAIEAQkA0BQrBkw42kIggQBAgCBAGCAEGAIFAlAoRgVQkN+YEgQBAgCBAECAIEAYKAbAgQgiUbbiQVQYAgQBAgCBAECAIEgSoRIASrSmjIDwQBggBBgCBAECAIEARkQ6BRCZZYLJZNyqZKJRKJGkdmscTRqykdQv+V+KtBQCwWN1rDVSNGfX5qhNZvnI5dVxAatOL0KtP9dRWyIeLXVZ4GBaohKgh5yiy2zAkbri5s5dygVfsPGAzZwrlJ8pGdYIlEoopaOFwrkUhEUVRdxxGcvHoPdKN/y7QqFAoBjUoBaSCIqgewPr+KxWKhUNhMxMbAVoptfapZTVqRxFUToal+oo/duIGwp6mkqqrcBhUMMqePQvSuUpVIjRAuQ61lSFJjRcRiMX04F353OLCe73hDyFxjpeofQSQSCYXC+ufT+DkwZsN/aS0aHzd2S5SdYLErh8y5Sc9t9RwI6JIkJSUJBAJ6SMP5GcM9UEZWimO8afXMkyFnPXNriORFRUUpKSkNkXNj5skizhUVFXh6Ky8vh1qw2MHYhYXFirMrWEPnhtuolgU12xasXn6Z25fejasvotn+WlXdqwqXuSI4w8zMzLi4uMLCQpmzIgnrg4AsBAsa79WrV6GhoUeOHDl9+nRERMSJEyf2798fGhq6b9++Q4cOHT9+/OzZs2fOnAGCIhaLnz17tmrVqrCwMBbfEzwkZWdnx8fHp6enl5WV1QcOelqxWOzm5hYeHk4PrL9fLBY/f/78y5cvkFVsbOz69evXrFkTGhpKUVRFRcWjR49Wr14dEBBw7tw5CKl/oQ2aQ1lZ2ZMnT/Ly8hq0lOozB1WNWCy+fv36ggULvLy8Nm7cmJ2d3Th6rDdv3iQmJuLeWL2oVf0Kn5hv3rzZtGnTsmXLIiMj69OZQZjMzMwzZ86kpaVRFPX58+fIyMiCgoLXr1+vW7fu119/jYqKwnyrKqkaLbysrCw0NHTFihUbN24EflxPPBmSx8XFBQcH37t3j6KoFy9ebNmy5fHjxxRFNeGXPRT95MmT4ODgFStWXLhwoTbNIRKJYmJiVq1aFRAQEBUVxdYQERcXd+DAgaNHj0ZERISHhx8+fDgsLOzYsWP79+/fvXt3WFjYtWvXsrKyZHuhYNBbu3btqlWrjh8/XkvYoQOkpKTs3Llz1apVW7dubXyi8Pnz5+fPnzP6EluPsbGxQUFBq1evhsEfsyJW8o+Li9u0adP8+fMDAgL2799/6dIl2dqOFWH+azORhWDBKLB///4uXbqoq6uj787U1NTS0rJdu3Y6Ojrfw1B8fDxFUcXFxR06dEAItWjRAsY1tjpTamrq77//PmnSpHHjxv38888zZ848ffp0QUFBPTuTWCx+8eIFQqhz5871zAr3LajyvXv32rRpM2TIEIAxOjp6xIgRCCFbW1uKovh8/rlz53r37o0QmjBhAkVR9VShicXipKQkFgem2NhYxjRw4sQJdXX1efPm1XLcxICw6IG56s8//1RSUvL19QVIz58/z9b0U6mo0KBxcXFdunT58ccfMzIy6tNV4MMjOjq6f//+CKFhw4bVpxvDis+uXbsGDBiQnZ0tEAgCAgI8PT1FItHNmzcdHBwQQuPHjy8pKamPzJXCIltgSUnJ3LlzVVVV5eTkHjx4QFEUW0MEyBMaGooQMjIyEgqF69evRwh17969XOLYZXK1rz60eFRU1MCBAxFCw4cPLyoqqrE5ysvLIyIievTogRDy9vamKKo+RBykFYvFN27c+PnnnzU1NRFCLVu29PT0HDdu3Pjx44cOHdq7d28Oh4MQgkma8frXpr4ikejKlSvQsd3d3WspM3SA169fz5kzByGko6MD36WN1l5isdje3l5HR+f9+/esFPru3Tt6Y927d2/UqFEIIWtra7YGT/jUjIuLs7W17d69e2ho6N9//z148GCE0OvXr/+lWs/a9LHmGUcWggU1gY4SHh5uYGCgqKg4ePBgmGCKiori4+MvXLjQs2dPBQWFa9euAcGys7ODl+Tly5f0hW1ocryMVSeDnqKiIhibFi9efOzYMXgPEUKLFi2iF1FX6OHFnj59OkKIx+PFxsZWmgPDwKXGNxDi3759u127dgMGDKAzJ21tbTc3N8wGsrOzFRUV586diwkWvSwQRro4/PLQfxIIBJMnT3727Blj4KZnSI8P0XBz4IpD5qmpqaNGjYIPWfxTWFiYgYHBzJkzGWMEJGFkzhCeLgbOsEYPIxUUJBQKvby81NXVs7KyXr165evr+/bt2+rnaelq1lg0XpLGHfXVq1fdu3e3s7NLT0/HINNrTfcz8qdXBCgRzD1dunQZNWoUg/1UgycjW+jA79698/Hx+fvvvymKevz48ezZs2NjY0GY/Px8c3PzSZMmwVtcDZWpqlC65FA6o5r0CIyfMEqQkN4K69ev19LSevHiBb3hqpKB0VfppdCTQHhiYuLYsWNdXV3T0tLevn07cuTI4cOHg6qsmupLV41eCgjACKG3BR0Eejj2w4eBWCw2NzcfN24cvcUZ2eKsIDw1NVVBQWHRokUMskKvOC6l9p4DBw4oKiouWbKEkeT69eu2trYHDx6kKAoTLCwSIzJ+xC0LMovFYmVlZfzdyKggTkX34DjOzs5mZmaZmZn0jkGPif31RADnA43bq1cvCwuLp0+f4k6LRcIh9CSV+kGkL1++jB49GtTJ9Gg6OjouLi4weNIzp8epsafhyNCjRowYoaSkVFxcDOGzZ89u06bNjRs3GOjRWxCPbJCEIQk9JiMCIyb8ipseHmvzV7oIXFBVuVVaND0f6Qgsdo/aVEp2glVeXi4Wi8PCwrS1tRFCP/30E6zI4FJfvnzZsmXL/fv3Q8jTp0/nzJlz5MgR3MZisbi8vByPbiKRiM45pKHBOWPPmcgzCCF1dfXIyEh4801NTXk8nq6uLrAinDlOUhuPWCwuLS3t1KmTi4uLnJzcggULsMyQXCwWCwQCMPYCY394PSoqKqrPH+T566+/YDKGbPPz801NTR0dHaEKQqHw1atXCgoKM2bMAJ0WLgsyr6ioAPAxRGCQCr/ivggMQCQSaWlp3b17VyBx8KLiDEF4nBzmeJwtlALZQtOcO3fOzs4uNTVVIBBAZaHVrl+/np+fj+sOLQuP4Ic8sSEtvMzwBS8SiegF4Uwq9QiFQjy+V1RUwGgCueXk5Li4uDg6OkoPYdJZlZeXQ1rAHFdZOiYOoW9NwH5o0JiYGNDUQm7Qq+ndm97PIUNca3o0YNjp6enW1tYeHh4wPoJsFRUVuDPXCBfEvHbtmoeHBxS3b9++WbNmgV8oFCYmJlpYWHh5eZWUlEArYDBxfUFCnAT3bXrnx7/iKkMHA3ihB4KVNEYbt6BQKKQXCtmuW7dORUUFFmWgWXFzQ3wsHmCFGw5qgUvH0UBsiFZWVjZ27FgYppKSknx8fHJycmDAxfEZHnjXAE+oCG4v3LFxU9LTQmQIoVcTx8FvMZ/Pz87KNjY29vT0BIIlEAjoHYYBOHw6Pnv2jMfjwTcYsGSQB/LHZB0XV6NHKBQKBIJdu3YhhGbNmlVWVlZUVATgQ/XnzZsHfA5GD9wuVdUOdxiIIBQKk5OT1dXVPT09hUIh1JRezaokLC0tFQgEjo6OpqamaWlpUC4dXpwQtwiE4EGyvLwcRj+BQADDIw7BQuJM6J6KiorExMSYmBjoV9B1IQc8ZtamCjB4Xrp0qVu3bmDXi4EtLi42NTXt3bu3SCSCdpTGE78y0OfxOEAXFfuh85iamjo7O/P5/JKSkrKysvj4+NOnT9Mrizst7s+QA967ht8sKJ1u2VxeXg7DEWRCHyhAQvyy1zhSQaGMCuLkDNnoUEMHwPMIgIans0pHHhicMXq1lA0DK5tHdoIlEAjEYvGxY8eAYPXr1y81NbWsrOzFixc7duyANSljY2NQYtOFg96G24+iqLy8PEzOxGJxfHw8vH70VJX6T506BWuRc+fOLSkpSU9P19HRkZOTMzAweP36NYMVVZqDdCDw30uXLo0fP/7evXsIIRMTk9o3Br1e0plXGlJaWmpoaIgJFkVRsbGxLVq08PHxqTQ+IxCXWFBQIM0tDh48yOPxkpKSGKmkH3E+FEXRdVT0D4Lhw4fDUqZ0chxCzweUmjA84Qgye/CLV1BQgBc96czDzc1t2LBhkD+8e5WWhSWED+JK4zAC8WuZkZGBU9EHLEb86vsezo2e6suXL1CpL1++dOnShU6wcMWx6R6uAj0H7If8o6OjdXR0Fi1alJycfObMGWNj41WrVsXFxVEUlZSUZGFhMWbMGJyE4cElCgQC/G5WX19GDpU+VlrxlJQUTBECAwMxwcLFVVRU5OTkQIY4B+xJS0vD5B5/XQiFwpSUFBhqcOCNGzewquD48ePDhw+HMbdSURmBGBBGeKWPOHJubi7WIuBAhqIXcjAxMfH09CwtLa00Q3ogVPzvv//m8Xh+fn6gwaL3h9TUVIhPD6TnUI0fCNbMmTPLy8v5fH5ubu79+/fj4uKEQuGWLVtAswWNRVFUYWEhHqjpZeGaZmdnw6InlJienq6iouLp6ckQADclIxweoThHR0djY2PM6uAnXBBjhMHDDj1CpZmzFVhNFTAyo0ePtrKyYpRYXl5uaGjo4ODACMcZ4ioUFBTQ+zkjPuPRxMSkmrcbZ05P9eXLF3pj0X+qvR9L++XLF+yvPjl+zQsLC/FrDkmwnBkZGRhG7Kk+W+lfsTx07YZ0NHZDZCdY0NexBmvw4MF8Pp+iqCVLlvB4vOzsbJFItH///vDwcJFItGfPnp9++snd3X3UqFGfPn2COpSWlu7Zs2f8eK9hw4YNHDhw0qRJ69evnzRpkp2d3fz582EuwRBXWu2C/IKAgIDVq1dDI125cgX41tChQ2VeIoRm6N+//82bN8vLy42MjDgczu3bt4FnQOumpaXNnDlz+vTp27Zte/PmTVBQ0KJFi2bPnn3r1i1ILt0JIOTz589+fn4+Pj5LliyB0Z+iqNLSUiMjI2mCBdz0xYsX06dPnzFjRnBw8NevXymKOnTo0HSJ++uvvwCWuLi4gICA4ODgzZs3L1u2bNq0adnZ2RkZGT4+PoqKinJycuPHj58/f/6aNWsqKipSU1MnT57s7e29bdu29+/f+/j4LFy48PHjx2KxuKioKDIyctu2bYGBgePGjdu9ezdmw3/88QcYNqmqqvr5+S1atOjo0aNisfjNmzeTJ0/29fXdtGkTfL6A1u23335bvHjxmjVrfHx8jh07BnqC7OzsrVu3ent7//rrr8+fPz98+PCSJUt8fX2PHDmC0ai0ofEA+uzZMz8/v/Xr1y9cuNDf3x/T6GfPnk2fPt3Y2NjMzMzPz2/jxo3JycmVshxoiIiIiB07dqxdu3bcuHHbt28vLS2VbjKGJNHR0cuXLw8JCQHT0fXr16enpxcVFa1cudLb23vu3Lnw3m7evHn69One3t7v37/fs2fPmjVrJk2adODAAdgEgEtJSkpatWqVv7//woULd+3adejQoeHDh48fPz4hIaGgoMDKygoTLHjRIiMj/fz81q5d6+3tvXfvXoZsjEd4a5KSkgwNDRFCV69eTUlJUVVVRQhduXKFoqjExEQTE5M5c+ZcvXp1+/btvr6+a9as+fDhA+QDQqakpCxZsmT16tXLli3z8fG5fv064Pn169dZs2Z5e3uvWLEiMTGRoqg//vhj6tSpPj4+YWFhkDlgsmvXrnv37q1du9bPz+/+/fvwkr5+/XqRxM38H/a+OyqqZHm4B5ghKQoCShZREVHAtKJIMCBGFLNgFhARzKDoKuoqZlEUxEVUJOeMgiTJGUmCiOQMIjnMMHO/s9Y5/d0zIOvqvvf2vR/zx5y+fburq6urq6urqvuamvr7+1+7du3QoUMbNmzo7e0dHBwkK1gEQdTV1dnb2z98+PD48eMHDhx4+/YtGUMfHx9ra+sHDx7Y2NicOHHCzs4OVghbW1sbG5t79+5dvHjx5MmTXl5egNXixYvfvn0LB260tLSSk5OxHYKNgJiTPTw8DA0NjYyMsrOzQ0NDHzx4cPTo0WvXrlVXVxME8fDhw+vXr+/evdvR0RGGCe9G4uPjjx49ev369RMnTlhaWpaVlWEfEJD3zZs3EIBsbW3t6+MrNklsx44dgP+NGzcMDQ0PHToE/a2pqTE0NDQ1NbWwsMCWY7KCBWoZg8F49OjRsWPHrl69eujQIYh/x/gM7eCwOQ4ODgghS0tLeNvS0rJv3z7gmffv30MQD0EQmZmZDx48uHfvnpGR0cmTJ7H5FuuO4eHhhw4dOnv27Pnz569cuQKu2Lq6unHjxunr6xcWFv7666+nT58+f/78p0+fhp2nGD1QsJYsWSInJ/f06dNbt25dvHgRmIetIovFsre3P3369OXLl01NTR8/fgxnO379+rtz587169dLS0ubmprs7e1v3rxpbm7u4uIC1MMTE9qFx7t37x48eNDY2LigoIAgiIiIiH379h09etTHxyctLe3UqVMwgrB9HXapAsNPZGTk9u3bEUL8/PxmZmYWFhbOzs6wUtDpdBkZGS0trY8fP165cuXMmTOnTp3CYg3QKCkpOXv2rLW19alTpywtLUGysSGMSREXF2dlZTVmzBgpKanTp09bWlqamZnt2rXL2NgY2AkqDpU/enp6+/fvT0lJefTo0aFDh4yMjMBTHxsba2Zmdvjw4fv370NUqJWVlfHXX0NDA8xfDw8PPF5hYWGmpqY2NjampqZ2dnb9/f0j7HWBCGlp6ebm5qdPn7506ZKVlRV0H165urqeOHHi0qVL5ubm169fh+1Kd3f306dP9+/ff/Hixbi4uN9//93Y2NjLy6usrOz69etGRkb3799PTEy8fv26ubl5bGwsVgbc3d1BihoaGj5//hwvKxj5vz3xNyhYEyZMQAjNmTMHThHOmjVLXFwcRgLQZTKZAQEBEJXJx8cH9v+Ojo5Lly4hhGRkZOzt7a9fvw6LgZqampyc3Pjx4/Py8vB0/dNuOzo6WlpaysvLCwgInD9/vqKigmzY+NPq5AIsFqupqWnGjBktLS0MBsPCwoJCocBuAGYLi8VqbGw0MzMbP378mDFjDAwMrK2tf//997Vr18rIyICfG2vlGDKwdU1NzZUrVxBCPDw8ePM3soJVXFxsaGjIyck5ffp0EEbe3t6qqqoIoatXrxIE0dPTo6Ojc/jw4YiIiPj4+Pv37yOEioqKmpubbWxs1qxZw8nJeejQocuXL9va2jIYjIaGhgMHDtBoNB4eHisrq507d0JINUEQzs7OEydOPHv2bGRk5LFjx4SEhK5fvw568+vXr2/evCkmJiYgIHDhwoWrV6/6+vqyWKySkhJTU1Mcpwn2yC1btsyfP9/Z2Tk2NtbGxkZZWdnGxoYgiNbWVicnpylTpiCEtLS0LC0tnZycjI2Nx48f7+joCEQbKjgwDaOiopSUlIyMjCIjI319fXfu3Llw4cKMjAyCIPLy8iwsLOTk5CZPnmxlZWVvbw/7eDapB+tNWFiYsLDwmTNnoqKirly5wsfHB64WtsLQLszz/Pz81atXnzx5MiEhITo6esuWLbNnzy4oKOju7ra1teXn50cIFRcXEwTh6OiooqKCEFq2bNmRI0eePn165coVfn5+c3NzvGzX1taqq6vPnTvXwcHhzJkzPDw8Ojo6cFYjOzu7p6dn5syZWMGChVxQUNDCwiImJsbOzm7s2LF37twZgVAY87i4uCdPnoBeHh4e/uzZM0iXl5crKyvPnj3bxMQENBg5OblFixbhY2Lv37/X1NRct26dv79/eHi4hYXFjBkz4Fhra2vrqVOnBAUF+fn5gfgxMTFr1qxBCO3cuRNUGVtbWyEhIQEBgdWrV0Nk5C+//NLV1VVaWjp37txly5Y9e/bM0tJSSEhowYIFvr6+ampqfX19WMGCkMEvX74YGhrKysrCOeXly5dLSkrGxMSAWHz16pWmpuatW7eSk5PDw8PV1NSWLl3a3d39+PFjDQ0NT0/P5ORkT0/PqVOnQnRgfX09hBARBFFWVga64LAjjqnHZDIDAwOXLl2KEJo/f/6xY8eePn1qZ2c3btw4XV3dkydPXr161dXV9ciRIwghOAgGDObr66uoqHjs2LGYmBhPT88NGzYsXbqUHBH44MGDGTNmHD58+OXLl/b29uvXr+fg4Ni3bx+YMO3t7efPn48QsrOzIwiivr7+2LFjCCEqldrV1QXThKxg0en0jo6Oo0ePysjI3LlzJyEh4eDBgxMnTgwPD/+rS8jjx48RQhoaGra2ttevX9+6dev8+fPT0tLwHGQymZ8+fVq4cOHSpUsjIiKcnJymTp2qqqoKqz5Mlvv3HwgJCR0+fDgsLMzLy4uDg0NXV5cgiNraWiEhIXl5eXNz80ePHt27d09GRkZXV7epqQlbGXFDOAEK1tKlS8eMGbNhwwY7OzsPD4/Tp09LSUnZ2triYgMDAwcOHFBSUnJ0dIyNjbW1tZ0zZ87FixcbGxtv376trq6OEJo9e3ZRUVFHRwcEBK9bt87Pzw/kG9tsgscnT54oKCgghIDzY2NjQU+aPn26kZHRvXv37t69KykpuWXLlpaWlmHVCCBIdHT0zZs3JSUlx4wZc/78+atXr3p5ecErBoMhKysrJSVlZGT06NEjR0fHmTNnLl68uLKyEpizoKBASUlJQ0MjODjYx8dn9uzZK1eurK2tHerahvIpKSk3btwYO3asmJjYb7/9du3aNWtr60WLFmF2goEYKn9sbW2NjIzCwsJcXFxARMMhoaSkJBMTE4SQoqIiGO9v3rw5efJkhNDmzZutrKzAqAHdf/78uZSUlJmZWXx8/K1bt0RERC5evIiVPzxYkABODg4OlpKS1tPT8/X1ff369aRJk5SVlbEmJy8vf+fOnZiYGCcnJ3V19d27dw8MDPT09Hh5ecFRsGXLlpmYmEhISCCEQkJCHBwcJk2axMfHt2rVKpg1CgoKYNG0sbEZP378xYsXY2Jibt68yc/PDztVwBz/syH5k49/g4IFB08EBAQWLVo0adIkhJCIiAg+AIUZNzo6mouLS1paOjc3lyCI3NzcMWPG8PLyghAhCMLGxoaLi0tfXz8oKCgiIgI2c7j6sP0cHBzs7e1tbGzcu3fvsmXL4AjSsWPHwEg2gvQcFhrwAZPJtLe3v3btGpRpaGigUqmioqJtbW2ADEbJzMwMAuqhZGpqKn7EESHDNqShoSEtLY3dAd9SsMguwrVr186ZMwcb/+Pj46lU6u3btwmCyM7O5uHhAREAzS1YsAA8QQRB3Lt3j0ajYashxmfnzp0cHBz37t0jCOLGjRu+vr4EQVhaWiKEYF4RBLF58+YxY8ZgwwZBECoqKlOnTsVAcGLSpEnLli0Da9zFixcRQnAQDArcv3+fSqXCESSCIB49esTBwUE+MyUrK6umpsZm48HAIVFfXz958uRFixbh/L6+vl9++UVdXR17BLS1tVesWIELDE0AS9y9exch5O3tDQVMTU05OTnLysrwyJIrghRwcnIaN24cbOlAiTx8+DDuo6GhIQ8PD6i/BEFERUXx8/PLyclhk/68efPGjx8PMQRMJvPRo0dkOq9fv3727NlNTU0Q9dLc3AwuQlhu09LSeHh49u7di7EyNTUdN24cmzkdv/2eREVFxcKFCzk4OF69egXlL1y4gEeNyWRqa2sLCQmRXQZ79+4VExMDJZIgCHNzc3Fxcdhrgs9x3LhxEDUIRD558iRCCMTr06dPnZ2dGQzGiRMnEEKYGxUUFJSVlQmCwKfDIAYLFKzGxkZFRUVhYWGgQ0tLy4QJE3bs2AEroomJyfTp0/HlIMXFxXv27BkYGFiwYMHmzZsxEUJCQszMzPAjrGrwSE7jAuQEDP2HDx+mTJnCz88PgosgiAMHDsChP1yYl5dXVVUVHktKSiZOnLh+/Xr8trm5WV5eXk9Pr7W1lcViBQUFUalUULihTGRkpICAAD7USRBEZGQkDw/P77//joGsXLlSQkICYuYIgiArWARBvHz5EiFEVjgWLVqkpaUF9Pl+SQgKlpqa2sOHDy9cuKCkpCQjIwMh3jhsMSsrCyGEw/siIyMRQg4ODrDkZ2Rk0Gi0NWvWYMylpKRkZGTAHsnHxyckJBQXFwdvf/sVWii6AAAgAElEQVTtN4QQGEeB2rgWToCCtXz5ciqVik2YBEEcOXKEm5sbNrSDg4N2dnb4nCPUhQASuBWiubl52bJlU6ZMKSkpaWxsXLly5Y0bN3ATIyS8vLx4eHhAVQX5xsXFJSkpmZSUBLXOnz+PEAJnwre6ACVVVVWBDuTmGAyGtLQ0FxcXFkeurq4IIQhWbmpqWrVqlaioKBZxCQkJHBwcwBgjNCcjI0OWhElJSVxcXM+ePQObxQjyB0xEd+/epVKpQFvAdtasWaqqqni6PXnyhEajaWlpEQQREBBw79492GwLCgrq6OjgDt64cWPMmDEgNNimGyg0ZWVlU6ZMUVBQwKJGR0eHQqG0t7eDP+rBgwcYWm5uLkLo0qVLkJOZmUmhUDQ1NVtbW9PS0q5evQpiBEwYJ06cYDKZ7u7ujo6OnZ2dQDfypNu1a5eIiAhehXErf2/ib1CwwIK1YMGC8PDwmJgYdXV1CQkJNgULDocjhMTFxWGVSk9PRwjx8fE5OjoC6W1tbWk0mrKy8ocPH8gbL3J43bc639jY2NDQ8OrVK0lJSYSQurr6u3fvYAjxaa9v1SXnA8sqKSlZWVn5fv15e3vTaDQqlerm5gYSBDap3d3dhw8fHjt2bFJSEoSUpqWl8fPzHz58GBS1YVdrOBa+fPlyCQkJPLQjK1iDg4OfP39eu3btrFmzwNI+ODgYHh6OEAIZUV9fLy4uLiYmdujQocePHyckJJSWloLPi06nX7x4kUqlJiUlQVAFaH5fvnzZsWOHmJgY7KoxBYqLi4ODg9vb2/Pz84OCguBYNVyr0d3dTafTZ86cKSMj09LSAtCgp52dnbKysurq6rA3EhAQmD9/PjgK+/v7IWafl5d3z549YMK5f/8+hULx9PSE46UsFmvOnDnKysrYfILxISdAXNrb20PQCYzUnTt3KBRKWFgYk8lsaGjQ0NBYsmRJU1PT0EBRAAWDUlNTExAQ0NnZWVxc7Ofnp6+vz83NDarG0KUIGnr16hWVSlVUVLS0tHz69GlxcXFlZeXnz5/BZQMKFjhKGAwGFLaysoL+9vX1bd68WVBQEBQFJpNpYWEhICCQkpLS3d3d399/5syZMWPG4MWjqakJFCwQZ6CU3L59OysrKzk5OTU19ezZswghvEqRqcSWhsBw6DWEhUJ3qqqqYKPc2toKZezs7Dg5OYEI+fn5HBwcoC2BE43JZMbFxSGEbt26xWKxOjo6zMzMhIWFsZk5JydHUFBw3759YFJlMBjm5uYUCiU1NZWM0tq1a7m4uNrb27u7u5lM5vr16xFC2DvMYrHIQe4DAwPJX389PT3x8fGPHz+WkpLS1tYGMXrnzh2E0OLFiy9cuODm5lZdXV1SUsJkMvX19RFC69ats7GxCQoKampqwvIE+g6yBafJ6LGloe/5+fliYmKbN28GZmYwGBcuXKBQKEArBoPR09MjJSU1e/Zs8url4eEBZ2XAdXj69GkqlQq3cG3ZsoVKpWZlZbFYLFCY2tvbpaSkcJA7g8EIDg7m5OSEHTb47nV1dUVERIZVsDo6Onbt2kWhUOzt7bOysqKjoz98+KCjo4MQAqVwKFez9RQ/gosQr0PJyclbt24FqyGDwairq2tra+vp6QkLCystLW1ubn716tXVq1dpNNrZs2dBEwKDZVBQEPhl6HR6ZGQk2MAaGxupVCpooj09PYODgw4ODhQKBXZ3QCiMCU4AWE1NzQkTJnz58gXHXH/8+JFCoYDRtKenR1ZWVlpamsVi9ff3w6ysqqqC09nA/3l5eTIyMvPmzdu6dauNjQ2eFLihoQkGg+Hh4cHJyRkWFgaDW1FRwcHBoampCbKLyWTeu3cPIRQcHDxCPF9vby+dTp8zZ464uHhDQwOdTscxCQwGQ0JCQlxcHA6Mg2zn4OC4f/8+QRCJiYkUCmXZsmU5OTkJCQnp6ene3t4UCmX//v3AxtALMuawxEhISIBVGB5jomMQQk+fPgUkR5A/vb29DAbDxsYGhxP09PQwmUwFBYX58+e3tbUBBCcnJwqFAlogtM5iseAmlEOHDmVnZ8fHx+fl5d26dQshBEKbbXzhEfjt1q1bwC2Dg4OJiYnR0dEsFktbWxshBAwPsqunpwc0ftD1Y2NjKRTK2bNnsWcAFmJra2sKhQLXF2DKHDx4EDDJyMhISUlJTU0F+0hubm5vb2/P1193d/fXQIU/Pjnzd/3+BgULgtzXrVsHyu+VK1d4eHiGbq9hoyMuLg7nxhsaGkC8btq0qb29vaSkBJZzExOTjo4OOO5BJtxQTgJnhL29PTYkEARhZmYGV7bgI3ggXL4nFgHKlJSUKCoqWllZXbt27bfffrty5crGjRs5ODjwNQqASX9/v6GhoZiYWEFBAeRkZmaOHTt2ZAULxLqWltb3K1hMJrOjo2PNmjVYwYKoF1jtgA9CQ0OVlZXBVDtp0qRTp07hRevy5ctUKpVs5CcIoqura9OmTdOmTfvy5QswJd5exMbG6ujobN++/dmzZ1paWhQKJS0tbXBwEMwGioqKkydPxoYZaB2C9EHBKi4uBvcfSDoAW1hYKCMjo6GhQV4aAwICWCwWgFVRUVFSUhpBwWIwGMeOHePm5oZ1Cw7xsVisFy9eUCgU8D82Nzdramqqq6uDFPjWogL5cXFx69ev19fXv3btmp6eHjc3d3h4OOjibFMLtOr+/v4XL16IiYkBkRUUFJycnPCe/sCBA9zc3DgSJSIigouL6/79+8AYDAZj8+bN48eP7+jogNZDQ0MRQmAlbW9vnzp16sKFC9vb24E9sIIFk2jdunUUCuXkyZO///67ra3tgwcPrK2td+/ezaYcs6H9rUcYkerqanl5+Q0bNmBX/sOHD7FIdXR05OTktLa2xjRksVj5+fkUCgWMQ93d3WQFCwzSgoKCIPdh52BsbEylUnNycoCqZHkK4UF0Ol1AQADWKsCKTcGCLly7dm3ZsmWnT5++cuWKmJjY8uXLm5qaBgcHOzo6Ll++zMPDAyOyYMGCiIgIOp1eV1cHXm+4dU9HR+db16x8i0Q4H7AqKCgQFhY2NTWFlZ4gCGtra4QQaEiwkZg8eTJcmNfR0bF79+6xY8cCO4Giz2Kxbty4gRByc3Pr7+9XUVGRkZEpLCzEZ/1qa2vFxcW3b9+OwwZCQkKwggU7irVr1w5VsEATKi4uVlNT4+fnP3ny5JMnT+7evevo6Hjy5EkzMzMID8KHXnHXvpWABc/MzAxYt6Ojw93dHbZYjY2Ntra2YNWor68/fPiwjo7OxYsXLSwsqFSqpaVld3f34OCguro6hULJy8sDCLghiKzg5eXV0dHBJ+bAYIbPgOPC5ASQXUtLS0hICEz4ALm/vx9cIhB7QKFQZsyYQT53VlVVpfT1V1dXB5wcGBhIo9GmTZtWVVUFJckNDZt2c3PDChZBEFVVVVQqFWJ8QXbZ2tpSKBTM0sMCgS7MnTtXQkICHxmBkhDkrqioiIXPq1evEEJguQkKCqJQKEuWLHFxcbl9+7adnd3t27d37dr18uXLYRvCQTWSkpJLliwBBiYIIvpNNFawWCzWt+QPlvPXr1/H0gCCqMgKFgRCUCiUmJgYEI8woCYmJhwcHDt37nz8+PHdu3ft7e0vXbp04MCB6OhojBhGG2gC2zAfHx+2BZrFYgkJCVGpVFweDlWsXr1aVFQUtg2gUdy/fx9aZzKZMCJnz56lUCiJiYkwv0CXBbY8d+7c48ePQYr++uuv+/btu3v3rqKi4syZM+F/0aJFycl/XMKHSUdG4AfSP65gsZ0iXLlyZX19PZ1Or6qqio2NZVNX4YptsGBhS/uzZ88EBQVVVVWhe1OnTsXePZhC8F9cXIyNPeQeMpnMZcuW8fHxKSgoVFVVAZVv376NEOLg4Fi8eDEUZjAY+G5MvGCQ4eA0qMmnT5++efNmQ0NDS0tLc3NzS0tLaWkpJyfnhAkTysvLcawAVrDwOpeRkfGnFiysYImLi+NOkRUUoBucIsQOFzYFC88QsGBVVlZWVFQUFxdHR0c/fvxYU1MTIYTDTa5cucLFxZWWlgaxvSAfOzo69PT0pk+fDusr2AiZTOadO3c4OTn37t2bmpra2dkJwSVghADRP2vWLBkZGbCsQGw4mM2lpKSWLFlCEMTHjx85ODjgUAxE1bBYrHfv3omLi2tpaZEVLHBE9vX1sVisP1WwWCzWpUuXaDSai4sL7HVgQsJG6vr16wRBNDU1/amCBQxw9+5dXl7eXbt2paSkNDU13blzh4uLawQFi8VilZWV5eTkFBYWhoSE3L9/X05OjkajYZvTsAoW9n0zGIxNmzaNHz++vb0d5u3Hjx+lpKSUlZWXLl06d+5cTU1NOAoOo9/Y2Dh79uyNGzeCgmVkZMTBwRESElJfX19TU1NbW9vQ0MC2emEe/tMEIFBVVSUvL79x40YwqhEEAR4WCGcGt8i5c+ewlAHm4eDg2Lp1K+zdQcGCzRLc083Hx0dWsA4dOsTLywsFwJZMEERhYaGcnJyMjIy2traqqqq2tnZhYSHGmcViQZA7+KRqa2s1NTWFhYUfPXpUWlr64cOHBQsWYI9wZmbmhw8fsrOz/fz8rly5MmbMGBERkZaWlrS0tPr6+tTUVE9PT4gOBLfyyHMf40BOQPdBwTpy5AjetIATHF/w3dfXJy0tDQfEenp6zM3N+fj4goODgVFBYly9ehUhBN+xUFdXl5SUhIBxEAg1NTUSEhLYgkUQBChYEKYNU2z16tUiIiLd3d1QBVyEoGA1NDSsXbtWVFQ0IiKirq6uqqqquroaO5X+EqvgaxrAoMJgMNrb27u6ugYHB1NSUg4dOvTx48fMzEx5eXklJSVfX9/q6uqEhITx48efOHECeGnTpk0cHBwpKSm4XbwW1NfX8/HxrV69Gt/dBfrc9ytY+Ip/MKNycXHJy8uDy56fnx+iF8ARz2KxPn36NGPGDGVl5bq6OiCas7OzoKCgsLDw7du3v9OzAQoWDm+oqqqi0WjgHoXlHCxY36NgzZs3T0JCAsKY7OzswB5Bp9MlJSXJF42SXWMxMX9Ynnbs2NHc3FxeXl5bW1tfXw8VyYxKTgPTSkpKqqmp4fkLCha+Mulb8gfflERWsGChnzFjxrx582DvihUsLADBlXH58mXYWNbU1FRVVYGwAsqTMYQ0KFhgDH7+/PlQbpk6dSonJydMW3j75csXbW3tiRMnglQBBevhw4f4tkiAeebMGQ4ODpCog4ODkLl9+3YODo7Y2Ni6ujosReG4hpmZmfnXn5mZ2dmzZ2Gf/APiYmgfCYL4cQULbi5wc3P71j1YbO1FRUWBgvXu3TsWi9Xe3r5jx47Fixd3dXVlZWUVFhZi1R5YH5jDyspqxowZa9asgQsI8DDAFIXNK0IoNTW1/+vPysqKk5OTg4MDggCqq6sPHz48Y8YMmP9sajIZQyaTCRqPiooKnBnBbwcHB1euXMnJyXnr1i1wjYHR3sjICMJQYIOYmpoqICBgYmICBmoyqhgUXMeybNkyKSmp9vZ2WHjgHixNTc2BgQHsU+Pm5jYxMQEbSU9Pz8aNG2fNmgXB+ywW6+7duxQK5d69e3Q6/c2bNwYGBvjugOrqalVVVQMDA+Ds3377jZubOy0tra+vz9nZ+dq1aywWq7m5efPmzdOnT//8+TOolRAnLiwsrK6uDoIDohw4ODg+ffpkY2MDYTGzZs2Sk5MDF+GcOXPAJIPv8RoYGGhsbJwxY4a8vHxnZ2dfXx9salNSUjg5OY2NjcG8D84dPz8/BoMBC4bK1x9sNIelG0EQCQkJCCHwu4G/cmBg4PTp0zw8PBEREeAiBAULXIRDZwhsttrb28XExMgBUnfv3uXh4QkMDPT29ga/GxkHIOOjR4/ggkQYypycHFFRUVtbW9gHHzx4kIeH5/3793AlTEREBJVKtbOzA9nU09OzZcuW8ePHt7W1wWyPjo6eM2dOeXl5REREQkICKKz4dpm6ujpwETY3NzOZTDC847UWEPDw8GDjUsxjIyfA7VVRUTFjxgzQ4QBnrGDR6fTm5mYODo5Vq1aB5IIVC0JD7t69C1Pv5MmToqKiOTk5YKQBhcDU1JTBYHR2dtLpdGNjYz4+vqysLOBhmM7Pnj0zMzNLTk6OiYlJSEiAJRmoBG2BizAjI4PBYECL1tbW0KO6uro5c+asWrUqMjIyMTFRR0cHZCsId1iWioqKNDQ0IPQeFMFbt25xcnL29/eTx3RkEuG34DzNz88XERE5cuRIV1cXDDe2YMFJmo6OjilTpigqKsIFUX5+fgghIFRnZ+fAwEBfX9/evXvHjx+fkJBAEIShoSEsAIODg8DJnz9/lpSU3LlzZ3t7O+D56tUrbm5ue3t7cPH09PRMmDBBXFy8q6sL9i2ZmZlcXFxHjx6F0YGFCkKF8JLm6uoKPtzqr7+RKcB2D9bAwABEBOIlytbWFgLLwLcCXjOCINLS0saMGXPu3LmwsLCIiIirV69SKJSHDx8CrUArdXJyqq2pramp4efnX716NcQp0+l0ULDgiiash2H6QwKmjJaWlrCwcFtbG51O7+/vZzAYb9++5eTkNDIyAiOimpqaiIhIc3NzX18f+LlKS0t5eXnx8fbAwEBtbe3k5GR9fX3YUAHLDRUUGAE6nQ4KVkhICPB5WVkZNze3np4enU6HsQMFKzAwEC42w3XJCSDC/PnzpaSk6urqBgYG5s2bB3Hx3d3dkydPVlZWhqWBwWBA+MeDBw/odPr79+9hawr7W5hEmZmZISEhg4w/PkE9dEzhii+496e3txdmd3TUHxYsMLr/YdD6hvzBCtbdu3c5OTmh1/39/Xl5eWJiYmpqao2NjVDG0dERDtfjyUsQRFJSEkIIPumBmTA1NdXHxwc7ozBZoEBwcDCNRjM3N+/v7wfPKUEQrq6uBQUF4PUuKSmBHtHp9J6eHiEhIQUFBVhhX79+DaY+mGLABnQ6He4xePv2LUhU4CuIu4WAPIzDy5cvP5T8EY/0r/v9uILV1dXV09Pj6Og4ceJEKpWqra394cOHL1++wJrKhnFvb29AQACVSpWUlExISGAyma2trbq6uoKCgps2bdqzZ8/evXv37NljbGx8584dsHPAN17k5ORAiwKhSZ4MTCZTS0sLzoZAcx8/foTjD2PGjAFZFhsby8nJiRCaOnUqWM6wUs+GITw6OzvLyMiAhgHaGHBwVFQUjh7AFffv3y8mJoZDm0tKSri5ueFmmmFZH1dcvHixkJAQfiQIQlBQcOHChTinubkZ34MFaBgaGoIJDTgVvKsQ5J6ens7Hx0f+ZqKKisrFixcBc5iuIA3Pnj375s0baGXdunUzZ84EasD8j4mJ4eTk3Lx5M2S2tLTMmjULIVRTU6OnpwcVd+zYwcvLC7N94cKF2AgnKCiITYYBAQEUCgWsStDWhg0bREREsOXS3t6eSqWC3RgKLFiwYMaMGZAeKjIgv6+vb+vWrYKCgviur6ysLBqNtmPHDsCHyWRCDBZAGAoHFKy2tjYajSYpKQlgCYKQlZXl4eF58eLFmTNnYCdKZjOYn0+ePOHl5YVdEfhY5eXlIWiUIIiDBw/SaDQcp/nq1SseHh44QACtrFmzRlRUFOjMYrGSkpLGjBlz7do1/68/Pz8/bA4B4EpKShs2bIC6bW1tK1asEBERgasBCIIoLy+XlZWtKP/jioS/+sODKy8vTw7EtrOzo1Kp+OIPMNLAJALjvJyc3MyZM9va2sAkA1oyJgiEtOO7TMFZz83NDYfLwOTOYrHc3NwUFBTu3Lnj6+sbFBQUEBAAaiIerMuXL+MTxHAeFpicIIigoCCE0NatWx89euTs7KyjoyMrK4uPfXz48EFAQKC2tnbBggXk8F4vLy9xcfHv95GR6QlrwIcPHyZOnEi+z+/8+fNUKhXzM0EQEyZMACMEQRBtbW0rV64ke9Jhq3306FHwVuTl5YFDELd18eJFHh4eHDbOYrHev3+PELp8+TKUiY7+Y4GUkZHBLt2amhoqlQqLGViOFRQUFi1aBOoIQRBOTk6bN2+urKz88uWLrq6uoqIibM0xqXHr5IS9vT03NzdcrUzOj46OlpaWBpu6np4ehUIBKyNBEMePH4fTDLdu3bK1tW1sbFywYMGkSZNwlEhMTIycnBzE7nBzc4PiDsB///13KpUKeuHICtbatWsRQhC4CXW1tLQEBATAB8piseDcz8mTJzHahoaGXFxc4N8MDw9ftGgRHB1tb29fvnz55MmTQfvE5YdNuLq68vDwQBg+xBfSaDR82R6YfqlUKojHb3UBJv6+fftoNBrsplRVVXGghaCgIJZ+cL6BSqXChgpf/QphTIDhhg0bYJ88wlomKSk5Z84c3KPYmFgqlQoWrJHlD/DPixcvyKdwLCwsODk54TwKwHRwcMAxhbiVvr6+nTt38vDwYLN0YWGhjo4OEAerXFAe/Hfd3d3bt2+n0WiYncrLy2VkZEo/lJaXl0+YMIFM6ps3byKE8JUQKSkpmFAYB4IgIOwY7z/BXtPa2qqqqiolJYXtOIWFhbKysi3Nf3xe81/3+xEFC9jlwYMHcB4Sm5HExcXHjRtHDnwDvBkMhpmZGTc3N5QUFxcHmzCYo3F1SNBoNAUFBZDsPT09cGBHW1sbLN5s0qGqqsrAwIBGo02ePFlFRUVAQAAhpKamho8/1NbWQqzckSNHIK6WvHYCesCmycnJcJYVISQsLIxPSw0ODhobG8PpSISQpKSksbFxa2vr7NmzoUciIiIODg6hoaHwWUYajbZu3TryTR7QCrSbkpKyYsUK6KmKikpOTk5ISMicOXMgR1tbOy4uzsnJSVpaGiFEoVC2bdsGLsjExERpaelFixZdunRJT09PV1dXVFSUk5NTX1//2bNns2fPVlJSMjQ0vHbt2ooVK3R0dEBJBYmgqakpJCSkp6enqqo6MDAAp6K4uLgQQhISEqdOnQKrRnd397lz5+B+gf3792toaNy6dUtFRWXcuHEHDx4EC1laWpqgoKC6uvqCBQuuXr3KYrGCg4Pnzp0L+MP5YSaT6ezsPGXKlKVLl5qYmCgpKamqqsKA1tTUmJqa8vLyIoTGjx9/7do1FxcX0JIRQlOmTGGLiWbj+8bGRn19/SlTpuzZs2fbtm2ysrJHjhwBxOLi4mbOnAneYWlp6Vu3boGkYGMYMBnCMRNVVVUzMzN1dfV79+7Jy8tzcXGtWrVq6PYAhOajR4+4ubnXrVt38ODBK1euzJ0719TUtLOzs6SkZMuWLdB9CQmJxMTEQ4cOwbGPMWPGrFmzBgyKUEBBQQEEZWVlpbi4OEJo7NixPDw8FAplzJgx2traVVVVUVFRsrKyCCEuLi5FRUVQvKqrq/X09MTExPbu3Xvw4EENDQ1/f3824nzPIxxxcHFxgbMgXFxcU6dOzcnJOXHiBOxDREVFzczMenp6+vr6zp49KykpuWnTpv3790+fPl1XV5dsPC8rK5s7d668vPzly5f19fXXrl0LHxtdtWqVq6srfGYKISQnJ7d3716sE1dUVADjcXNzAxuMGzdu5cqVnV9/q1atgsu6REVFf/vtt9bW1iVLltBotD179ujq6h44cMDS0pKfn59CoURHRxsZGYmKiq5evfr48eNWVlYKCgrXrl1jMpnTpk2bO3fumjVrLly4YGJiMnPmTAhA/h76kMuAhf7GjRsTJ06Ee1WUlZXLy8t37NgBtJKSkrKwsIDruWF81dTUYNvw8eNHXV3dqVOnHjx4cOPGjZMnTz537hysrCBtoqOjZ86cuWjRIktLy02bNq1fv56T6499oJycHKikDAbj8OHD/Pz8FhYW5ubmX4/HLkcITZs27f79+8+ePQP+4eTkNDAwgCj+lJSUhQsXTp069ejRo3p6eqtXr4ZNKZ1Oh50SHAdmmxHQZTjbOHv2bBgdhNCCBQuWLFmirq6urKwsISEBga1wgCs8PHzKlCnAKhoaGr/++uu2bduoVKqYmBi0WFpaqq6uPmnSJGNjY11dXQ0NjejoaDc3N+AQOIEYHBxsYmICw83Ly3v+/HnY/wxFD7a7U6dOtbCwMDAw2Llz54kTJ1RUVDQ0NNhCS/38/OTl5dXU1ExMTObPnz9r1qzk5OTc3FwVFRVgtgsXLjCZzMTERBkZGTjtrqmpCQeu2dYFQGPXrl2CgoIIITExsfPnz7u7u+MN/9atW4OCgoyMjOCKFj4+vsuXL3+rCwA8OztbRERk8eLFv/zyCwQ4hoSEwIURCKHly5cHBgaePn0avuRLpVINDQ1x3IuoqOjKlSuPHz++dOnSs2fP/nEclWCx4QyPYWFh8KVROAKSmpp65MgRCB7l5eXdvn07nPkdKn9Wr14NzqWBgYGOjo61a9eKi4ufO3du06ZN586dAzpISUnl5ubq6ekBPUVFRTU1NcmBcU1NTbt27Zo0adLu3bsPHDigpqbm6uoKvnLy5MJcB5da6+vrjx8/fvfu3fr6+r/88ouHhwf05e3bt/PmzVNRUTExMdHU1JwyZQpA6+josLa2BkKNGzdOR0cHQto/fPgAWizsRvT19cH8AZOuvLx81apVkpKSBw8e3Lt3r5aWFr7HBBtThrLfULT/Us6PKFiARH19fVpaWk5OTlFREdxBl5WVlZqaChftkJEAd3hmZub79+8LCgpycnJaW1vDwsIkJCRmzpx569YtOzs7uLbO3NwcLrSwsLAAonz+/LmwsJB8sTgZMkEQcOTN29v74cOHvr6++fn5OPgASjY3NxcWFuLtAlt1bGpqb2/PyMjIzc0tKiqC411QEnaT6enpRUVFhYWFycnJxcXFdDo9PT09Pz+/qKgoLS2tpqampaUlNTW1uLg49+vvW3dMtLe3Z2dnQ8X09PTOzs6WlpaMjIyioqL8/Pzs7OwvX77U1tamp6e/f/8+Nzc3Pz8fzDMsFqu8vDwkJCQsLOzdu3efPpVHRUUFBgZmZWXV1IAYN8wAACAASURBVNQUFhYWFxdHRUUFBwe/ffuWjQKNjY0hISE+Pj5w22FPT09qampBQUFhYWFqaipZ7Pb29mZmZvr4+Pj7+2dlZQ0MDHz8+DE6OhqrawRBlJaWenl5BQeHdHb+8WHa5uZmjH9OTg52L3748CE0NNTPzy8hIQGvr/39/cXFxdnZ2e/fv8/IyKioqKivr4duFhUVpaamjjBMMBwdHR2pqakBAQHBwcHv3r0DExqLxWpra8vIyMCdwrfIDB1u8HAVFhZ6e3v7+/unpaUxmcyKiorXr1/DtZlscwymemVlZVZWVllZWXBwcERERHp6OqyXPT09eXl5ubm579+/T01N/fLlS3FxcVZWVlFREXBCX19fZmZmfn5+YWFhRkZGTU1Nc3Pz6tWrN27cmJeXl//19+7du3v37lEolMOHD7e2tqanpxcUFOTn5wOHAD5fvnxJSEgAJyYO+xu2dyNk4pmbnp5eWFhYUFCQlpbW2dn54cOHrKys9+/fZ2VlwbXd4D3Jzc0NCgry9/fPyMiA8Asycerq6iIiIoKDgzMzM8vLy+Pj3wYEBKSkpDQ0NLx//z4nJ+f9+/eZmZmFhYXAFa2trQsXLjQ2Ns7OzobWMzMz4bvLx48fZzAY2dnZ7969KyoqysjIgKscGhsb4+PjPTw8QkNDq6ur4VxhfHx8X18fzMf8/PzAwMDIyMjs7GxoJTExsaqqKikpKSAgIDY29idpVVFRAeydl5eXnp7e29sLEqyoqCgrK6u0tJQ8fzMzM7HZ5vPnz0lJSX5+fqGhofn5+dh4iQlYWVn5+vXrkJCQ9PT06urqkOCQoMCg6OhobBXu6upKTEyEGfTx48d37/L8/f2joqIqKyvr6uqwiCgoKAARATdmRUVF+fn5xcXFYdshXLZZWlrKZkJg45Pm5ua0tLSsrCwQROnp6VlZWZmZmWlpaUlJSZmZmQUFBbD7ZzKZVVVVcMcVfCPry5cvb968ycnJwSe+m5ubY2JifHx84uLiQMLU19dnZGS8f/8+7+uvpaUFi4L09HQQTWwowSNQrLS0tK+vr6amJj4+PiAgIDExETs6yLU+ffoUHh7u6+v79u1bMG51dXWlpaXl5ubm5eWBDfjz589FX3/Z2dlZWVmYemQ4kC4sLIQFLjs7u6ysrKGhAcj+7t27/Pz8lpYWzOfp6enYlTEUDs4pKyvz9vYOCgoGSyRmnoKCguzs7Obm5tLSUpiJsFxC35lMZn5+vr+/f1hYWFZWFo6bxGDJiZaWFph0BQUFmZmZcIAMRC4M7rfkDwcHB/hbQeK1tra+efPG19c3Li7uy5cvUVFRAQEBr1696ujoyMvLy87OhimQlZWFjaaARmdnZ0pKiq+v76tXryBqAi+yZDwhDR3s7OxMTk729vZ+8+YNvvkFXtXU1Lx588bb2zsmJgZfGMRgMD59+gQkyvn6g3nX09NTVFSEJU9BQQFejwBaa2trfHy8t7d3fHw8tqEMxepvzPkRBevnm6fT6bDHhUgFOMUK/8+ePaNSqVu2bIEFbOS2gGojl/k/9fZnCPIzdf89RB4Ww2Ez/4P4/GnTWVlZ+JA/ubCZmdmMGTOGVTH//X0ctsVvZJI7MXwaKvr6/vHlUHwJFi5qYGCgoKCAH3Fi2Obw2z9N/GT1P4X/Vwv80/D5q/h/f/l/W0//tKE/LfD9nfpPlRy2C8NmfieGI8gfBQUFsGWAdWMExehbbf0AYj9Q5Vutj5z/b2uIjMaPK1hkqxpO40N25DZgnOAV/nd3d4fwqYiICDyciYmJGhoaECKKb7EC4GwA8SPbW7ZHaHpoJq5OTkAxMPMOm0+2AOPCOAFd+9O2cHkYb/Ljt3IAGcwfOEGeADhzKALkVxgUuV3cWXJdci2cHtoiGc63irHls1UhP2JMvpVgw3AEyN+CgFkCCgAE8v/IFXEt3DQb/uRHDBYygUPq6+sXLFiwYcMGcsTGu3fvZGRkDh48CGcO8DcDMDJsHcf5P5AYFsOh3MvWIu4vbhHnDC3J1gT4CLq6ukRERMzMzMhb3vT0dFFRUfjIHe41BogTeNSG5gA+I+RjhH8gAWDxPxsaOB8ncBOQg3HD+TjxLeqRC5DLYFC4LZwYuQrGGRcbNgHQQOqSmQGEHpbGGA0MBPcUY8vWIuRjbIcmoDkMcNgEroURIDeHq+BMjBVGhtwKhobLYwjkBLnYyGkycDIEtjRuDhIjwGQDSK6I02zA4ZENJu4+PjU5gvw5cOAAhIsAEAwftzgs2rgYGQGcZns77CO5OdwWhoBzcILcKXLdEfJHhjYsVn9L5o8rWD/ZPIPBePPmzbp162RkZISFhfn5+QUEBCZOnKihoeHj4wMW9Z9sYrT6KAX+aRQAGVFRUXHgwAFxcfG5c+du2rRJVlZ28uTJN27cAPMVWY780/D/YXygU/n5+Tt27Jg4ceL8+fM3btwoLi6upKQEAXP/k73+YXKNVhylwL+CAv9n5c+/gpjfA/M/pmABcr29vZ8/f66rq/v06VNtbW1ra+sIHvHv6c9omVEK/FdQoL+/Hy4ZhwAUuFH9vwLzn0Syt7e3qampuLg4IyPj06dPnz9/hpiPnwQ7Wn2UAqMU+H4K/J+VP99Por+l5H9Ywfpb+jAKZJQCoxQYpcAoBUYpMEqBUQr8oygwqmD9o4ZjFJlRCoxSYJQCoxQYpcAoBf4XKDCqYP0vjOJoH0YpMEqBUQqMUmCUAqMU+EdRYFTB+kcNxygyoxT476bACIeY2M77/Hf3c0Ts2Ygw9HHE2qMvRykwSoH/EQqMKlj/IwM52o1RCoxSYJQCoxSAj4mR70D5Fk3g06jfejuaP0qBn6fA/4KCxWKxSkpK4FpkuKv3/+yRbzqd/q0LWplMZlFR0evXr8PCwoZ+OfvHOKmgoCAyMjIgIIB81fuPgfqZWt3d3VlZWSEhIbGxsXDN8Z8yQHt7e2JiYlhYGHxj4Wda/5+p29nZSb6h6i/1Cwje3d1dWVlZXV1dX19fU1MD6erq6oqKivLycrjFHp8Z7Ojo+J5V8C+h8TcWhh719PR0d3f/KTvhdlksVkdHR319fWVlZUVFRVVVVU1NTWNjY11dXVVVVeXXX21tLVw8XVtbGx8f7+vrW1hY+P1N4Lb+CYmurq7MzMzw8PCIiIh/yAHw0tLSa9euWVtbp6enw/VdbIQCUn/69OnRo0fnzp17+/btyHfcs1X/r3gcGBgY9srifyzyPyN8/rGdIgjif0HBotPpxsbGfHx88MFw+LLsXyV6f3//f/XlWyAjrl69Ki8vP+xHTHt6evbs2UOj0ahUKnwv82fECgip3bt302g0hFBQUBBc8vZXyf6T5eGK2nfv3sEHDWfMmPHu3buRGQAwj4mJgU8oTp069T+C+U92/G+sDgSpqalZunSpsbHxjy2TwEuBgYHTpk3DXx2dNm3avHnz5syZIy0tDd8J5ePjA1383bt3c+fOtba2/hs78jeCYrFYdDqdyWTu3r3bwsJi2O9asjUHZOzs7Ny5cydCSFxcfMqUKbNnz4Yvpo0ZM0ZRUXHatGnwFci5c+e2tbU5OjrCZystLS1B1wQgbJD/mY8w9TIzM3V0dBBCUlJS//HNLYvFSkxMFBAQWLNmzbx58/j4+MhfzwQyAtrp6enCwsLbt2+HTwHCFz+x6v+XCD44ONjb2/tjdf9SQ99ZGDA5derUrFmz4GNB31nxP1IMGL6hoUFbW/vAgQMjfwUIMBwcHIQv0f1dCLNYrN7e3p9ZDUfA5G9QsP4hQsHPz4+Li8vNzY1tff1T9OALuHZ2diEhId+aJ38KZCiJf6DKUCCQ8y1Q5HzQDpcuXcrLy+vi4vItUPAlosDAQDCkf6vY9+c/efKERqPhr2Z+f8XvL0nu5tBaeMgMDAxkZWXhC1P42wBDy5NzJCUllZSUhipYI7dIhvCn6b8EaoTCP/bqT9HDkyUqKkpCQkJaWhq+lTFCc38K08nJiUqlnjx5kq3k1atXqVQqfPDRxcWFn59/4cKFbGXYHn8GjZ8BBdLW19eXm5vb1NT0+xWs1tbW1atXb9u2DX8ELSAggIuL6+jRYxifc+fOzZ8/Pz09nSCId+/eCQoKWlhYQIsj9HeEVxjyvzmBg+o0NTUlJCS+xy7+d/ViWDhdXV0aGhpTp07t6+u7e/fuokWLQMHC0gBuM2cymZs3b+bl5W1vb3d2dp4xYwZ8sJLt8nQ2Yg5tEcpnZWXZ2NgM/VInW/W/+ji0OYCAaf4tgNDZWbNm8fLygpwnl/wWWHIZcnqE8iO8IkMYNo3rArZxcXEyMjJiYmL19fVs0hiXBDgsFis+Pv7XX3/91vrFVn7Y1tky+/r6Lly4ADzwA9XZoLE9/pSChdc2MlHIbAoFyF+YITdP5hVyeigEgiDIbQEQMlg6nR4WGoYQevnyJawZZIBk9MgIQBpEoZaW1p07dwYHB8mts7XL9mooKGgIo8qGw5+WB/jkWhgUGRNyAQwTMouKip48eUJW8MlUGhgY8HD3QAj5+/sTBAF7dDJLQWEMExLk5tgowGAw3N3dOTk5w8LCvkVkMkycxjBxgg3yUEqSSYERwxSj0+kHDx6UlJQEBYvBYLCVZ4MP4RcqKiqKiooYc4we5JApw0YTMgKQZoNPHi8Mf1ggbG1hggxtYoRX5NZxmtwdNmqwYQJvXVxckpOToddQl5xmw5MNAjz29/fT6XQnJyeEkKmpKXxdFFR/UCDk5OTS0tJghjo4OBQUFJDhDIs5tItfQXk21w8btkAoPHxkurHBIbeO00CN2tpadXV1Li6uU6dO4W+K4zJDE9BcZWXlunXrioqKmExmd3c3nU738vICatDp9I6OjsHBwcbGxl27dsF2rqCgYOLEiSdOnIDJCGyDMYdWvh9/8ojjWjgBwNk4gY0gw0LAZXBdnOjr62MwGCtXrpSUlKyuroZ8MhBMKDY0IB9DhrfkMrgiWwJXIdMKKjY0NIiKiuro6MCema0iLt/a2jp//vw1a9Y0NzfjMrhHUAznsz2SiwFL+/j4LFy4sK2tDb4HhdmVDQJ5TMn0Gdpxtg5iOGTikKHhApAA4Dk5OU+fPgUMh6I0QnUMjdxTMkpk5IcSB0PG1XHiWwIECri6uiYkJODCw3YWKHzu3DkDAwMmk9nf34+bG4oJmRqQxr3ACUCpt7dXRkYmIyMDjyAmws8nflzBwn0DOYL7MAJOuAp0DEoODAyQyTpCdfIrMijIfxP1BitYmLH6+/uxy2NoFYxGa2vrxIkTHRwcyE3gtwRB4E/csxVge8RNwBed4C3OZCv8/Y/d3d1DC7NYrD81qA5t2t/PHytYQ2EOzcEQvuU/dXNzG1nBGgoT52DgOAcn8CsWi4UDgzCfkIcGVzEyMpKUlCwpKcE5IyQA/uzZs7GChVskDze5RTZouDydTsdpnMCFycouziQncJW+vj7MZpCJXxEEwcbG+NXAwMC/OZIJN03uBaQBk6dPnyKEjhw5AlN7cHCwpqams7OTwWAYGhqGhoYOrTjsgJJ7PWyV78nE2Pb19WFLxggVWSzWwMBAX1+f4deflJSUsbHx91uwiouLNTU1YXuNF2CgBkEQAwMDgIOxsfGLFy8IgigsLBQVFT127P/bt9hw+6v4s1X/Vz8CZVauXCkuLg5WHNwixpw8uH19feR8XPg7E7gueXHFi3Fvb6+oqKipqSlAG6pmwXSm0+lKSkoHDx7ExTBj4Fk2FB9skmR7ZW5urqKiwpb5w4+4g2yY4Pze3l6MCc78zuYGBwfxUvKdddnGi1zrh9EgCGLklQu30vf1R+5ddXW1qqrq8ePHyZnkNO4gOXPkdHR0NJVKLSsrG7nYj739cQWLIIji4uLAwMAXL164uLgEBQVFRUUxGIy4uDgfHx8XF5fy8vK4uDhfX18/P7/8/Pyhy0BVVZW/v/+LFy8eP34cHh4ORO/u7k5ISPD29g4ICKivr8/Ozvby8nJ3d2fb7EJvMzMzAwMD/f39ExMTXV+6YgWLIIgvX75ERER4eno+e/bMx8cnPDy8vLycTc8FYZqSkrJs2TKEkJmZWVRUVEREBISgwjAXFBT4+Pg4Ojo6OTklJCRAJuYAMtEhs7W1NT093dfX19bWNiAggE3okMtD+sOHD15ffwUFBSUlJZ6enjExMT09PQCtqKjIw8PjyZMnTk5O+fn5IKp6e3vj4+M9PDycnZ2DgoKeP39eVVVFEERubq6vr++LFy/YlIy0tDSgUnJy8mOHxxQKxc/PjyCIlpaWoKAgT0/PkJAQwLOgoMDPz8/FxQWiuEAeVVdXp6SkuLq63r9/PyoqCs8rwH8ECxZ0oampKSAgwM3NLTk5ubGxEZAJDg4Gh0JJScmbN2+ePXsWFRXFpl709PRERka6ubk5ODi4u7uDawlQAshdXV0xMTF+fn7BwcE5OTn6+vpSUlKFhYUEQbx//97Pz8/d3T0xMRFkcWxsrJ+f3/Pnz1tbW7HEV1JSwgoW8EZiYqKLi8ujR488PDyG2qvx8AECdXV1qampHh4etra2ERERbHM7KSnJx8fH2dnZzc0tNDQUXELDamwNDQ2hoaHu7u7Pnz/39/ePiIhobGyEtuh0elxc3IsXL+zt7X18fMh77tbW1qCgIGdn50ePHr158wbGpaenJzQ01MPDIywsrKWlJT4+HqZPRUUF4Iy7AAnI7O7ujoyM9Pz6A8leV1cXEBDg7e0dGxvb1tb26tUrNze34OBgQGBYUGATJQgCFCyyENy6dWtqaipBEBEREbm5uYC5p6cnWFIxSuXl5YGBgS9fvgwPDy8pKfH29j516pSfn19zc7Ovr6+Hh8fr169BSmRmZvr5+b148aK0tJQgiIaGhpCQEE9Pz7S0tMrKyuCvPxhoeOvv7//06VN7e/v4+PiRo51AK3J0dDx27Fh8fLysrKyhoeH3K1i1tbU2NjYsFgs+2k0QhI+PD1nBglXf3d397du3wKjCwsK//vprcXFxcnKyk5NTYmIi2+La0NDwp/gDX1VWVvr7+798+TI3N7eioiI6OtrZ2TkyMrKtrQ2GLCUlxcfHx9XVFaZJd3d3UFCQj4+Pu7s7IFZSUuLl5eXj45OTk9PY2BgVFeXr6xsTEwPcBUdJXFxcsrKyQCmBfG1tbUlJyfj4+KSkJC8vL19fXzAkk/mkpqbG19f36dOnDg4OSUlJEOWWlpbm6ekZGhpaXl6elpbm5eWVn5+P1R3MGJCAPpaVlQUEBPz+9RcZGQlDw2Qyy8rK3N3dx40bp62tHRERAeYQMgJgtygvL3dzc5OVlV2+fHlYWFhycjJwFPCb49cfhGTh1gsLC2NjYx2+/kAwAtjKyspff/0VIaSsrBwQEBAaGgpvP378CMInISEB0IuPjwfh09TURBBERUWFn5+fh4dHbm5uWVmZr69vREREZ2cngP306ZOXl5eTk5Ojo2NmZiZWH2tqaoKDg11dXUFKvHr1CnM4RhUgZGRkeHt7P3/+/NOnT/BqYGAgOjoaxJGnp2dYWBhZwuPqWDASBJGXl+fm5ubo6Oji4pKUlISXbzqdHhsb6+7u7uDg8PLlS5iAsJKGh4d7enoGBga2trYmJCT4+Pi8fPmyrKwMj8LAwEBSUhIsyqA5fPr0aWBgICoqysvLy8PDA4vQhoaGwMBANze3Z8+ewfLBYDCSkpJ27NiBENqwYUNkZOTr169h4QP8s7OzXV1dHz9+/Pz5c4xVXl4erGjl5eWFhYWvX79+8uRJcnIyTLGOjg43NzdxcXEeHp6HDx9GRERg+UCmyc+kf0TBAkYvKipasWLFwYMHfXx8Hjx4ICEhsWTJkoGBAQMDAx4eHoSQjo7OypUrjYyM1q1bJy8vf+XKFdjrA7mzs7M1NTXV1dWfPHliaWk5Y8YMU1PTtra2urq6Q4cOcXJyIoQOHDhgYGBgamq6YMGCyZMnx8bGQl0mkzkwMGBraztt2rQNGzacOXNmx44dCxYs4Obmfv78OSjIZ8+eVVNTe/HihZubm46OjrCwcEREBNlxC6C6u7sPHz6srKzMxcU1ffr0devWLV26FFQZgiACAgJmzpy5bds2R0dHExOT6dOn3717l01LA+qDz6K9vX3fvn2zZs16+PDhsWPHBAQEdu7cOXJQy7Nnz2RlZRFC69at27x5s7CwMEIIfOehoaHTpk1btWqVq6vr2rVrJSUl4cjb7du3Fy1aZGNj4+Pjc/z4caxW/vbbb1D99u3b0Lu+vj4bGxs5ObnNmzefOXNm27ZtysrK/Pz8Xl5eEP+hrKyMEJKWlgaBe+fOHYi6tbCwAD9OQ0ODpqbm0qVLHz9+vGfPHl5eXgsLC7I1awQFCyRaamqqvLw8QmjOnDlnz541MjIyNTUVFRVdvXq1t7f3kSNHjh49um3bNoQQcAhM8s+fPx85cmTq1KlXrly5e/eulpaWmpoaWWrX1NTs27dPXl7e0NDw6NGjW7ZsmThxoqKiYk5ODkEQv//+u5SUFEJoy5YtX758YTKZ+vr6HBwcCKGYmBg8YbCCBYxhbW09YcKE06dP29nZzZw5U0tLq6GhAbzGuAqMPpPJbG5uXr169eLFix8/fmxoaMjHx3fkyBG8NPr7+6uoqNy4ccPHx8fCwoKXl9fS0hIHPGGeIQiitbXV0NBw5cqV7u7uz549W7hwoYyMTHp6Olgof/31V1FR0dOnTzs6OsrIyKxfvx6kal1dnZ6enqys7P379y9cuCAgIHD+/Hkmk1lXV6eqqooQEhISOn36tJ6e3uHDh2fNmjVv3jzQ8LBxl4xDZWWlnp4e+voDxTcqKuqXX35BCE2ZMuXkyZNbtmwxNzcXEhLavHkzfD0QGIxMFqxggYtw7969Hz9+rKqqys7OVlZWzszMxIXz8vI0NDQQQiIiIjizoKBg3tefmZnZ3LlzZWVlN2zYsHv37s2bN6enp0+bNg1WMtCzz507B1Hzd+7cIQgiLi5u+fLlwGMgChBCQPBPnz6tWLFCQUHh8ePHJ06cGDt2LFQZdhWHzE+fPq1duzYxMbGhoUFYWPg7LVi4IzgB0NgULPwWEsXFxWJiYlu3bj1w4ICZmdnOnTsnTJhw/PhxPExlZWXfgz+oR4GBgdLS0gihFStWGBoa7t69e+/evZMmTTIwMADN2MzMTFBQECFkY2NDEERNTQ1wC0Lo8+fPdDodTxxNTU0rK6vDhw/r6+tTqVQLCwt/f/8tW7YcP35cRUWFn58/MjISmxhXrlwpJCS0atWq/fv3W1lZaWhoKCgoQAHoZlFR0ZIlS1RUVJycnExNTceOHfv06dOenp7Tp0+PHTuWj49PX19/48aNcDgAbAlsAwSLTnJy8vz583V0dB4+fGhpaSkvLw/nAwYHB93d3TU0NKhU6oQJE5YtW2ZqasoWAgEk9fb2Xrhw4fjx48XExJYvX25iYtLS0pKWliYrK6utre3k5LR79+4JEya8evUKMI+NjZWSkjIzM7Ozs1u8eLGoqCjsCphMpr+//4IFCyQlJceOHaujo6Ompnbr1i0Wi/Xy5UsQPrq6urBX37t3LxcXF0IIQilAOCCEli1btnXrVjk5OYTQ48ePCYJISUmZM2fOokWLnj9/vmvXrgkTJri7uxMEUVtba2BgsH79ek9Pz6dPnyorK0+fPh2ENlAGsAWRa2FhMXbsWITQ06dPsTt+zpw59vb2np6e+/fv5+Pje/jwIXk1hOo4Rs3Hx0dSUnLp0qWPHj0yMTGhUqnOzs7g9bayspoyZYqVlZWdnR0cJsjKyiIIor29XVNTk0Kh8PDwWFhYbNiwwdTUdO7cuYqKivHx8SDVf//9919++eXy5cu+vr4WFhYIIQcHh/b29i1btoDwgaj87u5uXV3d7du3u7i4ODk5wQGm9vZ2c3NzLS0tDg4OSUnJ1atX6+rqBgcHQy9cXV0lJSW3b9/+8uVLNTW1mTNngri7fv06rIkGBgY7duwwNjbevn27kJCQhYUFk8n8+PHjypUrFRQUODg4Fi1atHz58j179nR1dWGlFsjyM/8/omCBVm5mZjZ//nxsz0hOTt6zZw8cJHF2dubl5V2yZAnG7MqVKwgh0E5gYs+cOZNcID8/X0RExNLSEqbB1atXOTk51dXVYSvf3t4+ceLE9evX4wXszp07CCFXV1fchJ6eHjc397NnzwiCSE9PFxISgshrkPtmZmagVWDJBRVhGmdlZVGpVIjfwgDz8/M5ODhOnDiBcyIiIhBCI8DJycnh5ORcu3YtVAkNDUUIOTo6kncGGBpOsFgsGo0mJCQUERHx4cOHlStXxsbGlpWVCQkJbdiwARdbvXr1L7/8UlFRIS8vf/nyZZyvra0NKBEEkZyczMfHZ2dnB+uftbU1Qgi4EMpra2vz8/P7+vri6lu2bFFQUADzHkEQaWlpPDw8+HhXQEAAQgiCCgmCuH79OkIoISEBVx9BwSJrorB+nzt3Diq+ePGCg4Nj4sSJGRkZkAPh+bW1tfBoYmLCxcVFPgWzfPlyFRWVlpYWFovV3t6+atWqsWPHYrRLSkokJCQUFRUhVpEgiObmZllZ2V27duFd0fXr12k0Ghl5rGAxmUw3NzeEkLOzMyDQ2Ng4duxY8N2QFUoslaKiosDqCeUdHBwQQuHh4UB5RUVFst/H3t7+1KlTbAoWsKK/v//EiRNBLyQIoqmp6eDBg1FRUQRB3Lp1izx8NTU1HBwcN2/epNPpenp6VCoVn8SGYUpOTgZkTpw4gRDS09ODx5ycHISQubk51oEgH/8Dzrq6uuPGjYNNNrySkpKiUqlnz56FRw+PPwL4wLfFNo+gAGxzHR0dEULz5s07+/U3efJkKpX6/v17aB2vBwpff1Cxv79fW1tbSEgI3hYUFHBwcIASgDlES0tLVVUVXwgSGhpKo9FgcgEQ0BWuXr3a3d29efNmW1vb/v5+DQ2NCRMm4EYdHByoVCoOaoaK8A+78O7u7gMHDty4cQM2xlocsQAAIABJREFU8ZMmTTIyMgIpN6xOSYYA0xzv9UdQsBgMBhQrKCiYOXMmQgi2fwRBGBgYIIRAkPb396urq38n/tBcW1vbkiVLYOkC3ED2wiIHyigfH5+trS3uzpo1a4SEhPBXt1tbW2k0moSEBJw1JgjizJkzCKFNmzaBwaC8vFxYWNjAwIBOpwMbrFu3DiEEm1todNOmTUJCQuB26OzsnD9/vrS0NKbV9evX+fn5wb4SFBREoVA0NDQqKiqcnZ319PQgHw8ZUBUczaKiomSR+OHDB4TQzZs3MWQhIaHdu3fjR7YEhjl37lwDAwN429DQMGnSJC0tLVx448aN0tLS4Nw/fPgwQghvD1RVVSdPntzT04NHeenSpeSFDEbhy5cv06dP37ZtG/aF3bt3j0ajvXnzBrciIiJCoVCeP3/e3Ny8atUqX1/fz58/T506de7cubiMubm5iIhId3f3ixcvyPEPlZWVe/fuHdYoDn0MCwvj5uaGI19dXV2TJk2ytbXFYC9dugREY5vF8BgbG8vPz79nzx4oX1ZWNn78+I0bN7JYrMuXLyOEsLAiCGLLli1ycnJg7CcI4sKFCxQKZcWKFQCqrKyMQqHo6+uDQFZWViYbtrds2WJvbw+tbNu2jZ+fHxgsIiKCQqFg4UYQxOTJk0EId3Z2IoQuXryI+wIsTaPRsLxlsVhKSkq6urqgLeTl5YEJA2+tN23ahBDCXoKbN2/y8PDgLpAh/3z6RxQs6OqlS5cgeDMiIiL7/7X33VFRJNv/NQxDEAkiYBYxLIiIKAgqBhQVZVcliKCgYFjDKqIYkSQ55zig+DCsYlhhRZSgrgEJIiJIkkWSSBCUDBP7d573fOv0GZR1Xd8+9/d6/uAU1dW3bn2quurT996qfvr07du3t2/f7ujoYLFYCQkJNBotKysL5iwIE1FQUJCTkwN/DbzjJiYmgleYxWJxOJytW7fKysoWFBSwWCxvb28ajQa8AUbz9OnTIZaQIIju7m4pKSkVFRUIa2Cz2Vwu9+bNmwgh2EBXUlIiKio6f/78pKSkR48eNTQ0FBYWPn36VCBADxZLNpudkpLCYDACAwMHBgb6+/uhgWZmZkJCQs3NzewPPxaL1d3dPXnyZGVl5cELFcxWXV1dnp6e//rXv7q7u0tLS8+dOyclJWVrawtPI57RyN3G4XC6urpoNJqamhpevPHi6u3tXVdX9+LFi6amJqD8586dMzAwGDduXFxc3G+//QYHgL169YrH47HZ7OzsbHFx8eDgYD6f397eLiwsPHfuXIIgIPqYz+dfvHgRx2DxeLz379+bm5tPnjwZJjXw8AoLCzs4OICSjY2Nhw4dyszM7OjoKCoqCgoKYjAYERERuC1/SLB4PF5bW5uxsbGcnFxhYSFYH2/cuCEqKgphOnBAhpWVFY1GAzVqa2tFRETgsYRJnMvl/vrrrwghJpNJEERKSgpCCJTExylt27aNHOT+8uXLyZMnm5ubA8FisVjBwcEIoXv37mH8McHq7e1VUVEZPXr0s2fPqqqqampqnj17Nnv2bBUVFYibwVMzZo2tra3Hjx//9ddfOzs7i4uLo6KiREVFfXx8oK/nz58vLS0dEBBw586dsrKyhoYGsD5i3HBfZ2VlIYQMDQ2vX7+ek5Pz5s2bR48elZWVdXd3a2lpSUlJPXjwoKampqysrK2tbcyYMWpqavn5+fACU1dXV11d/erVq6KiIoTQzp074RymY8eOMRiM5ORkeACfPXs2YsQIWFG4XC5ZB4ACusDc3FxaWhoYDJfLZbFYqqqq4uLi0GXgrKTT6UA+BKZmkANtj4uLQwgZGRmlp6dnZWUFBwdPmDABJmWAkcPhDAwM6OjoTJs2DZTp6uqSkZGZPXs2GEUaGhpGjhw5d+5ceCMHe+Hy5cvnzJkD8yCbzU5OTkYIhYeHwy0sFmvRokViYmJgMAZ9Hj9+DA6FhoaGqqqq2traO3fu4BcGvEZCYdDt9OnTu3btamxs5PP5paWlo0eP3r17NzR2MG5w46f+DkGwMNUuLy+fMGHCsmXLOjs7ORwOi8VycnISFhYGuP6s/sBC5s2bp6ysXFtbCwLj4+OFhYXhnYrNZqenp4uIiIAZDzaoW1lZSUhIAMHicDjAWpYuXUoQRE9PD5fLDQwMBBsGzLcNDQ2zZ89esWJFW1sbYGJgYCApKfn27Vs2mw1TTVFRkbCw8LZt2wiCyMz8d3SspaVlfX19RUVFfX399evXEUKenp5sNvvKlSs0Gg3e6ODVfTCe0FPHjx9HCOXn58MOFcjU19cXExMbGBjgcrl1dXVABQYGBvBrFVkaRDHX19erqamtW7eupaUFAIeBBE9TdXU1UFKgLw8fPrS1tX394ZeTkwMmxmfPnkEP1tfX6+rqamlpvX37FgKGOBwOl8t99eqVsrKysbExECwWixUREYEQgkkA5rTRo0fLyMiQ1YPJ2dbWtr6+vqys7PXr1/Cukpqa+ttvvwHHTUlJycvLa25uvn//PlhzBYYl+F5v3bpFp9PBZNDf3z9x4kQlJSVYMqqqql6+fAnTIHlaA/MVn8+3sbGh0WjgY2Wz2c3NzY6OjhkZGe3t7XJycitXroQFHV4SsrOzwSDK5/N7e3s9PDzodDpY3VgsVnl5+fjx4w0NDSEiZdmyZfLy8tHR0Xfv3i0rK8vMzKyoqIBRunnz5uHDh0OL4CE1Nja+cuVKfn5+XV1dSkoKVFpSUgLvigMDA52dncAc7OzswFxXW1tbXFzc2tq6YcMGGo325MkTDodTWFg4duzY1atXQ7A4i8U6evQonU6Hw+cGBgaOHz8uKiqam5sLqz8ZE3LvfFn6SwgWzHrV1dXg2UEIjRw58scffwR0CIIAgnXp0iXc9729vUZGRtLS0jk5OQMDA1u2bJGSkrp9+zZ0Kkz6Li4uCCF4bYIhnpqayuPx4PVx+vTpc+fOBYtrRkaGuLj45s2b8RTJ5/PBAgFDqre3NywsDJwIYInFBzVhlQAvmDrT0tIYDEZoaCgZxPHjx9PpdHJOZ2engYHB2LFjB79jkW1Uv/76q729/YkTJ/bs2SMsLAzbqYawOnZ3d9Pp9A0bNsBQ43K57969s7KyEhYW3rp1K5PJDAwMPHXq1M6dO3V0dAoKCm7fvj158mSwqaqrq8fGxuJQxOzsbDExseDgYGAhwsLCe/bswcGefD4/OjqaRqPh2Jfe3l4ywSII4rfffsMEC8BhsViJiYkHDhxwdHTctGkTjUYjv/7+IcEiCKKzs9PQ0FBVVRVHAqWnp9PpdLDDAZ3dunUrjUYD78C//vUvBoPh7OwMSxRgW1RUJCQkBKGpR44cYTAYZ86cgbUf+K65ubmioiK4ESHQgUywCIIAq+dHCda7d++EhYVlZWWjoqKCgoIiIiICAgJWr16NDWACwwYeQiCsgIy1tTWNRvP09AS07927N336dOij7777ztPTs6+vT0AI/NvR0eHi4sJgMBBCwsLCa9asycrKAmOkkpLSuHHjXF1dY2JigoKCYmNjwYgdGho6fPjwefPmBQUFhYWFRURE+Pj4aGlpnTx5kiCI/v5+e3t7aWnp3NxcqOLFixfy8vIbN26EVUFADVg14WVUSkoKCBaUUVFRGT9+PB66MJl+9N0XHhN4HiEGi2z6NTAwAOsa4AZ/NTU1McECC5aCggLE4YHJbdeuXdgJ9f79ezLBIgiCTLBgYdbR0Rk/fnxrayuPx4MQqICAADqdrq+vHxYWFhwcHBUVdfLkyblz58KTTuaIMNKKi4uN1hmBMYnL5RYVFY0ePRqO54FJD9Nr8rTwqfRnEqwxY8bY2NhgQgAvruB2+Xz9sWJNTU1z5sxZunQpDtc7c+YMQigpKQn0zMrKwgQL7rKwsMAEiyCIqqoqGo1mYmLC5XIhGBQenJSUFAChtbV19uzZ+vr67e3tME7I5mR4iR0YGJCQkPjuu+8IgnBzc6PT6YaGhpGRkcHBwTExMY6Ojtra2uAUA1YRFxcH+8LwZEVGFaaIpUuXCgkJNTQ04AHJ5XJ//PFHMTExGF1NTU0yMjImJiZ4SJOFYIhaWlpmzpxpbGwMq4mpqamQkND+/fuZTGZwcHB4ePj+/fu1tLRycnKgdbW1tX5+focPH3ZwcNDS0ho5ciQ8WXw+v6WlZeHChXPnzu3s7CTXVV9fr6ysbGRkhC1Y4eHhmGABjAoKCrq6uhBbAodpHThwgE6nr1+/Hp53JpNpb2+vo6OTnJw8MDBgZ2dHo9EQQmJiYqamprAbd/CzDDlpaWmYYPH5/KtXr8IBbOBGj42NHYwzDNfm5uZFixbJy8sXFBTA6ozblZycDIEQZB5cX18vJCQEFIrNZru6uoqLi9+7dw/UqK6uVlRUhKsEQaSmpuJZUVVVNTQ0tLu7G+q1tLTEFiyCIHbt2gWNFRcX//777x89egRqAPsHMxhMI69fvzYwMBAREbG1tYW18vTp05aWlkuWLIE4s6KiIjk5uV27duEY3xMnTiCEcFS3o6OjqKioQOAdbvVfTHwJwYJHqKmpqbKyMiEhwdbW9vvvv4cICXiqgWBdvnwZK9fX17d+/Xrw3LPZ7E2bNmGChXfUgz8rPj4enklwuID1hc/nT58+XUtLCwJQ0tLSxMXFt23bRiZY4L8AC1Zvb++rV69u377t4uKyefPmUaNGIYQgshvfArrBJHvr1i0GgwG8pL29vbCwkM/njxkzhsFgkEcweKbk5eXBbQEjA+RApE5DQ8PGjRslJCT27t376NGjO3fujBo1CgfJYjQEEt3d3cLCwlZWVmDV4PP53d3dmzdvhsi7wsLC/A+/wsLCt2/fdnd3v3//Pjs728vLa/v27TNnzkQIhYaGQkPIBOvatWsfDuDZj40WfD4/NjYWB7nD6kUmWHDKiJCQEHiFuFzu8+fP9fT05OXlXV1dCwoKLl26BKY+3ATYRQhbw8hYQQF4BoBgqampNTQ0QH56erqQkJCXlxeYWAiCsLGxQQjBuTXx8fEiIiIfJVjg9jp48CC4dMkEy8LCYuLEieCKGkyw+Hx+UFAQQuju3btY+ZkzZ6qqqhIE0d7ePmzYsHHjxpWXl+fm5j558uTp06dlZWVgZBZoF2wRLS8vX716tays7PHjx/Pz85OTk8XFxV1dXWFU1NfXFxcXh4SE7NixY+7cuQihH3/8EdstsALg7qyurv7ll1+OHz9uYWEhKSk5fPjwwsLCV69eTZgwYc6cObdv3y4sLHzy5El+fj7s/09MTBQREdmyZUteXl5BQcGTDz9MXtls9sGDB0eMGAGLNEEQJSUlI0eO/EOCZWZmNphgKSkp4fXs0aNHCKE/tGDhYxr6+/vZbDaLxUpJScGuPbzOkQkWQRDnzp2j0+mWlpYeHh6GhoYLFiwoKSkBqgR7VsgEi8/ngxUzLCwMpgiCIMAkBqsmDLzQ0FA6nW5ra5ufnw8oFRQUgPcNdwEkYFrw9PTU0NAICAhwd3f39fXdvXu3hITEnDlzjh8/fu/ePXySgsB4EBCF/4WRkJSURA5yF7haUVExevTo7du341gLmAah7z5ff4xqc3PznDlzli9fjrfXJCQkDEGwuFzupk2bJCQk2traALSqqiohISELCwu8nRZc1TjioqWlZfbs2cuWLcMWLCBY2KACT6WEhMSoUaP4fL6Xl5ewsPDRo0ehC/Lz8wsKCl6/fg34XLlyBbsXyRtyMVCYLS1evFhISKi+vh4PSCBYDAYDoiDevHnzBQTLwsJCSEjo3Llzz58/Bw2Lioqwo/zSpUsjR47U0tI6e/ZscXHxvn37JCUlwbgFDv1FixZpaWl1dXWxWKySkhIIcqitrSUTLD6fDxYsCO0CnBUUFMBMCC+QHA7n8OHDwsLCnp6eBQUFeXl5+fn5T58+bWpq4nA4LS0t1dXVENC5YcMGUVFRBQUFbEgjYwWDk0yweDxebW1tbm6ur6/vtm3bVFRUcBAeeTUErZqbmxcuXAhBC9imBew/JSVFXFx83759eKzy+fy6ujoajQYH2gHBkpCQePjwIahUXV09YcKE1atXw2v2+/fvnzx54uvru3PnztmzZyOE3NzcgK6RCVZra+u7d+8uX7584sQJY2NjCQkJhBBM7BUVFTQaDbyBtbW1+fn57e3tEF2QmJgIa2VeXl5RUVFnZydAUVJSIicnB94S0MrBwYFMsJycnIBg8fn8V69ewRaxz3zGych/NP0lBAveJ+zt7cHFCy/NwNBhrxAQrFu3bnG5XHiV5HA4SkpKMjIyMP1FRkbiCKru7m7wwsC7SEZGBpvNBgvWjRs34C2Ky+VOnz5dW1u7ubkZHFsMBmPevHlgugBuAQFPiYmJHA4nLy9v/fr1uMEPHjyYMmWKjY3NYNce3JuamioiIhIaGgpeNjs7OzabvW7dOoRQa2srGJZgqVBSUlJUVATeTe4DkAMhStivXFFRMWrUqL179xYUFODNQeS7YMXt6OhgMBhbtmxhffiB2jExMRgifEtVVdXly5c9PDzwPsHS0tIlS5bo6+vDc/jo0SNxcfGgoCA2m/327VvYagAzFGh44cIFsGDBv/39/Zs2bZo6dWpVVRW801y6dAks9vAv7NrA3us7d+6IiIiEhITk5OTk5uay2ewLFy4ICwvfuHHjo5MjvJ+1t7d///33M2fOrKmpASv97du36XQ6+AhgGd66dStCqLKyksvlVlVVIYSAEIB/ls/np6b++5wzZ2dngiBgUgZTSnd3N0wTW7duhWh9mLhra2u/++47CwsLcL4QBLFz506EEHwZA9YADQ2NmTNnslisnp4eLS2tUaNG4bccGCp5uXmQg7sAD6Fdu3bhI+wJgsjJyREXF/f09MzJyQFWCkOdIIi6ujorKyspKSnsFoQuBj2vXbuGN5YTBHHz5k05OTl3d3eCIDQ1NcePH4/lwF3p6elgY8ChfpAP3h8+n9/T02Nvbz9ixIgnT54A7y8qKlJQULCwsMBgwi34L7h1NmzYICMjA6cZgYtwxowZU6ZMAY8G7OIBgoWDb7AESICrEQiWra0tnIMFEze5JDxQ2traysrKEDPAZrODg4N37twZGBh4+PDhiIgIaPXAwADc3tnZuWrVKk1NzYaGBhi6ERERNBotOjqaw+FAFIGOjo6ysnJzczN+Oy8tLUUIbd26lVx7V1cXkGyyYtC/FRUV8fHxTCYzMjIyKirK1tZWVFR06dKl/v7+jx8/hv4iM0WyWIE0eN9YLBYOcmexWF1dXbhSoOllZWVjxozZsWMHbCJjs9lgwcrPz+dwOJ+vPxAs2OWgqam5YsUKbMkDgnXx4kV4TO7duycuLh4QEIA7UV1dHRx8MA+Xl5fT6XQYLfBwAcGC2ZjL5TY1NYEFC7aAEARhYGAgIyPT0tICzl/wM4qIiKxZswYiYhFC5OAb2Ntx584dDocDFqyEhASYYAVghH9hxQFPEFhWYCQTBAFRz42NjVwut76+HvZhsFgs8oOMZcJ09ObNG3V1dWNj49bWVnCAIoR++eUXXIwgiMLCwob6hvb2dmlpaWVlZXzYiqOjo6ys7N0Pv5qamo6OjgULFmhra/f29jY1NTk7O8OrZm1traqqqrGx8fv372G47t+/n0ajZWZmgmeNw+GMGjVKX18fnFxQNUS5QOghVqampqa0tBSoFc68evXq8OHD4VwhGJb4EuxgTUtLExYWhtWwu7tbX18fF3jx4sWiRYtmzZoF6wJ5ZgNRW7ZsQQgBScIBMxkZGeXl5SNGjNDX14dwFJjwwYu9Z88eeF0/efLk8OHDHzx4AHaTysrKSZMmGRgYsNnssrIyR0dH+NIGQRAvX75ctWqVrq5uVVUVm822tLSUlJR8+fIlj8e7evUqTIBAPZOTk8eNGwfm+fLycoSQnZ0di8W6fv26ubk5QRD29vZCQkLkN2eCIPLz83///XcOh/P8+XN5efm9e/fCN6/YbLajoyNC6Pnz59A1J06cGDZs2LNnz/r7+xMSEuAFUgBVjN6fTXwJwQLKaWNjM2vWLGzZbm1tlZSUfPz4MUEQp0+fptPpe/fuxdrAQ47D0KqqqiZPnrxo0SJcoKCgYNiwYebm5mBrDQ0NZTAY+EWBIAgNDQ1NTU1cHpY3oPCQuXHjRjExMbBgPXjwYNiwYeTPzMGOJLw6Yjkw35WVlYmLi0Mo98WLF4ODgzkcDhyPQfZ0XLv270OkoO+hb0AOfqOysbEREhIC4y1BEPDW4ujoGBcX5+3tDV8JJA9orIaoqCgOKoQC1dXV06ZN09DQwPbYkpKSTZs2hYeHz5s3jxzkvnPnzh9++AFse0VFRcOHD4dAJYIg4KsdsGcVBuvatWslJCTAYQqr7549e6SlpbF796effiLHjc6aNUtSUhLHGwJHiY2N9ff3j4yMJAji2rVroqKiwMDw4oHbhXPWrFkza9Ys/OqTl5cnIiICJkMovHPnTgaDgScyMzMzOp2OY9gJgli8eLGcnBx4AJuamtTV1VVVVbHAioqK7777TllZGd4gORxOT0+PiorKihUrQD6LxZoyZQqdTsfxqgRBzJgxAyLqCIIAFzMOwwdP98qVK/t6//0VBdxrOIxPT09PREQER+VDhFxoaGhQUFB8fLy8vDx5RUlKSpKTkxMgWDCEfv755xEjRpBH8uzZs52cnMCfixDy9/fHeMbExJibm/f39wMfJbdl//79OHLu6NGj8vLyYA6E8FIZGRl88A9uCxYLCQsLi+HDh5Mzp02bpqioiHOKioqwJ508/nEBSFy8eJHBYOCHHdtlBYrNmTNnypQpkNnf329ra2thYfHy5Uv4YF9FRQV0LlTEZrNNTEwUFRVxiBVsm4CIexCycOHCSZMmkWvh8XirV69GCOFHgMfj7dixYwgvJ/n2qqoqaWlp2FEL+efPn1+7du0vv/yCBza5/EfTycnJDAaDPBigGLSrqalp/PjxO3fuxPd6eXkxGAz8jZfP1x+WBA6HM3fu3JUrV2KBSUlJDAYDB9GXl5fjmCeCIF69eoUQkpOTw/uHuru7wT6KJcDOAOym4fF4AlVA1DB52+COHTsQQuDsHhgYWLJkiZiYGN5U39fXt3nzZrA+QngGuBdwjQIJIEbPnj2Tk5PDWzfgYBqEELw5wy2SkpJmZmYCt+N/MUQaGhq4WFVVlbKy8owZM/Dk09HRoaqqWlJcAss5GGDgZXjixInjx49PTk4+duxYZmYml8s1NTWFUdfY2Ojg4ACOp56eHnV19cWLF+MaVVVV6XQ6xpAgCIHIerCjw2seHuQtLS0mJiYPHjz4+eef5eTkKisrcVumTZsGOwEFhiI83b/99puoqCiEgnR1dQ0bNgzQhtvd3NzgCxYsFos8GwAFT0tLk5KSsra2xnXBsH/37h3snsGmcYIgjIyMxMXFMW3y9PSUlpbG//b29o4dO9bIyAisferq6rC3FyQfP34cdmoTBGFtbT1s2DB4KM6ePTtixAjyDqdJkyZBY2tqasTExMDbk5GRAaH3Dx48UFBQADYPku/fv29kZAT8oaamZvTo0fv378fNcXZ2ZjAY2JIN5AS8Uh4eHnhxxOX/SuJLCBZMfJs3b2YwGLt27WIymWfOnIETGcDlfPr0aQaDoaKiYm9vn5CQcPjwYXl5+T179sCGeejR9PR0NTU1IyOjsLAwCIz44YcfXr582d7eHhISApuNdXR0Ll++nJ6eDqs+WDVevnwJjhVzc3NlZWU/Pz8mk7lv376pU6cihMaPHx8WFgYxHGvXrvXx8UlKStq9e/fSpUvBTz94YYBnwMTEREZGxsvLS1NTE8LhgSFNmTJlx44dkZGR9vb2U6dOtbe37+npERjT2D5/+fJlWVlZDQ2NqKioo0eP7tmzZ/78+aNHj1ZRUUlNTSWv07jPMjIyNm3ahBASFxc/cOAA2LpBpaysrBkzZujr64eHh/v5+a1evTouLq68vFxVVfW7777z9/e/dOmSq6vrrFmzrl69yuPxTp8+raurixCaNm0acMS3b9+uWbNmxowZgYGBsbGxe/bsUVRUhL33YWFhUEtaWpqMjIypqenPP/984sQJfX19CQkJBQWFI0eONDc3g1vt+++/ZzKZBw8e3LVr17Rp08aNG6elpfX8+XN/f/8ZM2aAXz8oKAiTD2gdhBqUlZUB90UIrVq16vHjx2lpaXA8xKRJkzw8PPLy8nx9fcXFxeGsipSUFA6H09TU9P3332tqanp4eAQHB69bt27mzJk4kA72M6urq69cuZLJZAYFBVlYWMDHKHV1de/cuQNNc3FxGTZs2IkTJxITE3ft2gXnDsyaNSsqKio/P9/e3h5ipA4cOAA97ubmNn78+N27dzOZzBMnTpiZmcG2aoExA/+Gh4eLioouW7YsNjbW3t5+9+7dampq48aNU1NTKy4ulpOTU1VVPXHixIULFwIDA3V0dGCCA8UAH5ADb/AWFhZBQUEXL160tLQ0NDQsKyuDYFVXV9cJEyb8+OOPp06dcnBwWL16NcQKtLe3r1+/ftq0ac7OzrGxsTt37rS0tGxpaWlra9uzZw/4xBcsWHD58uWioqIlS5YghKSlpY8ePQq2TzyAIVFVVeXi4gKflTQ3N3/x4kVBQQGcAALviw8fPjxz5gxsTxs7dqy3tzeE/eHZGdqVm5t74MAB6FxZWdnt27c7OjrCsgol4W9RUREG39bWFgrA2wuDwZD68INt/xCRCcLPnz8/fPhwGxubCxcuHDp0aMmSJcLCwoqKit7e3vfu3YMIToSQtbU1k8nEvo/GxkYDAwM1NTUPD4+oqKgtW7bs3r27o6MDx1ThJxGe4oGBAQ6HU15e7ubmBh+qk5WVtbCwgJFga2sLB0BgAMm3QxraWF9fb//hp6amBgzG1tZ2z549EAIIlrlr164tXboUITRixAgbG5uGhgZ/f3/YVb506dLTp0/z+fzP1B9elh49erR27VoI5jMxMSktLU1MTIQtVLMYwwI1AAAgAElEQVRnzw4JCenu7h4YGFi7dq2CggKc+WRubg5hMYaGhklJSZmZmSBBTEzM29s7Ozs7JCQEJMycOfPXX3/97bffgNqKioqamJgAn1BVVV2xYsWWLVs8PT3j4+N37NihoqJCXtGrq6v19PQ0NDR8fHwiIiI2btx46NChtra2uLg4cBWpq6s7OTnB+TjkZwTDC4BfuXJFRUXFzMwMTidRVVW1srICW93du3fhfVJUVHTfvn0Q1E8WhSEC/YcPH25ubg5+z4cPH86cOVNPTy84ODgkJGTTpk2urq48Hq+3t3f58uWwnyY8PHz37t0WFhYjRoxQUlKytraGeJgrV65AsOzGjRvhk0cw9ry8vOBwlsTExD179sAWVzjB5+7du/v27YPJx9bW9uLFi/D2C0fnaGtrz58/PzAwMCQkxNTUFKI1wOdjZWUVEhLy888/m5mZGRkZwVsxeShCOjw8HIauurp6XFxcb28vQmjhwoVubm6XLl1ydHScN28exOQJzGxYjZiYmFGjRq1fvz46OvrYsWMLFiwA9vz+/fuNGzeqq6s7OzuHhYVZWFhMnz4d4p57enoOHDgAkV7a2tr/+te/ysvLV65ciRASERE5ceLE9evXFy5cOGHCBD8/v0uXLrm7u6urq1+6dKm+vt7V1RWOdjIxMXn+/Dl8z23Dhg1RUVHnz5/ftm3bkiVLYN9JX18ffHfYwcFh/fr1cA4FxMJOmTJl7dq1MTExHh4eq1atOnv2LJfLvXjxIsx+I0eO3LFjR2Njo7e3t6ysLBxlAvte37x5M3HiRC0tLScnp8WLF8PnFvDMhkfglyW+hGBBr/zyyy+XLl06deqUt7d3eHh4aGgo9lsnJCQICQn5+PiEh4cHBAQ4O7tcuHABhh0Ye0D7goICNzc3V1dXZ2fnmJgYcAq0tbUlJib6+fnFxcV5eHjcunULHvLw8PDo6GhXV1dMbN+9excZGenq6hoSEhIXF5eenn7A7sDhw4dh42tAQEBaWpqbm1tYWFhAQAAc6/dRjGCqraqq8vDwOHToUGRkJBiNYLBmZGQ4ODi4uLicPHny559/Bs0How85bDb71q1bx44dc3FxcXR0LC4urqysdHV1DQwMBCPQ4Bshmio2NjY0NNTHxwd2U+OoqcLCQpcPPx8fH9hT2d7efvXq1bNnzwYFBYWEhHh7e4PpHoxJPj4+sbGxAQEBMLzgvSE0NNTNzS0kJOT06dNpN9MAJTjPArzsN2/edHR0DA0NZTKZ6ekZkZGRBw4cCAgIaGtrY7PZly9fPnjwoLu7u5OT05s3b3Jyco4dO3bq1Ck+n3/69OmAgICYmBhfX18YrIOf9urqag8Pj8jIyJiYGBcXl6KiogcPHpw8eTIuLi4wMJDJZJaUlMTHx0Pazc3t7t27MC22traGhYU5Ojq6uLj4+vpCxCJcAhifP3/u7u7u7e0dEhICJ9YeOHDA3t4emDSw8Pj4eFdX14iIiISEhKysO66urnZ2dhBO4efnF/Xh5+Pjgw93uHbt2uHDh728vIKCgvDxLQLDBmrv6+u7fv36oUOHTp486eTkVFtb+/TpUwcHh6ioKAi5yMjI8PX1DQwMDA4OBsOqgBzA6vnz51FRUdevX3d3d4fnBaxZUAuHw4EzY7y9vX18fMiRmG1tbcDjfX19Q0ND4fyq9+/f+/n5hYaGxsTEuLu737p1q6KiwsnJKT4+HhYPgeN2QYfXr19HRESEhIQwmcyTJ09WVVWVlpb6+voCPn5+fk+fPv3ll198fX2ZTKavr29cXBy8LeDxDHJevHjh4+MD8fihoaGgCTlqHspXVFT4+fmBG87b27upqam6utrQ0NDGxiYqKio6OjoyMtLT03PmzJkSEhL379+HUcpisa5evQoze1xcXGZmVmBgkJ2dXXR0TH5+fkRERFhYWExMjJeX1+XLl8krx5s3b4KDg48fP+7r6xsZGQm2XvJAJfcLBoTJZHp7e8fGxsIbIJzuUVdXd+rUqaEPfYY2NjU1+X34BQQEMJnM0NBQb29vNzc3eFEGn1dGRoa7u3tMTEx4eLiHh0dzc/Pp06f9/f2h6mvXroGoz9EfnovCwkJ3d3foNVdX199//z05OdnT0zMuLs7X1zcxMRHMVLW1tb6+vp6enhERERcvXrx169bhw4ePHTuWmpr6+PFjd3d3eGlhMpnPnz9PTEz09/ePi4s7efLk3bt38/PzPT09oQoXFxcwDV64cKG+vv7evXsBAQHQTHh2AExoRV1dXUBAgIODg6+vb0xMDERZXLlyxcfHh8lk+vv7h4WFAbCf6hpoY15enrOzs5OT08mTJ+Pj43GUZG5uroeHB/SXp6cnGM/gFuhfDBHoD2vH/fv3oS9KSkpcXV1PnDgRHByMz2ohCKK2tjYyMtLOzg6A7evru3DhwrFjx/Bemd7e3gsXLuzfv9/NzQ0sN/BodHV1nT592tXVNTw8PCEh4c6dO25ubnZ2domJiXl5ef7+/lFRUREREV5eXvhDrgBUZWWll5eXo6Ojj4/PmTNnwK6Wk5PDZDKvXr3q4eERERERGBiIp6zBo/f8+fN+fn4xMTH+/v5Xrlxhs9kBAQGZmZnwshoYGIg3OZHvhTT2xsDC5+bm5uHhQV49Ozo6YmNjHR0dXV1dPT098Zev+vr6AgICQkJCYmNjPT09r1+//urVKycnp7i4uODg4NDQ0Ozs7PT09DNnzoSGhgYHB/v4+ODzrmH3A0w+lZWVcMbpmTNnYGbz8fGBE78An8bGxoCAgL1790ZFReETuQmCePDgwYkTJ9zd3X18fG7evAmj6ObNmx4eHjExMWFhYZ6eni0tLUwmMyAgIDY21tvbG3+r8f79+4cOHXJwcIB3+E+NwMFw/WHOlxAsAaFkbSB0A045gnFDvvqpNBZILoAzP5r4/JLw/AyOLyaLhZ4j50Aa14ITmOMPLkzOIc/v5Pw/lcaVfko97CD7lFgs4VMFyPlD6zz0VbKcr5L+qOZkHT5a4K9UjWcWbI8U8Oh9SjhZq8FlsId38KXBOVgUtI7c73ipgEsfbf5HMwfX8g3mODo6SkhICChWXl4+derU3bt34zB2KIChECj/0X8/igkZ2I/e9e1kfnX9cdv/FIyfDwhWmCwfZ36+nI+WxDJxK8hP60dv+ZxMLFagMLkWgUufqnfoWwYLEcj5HKCwtp9TWEA+no6Gvhe3AheDHJxPFov1IWcOncZqDF2MPAODJh9VgNwXWOE/lIwLCMj8AglY1ODElxMseK3E4AJth39PnTpFp9Px6ZewFUigGcB4YEUBbwhGEMIAIYwR7LrkHCwH7zAiB3ngjwCSP9yIMwe3H+fg8th6BFwKzjXB2uLa8Y3kBAQYAgh4/y1E+n/qRhwwCO0V6F24F2sCQsh4ktP4eBgcvgojDy/b2EdJBgR0BsmANtwFOMBV0Aquwr0gk9wvuBYyIKA5NA03kNxkiIUky8EjCoqBNDjqTEAybgUoCVdxEzBokI/bDh1N1oG8OwzvacXCBSrF/w6NDDwd5CH9qQGAhxbWHyOAm4DxJ18i44PVhkcJAw5o4H8xCLgVkIC24GLg6sX/Aj4fHV0CcgRQhXE4uOECxXg8HnwRGSI4scwbN26MHDkSYi8gohaPMdzLuDc/OoRAlMBEIfCI4erICQFA8AgBzT9TAhlAnCbfS4YUNxCXxH39+foLAAuDEAvE6AkIhGEGkJIlfPTZJBfA+1qgJO4UPGeSIRWoFE84ZPUGDxWyBEjjKRE0wbcIKIbRE5AgUAx3BxYLXU8Wi3GDxwdah+VDecAQS8NPLtSOnzs8XHGrB89sWBPyugZdSdaEXJdAGwc/C7hegRVB4Eb8LzQKqhCYCcmwC3S0QKPIcxGMELJYGA/wWktWGJAXGC0Ybbw2wbSJuwn+hWJYMmSStQIFcA4Wixd9jDCG4i8mvpxg4Yrxqz9ouXPnznHjxiGEpk6devjwYTIE+BZyAt9Ozvz8NPn2wXWRr36OzMES4K5P5X9K5p8t/yk5Q9f++bWQcfjoXQIFBMqQrw6t6le/+odVY1U/VZKcjwsPrednFoNpdOjCQ18lq0HWk5wP6U/JGfquwXK+zRzYOLxgwQI9Pb1du3bt3r174cKFurq6bm5uAvFS5PaS03/Yrj9V+A+l/f0Fvrr+ZIGfGl1f0MyhRZEr/QLh+Jaha8HF/mziU2L/UO1PFSDnk9N/qNinCn8q/w8FQoFPNfBTt3+q/N+jxhC1DKHYp9oydP4QdQ1949BXvwLBwhVAm9PS0m7cuHH//v3U1FS8vR+XoRIUAhQCFAIfReDRo0fwedf4+PgzZ87gkxE+NZl+VAiVSSFAIUAh8I0g8DUJ1jfSJEoNCgEKgX8WAkNQqCEu/bPaSGlLIUAh8L+GwNcnWODuBcftEH7i/zWgqfZSCFAIDIEABH6RoyI+epLCEBKoSxQCFAIUAt8UAl+fYH1TzaOUoRCgEKAQoBCgEKAQoBD4+xGgCNa/MYcAt/+WM+Iv1v7fvf0/N2T/Q1GH/zmFKckUAhQCFAIUAhQCGAGKYP0biv8WtcLcDvfHn038dRbyX2z7EI396+0aQjh1iUKAQoBCgEKAQuA/jcD/IsESWLz5fH53d3dtbe2LFy/wqaT/adyxfB6P197eXllZCV8f+3y6AyVZLFZbW1tJSUlTU9Pn34tr5/P5XV1dFRUVVVVVEP7yBUKwtK+Y6O/vb2pqKikpwZ9B/IrCKVEUAhQCFAIUAhQC/2kE/hcJFsYUKEVxcfHSpUslJSWVlJTg271/J8m4cuXKxIkTpaSk4FuVn3/QGXx6KCgoaOzYsZKSkl5eXvhjRLiBQySg7QUFBZMnTx4+fLihoeGbN2/gGLch7vobLsHGCGtraxkZGUlJSYGv3P8NClBVUAhQCFAIUAhQCPx1BP6HCBY2XCUmJsKXxYBk9PX1dXR06OjoCAsLNzQ0/M0ew4GBAfigvZ6eHvnLAJ/TtVwuF76Ix2AwDh8+DOTs89khnHhbWlqqrq6uoaHR3Nz8LRAswL+rq8vLywshBJ/7/Rw0qDIUAhQCFAIUAhQC3w4C/0MEC0wjTU1NRkZGFy5cwGwGDD8WFhZSUlL19fV/M8EiCILNZo8ZM0ZfXx+r9JnjAwhiVVWVvLz8oUOH/izBAio2MDBgZGSkrq4OH4f/Fk7WAMVu3LghJCSEP7j0mZhQxSgEKAQoBCgEKAS+BQT+hwgW0JE7d+7MmDEDPpoNCznEXZmbm0tJSYEF62/umL6+PkywPt/+BB9aIgiioqLirxCsnp6edevWqaurv3379m9u+NDVpaSkfAHB+lMADq0AdZVCgEKAQoBCgELgixH4SwSLz+ezWKyXL1+mpaUVFxd/NEK8s7MzJyfnxo0b5eXlYCvq7+9//fp1ZWVlR0dHX19fTU1Ne3s7thvx+fyOjo7s7Oxbt27V1dUBK4Lm9fX1vXjx4saNG9nZ2c3NzdXV1bCacjic33//PT09/e7du/X19cXFxVgaxgWOP62trV21ahVCKDk5ubu7u7Ozk8fjYYIlLS1dX1/f19dXVlZWW1s7OByqp6fnyZMnKSkpOCQcyycn2Gx2d3d33//9BgYGenp6+vv7e3t7+/v7CYLo7e3t6+uDvwRBYILF4/EgYr21tXUwUejq6srLy0tNTf39999BNwDnowRrYGCgrKwsOTm5qKgIYCdrCGmoAggWuAh7e3tfvHhRW1s72I7V19dXXFycmppaXFxM/hZ6V1dXQ0PDixcv3r9/Dxa49vb2ly9fQsw+j8fr7e2tq6urqqrq6enp6uqqqanp6urCynA4nMbGxqdPn7a2trJYrGfPnkVHR1dWVhIEMZhg8Xi8zs7Ohw8fZmVltbS0CEDU1dX19OnTlJSU0tLS8vJy8Hjy+fzW1tbs7OyUlJSampqioiLoAqwAlaAQoBCgEKAQoBD4TyDwhQQLW4O0tbW3bdtmb28vJyenp6dHtgDxeDwmkyktLW1sbHz8+HENDY1ly5a1tbXt37+fTqfLycnZ29ubmpoihFRUVCoqKmB59vX1VVJS2rFjx+HDhydNmvTTTz/Byl1TU7N48WJra+vo6Oh9+/ZJSkpu374dbrG2tl6+fHlUVJSzs/OYMWMmT54MfjeMF2j74MEDHR0dKSkphNDEiROnT5++evXqly9fQjEzMzNZWdmoqChTU9P169ePHDnSzMysra0NC0lNTZ0wYcL69esdHBzGjRu3bds2+JY4eZkH3pOcnDx16lRpaWlxcfG1a9e6urqqqKhISUlJSkoeOXKEIIgVK1ZISkrKyclZWVkBwRo/fryuri6TydTX1zc0NJSWlvbx8QFSCHFRTCZTSUlp8+bNJ06cmDZtmpWVVUtLC+gmQLAIgnjx4sWsWbMWLVrk7u6upqa2aNGi9vZ2sp5wI+R0d3evXbtWR0cnICDghx9+WLVq1dixY21tbaHt0KLbt29PnjzZyMjI2dlZU1Nz0aJFtbW1IERTU5NGo9Hp9GvXrhEEkZWVJSoqihBSV1cnCKKxsXHjxo00Gm3q1KlHjhwBdrt8+fKmpia4um7duqlTp65cuVJZWVlTU9PKykpVVdXMzIzFYqWmppItWP39/UeOHNHW1vbw8Fi2bNnw4cPDwsJ4PB6wvYcPH+rq6u7YsSMuLm7r1q0IIT8/P4IgoqOj586d6+DgEB4ebmhoiBB6+vQp7lMqQSFAIUAhQCFAIfAfQuALCRZYfX744QeEUElJCSyWCgoKpqamfX19oGtkZCRCKCIiAv719fVFCEHw0+3btxFCqqqqv/76a1hYmKys7N27d7lc7vHjx8XExPAnoltaWpSUlLZv387n8y0tLU1MTLAxJigoaMeOHTwe7+rVqxISEhid3NxcJSUlIF4ClAJoloeHB0IIgtzhLmzBQggtWrQIiEV6ejqNRvPy8oIyeXl5CCE7Ozv499mzZwwG4+TJkwJMDrvtWltbtbW15eTkgDjW1dWNHz/e0NAQ236mTp2Khff19Y0bN05UVHT//v0g38HBQUxM7O7du8CuIiIi6HT61atX4WpXV9ecOXNMTU17enoIgigvLye7CN++faugoAAh8wRBsFisMWPGGBsbDw5gB3w6OjpsbGwQQjY2NmBbgo47f/48VJeamioiIhIWFgb/EgRhbGw8c+ZM7FL09PRkMBjXr1+HAu3t7QoKCpqamtALBEGAwHnz5j1+/Hj//v3Tpk0rLi7m8XimpqYyMjLl5eUEQVy4cIFGo124cKGqqio/P5/H4/36669kglVRUYEQsrCwgFp8fX3pdHpqaipYBA0NDYGtwlUjIyMPDw82mz1q1CgYcpCvqKiYnZ0NaeovhQCFAIUAhQCFwH8OgS8kWEB0PDw8NDQ0KisrYSOetra2rKwsWFZ6enpkZGQmTZoE6zqfz09KSlq6dGl5eTmPx3v27BlCyMjIiCCIzs7Od+/eEQSRl5cnJSUFdilwLREE4erqihCqrKzcsmWLjIxMSkpKS0tLZ2dnXV1deHg4l8s9deoUQigyMvL169cdHR21tbWenp7wXTMB1AYGBng8nrOzM0IoPT2dy+UODAyAl5MgCEtLS4QQrL48Hq+hoUFMTGzz5s3AEpYtWyYtLf38+fOenp7e3t7Xr1/Pnz9fTU0NNuIJMDlgUU5OTuCLhDKqqqoMBqOuro4giIGBAQ0Nje7ubvDE9ff3K3z49ff3czgcHo8HbCMuLo4giJKSkrFjx65Zs4bP58O2QT6fHx0djRC6ePEiQRClpaVAsKDe/fv3I4SuX7/e83+/Xbt2CQsLd3V1CTj+QO2uri4LCwtxcfE7d+4AbqmpqTQazdPTE/ji/PnzFRUVGxsboSE8Hi83N5fBYLi6ukLnxsbG0ul0HNYG5HLmzJnQUh6Pd+XKFYTQiRMnCIJob2/v7u4mCOL169fTpk1bs2ZNe3s7l8t9/fo1NBMTdAEXYXNzs6am5rFjx/h8fmdnZ0ZGBo1Gc3BwIAiiubl5xYoVU6ZMycvLa21t7e7uPnPmzJ07d2praxUUFIyMjCorK9va2np7e729vaurqwUGBvUvhQCFAIUAhQCFwFdH4AsJFtaDzWZnZmb6+Pi4u7tPnjxZSkoKjlO6c+eOiIiIkZHRRxf1goICWHH5fP7Ahx+fzz937hyNRvPz8wPSw2az+Xz++fPnhYSEoqOjU1JSJk6ciBAaNmyYvr7+hQsXgB80NzdPnToVffhpa2t7eXnB+i1AesCWQxCEi4sLQigzMxM3ASxYmzZtQgi9evUKSMO7d+/ExMQ2bdoELrzhw4dLSUkxmcyYmJhTp05FRETo6OjMnz8fjsEUaCOYyrKzsyUlJS0tLQmCOHfunI6OjpCQ0Llz5wiCSEhIcHFxwdasvr6+UaNGzZgxg8/nA3O9fv06QigmJoYgiNTUVITQ/v37ASvw2aWlpdHp9KNHj/L5/MrKSjk5uUOHDgE10dbWBtvb6dOn4+LimEymqanpuHHjysrKBELTAKLu7m4TE5MpU6ZA5BO4+Wg0GvCnwsLCMWPGrFu3Dhy1cNRFd3e3lJTU4sWLoeFxcXGYYBEE0dbWpqWlhQkWQRDnz59HCJ06dQqawOVy+Xz++/fvtbW1NTU1Yffi8+fPxcTEfvrpJ2C9H43BIgiitrY2ISHBx8dn8+bNCCFbW1voxzNnzoDzV0ZGZu3atbdu3YJeAHKJEJo0adLWrVtLS0txvw8eIfgSlaAQoBCgEKAQoBD4iwh8IcGClTU7O/u7775TUFA4efJkaWnpwoULxcXF4azOS5cuCQsLb926FS9jvA8/UPfJkycIIfCRsT78sCMpJCQEyBAQrIsXLwoLC+/bt6+3t/fRo0fW1taqqqoKCgoIoX379kHJuro6Ozu7efPmjRs3DiE0a9as3t5eICJkdIBIAcHKyMggCKKpqamnpwc4zcaNGz9FsLq6uuh0+siRI+/du5ecnHzjxo3U1NTbt2+Dbws3ENeF7WerVq0SFxfv6OgwMTFJT0/X1NRcsGABQRAGBgYFBQW4fF9f3+jRo7W0tLBPjUywkpKSEEIQvDUwMICDooYNG2ZhYTEwMFBdXT1y5EhMsFRUVGg0WlJSUlpa2o0Pv7S0tIKCAiAcuFJMtrq7u42NjZWVlXFYVVZWFo1Gc3FxIQgiPT1dWlrazMwMqCQ0rb+/X1paevTo0aDMHxKsc+fO4ROtAG3oi+PHjyOEwsPDnzx5smXLllGjRpEDpAQsWL29vWB9XLVqVUZGBpA2GAPA0dPS0oyMjGbOnCkrK4sQCg4O7uvr6+npiY6O1tfXnzp1qri4ODZSknGg0hQCFAIUAhQCFAJfHYEvIViwVFdVVU2aNGnKlCmwB5AgCENDQxERkbdv32ZnZ798+VJISAj4BKZWPB4Ptv7l5+cjhLy9vSFIGRbdGzduIIScnZ2BNsFuLyaTiRC6dOnSkSNHYGMaRB2tW7cOIdTV1ZWSknLlyhXApamp6dChQyNGjABDkQDHgkXd2dmZRqOlpaXxeLzdu3dD5BCPxwMLFqjH5XLBgmVpaQnbDNXV1UeOHCmAfktLC2gukA/68/n8wMBABoPh5ORkaWnZ2Njo4eFBo9Fu3ry5atUqMAgBy+np6Rk7duzcuXN5PB6bzebxeJhg8Xi8+/fvI4S2bdtGEET/hx9BENeuXcNOt4qKCjk5ucOHD/f29hIEsWXLFoSQgCOstbUVu96wtmCO6uzsNDExUVZWrqmpgZzMzEywYPF4vMbGRkVFxQULFsCOS/BgNjY2Dhs2TE9PD8hlfHy8sLBwcnIyj8fj8/nNzc2KioqzZ88G4x+Pxzt79iwQLLxnE/pi9+7dGzduNDc3nz9//ubNm2HDAQjh8XiYYEGOn58fDl0Hz6mQkNCRI0d6enqSkpLOnj374sULaFp2dvbs2bOXLl16/fr1xMTEzs5OYK6XL18eNWrU1q1bwXY4mHFiZKgEhQCFAIUAhQCFwF9E4EsIFqyOEPzk6+sLGgwMDCgqKiL0b4EaGhodHR16enoQPoVVDAsLgzjukpISISGhwMBAuAT2sOrq6lmzZmloaGDGxufzDQwMJCUlW1pa9PT0fvrpJyzq6dOnwsLCzc3NQUFBampqmOj09vZOnDjx9OnT2BqEbwG1/f39EUI3b94kCMLKygqfY2llZSUkJATRYBA8NGzYMBw3DRH6OGAf/GhGRkYdHR2w/ONaIAFesBcvXqipqSGEfHx8uFxuSUmJ/IdffHy8APmTl5efO3cuFnLr1i0hIaHExESCIOrr65cuXaqkpATxW1DGxsZm2LBhWVlZBEHU1NQoKCgcO3YMLv3yyy8MBmP37t1YWnNzs56eHuZPOB/b3kxNTVVUVDDsOTk5dDodduERBGFtbY0Qys3NxTcGBAQghABkgiCuXr2KEMI0Nzc3FyGkq6uLy1+7dk1ISAgHaWECqqen5+rqCnSHx+Nh0gnj4datW8LCwvCpnL6+Pi0tLXl5edh+SBAEGPZ8fX0fP36sra29ffv2gwcP4k0Gvr6+CxcuPH/+/OzZsyEQHpRZu3athYVFX19fZ2enj49PbGwsBgFrSyUoBCgEKAQoBCgE/joCX0KwOBwOn89/+PChoqLi1KlTr1+//uDBAzc3t/nz5yOEPD09TUxMOjs7a2pq1NTUtLW1MzMzHz9+HB8fr6OjU15eXlRUBL4hfX39W7duga0FCMedO3cgVubevXvZ2dkODg5jxowBWqOrqysrK3v+/PnCwsLc3NytW7caGxvz+XwgTGFhYfn5+QUFBX5+frNnz66trcUfxsEYgcUiJydHSkrKxsbm5s2bZmZmubm579+/z8zMnDRpEviVXr58WVpaGhISAoE7qampvb29LBZrw4YNkydPTkpKys/Pv3Xrlo2NDYSBY26HK4IE5K9fv57BYGB2smDBAhqNRjYv1dbW/vzzzwghUVHR27dv19TU5OXlAYDiDQQAAAeBSURBVKdZu3bt48ePCYIoKChQVlY2NTXNysp6/Pixp6fn2LFjfXx82Gz2ixcvwHE2d+7cq1evgpnKyclJXl4+ODg4Jyfn/v37hw8fxlsHsJLg7Gtrazt16pSioiKDwfDw8Hj16lVVVdWuXbsQQsuXL8/Kyuro6Hjz5s3ixYuXLl1648aN3Nzcc+fOTZkyBaLTQBocJW9kZPTs2bP79+/v2LFDSkpKWlo6Li6uvLy8sLAQrIPW1taZmZkQogfmSXd3d4TQmDFj4Fs9urq6Xl5eECb1+PFjMMWZm5v/9ttvLBbr4MGDQkJCe/fuzc3NvXbtmqWl5aRJkzQ1NT08PPbt27d27VolJaWMjIyioqJ79+4BdcvJyZGVlTU2Nn706FFxcXFaWtr06dOTkpIIgiguLoa4PQGmi/GhEhQCFAIUAhQCFAJ/BYEvIVh4w39qauqyZcuMjY3NzMx++umn2traffv2zZkzJy0tDXSqqKiwsrIyMDCwtrY2NjZ+/vw5QRAhISH6+vrm5uaGhoYQnATWJrBbFBYWmpiYrFmzZtOmTYaGhrdv3wY/mp2dnbu7+5YtW/Z9+FlZWcFSnZycvH///j179vz4448HDhwwNjYmf2fwo9DEx8cbGBjo6OhERkYSBFFZWWllZfX999+bm5svW7bswoULTCbzhx9+MDU1XbNmjaGhYVNTE5/P53A4Tk5OCxcu3L59+8aNG4H2gc4frQW8aUlJSYcOHQK7CzjLjh49CmH4cNeNGzdWrly5YcMGIyMjMzOztLQ0JycnAwMDCwuLJUuWHDlyBHhhRUXFxo0b4TACAwMDMBfx+fyYmJjly5dv2LBh3bp1a9asaWlpgfJxcXG6urpWVlbW1tZHjx7FEVdYVdD8+fPny5YtMzU1NTMz09XVTU5Ovnjx4oIFCzZt2rRq1art27fX1NTAjr89e/YYGBhs3rx55cqVoaGheAyAwGvXrq1YscLOzm7v3r2urq4+Pr7z5s0zMDBISEjw9/fX09OzsLBYtWqVpaVlYWEhxPK/efNm165d2traJiYmlpaW69evX7ZsmYiIiJKSUktLCxyaZWZmZmBgsHPnTthtevDgwUWLFm3evNnIyOjSpUuPHj1auHDhvn37SktLQ0NDd+/evWPHjkOHDllaWh48eLC3t/f33393dHTcs2fP1q1bjx8/bmRkFB4eDgpzudy9e/e6u7tjQKgEhQCFAIUAhQCFwFdE4AsJFvbFQJzNR/0sZPIB7jmBVXlwM3BYDD5jc7Cn7/379x+trvfDD2R+tMBHLw1RkqwejlsnCAIfRP4p2xX5xq+SxrCAcw1kkjMFasFWGazqEIUF7h38L7kfMTXEVZCpG3aw9vX1DwEsDAY7O7vx48cDgcOV5ufni4mJYRqE88kJCLcn55DT5LNhyZrjfHIm+UYqTSFAIUAhQCFAIfAVEfhyggXUBxZLsElAjDaLxcLLOQQ1w1o7MDAA+RwOB7Z9DQwMkAtDq7hcLovFgltYLBZeyEE4cBo4xwFWSrAtYa4zWOBgsCCQHE51An7AYrGwSlwul6whPjUAKgV9OBwOrnGwfHKOwEFZIJxcAI7jgtpBeTabDf/29/fjWshIAiwA0UdVheNP4V4ynuR6IQ2bDHB13A+//v5+rA/UArVD97FYLNjgSZaGAeFyuXAV+gvaSxaI49wPHDigrKwMp9RiUbW1tUJCQvHx8eQewWMMdy7nww9E4d7EV8lYQfNhqGAloToYfrhqKkEhQCFAIUAhQCHwFRH4SwQL9Bgc7SSg3x8WECgPpAeWdnwJ//spaZ/KxxLIiT9V+KvcSBbyxek/q/afLT+0YkNLg6vQTUOXhG0B1dXV69ev/+GHHzw9Pc+ePZuQkODm5qanp2dqaootYYP1AVMiHgy4AM4R2HOA9aEMVxgrKkEhQCFAIUAh8Dcg8BUI1t+gJVXF/2cIAO/p7OwMCQnZuHHjvn374MiG6OhobLT7/6zJVHMoBCgEKAQoBP6nEKAI1v9Ud39DjR3CpITNUd+QupQqFAIUAhQCFAIUAn8GAYpg/Rm0qLJfFQEctAcuxY9Gd33VCilhFAIUAhQCFAIUAn8TAhTB+puApqoZAoGhY7aGuJG6RCFAIUAhQCFAIfBtIkARrG+zXyitKAQoBCgEKAQoBCgE/sEIUATrH9x5lOoUAhQCFAIUAhQCFALfJgIUwfo2+4XSikKAQoBCgEKAQoBC4B+MAEWw/sGdR6lOIUAhQCFAIUAhQCHwbSJAEaxvs18orSgEKAQoBCgEKAQoBP7BCFAE6x/ceZTqFAIUAhQCFAIUAhQC3yYCFMH6NvuF0opCgEKAQoBCgEKAQuAfjABFsP7BnUepTiFAIUAhQCFAIUAh8G0iQBGsb7NfKK0oBCgEKAQoBCgEKAT+wQhQBOsf3HmU6hQCFAIUAhQCFAIUAt8mAhTB+jb7hdKKQoBCgEKAQoBCgELgH4wARbD+wZ1HqU4hQCFAIUAhQCFAIfBtIkARrG+zXyitKAQoBCgEKAQoBCgE/sEIUATrH9x5lOoUAhQCFAIUAhQCFALfJgL/D0YSvpNwsEo6AAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hourglass 모델과 Simplebaseline 모델의 구조는 다음과 같다.\n",
    "![image.png](attachment:image.png)\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZAKAR\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:461: UserWarning: To make it possible to preserve tf.data options across serialization boundaries, their implementation has moved to be part of the TensorFlow graph. As a consequence, the options value is in general no longer known at graph construction time. Invoking this method in graph mode retains the legacy behavior of the original implementation, but note that the returned value might not reflect the actual value of the options.\n",
      "  warnings.warn(\"To make it possible to preserve tf.data options across \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 10.5148659 epoch total loss 10.5148659\n",
      "Trained batch 2 batch loss 10.3108768 epoch total loss 10.4128714\n",
      "Trained batch 3 batch loss 9.89051056 epoch total loss 10.2387514\n",
      "Trained batch 4 batch loss 8.35937881 epoch total loss 9.76890755\n",
      "Trained batch 5 batch loss 8.43074799 epoch total loss 9.50127602\n",
      "Trained batch 6 batch loss 8.34402657 epoch total loss 9.3084\n",
      "Trained batch 7 batch loss 7.45185757 epoch total loss 9.04318\n",
      "Trained batch 8 batch loss 8.31870842 epoch total loss 8.95262146\n",
      "Trained batch 9 batch loss 7.89187 epoch total loss 8.83476\n",
      "Trained batch 10 batch loss 8.06293106 epoch total loss 8.7575779\n",
      "Trained batch 11 batch loss 8.0234108 epoch total loss 8.690835\n",
      "Trained batch 12 batch loss 7.67045 epoch total loss 8.60580254\n",
      "Trained batch 13 batch loss 7.56365108 epoch total loss 8.52563667\n",
      "Trained batch 14 batch loss 7.64938593 epoch total loss 8.46304798\n",
      "Trained batch 15 batch loss 7.35198212 epoch total loss 8.3889761\n",
      "Trained batch 16 batch loss 7.44528437 epoch total loss 8.32999611\n",
      "Trained batch 17 batch loss 7.18178 epoch total loss 8.26245403\n",
      "Trained batch 18 batch loss 6.61694908 epoch total loss 8.17103672\n",
      "Trained batch 19 batch loss 7.32177258 epoch total loss 8.12633896\n",
      "Trained batch 20 batch loss 7.68495226 epoch total loss 8.10426903\n",
      "Trained batch 21 batch loss 7.02174664 epoch total loss 8.05272102\n",
      "Trained batch 22 batch loss 7.4999361 epoch total loss 8.02759457\n",
      "Trained batch 23 batch loss 6.58795881 epoch total loss 7.96500111\n",
      "Trained batch 24 batch loss 6.12685537 epoch total loss 7.888412\n",
      "Trained batch 25 batch loss 6.68818188 epoch total loss 7.8404026\n",
      "Trained batch 26 batch loss 6.1751976 epoch total loss 7.7763567\n",
      "Trained batch 27 batch loss 7.08348656 epoch total loss 7.75069475\n",
      "Trained batch 28 batch loss 7.240592 epoch total loss 7.73247623\n",
      "Trained batch 29 batch loss 6.854249 epoch total loss 7.70219278\n",
      "Trained batch 30 batch loss 7.51455688 epoch total loss 7.69593811\n",
      "Trained batch 31 batch loss 7.45022774 epoch total loss 7.68801212\n",
      "Trained batch 32 batch loss 7.5138793 epoch total loss 7.68257046\n",
      "Trained batch 33 batch loss 7.38819 epoch total loss 7.67365\n",
      "Trained batch 34 batch loss 6.71551609 epoch total loss 7.64546919\n",
      "Trained batch 35 batch loss 7.25066853 epoch total loss 7.63418913\n",
      "Trained batch 36 batch loss 7.22000885 epoch total loss 7.622684\n",
      "Trained batch 37 batch loss 7.19470692 epoch total loss 7.61111689\n",
      "Trained batch 38 batch loss 6.48474789 epoch total loss 7.58147573\n",
      "Trained batch 39 batch loss 7.00862694 epoch total loss 7.56678724\n",
      "Trained batch 40 batch loss 7.24528694 epoch total loss 7.55875\n",
      "Trained batch 41 batch loss 7.08672428 epoch total loss 7.5472374\n",
      "Trained batch 42 batch loss 7.14750671 epoch total loss 7.53771973\n",
      "Trained batch 43 batch loss 7.03883648 epoch total loss 7.52611828\n",
      "Trained batch 44 batch loss 7.54294109 epoch total loss 7.5265\n",
      "Trained batch 45 batch loss 5.43303394 epoch total loss 7.47997904\n",
      "Trained batch 46 batch loss 6.90793657 epoch total loss 7.46754313\n",
      "Trained batch 47 batch loss 7.15742254 epoch total loss 7.46094465\n",
      "Trained batch 48 batch loss 7.09517097 epoch total loss 7.45332479\n",
      "Trained batch 49 batch loss 7.0519433 epoch total loss 7.44513321\n",
      "Trained batch 50 batch loss 5.77972317 epoch total loss 7.41182518\n",
      "Trained batch 51 batch loss 6.85957146 epoch total loss 7.40099621\n",
      "Trained batch 52 batch loss 7.28150511 epoch total loss 7.39869785\n",
      "Trained batch 53 batch loss 7.37367916 epoch total loss 7.39822626\n",
      "Trained batch 54 batch loss 7.27439117 epoch total loss 7.39593267\n",
      "Trained batch 55 batch loss 6.80487394 epoch total loss 7.3851862\n",
      "Trained batch 56 batch loss 6.65883541 epoch total loss 7.37221575\n",
      "Trained batch 57 batch loss 7.01506376 epoch total loss 7.36595\n",
      "Trained batch 58 batch loss 7.14243221 epoch total loss 7.36209631\n",
      "Trained batch 59 batch loss 6.98786068 epoch total loss 7.35575342\n",
      "Trained batch 60 batch loss 6.76719427 epoch total loss 7.34594393\n",
      "Trained batch 61 batch loss 6.95784473 epoch total loss 7.33958149\n",
      "Trained batch 62 batch loss 6.58048153 epoch total loss 7.32733774\n",
      "Trained batch 63 batch loss 6.85959625 epoch total loss 7.31991339\n",
      "Trained batch 64 batch loss 6.94489574 epoch total loss 7.31405354\n",
      "Trained batch 65 batch loss 5.70861101 epoch total loss 7.28935432\n",
      "Trained batch 66 batch loss 6.92428589 epoch total loss 7.28382301\n",
      "Trained batch 67 batch loss 6.31383705 epoch total loss 7.26934576\n",
      "Trained batch 68 batch loss 6.744874 epoch total loss 7.26163292\n",
      "Trained batch 69 batch loss 4.4538784 epoch total loss 7.22094107\n",
      "Trained batch 70 batch loss 8.51395 epoch total loss 7.23941278\n",
      "Trained batch 71 batch loss 7.440557 epoch total loss 7.24224615\n",
      "Trained batch 72 batch loss 6.21773434 epoch total loss 7.22801638\n",
      "Trained batch 73 batch loss 5.98858 epoch total loss 7.21103764\n",
      "Trained batch 74 batch loss 6.56650543 epoch total loss 7.20232821\n",
      "Trained batch 75 batch loss 6.69037485 epoch total loss 7.19550228\n",
      "Trained batch 76 batch loss 6.66109467 epoch total loss 7.18847036\n",
      "Trained batch 77 batch loss 6.85436535 epoch total loss 7.18413115\n",
      "Trained batch 78 batch loss 6.9316473 epoch total loss 7.1808939\n",
      "Trained batch 79 batch loss 4.58079243 epoch total loss 7.14798164\n",
      "Trained batch 80 batch loss 6.72974 epoch total loss 7.1427536\n",
      "Trained batch 81 batch loss 7.37192392 epoch total loss 7.14558315\n",
      "Trained batch 82 batch loss 7.20574379 epoch total loss 7.14631701\n",
      "Trained batch 83 batch loss 6.90990829 epoch total loss 7.14346886\n",
      "Trained batch 84 batch loss 6.76980782 epoch total loss 7.13902044\n",
      "Trained batch 85 batch loss 6.69266129 epoch total loss 7.13376951\n",
      "Trained batch 86 batch loss 6.81768417 epoch total loss 7.13009453\n",
      "Trained batch 87 batch loss 6.40938234 epoch total loss 7.12181\n",
      "Trained batch 88 batch loss 6.25402355 epoch total loss 7.11194897\n",
      "Trained batch 89 batch loss 6.6373167 epoch total loss 7.10661602\n",
      "Trained batch 90 batch loss 6.48410797 epoch total loss 7.0996995\n",
      "Trained batch 91 batch loss 6.94656754 epoch total loss 7.09801722\n",
      "Trained batch 92 batch loss 7.09636402 epoch total loss 7.0979991\n",
      "Trained batch 93 batch loss 7.04584599 epoch total loss 7.09743834\n",
      "Trained batch 94 batch loss 6.98874378 epoch total loss 7.09628248\n",
      "Trained batch 95 batch loss 6.89556789 epoch total loss 7.09416962\n",
      "Trained batch 96 batch loss 6.50453472 epoch total loss 7.08802748\n",
      "Trained batch 97 batch loss 6.84020424 epoch total loss 7.08547258\n",
      "Trained batch 98 batch loss 6.40554285 epoch total loss 7.07853413\n",
      "Trained batch 99 batch loss 6.79409885 epoch total loss 7.07566118\n",
      "Trained batch 100 batch loss 7.29022646 epoch total loss 7.07780695\n",
      "Trained batch 101 batch loss 7.06849813 epoch total loss 7.07771444\n",
      "Trained batch 102 batch loss 6.75039768 epoch total loss 7.07450581\n",
      "Trained batch 103 batch loss 6.19880199 epoch total loss 7.0660038\n",
      "Trained batch 104 batch loss 6.86061859 epoch total loss 7.06402874\n",
      "Trained batch 105 batch loss 6.73656654 epoch total loss 7.06091\n",
      "Trained batch 106 batch loss 6.93181324 epoch total loss 7.05969238\n",
      "Trained batch 107 batch loss 6.74984074 epoch total loss 7.05679655\n",
      "Trained batch 108 batch loss 7.07142544 epoch total loss 7.0569315\n",
      "Trained batch 109 batch loss 7.14426041 epoch total loss 7.05773306\n",
      "Trained batch 110 batch loss 6.80633545 epoch total loss 7.05544758\n",
      "Trained batch 111 batch loss 6.79938316 epoch total loss 7.05314064\n",
      "Trained batch 112 batch loss 6.74615765 epoch total loss 7.0504\n",
      "Trained batch 113 batch loss 5.77216482 epoch total loss 7.03908777\n",
      "Trained batch 114 batch loss 7.24011755 epoch total loss 7.04085112\n",
      "Trained batch 115 batch loss 6.56010818 epoch total loss 7.03667116\n",
      "Trained batch 116 batch loss 6.638237 epoch total loss 7.0332365\n",
      "Trained batch 117 batch loss 6.9150815 epoch total loss 7.03222656\n",
      "Trained batch 118 batch loss 7.07033348 epoch total loss 7.03254938\n",
      "Trained batch 119 batch loss 6.08832645 epoch total loss 7.02461481\n",
      "Trained batch 120 batch loss 7.44647312 epoch total loss 7.02813\n",
      "Trained batch 121 batch loss 7.45111704 epoch total loss 7.03162575\n",
      "Trained batch 122 batch loss 7.27142906 epoch total loss 7.03359127\n",
      "Trained batch 123 batch loss 7.13876057 epoch total loss 7.03444624\n",
      "Trained batch 124 batch loss 6.96736336 epoch total loss 7.03390503\n",
      "Trained batch 125 batch loss 6.91594028 epoch total loss 7.03296137\n",
      "Trained batch 126 batch loss 6.94567394 epoch total loss 7.03226852\n",
      "Trained batch 127 batch loss 6.79515457 epoch total loss 7.03040171\n",
      "Trained batch 128 batch loss 6.87990665 epoch total loss 7.02922583\n",
      "Trained batch 129 batch loss 6.80440331 epoch total loss 7.02748299\n",
      "Trained batch 130 batch loss 6.46297646 epoch total loss 7.02314043\n",
      "Trained batch 131 batch loss 6.65573502 epoch total loss 7.02033567\n",
      "Trained batch 132 batch loss 6.3420639 epoch total loss 7.01519728\n",
      "Trained batch 133 batch loss 6.95505333 epoch total loss 7.01474524\n",
      "Trained batch 134 batch loss 7.10831642 epoch total loss 7.0154438\n",
      "Trained batch 135 batch loss 6.94234753 epoch total loss 7.01490211\n",
      "Trained batch 136 batch loss 6.78850269 epoch total loss 7.01323748\n",
      "Trained batch 137 batch loss 6.83207083 epoch total loss 7.01191521\n",
      "Trained batch 138 batch loss 6.95814466 epoch total loss 7.01152563\n",
      "Trained batch 139 batch loss 7.35680866 epoch total loss 7.01400948\n",
      "Trained batch 140 batch loss 6.57615089 epoch total loss 7.0108819\n",
      "Trained batch 141 batch loss 6.75365591 epoch total loss 7.009058\n",
      "Trained batch 142 batch loss 6.55714703 epoch total loss 7.00587511\n",
      "Trained batch 143 batch loss 6.89970446 epoch total loss 7.00513315\n",
      "Trained batch 144 batch loss 6.23672152 epoch total loss 6.99979639\n",
      "Trained batch 145 batch loss 5.55171585 epoch total loss 6.98980951\n",
      "Trained batch 146 batch loss 5.73522186 epoch total loss 6.98121643\n",
      "Trained batch 147 batch loss 5.40213776 epoch total loss 6.97047472\n",
      "Trained batch 148 batch loss 4.89848423 epoch total loss 6.9564743\n",
      "Trained batch 149 batch loss 6.07348537 epoch total loss 6.95054865\n",
      "Trained batch 150 batch loss 7.03920507 epoch total loss 6.95113945\n",
      "Trained batch 151 batch loss 6.91563892 epoch total loss 6.95090437\n",
      "Trained batch 152 batch loss 7.33978462 epoch total loss 6.95346308\n",
      "Trained batch 153 batch loss 5.44351292 epoch total loss 6.94359398\n",
      "Trained batch 154 batch loss 5.06331062 epoch total loss 6.93138456\n",
      "Trained batch 155 batch loss 6.5826993 epoch total loss 6.92913485\n",
      "Trained batch 156 batch loss 5.70938206 epoch total loss 6.92131567\n",
      "Trained batch 157 batch loss 6.00071812 epoch total loss 6.915452\n",
      "Trained batch 158 batch loss 6.67963314 epoch total loss 6.91396\n",
      "Trained batch 159 batch loss 7.06387138 epoch total loss 6.91490221\n",
      "Trained batch 160 batch loss 6.57910347 epoch total loss 6.91280365\n",
      "Trained batch 161 batch loss 6.60402584 epoch total loss 6.91088581\n",
      "Trained batch 162 batch loss 6.23494339 epoch total loss 6.90671349\n",
      "Trained batch 163 batch loss 4.02216864 epoch total loss 6.88901711\n",
      "Trained batch 164 batch loss 6.20674 epoch total loss 6.88485718\n",
      "Trained batch 165 batch loss 6.7289896 epoch total loss 6.88391256\n",
      "Trained batch 166 batch loss 6.39065123 epoch total loss 6.88094091\n",
      "Trained batch 167 batch loss 6.72983408 epoch total loss 6.88003635\n",
      "Trained batch 168 batch loss 6.70591593 epoch total loss 6.879\n",
      "Trained batch 169 batch loss 6.65848064 epoch total loss 6.87769508\n",
      "Trained batch 170 batch loss 6.59752321 epoch total loss 6.87604713\n",
      "Trained batch 171 batch loss 6.36452246 epoch total loss 6.87305546\n",
      "Trained batch 172 batch loss 6.81683493 epoch total loss 6.87272882\n",
      "Trained batch 173 batch loss 6.6381855 epoch total loss 6.87137318\n",
      "Trained batch 174 batch loss 6.38723755 epoch total loss 6.86859083\n",
      "Trained batch 175 batch loss 6.65011406 epoch total loss 6.86734247\n",
      "Trained batch 176 batch loss 6.41863155 epoch total loss 6.86479235\n",
      "Trained batch 177 batch loss 6.56378365 epoch total loss 6.86309242\n",
      "Trained batch 178 batch loss 6.51722431 epoch total loss 6.86114931\n",
      "Trained batch 179 batch loss 6.35345697 epoch total loss 6.85831308\n",
      "Trained batch 180 batch loss 6.76665878 epoch total loss 6.85780382\n",
      "Trained batch 181 batch loss 6.61877489 epoch total loss 6.85648298\n",
      "Trained batch 182 batch loss 6.52221203 epoch total loss 6.85464668\n",
      "Trained batch 183 batch loss 6.59343338 epoch total loss 6.85321856\n",
      "Trained batch 184 batch loss 6.78545189 epoch total loss 6.85285044\n",
      "Trained batch 185 batch loss 6.53141737 epoch total loss 6.85111237\n",
      "Trained batch 186 batch loss 6.83832073 epoch total loss 6.85104418\n",
      "Trained batch 187 batch loss 6.5797472 epoch total loss 6.84959316\n",
      "Trained batch 188 batch loss 6.64913654 epoch total loss 6.84852695\n",
      "Trained batch 189 batch loss 6.71204519 epoch total loss 6.84780502\n",
      "Trained batch 190 batch loss 6.34693432 epoch total loss 6.84516859\n",
      "Trained batch 191 batch loss 6.58084393 epoch total loss 6.84378433\n",
      "Trained batch 192 batch loss 6.43659735 epoch total loss 6.84166384\n",
      "Trained batch 193 batch loss 6.22441864 epoch total loss 6.83846569\n",
      "Trained batch 194 batch loss 6.87432241 epoch total loss 6.83865\n",
      "Trained batch 195 batch loss 6.80475235 epoch total loss 6.83847666\n",
      "Trained batch 196 batch loss 6.91043949 epoch total loss 6.83884335\n",
      "Trained batch 197 batch loss 6.7090292 epoch total loss 6.83818436\n",
      "Trained batch 198 batch loss 6.75517178 epoch total loss 6.83776474\n",
      "Trained batch 199 batch loss 6.83325577 epoch total loss 6.83774233\n",
      "Trained batch 200 batch loss 6.44793749 epoch total loss 6.8357935\n",
      "Trained batch 201 batch loss 6.12457 epoch total loss 6.83225489\n",
      "Trained batch 202 batch loss 5.80632687 epoch total loss 6.82717562\n",
      "Trained batch 203 batch loss 6.39411449 epoch total loss 6.82504272\n",
      "Trained batch 204 batch loss 6.92693615 epoch total loss 6.82554197\n",
      "Trained batch 205 batch loss 6.76346874 epoch total loss 6.8252387\n",
      "Trained batch 206 batch loss 7.47816849 epoch total loss 6.82840824\n",
      "Trained batch 207 batch loss 7.02643776 epoch total loss 6.82936525\n",
      "Trained batch 208 batch loss 6.29213 epoch total loss 6.82678223\n",
      "Trained batch 209 batch loss 7.58553505 epoch total loss 6.83041286\n",
      "Trained batch 210 batch loss 5.75254154 epoch total loss 6.82528\n",
      "Trained batch 211 batch loss 6.51114845 epoch total loss 6.82379103\n",
      "Trained batch 212 batch loss 6.43705273 epoch total loss 6.82196665\n",
      "Trained batch 213 batch loss 6.48094749 epoch total loss 6.82036591\n",
      "Trained batch 214 batch loss 6.69630337 epoch total loss 6.81978607\n",
      "Trained batch 215 batch loss 5.13589764 epoch total loss 6.81195402\n",
      "Trained batch 216 batch loss 6.48819065 epoch total loss 6.81045485\n",
      "Trained batch 217 batch loss 7.21892118 epoch total loss 6.81233692\n",
      "Trained batch 218 batch loss 6.8480463 epoch total loss 6.81250048\n",
      "Trained batch 219 batch loss 6.83154154 epoch total loss 6.81258774\n",
      "Trained batch 220 batch loss 7.06776571 epoch total loss 6.81374741\n",
      "Trained batch 221 batch loss 6.60423851 epoch total loss 6.81279945\n",
      "Trained batch 222 batch loss 6.81554413 epoch total loss 6.81281185\n",
      "Trained batch 223 batch loss 7.15933609 epoch total loss 6.81436539\n",
      "Trained batch 224 batch loss 6.39759064 epoch total loss 6.81250477\n",
      "Trained batch 225 batch loss 6.08064938 epoch total loss 6.80925226\n",
      "Trained batch 226 batch loss 6.32128048 epoch total loss 6.80709314\n",
      "Trained batch 227 batch loss 6.11483574 epoch total loss 6.80404377\n",
      "Trained batch 228 batch loss 5.32178497 epoch total loss 6.79754257\n",
      "Trained batch 229 batch loss 5.96245766 epoch total loss 6.79389572\n",
      "Trained batch 230 batch loss 6.44826317 epoch total loss 6.79239273\n",
      "Trained batch 231 batch loss 6.54939318 epoch total loss 6.7913413\n",
      "Trained batch 232 batch loss 6.74388027 epoch total loss 6.79113674\n",
      "Trained batch 233 batch loss 6.8272419 epoch total loss 6.79129171\n",
      "Trained batch 234 batch loss 6.21796513 epoch total loss 6.78884172\n",
      "Trained batch 235 batch loss 6.71329927 epoch total loss 6.78852034\n",
      "Trained batch 236 batch loss 6.67423964 epoch total loss 6.78803587\n",
      "Trained batch 237 batch loss 6.7516346 epoch total loss 6.78788185\n",
      "Trained batch 238 batch loss 5.35326672 epoch total loss 6.78185415\n",
      "Trained batch 239 batch loss 5.43861628 epoch total loss 6.77623367\n",
      "Trained batch 240 batch loss 5.6016655 epoch total loss 6.77134\n",
      "Trained batch 241 batch loss 4.89393568 epoch total loss 6.76355\n",
      "Trained batch 242 batch loss 4.39661026 epoch total loss 6.75376892\n",
      "Trained batch 243 batch loss 4.78250694 epoch total loss 6.74565649\n",
      "Trained batch 244 batch loss 4.37634563 epoch total loss 6.73594618\n",
      "Trained batch 245 batch loss 5.59185219 epoch total loss 6.73127651\n",
      "Trained batch 246 batch loss 6.79542637 epoch total loss 6.73153734\n",
      "Trained batch 247 batch loss 6.65908527 epoch total loss 6.73124361\n",
      "Trained batch 248 batch loss 6.88444901 epoch total loss 6.73186111\n",
      "Trained batch 249 batch loss 6.33367443 epoch total loss 6.7302618\n",
      "Trained batch 250 batch loss 7.33736753 epoch total loss 6.73269033\n",
      "Trained batch 251 batch loss 5.62303448 epoch total loss 6.72826958\n",
      "Trained batch 252 batch loss 5.52914524 epoch total loss 6.72351122\n",
      "Trained batch 253 batch loss 4.62817335 epoch total loss 6.71522903\n",
      "Trained batch 254 batch loss 6.67358398 epoch total loss 6.71506548\n",
      "Trained batch 255 batch loss 6.0595274 epoch total loss 6.71249485\n",
      "Trained batch 256 batch loss 6.55807 epoch total loss 6.71189165\n",
      "Trained batch 257 batch loss 6.79365826 epoch total loss 6.71221\n",
      "Trained batch 258 batch loss 6.69995308 epoch total loss 6.71216249\n",
      "Trained batch 259 batch loss 6.79841518 epoch total loss 6.7124958\n",
      "Trained batch 260 batch loss 6.42872143 epoch total loss 6.71140432\n",
      "Trained batch 261 batch loss 6.36684 epoch total loss 6.71008396\n",
      "Trained batch 262 batch loss 6.77023125 epoch total loss 6.7103138\n",
      "Trained batch 263 batch loss 6.53753185 epoch total loss 6.70965624\n",
      "Trained batch 264 batch loss 6.40576363 epoch total loss 6.70850515\n",
      "Trained batch 265 batch loss 5.91883659 epoch total loss 6.7055254\n",
      "Trained batch 266 batch loss 6.4902029 epoch total loss 6.70471621\n",
      "Trained batch 267 batch loss 6.41029644 epoch total loss 6.70361328\n",
      "Trained batch 268 batch loss 6.34111309 epoch total loss 6.70226049\n",
      "Trained batch 269 batch loss 6.40745258 epoch total loss 6.70116472\n",
      "Trained batch 270 batch loss 5.26554489 epoch total loss 6.69584751\n",
      "Trained batch 271 batch loss 6.56751728 epoch total loss 6.69537354\n",
      "Trained batch 272 batch loss 6.55849075 epoch total loss 6.69487047\n",
      "Trained batch 273 batch loss 6.69009 epoch total loss 6.69485283\n",
      "Trained batch 274 batch loss 6.64595 epoch total loss 6.69467449\n",
      "Trained batch 275 batch loss 6.57869148 epoch total loss 6.69425297\n",
      "Trained batch 276 batch loss 5.2802906 epoch total loss 6.68913\n",
      "Trained batch 277 batch loss 6.5446043 epoch total loss 6.68860769\n",
      "Trained batch 278 batch loss 6.36740494 epoch total loss 6.68745279\n",
      "Trained batch 279 batch loss 7.05297 epoch total loss 6.68876266\n",
      "Trained batch 280 batch loss 6.78416777 epoch total loss 6.6891036\n",
      "Trained batch 281 batch loss 6.60749912 epoch total loss 6.68881321\n",
      "Trained batch 282 batch loss 6.76647615 epoch total loss 6.68908882\n",
      "Trained batch 283 batch loss 6.63178158 epoch total loss 6.68888617\n",
      "Trained batch 284 batch loss 6.76425838 epoch total loss 6.68915176\n",
      "Trained batch 285 batch loss 6.30928898 epoch total loss 6.687819\n",
      "Trained batch 286 batch loss 6.45900774 epoch total loss 6.68701887\n",
      "Trained batch 287 batch loss 6.636 epoch total loss 6.68684101\n",
      "Trained batch 288 batch loss 6.51504517 epoch total loss 6.68624449\n",
      "Trained batch 289 batch loss 6.28279161 epoch total loss 6.68484879\n",
      "Trained batch 290 batch loss 6.67630577 epoch total loss 6.68481922\n",
      "Trained batch 291 batch loss 6.54794407 epoch total loss 6.68434906\n",
      "Trained batch 292 batch loss 6.40279436 epoch total loss 6.6833849\n",
      "Trained batch 293 batch loss 5.82958221 epoch total loss 6.68047094\n",
      "Trained batch 294 batch loss 6.22392893 epoch total loss 6.67891788\n",
      "Trained batch 295 batch loss 6.57849312 epoch total loss 6.67857742\n",
      "Trained batch 296 batch loss 6.90383053 epoch total loss 6.67933798\n",
      "Trained batch 297 batch loss 6.60051584 epoch total loss 6.67907286\n",
      "Trained batch 298 batch loss 6.55250263 epoch total loss 6.67864799\n",
      "Trained batch 299 batch loss 6.48126888 epoch total loss 6.67798805\n",
      "Trained batch 300 batch loss 6.80198765 epoch total loss 6.67840147\n",
      "Trained batch 301 batch loss 6.36329842 epoch total loss 6.67735434\n",
      "Trained batch 302 batch loss 6.74562168 epoch total loss 6.67758036\n",
      "Trained batch 303 batch loss 6.1772213 epoch total loss 6.67592907\n",
      "Trained batch 304 batch loss 6.09519291 epoch total loss 6.67401886\n",
      "Trained batch 305 batch loss 6.91817665 epoch total loss 6.67481947\n",
      "Trained batch 306 batch loss 8.04962826 epoch total loss 6.67931271\n",
      "Trained batch 307 batch loss 6.49959612 epoch total loss 6.67872667\n",
      "Trained batch 308 batch loss 5.67089224 epoch total loss 6.67545462\n",
      "Trained batch 309 batch loss 6.27031755 epoch total loss 6.67414331\n",
      "Trained batch 310 batch loss 6.08313 epoch total loss 6.6722374\n",
      "Trained batch 311 batch loss 6.16265917 epoch total loss 6.67059851\n",
      "Trained batch 312 batch loss 6.23533249 epoch total loss 6.66920376\n",
      "Trained batch 313 batch loss 6.61079597 epoch total loss 6.66901684\n",
      "Trained batch 314 batch loss 6.65894794 epoch total loss 6.66898489\n",
      "Trained batch 315 batch loss 6.76169872 epoch total loss 6.66927958\n",
      "Trained batch 316 batch loss 6.01303959 epoch total loss 6.66720247\n",
      "Trained batch 317 batch loss 6.60929585 epoch total loss 6.66702\n",
      "Trained batch 318 batch loss 6.54551744 epoch total loss 6.66663742\n",
      "Trained batch 319 batch loss 6.82242966 epoch total loss 6.66712618\n",
      "Trained batch 320 batch loss 6.4846034 epoch total loss 6.66655588\n",
      "Trained batch 321 batch loss 5.19824839 epoch total loss 6.66198158\n",
      "Trained batch 322 batch loss 6.42525673 epoch total loss 6.66124678\n",
      "Trained batch 323 batch loss 6.51265478 epoch total loss 6.66078663\n",
      "Trained batch 324 batch loss 6.88983631 epoch total loss 6.66149378\n",
      "Trained batch 325 batch loss 6.34811592 epoch total loss 6.66052961\n",
      "Trained batch 326 batch loss 6.07037544 epoch total loss 6.65871906\n",
      "Trained batch 327 batch loss 5.91383648 epoch total loss 6.65644121\n",
      "Trained batch 328 batch loss 6.14234924 epoch total loss 6.65487385\n",
      "Trained batch 329 batch loss 5.16641712 epoch total loss 6.65034962\n",
      "Trained batch 330 batch loss 5.97232151 epoch total loss 6.6482954\n",
      "Trained batch 331 batch loss 5.64003849 epoch total loss 6.64525\n",
      "Trained batch 332 batch loss 6.31594 epoch total loss 6.64425755\n",
      "Trained batch 333 batch loss 6.47473192 epoch total loss 6.64374876\n",
      "Trained batch 334 batch loss 6.84833241 epoch total loss 6.6443615\n",
      "Trained batch 335 batch loss 6.45203686 epoch total loss 6.64378786\n",
      "Trained batch 336 batch loss 6.75015974 epoch total loss 6.64410496\n",
      "Trained batch 337 batch loss 6.10688591 epoch total loss 6.64251089\n",
      "Trained batch 338 batch loss 6.54701805 epoch total loss 6.6422286\n",
      "Trained batch 339 batch loss 6.59188652 epoch total loss 6.64208\n",
      "Trained batch 340 batch loss 6.41727495 epoch total loss 6.64141846\n",
      "Trained batch 341 batch loss 7.10068512 epoch total loss 6.64276505\n",
      "Trained batch 342 batch loss 6.94329166 epoch total loss 6.64364386\n",
      "Trained batch 343 batch loss 7.69229889 epoch total loss 6.64670134\n",
      "Trained batch 344 batch loss 7.64575195 epoch total loss 6.64960575\n",
      "Trained batch 345 batch loss 6.85724545 epoch total loss 6.65020752\n",
      "Trained batch 346 batch loss 6.52069092 epoch total loss 6.6498332\n",
      "Trained batch 347 batch loss 6.58740377 epoch total loss 6.64965343\n",
      "Trained batch 348 batch loss 6.55869293 epoch total loss 6.64939165\n",
      "Trained batch 349 batch loss 6.88859463 epoch total loss 6.65007734\n",
      "Trained batch 350 batch loss 7.00475359 epoch total loss 6.65109\n",
      "Trained batch 351 batch loss 6.75631618 epoch total loss 6.65139\n",
      "Trained batch 352 batch loss 6.75764704 epoch total loss 6.65169191\n",
      "Trained batch 353 batch loss 6.79749584 epoch total loss 6.65210533\n",
      "Trained batch 354 batch loss 6.57306 epoch total loss 6.65188169\n",
      "Trained batch 355 batch loss 6.86128044 epoch total loss 6.65247154\n",
      "Trained batch 356 batch loss 6.55841446 epoch total loss 6.65220737\n",
      "Trained batch 357 batch loss 6.40466499 epoch total loss 6.65151405\n",
      "Trained batch 358 batch loss 6.60078573 epoch total loss 6.65137243\n",
      "Trained batch 359 batch loss 6.33859348 epoch total loss 6.65050125\n",
      "Trained batch 360 batch loss 6.90454626 epoch total loss 6.65120697\n",
      "Trained batch 361 batch loss 5.73005867 epoch total loss 6.64865541\n",
      "Trained batch 362 batch loss 5.65455341 epoch total loss 6.64590931\n",
      "Trained batch 363 batch loss 5.65081501 epoch total loss 6.64316797\n",
      "Trained batch 364 batch loss 6.7319622 epoch total loss 6.64341164\n",
      "Trained batch 365 batch loss 6.36726 epoch total loss 6.6426549\n",
      "Trained batch 366 batch loss 6.69396 epoch total loss 6.64279509\n",
      "Trained batch 367 batch loss 6.72451162 epoch total loss 6.64301777\n",
      "Trained batch 368 batch loss 6.84160805 epoch total loss 6.64355755\n",
      "Trained batch 369 batch loss 6.7141304 epoch total loss 6.64374876\n",
      "Trained batch 370 batch loss 6.27798462 epoch total loss 6.64276028\n",
      "Trained batch 371 batch loss 6.97034836 epoch total loss 6.64364338\n",
      "Trained batch 372 batch loss 6.91315508 epoch total loss 6.64436769\n",
      "Trained batch 373 batch loss 5.44326687 epoch total loss 6.64114809\n",
      "Trained batch 374 batch loss 7.04448605 epoch total loss 6.64222622\n",
      "Trained batch 375 batch loss 6.33327341 epoch total loss 6.64140224\n",
      "Trained batch 376 batch loss 6.73899841 epoch total loss 6.64166212\n",
      "Trained batch 377 batch loss 6.57433319 epoch total loss 6.64148283\n",
      "Trained batch 378 batch loss 6.55510664 epoch total loss 6.6412549\n",
      "Trained batch 379 batch loss 6.64949799 epoch total loss 6.64127636\n",
      "Trained batch 380 batch loss 6.81533861 epoch total loss 6.6417346\n",
      "Trained batch 381 batch loss 7.0583477 epoch total loss 6.64282799\n",
      "Trained batch 382 batch loss 6.75658131 epoch total loss 6.64312601\n",
      "Trained batch 383 batch loss 6.80623913 epoch total loss 6.64355135\n",
      "Trained batch 384 batch loss 6.64422607 epoch total loss 6.64355326\n",
      "Trained batch 385 batch loss 6.56628799 epoch total loss 6.64335299\n",
      "Trained batch 386 batch loss 5.36745358 epoch total loss 6.64004755\n",
      "Trained batch 387 batch loss 6.68586159 epoch total loss 6.64016581\n",
      "Trained batch 388 batch loss 6.29283333 epoch total loss 6.63927031\n",
      "Trained batch 389 batch loss 7.48939514 epoch total loss 6.64145613\n",
      "Trained batch 390 batch loss 6.00959778 epoch total loss 6.63983583\n",
      "Trained batch 391 batch loss 6.49460125 epoch total loss 6.63946438\n",
      "Trained batch 392 batch loss 6.49869347 epoch total loss 6.63910532\n",
      "Trained batch 393 batch loss 6.64472485 epoch total loss 6.63911963\n",
      "Trained batch 394 batch loss 6.60879135 epoch total loss 6.63904285\n",
      "Trained batch 395 batch loss 6.88447189 epoch total loss 6.63966465\n",
      "Trained batch 396 batch loss 4.97514486 epoch total loss 6.63546085\n",
      "Trained batch 397 batch loss 6.55654335 epoch total loss 6.63526249\n",
      "Trained batch 398 batch loss 6.21154 epoch total loss 6.63419771\n",
      "Trained batch 399 batch loss 6.40935 epoch total loss 6.63363409\n",
      "Trained batch 400 batch loss 6.40242767 epoch total loss 6.63305616\n",
      "Trained batch 401 batch loss 6.59941387 epoch total loss 6.63297224\n",
      "Trained batch 402 batch loss 6.16496372 epoch total loss 6.6318078\n",
      "Trained batch 403 batch loss 6.49866629 epoch total loss 6.63147783\n",
      "Trained batch 404 batch loss 6.41942692 epoch total loss 6.63095284\n",
      "Trained batch 405 batch loss 6.19400692 epoch total loss 6.62987423\n",
      "Trained batch 406 batch loss 6.39630938 epoch total loss 6.62929869\n",
      "Trained batch 407 batch loss 6.78061962 epoch total loss 6.62967062\n",
      "Trained batch 408 batch loss 6.54859829 epoch total loss 6.62947178\n",
      "Trained batch 409 batch loss 6.46022892 epoch total loss 6.62905788\n",
      "Trained batch 410 batch loss 6.45561314 epoch total loss 6.62863493\n",
      "Trained batch 411 batch loss 6.61807108 epoch total loss 6.62860918\n",
      "Trained batch 412 batch loss 6.36532784 epoch total loss 6.62797\n",
      "Trained batch 413 batch loss 6.81697083 epoch total loss 6.62842751\n",
      "Trained batch 414 batch loss 6.51572275 epoch total loss 6.62815475\n",
      "Trained batch 415 batch loss 6.42125607 epoch total loss 6.62765598\n",
      "Trained batch 416 batch loss 6.26255178 epoch total loss 6.62677813\n",
      "Trained batch 417 batch loss 6.50202847 epoch total loss 6.62647867\n",
      "Trained batch 418 batch loss 6.12654352 epoch total loss 6.62528276\n",
      "Trained batch 419 batch loss 6.44565105 epoch total loss 6.62485361\n",
      "Trained batch 420 batch loss 6.89294577 epoch total loss 6.62549257\n",
      "Trained batch 421 batch loss 6.22621346 epoch total loss 6.62454414\n",
      "Trained batch 422 batch loss 6.77853489 epoch total loss 6.6249094\n",
      "Trained batch 423 batch loss 6.44459248 epoch total loss 6.62448263\n",
      "Trained batch 424 batch loss 6.58923054 epoch total loss 6.62439966\n",
      "Trained batch 425 batch loss 6.7240653 epoch total loss 6.62463427\n",
      "Trained batch 426 batch loss 6.44302464 epoch total loss 6.62420797\n",
      "Trained batch 427 batch loss 6.41963482 epoch total loss 6.62372875\n",
      "Trained batch 428 batch loss 6.44153118 epoch total loss 6.62330341\n",
      "Trained batch 429 batch loss 6.82000971 epoch total loss 6.62376213\n",
      "Trained batch 430 batch loss 6.79869747 epoch total loss 6.62416887\n",
      "Trained batch 431 batch loss 6.9297533 epoch total loss 6.62487745\n",
      "Trained batch 432 batch loss 6.87107468 epoch total loss 6.62544775\n",
      "Trained batch 433 batch loss 6.84558535 epoch total loss 6.62595606\n",
      "Trained batch 434 batch loss 6.5475235 epoch total loss 6.62577581\n",
      "Trained batch 435 batch loss 7.20399523 epoch total loss 6.62710524\n",
      "Trained batch 436 batch loss 6.95376968 epoch total loss 6.62785482\n",
      "Trained batch 437 batch loss 7.04806852 epoch total loss 6.62881613\n",
      "Trained batch 438 batch loss 6.63317 epoch total loss 6.62882614\n",
      "Trained batch 439 batch loss 7.34159 epoch total loss 6.6304493\n",
      "Trained batch 440 batch loss 6.47665 epoch total loss 6.6301\n",
      "Trained batch 441 batch loss 6.67013645 epoch total loss 6.63019085\n",
      "Trained batch 442 batch loss 6.43377972 epoch total loss 6.62974644\n",
      "Trained batch 443 batch loss 7.43741131 epoch total loss 6.63157\n",
      "Trained batch 444 batch loss 6.94911146 epoch total loss 6.63228512\n",
      "Trained batch 445 batch loss 7.43083143 epoch total loss 6.63408\n",
      "Trained batch 446 batch loss 7.06846905 epoch total loss 6.63505363\n",
      "Trained batch 447 batch loss 7.02989388 epoch total loss 6.63593674\n",
      "Trained batch 448 batch loss 6.91160774 epoch total loss 6.63655186\n",
      "Trained batch 449 batch loss 6.67770195 epoch total loss 6.63664389\n",
      "Trained batch 450 batch loss 6.95539856 epoch total loss 6.63735199\n",
      "Trained batch 451 batch loss 6.65290642 epoch total loss 6.63738632\n",
      "Trained batch 452 batch loss 6.6808033 epoch total loss 6.63748264\n",
      "Trained batch 453 batch loss 6.94790363 epoch total loss 6.63816786\n",
      "Trained batch 454 batch loss 6.70104218 epoch total loss 6.63830614\n",
      "Trained batch 455 batch loss 6.77942562 epoch total loss 6.63861656\n",
      "Trained batch 456 batch loss 6.74 epoch total loss 6.63883877\n",
      "Trained batch 457 batch loss 6.71456909 epoch total loss 6.63900471\n",
      "Trained batch 458 batch loss 6.70134735 epoch total loss 6.63914108\n",
      "Trained batch 459 batch loss 6.63007927 epoch total loss 6.63912153\n",
      "Trained batch 460 batch loss 6.83454847 epoch total loss 6.63954592\n",
      "Trained batch 461 batch loss 6.87257671 epoch total loss 6.64005136\n",
      "Trained batch 462 batch loss 7.12024403 epoch total loss 6.64109087\n",
      "Trained batch 463 batch loss 6.69424915 epoch total loss 6.64120626\n",
      "Trained batch 464 batch loss 6.43981075 epoch total loss 6.64077187\n",
      "Trained batch 465 batch loss 6.28432941 epoch total loss 6.64000559\n",
      "Trained batch 466 batch loss 6.924016 epoch total loss 6.64061499\n",
      "Trained batch 467 batch loss 7.09611511 epoch total loss 6.6415906\n",
      "Trained batch 468 batch loss 6.58911467 epoch total loss 6.64147854\n",
      "Trained batch 469 batch loss 7.15427303 epoch total loss 6.64257193\n",
      "Trained batch 470 batch loss 6.67991447 epoch total loss 6.64265156\n",
      "Trained batch 471 batch loss 6.5749836 epoch total loss 6.64250755\n",
      "Trained batch 472 batch loss 6.47438192 epoch total loss 6.64215136\n",
      "Trained batch 473 batch loss 6.74899578 epoch total loss 6.64237738\n",
      "Trained batch 474 batch loss 6.99385738 epoch total loss 6.64311886\n",
      "Trained batch 475 batch loss 6.88657188 epoch total loss 6.64363146\n",
      "Trained batch 476 batch loss 6.74741 epoch total loss 6.6438489\n",
      "Trained batch 477 batch loss 6.76351643 epoch total loss 6.64409971\n",
      "Trained batch 478 batch loss 6.91511 epoch total loss 6.64466667\n",
      "Trained batch 479 batch loss 6.46140146 epoch total loss 6.64428425\n",
      "Trained batch 480 batch loss 6.4390192 epoch total loss 6.64385653\n",
      "Trained batch 481 batch loss 7.1743083 epoch total loss 6.64495897\n",
      "Trained batch 482 batch loss 6.68342876 epoch total loss 6.6450386\n",
      "Trained batch 483 batch loss 6.68570805 epoch total loss 6.645123\n",
      "Trained batch 484 batch loss 6.9071331 epoch total loss 6.64566469\n",
      "Trained batch 485 batch loss 5.78747463 epoch total loss 6.64389515\n",
      "Trained batch 486 batch loss 5.7922616 epoch total loss 6.64214277\n",
      "Trained batch 487 batch loss 6.67616 epoch total loss 6.64221287\n",
      "Trained batch 488 batch loss 6.47098684 epoch total loss 6.64186192\n",
      "Trained batch 489 batch loss 6.58257723 epoch total loss 6.64174032\n",
      "Trained batch 490 batch loss 6.90456581 epoch total loss 6.64227676\n",
      "Trained batch 491 batch loss 6.62604666 epoch total loss 6.64224339\n",
      "Trained batch 492 batch loss 6.70050716 epoch total loss 6.64236164\n",
      "Trained batch 493 batch loss 6.44736958 epoch total loss 6.64196587\n",
      "Trained batch 494 batch loss 7.25998402 epoch total loss 6.64321709\n",
      "Trained batch 495 batch loss 5.89229 epoch total loss 6.64170027\n",
      "Trained batch 496 batch loss 6.56563282 epoch total loss 6.64154673\n",
      "Trained batch 497 batch loss 6.7052145 epoch total loss 6.64167547\n",
      "Trained batch 498 batch loss 6.4973259 epoch total loss 6.64138556\n",
      "Trained batch 499 batch loss 6.62923431 epoch total loss 6.64136076\n",
      "Trained batch 500 batch loss 6.63930464 epoch total loss 6.64135695\n",
      "Trained batch 501 batch loss 5.51934 epoch total loss 6.63911724\n",
      "Trained batch 502 batch loss 6.54273462 epoch total loss 6.63892508\n",
      "Trained batch 503 batch loss 6.46163034 epoch total loss 6.63857269\n",
      "Trained batch 504 batch loss 6.42803764 epoch total loss 6.63815498\n",
      "Trained batch 505 batch loss 6.40453529 epoch total loss 6.63769245\n",
      "Trained batch 506 batch loss 6.89152336 epoch total loss 6.63819408\n",
      "Trained batch 507 batch loss 6.47947216 epoch total loss 6.63788128\n",
      "Trained batch 508 batch loss 6.98683214 epoch total loss 6.63856792\n",
      "Trained batch 509 batch loss 6.9960475 epoch total loss 6.63927031\n",
      "Trained batch 510 batch loss 6.71239 epoch total loss 6.63941383\n",
      "Trained batch 511 batch loss 6.70088243 epoch total loss 6.63953447\n",
      "Trained batch 512 batch loss 6.83558846 epoch total loss 6.63991737\n",
      "Trained batch 513 batch loss 6.26853561 epoch total loss 6.63919353\n",
      "Trained batch 514 batch loss 6.13832188 epoch total loss 6.63821936\n",
      "Trained batch 515 batch loss 6.06699085 epoch total loss 6.63710976\n",
      "Trained batch 516 batch loss 6.86470747 epoch total loss 6.63755083\n",
      "Trained batch 517 batch loss 5.9248395 epoch total loss 6.63617229\n",
      "Trained batch 518 batch loss 5.89118385 epoch total loss 6.63473415\n",
      "Trained batch 519 batch loss 6.15705585 epoch total loss 6.63381338\n",
      "Trained batch 520 batch loss 6.46111298 epoch total loss 6.6334815\n",
      "Trained batch 521 batch loss 6.56551933 epoch total loss 6.63335085\n",
      "Trained batch 522 batch loss 6.58916 epoch total loss 6.63326597\n",
      "Trained batch 523 batch loss 6.32874489 epoch total loss 6.63268423\n",
      "Trained batch 524 batch loss 6.67461777 epoch total loss 6.63276386\n",
      "Trained batch 525 batch loss 6.18411446 epoch total loss 6.63190937\n",
      "Trained batch 526 batch loss 5.94873524 epoch total loss 6.63061047\n",
      "Trained batch 527 batch loss 6.26983356 epoch total loss 6.62992573\n",
      "Trained batch 528 batch loss 6.63255596 epoch total loss 6.62993097\n",
      "Trained batch 529 batch loss 6.69777727 epoch total loss 6.63005924\n",
      "Trained batch 530 batch loss 6.6011982 epoch total loss 6.63000488\n",
      "Trained batch 531 batch loss 6.3900938 epoch total loss 6.62955332\n",
      "Trained batch 532 batch loss 6.49869061 epoch total loss 6.62930727\n",
      "Trained batch 533 batch loss 6.39504337 epoch total loss 6.62886763\n",
      "Trained batch 534 batch loss 6.83161974 epoch total loss 6.62924719\n",
      "Trained batch 535 batch loss 6.38694477 epoch total loss 6.62879467\n",
      "Trained batch 536 batch loss 6.51979 epoch total loss 6.62859106\n",
      "Trained batch 537 batch loss 6.46910286 epoch total loss 6.62829399\n",
      "Trained batch 538 batch loss 6.29942656 epoch total loss 6.62768221\n",
      "Trained batch 539 batch loss 6.63312864 epoch total loss 6.62769222\n",
      "Trained batch 540 batch loss 6.57878113 epoch total loss 6.6276021\n",
      "Trained batch 541 batch loss 6.67054272 epoch total loss 6.62768126\n",
      "Trained batch 542 batch loss 6.80173731 epoch total loss 6.62800264\n",
      "Trained batch 543 batch loss 6.30225277 epoch total loss 6.62740278\n",
      "Trained batch 544 batch loss 6.96087646 epoch total loss 6.628016\n",
      "Trained batch 545 batch loss 6.518363 epoch total loss 6.62781477\n",
      "Trained batch 546 batch loss 6.59226131 epoch total loss 6.62774944\n",
      "Trained batch 547 batch loss 6.9090395 epoch total loss 6.62826347\n",
      "Trained batch 548 batch loss 6.86527824 epoch total loss 6.62869596\n",
      "Trained batch 549 batch loss 6.31065178 epoch total loss 6.62811661\n",
      "Trained batch 550 batch loss 6.51079226 epoch total loss 6.62790298\n",
      "Trained batch 551 batch loss 6.69140625 epoch total loss 6.62801838\n",
      "Trained batch 552 batch loss 6.67282963 epoch total loss 6.62809944\n",
      "Trained batch 553 batch loss 6.44489384 epoch total loss 6.62776804\n",
      "Trained batch 554 batch loss 6.72166252 epoch total loss 6.62793779\n",
      "Trained batch 555 batch loss 6.65626907 epoch total loss 6.62798882\n",
      "Trained batch 556 batch loss 6.3746624 epoch total loss 6.62753296\n",
      "Trained batch 557 batch loss 6.37867737 epoch total loss 6.62708616\n",
      "Trained batch 558 batch loss 6.56534386 epoch total loss 6.62697601\n",
      "Trained batch 559 batch loss 6.38478851 epoch total loss 6.62654257\n",
      "Trained batch 560 batch loss 6.20766926 epoch total loss 6.62579489\n",
      "Trained batch 561 batch loss 6.12794065 epoch total loss 6.62490749\n",
      "Trained batch 562 batch loss 5.87226629 epoch total loss 6.62356806\n",
      "Trained batch 563 batch loss 6.55963564 epoch total loss 6.62345457\n",
      "Trained batch 564 batch loss 6.64293146 epoch total loss 6.6234889\n",
      "Trained batch 565 batch loss 6.98512077 epoch total loss 6.62412882\n",
      "Trained batch 566 batch loss 6.33729267 epoch total loss 6.62362242\n",
      "Trained batch 567 batch loss 6.382617 epoch total loss 6.62319708\n",
      "Trained batch 568 batch loss 6.34464264 epoch total loss 6.62270689\n",
      "Trained batch 569 batch loss 6.24979687 epoch total loss 6.62205124\n",
      "Trained batch 570 batch loss 6.40039492 epoch total loss 6.62166262\n",
      "Trained batch 571 batch loss 6.63099766 epoch total loss 6.62167931\n",
      "Trained batch 572 batch loss 6.75208426 epoch total loss 6.62190723\n",
      "Trained batch 573 batch loss 6.30142689 epoch total loss 6.6213479\n",
      "Trained batch 574 batch loss 6.37571239 epoch total loss 6.62092\n",
      "Trained batch 575 batch loss 6.66052055 epoch total loss 6.62098885\n",
      "Trained batch 576 batch loss 5.90955067 epoch total loss 6.61975384\n",
      "Trained batch 577 batch loss 6.48804855 epoch total loss 6.61952543\n",
      "Trained batch 578 batch loss 6.18049622 epoch total loss 6.61876583\n",
      "Trained batch 579 batch loss 6.18563843 epoch total loss 6.61801767\n",
      "Trained batch 580 batch loss 6.17606258 epoch total loss 6.61725569\n",
      "Trained batch 581 batch loss 5.41038227 epoch total loss 6.61517859\n",
      "Trained batch 582 batch loss 5.49393749 epoch total loss 6.61325169\n",
      "Trained batch 583 batch loss 6.48337173 epoch total loss 6.613029\n",
      "Trained batch 584 batch loss 6.65482044 epoch total loss 6.61310053\n",
      "Trained batch 585 batch loss 5.5994072 epoch total loss 6.6113677\n",
      "Trained batch 586 batch loss 6.27778339 epoch total loss 6.61079884\n",
      "Trained batch 587 batch loss 6.80771542 epoch total loss 6.61113405\n",
      "Trained batch 588 batch loss 6.05434227 epoch total loss 6.61018705\n",
      "Trained batch 589 batch loss 6.7838583 epoch total loss 6.61048222\n",
      "Trained batch 590 batch loss 6.86849785 epoch total loss 6.61091948\n",
      "Trained batch 591 batch loss 6.63402748 epoch total loss 6.61095858\n",
      "Trained batch 592 batch loss 6.58261251 epoch total loss 6.61091042\n",
      "Trained batch 593 batch loss 5.97865582 epoch total loss 6.60984421\n",
      "Trained batch 594 batch loss 5.6006546 epoch total loss 6.60814524\n",
      "Trained batch 595 batch loss 6.14372921 epoch total loss 6.60736465\n",
      "Trained batch 596 batch loss 5.53264093 epoch total loss 6.60556173\n",
      "Trained batch 597 batch loss 5.34917402 epoch total loss 6.60345697\n",
      "Trained batch 598 batch loss 6.48807621 epoch total loss 6.60326433\n",
      "Trained batch 599 batch loss 6.12726307 epoch total loss 6.60246944\n",
      "Trained batch 600 batch loss 6.55796051 epoch total loss 6.60239506\n",
      "Trained batch 601 batch loss 6.6324935 epoch total loss 6.60244513\n",
      "Trained batch 602 batch loss 6.50402927 epoch total loss 6.60228205\n",
      "Trained batch 603 batch loss 5.08892536 epoch total loss 6.59977198\n",
      "Trained batch 604 batch loss 5.05416679 epoch total loss 6.59721327\n",
      "Trained batch 605 batch loss 5.55300331 epoch total loss 6.59548712\n",
      "Trained batch 606 batch loss 5.66551638 epoch total loss 6.59395266\n",
      "Trained batch 607 batch loss 5.04966211 epoch total loss 6.59140825\n",
      "Trained batch 608 batch loss 6.94536591 epoch total loss 6.59199047\n",
      "Trained batch 609 batch loss 6.66336107 epoch total loss 6.59210777\n",
      "Trained batch 610 batch loss 6.54483509 epoch total loss 6.59203\n",
      "Trained batch 611 batch loss 6.23758364 epoch total loss 6.59145\n",
      "Trained batch 612 batch loss 6.64773083 epoch total loss 6.59154177\n",
      "Trained batch 613 batch loss 7.09252787 epoch total loss 6.59235907\n",
      "Trained batch 614 batch loss 6.26069164 epoch total loss 6.59181929\n",
      "Trained batch 615 batch loss 6.71162891 epoch total loss 6.59201384\n",
      "Trained batch 616 batch loss 6.75186825 epoch total loss 6.59227371\n",
      "Trained batch 617 batch loss 6.80304813 epoch total loss 6.59261513\n",
      "Trained batch 618 batch loss 6.30357838 epoch total loss 6.59214735\n",
      "Trained batch 619 batch loss 6.32357597 epoch total loss 6.59171343\n",
      "Trained batch 620 batch loss 6.3002 epoch total loss 6.59124327\n",
      "Trained batch 621 batch loss 5.22553635 epoch total loss 6.58904409\n",
      "Trained batch 622 batch loss 6.42962265 epoch total loss 6.58878803\n",
      "Trained batch 623 batch loss 6.71207237 epoch total loss 6.58898544\n",
      "Trained batch 624 batch loss 6.47491 epoch total loss 6.58880281\n",
      "Trained batch 625 batch loss 6.37623024 epoch total loss 6.58846331\n",
      "Trained batch 626 batch loss 5.86431789 epoch total loss 6.5873065\n",
      "Trained batch 627 batch loss 6.34069777 epoch total loss 6.58691311\n",
      "Trained batch 628 batch loss 6.88903522 epoch total loss 6.58739471\n",
      "Trained batch 629 batch loss 6.26705313 epoch total loss 6.58688545\n",
      "Trained batch 630 batch loss 5.93146324 epoch total loss 6.58584547\n",
      "Trained batch 631 batch loss 5.13748837 epoch total loss 6.58355045\n",
      "Trained batch 632 batch loss 6.62192631 epoch total loss 6.58361101\n",
      "Trained batch 633 batch loss 6.52870274 epoch total loss 6.5835247\n",
      "Trained batch 634 batch loss 6.48436689 epoch total loss 6.5833683\n",
      "Trained batch 635 batch loss 6.52415419 epoch total loss 6.58327484\n",
      "Trained batch 636 batch loss 7.05582047 epoch total loss 6.58401728\n",
      "Trained batch 637 batch loss 6.15635872 epoch total loss 6.58334589\n",
      "Trained batch 638 batch loss 6.44433689 epoch total loss 6.58312798\n",
      "Trained batch 639 batch loss 5.16057777 epoch total loss 6.58090162\n",
      "Trained batch 640 batch loss 5.23100662 epoch total loss 6.57879257\n",
      "Trained batch 641 batch loss 6.24975395 epoch total loss 6.57827902\n",
      "Trained batch 642 batch loss 6.13089371 epoch total loss 6.57758188\n",
      "Trained batch 643 batch loss 6.68430424 epoch total loss 6.57774782\n",
      "Trained batch 644 batch loss 6.47705078 epoch total loss 6.57759142\n",
      "Trained batch 645 batch loss 5.91463184 epoch total loss 6.57656336\n",
      "Trained batch 646 batch loss 6.63846731 epoch total loss 6.5766592\n",
      "Trained batch 647 batch loss 5.80955124 epoch total loss 6.57547379\n",
      "Trained batch 648 batch loss 5.99145603 epoch total loss 6.57457304\n",
      "Trained batch 649 batch loss 5.86569309 epoch total loss 6.57348061\n",
      "Trained batch 650 batch loss 6.10787487 epoch total loss 6.5727644\n",
      "Trained batch 651 batch loss 5.63399601 epoch total loss 6.57132196\n",
      "Trained batch 652 batch loss 6.27321863 epoch total loss 6.57086515\n",
      "Trained batch 653 batch loss 7.01403284 epoch total loss 6.57154417\n",
      "Trained batch 654 batch loss 6.3082962 epoch total loss 6.57114124\n",
      "Trained batch 655 batch loss 6.9797 epoch total loss 6.57176447\n",
      "Trained batch 656 batch loss 5.04465866 epoch total loss 6.56943655\n",
      "Trained batch 657 batch loss 6.01513433 epoch total loss 6.56859255\n",
      "Trained batch 658 batch loss 4.86333084 epoch total loss 6.56600094\n",
      "Trained batch 659 batch loss 5.20086 epoch total loss 6.56392908\n",
      "Trained batch 660 batch loss 6.81965542 epoch total loss 6.56431723\n",
      "Trained batch 661 batch loss 7.04482889 epoch total loss 6.56504393\n",
      "Trained batch 662 batch loss 6.00793934 epoch total loss 6.56420231\n",
      "Trained batch 663 batch loss 6.55732918 epoch total loss 6.56419182\n",
      "Trained batch 664 batch loss 6.24690771 epoch total loss 6.56371403\n",
      "Trained batch 665 batch loss 6.65001 epoch total loss 6.56384373\n",
      "Trained batch 666 batch loss 6.56855631 epoch total loss 6.5638504\n",
      "Trained batch 667 batch loss 6.36386395 epoch total loss 6.56355047\n",
      "Trained batch 668 batch loss 6.10428524 epoch total loss 6.56286335\n",
      "Trained batch 669 batch loss 5.86789513 epoch total loss 6.56182432\n",
      "Trained batch 670 batch loss 7.05945301 epoch total loss 6.56256723\n",
      "Trained batch 671 batch loss 6.35776091 epoch total loss 6.56226206\n",
      "Trained batch 672 batch loss 6.61198711 epoch total loss 6.56233597\n",
      "Trained batch 673 batch loss 5.58744621 epoch total loss 6.56088734\n",
      "Trained batch 674 batch loss 6.69944048 epoch total loss 6.56109238\n",
      "Trained batch 675 batch loss 6.34656906 epoch total loss 6.5607748\n",
      "Trained batch 676 batch loss 5.9529171 epoch total loss 6.55987597\n",
      "Trained batch 677 batch loss 3.93751049 epoch total loss 6.55600214\n",
      "Trained batch 678 batch loss 5.90996599 epoch total loss 6.55505\n",
      "Trained batch 679 batch loss 6.6316309 epoch total loss 6.55516291\n",
      "Trained batch 680 batch loss 6.59935284 epoch total loss 6.55522728\n",
      "Trained batch 681 batch loss 6.49447203 epoch total loss 6.55513859\n",
      "Trained batch 682 batch loss 6.56081104 epoch total loss 6.55514717\n",
      "Trained batch 683 batch loss 6.33541 epoch total loss 6.55482531\n",
      "Trained batch 684 batch loss 6.08092737 epoch total loss 6.55413294\n",
      "Trained batch 685 batch loss 6.44973564 epoch total loss 6.55398035\n",
      "Trained batch 686 batch loss 6.36982155 epoch total loss 6.55371141\n",
      "Trained batch 687 batch loss 6.71696138 epoch total loss 6.55394888\n",
      "Trained batch 688 batch loss 7.32682419 epoch total loss 6.55507231\n",
      "Trained batch 689 batch loss 6.84675264 epoch total loss 6.55549526\n",
      "Trained batch 690 batch loss 6.7076664 epoch total loss 6.55571556\n",
      "Trained batch 691 batch loss 6.78906345 epoch total loss 6.55605364\n",
      "Trained batch 692 batch loss 6.5421629 epoch total loss 6.55603313\n",
      "Trained batch 693 batch loss 6.44116783 epoch total loss 6.55586767\n",
      "Trained batch 694 batch loss 6.38727283 epoch total loss 6.55562449\n",
      "Trained batch 695 batch loss 5.50918961 epoch total loss 6.55411911\n",
      "Trained batch 696 batch loss 5.46284866 epoch total loss 6.55255127\n",
      "Trained batch 697 batch loss 5.45010757 epoch total loss 6.5509696\n",
      "Trained batch 698 batch loss 6.89555073 epoch total loss 6.55146313\n",
      "Trained batch 699 batch loss 5.76395798 epoch total loss 6.55033684\n",
      "Trained batch 700 batch loss 6.72713375 epoch total loss 6.55058956\n",
      "Trained batch 701 batch loss 6.61190796 epoch total loss 6.55067682\n",
      "Trained batch 702 batch loss 5.79961491 epoch total loss 6.54960728\n",
      "Trained batch 703 batch loss 6.55023432 epoch total loss 6.54960823\n",
      "Trained batch 704 batch loss 6.50115871 epoch total loss 6.54953909\n",
      "Trained batch 705 batch loss 6.47590303 epoch total loss 6.54943466\n",
      "Trained batch 706 batch loss 6.54725075 epoch total loss 6.5494318\n",
      "Trained batch 707 batch loss 6.55237722 epoch total loss 6.54943609\n",
      "Trained batch 708 batch loss 6.18268061 epoch total loss 6.54891777\n",
      "Trained batch 709 batch loss 5.84905052 epoch total loss 6.54793072\n",
      "Trained batch 710 batch loss 6.55176497 epoch total loss 6.54793596\n",
      "Trained batch 711 batch loss 6.6018095 epoch total loss 6.54801226\n",
      "Trained batch 712 batch loss 6.2671504 epoch total loss 6.54761791\n",
      "Trained batch 713 batch loss 6.54172421 epoch total loss 6.54760933\n",
      "Trained batch 714 batch loss 6.82605648 epoch total loss 6.54799938\n",
      "Trained batch 715 batch loss 6.61729097 epoch total loss 6.54809618\n",
      "Trained batch 716 batch loss 6.19224644 epoch total loss 6.54759932\n",
      "Trained batch 717 batch loss 6.35393715 epoch total loss 6.54732943\n",
      "Trained batch 718 batch loss 6.73973894 epoch total loss 6.54759741\n",
      "Trained batch 719 batch loss 6.64476204 epoch total loss 6.54773188\n",
      "Trained batch 720 batch loss 6.6143074 epoch total loss 6.54782438\n",
      "Trained batch 721 batch loss 6.13979959 epoch total loss 6.54725838\n",
      "Trained batch 722 batch loss 6.88532 epoch total loss 6.54772663\n",
      "Trained batch 723 batch loss 6.2419548 epoch total loss 6.54730368\n",
      "Trained batch 724 batch loss 5.58107853 epoch total loss 6.54596949\n",
      "Trained batch 725 batch loss 7.28216839 epoch total loss 6.54698467\n",
      "Trained batch 726 batch loss 6.13384676 epoch total loss 6.54641581\n",
      "Trained batch 727 batch loss 6.93812656 epoch total loss 6.54695415\n",
      "Trained batch 728 batch loss 5.92559814 epoch total loss 6.54610109\n",
      "Trained batch 729 batch loss 6.40057373 epoch total loss 6.5459013\n",
      "Trained batch 730 batch loss 6.264081 epoch total loss 6.54551506\n",
      "Trained batch 731 batch loss 5.9049 epoch total loss 6.54463863\n",
      "Trained batch 732 batch loss 5.40843582 epoch total loss 6.54308605\n",
      "Trained batch 733 batch loss 5.45652676 epoch total loss 6.54160404\n",
      "Trained batch 734 batch loss 5.4583168 epoch total loss 6.54012823\n",
      "Trained batch 735 batch loss 5.40461826 epoch total loss 6.53858376\n",
      "Trained batch 736 batch loss 4.88837624 epoch total loss 6.53634119\n",
      "Trained batch 737 batch loss 5.15704536 epoch total loss 6.53447\n",
      "Trained batch 738 batch loss 4.64280605 epoch total loss 6.53190613\n",
      "Trained batch 739 batch loss 5.29851055 epoch total loss 6.5302372\n",
      "Trained batch 740 batch loss 5.41055 epoch total loss 6.52872419\n",
      "Trained batch 741 batch loss 5.2986927 epoch total loss 6.52706432\n",
      "Trained batch 742 batch loss 5.2443018 epoch total loss 6.52533531\n",
      "Trained batch 743 batch loss 5.24073458 epoch total loss 6.5236063\n",
      "Trained batch 744 batch loss 5.88101482 epoch total loss 6.52274227\n",
      "Trained batch 745 batch loss 6.62459183 epoch total loss 6.52287912\n",
      "Trained batch 746 batch loss 7.35787582 epoch total loss 6.52399826\n",
      "Trained batch 747 batch loss 6.50150108 epoch total loss 6.52396822\n",
      "Trained batch 748 batch loss 6.82269239 epoch total loss 6.52436781\n",
      "Trained batch 749 batch loss 6.80888748 epoch total loss 6.52474785\n",
      "Trained batch 750 batch loss 6.58366203 epoch total loss 6.52482605\n",
      "Trained batch 751 batch loss 6.97340107 epoch total loss 6.525424\n",
      "Trained batch 752 batch loss 7.33005238 epoch total loss 6.52649403\n",
      "Trained batch 753 batch loss 7.15042782 epoch total loss 6.52732229\n",
      "Trained batch 754 batch loss 7.0909462 epoch total loss 6.5280695\n",
      "Trained batch 755 batch loss 6.73500824 epoch total loss 6.52834368\n",
      "Trained batch 756 batch loss 6.96629333 epoch total loss 6.52892303\n",
      "Trained batch 757 batch loss 6.76932859 epoch total loss 6.52924061\n",
      "Trained batch 758 batch loss 7.51280451 epoch total loss 6.53053808\n",
      "Trained batch 759 batch loss 6.7676878 epoch total loss 6.53085041\n",
      "Trained batch 760 batch loss 6.92971325 epoch total loss 6.53137541\n",
      "Trained batch 761 batch loss 6.84310055 epoch total loss 6.53178501\n",
      "Trained batch 762 batch loss 6.81847954 epoch total loss 6.53216124\n",
      "Trained batch 763 batch loss 6.93040371 epoch total loss 6.5326829\n",
      "Trained batch 764 batch loss 6.58940458 epoch total loss 6.53275681\n",
      "Trained batch 765 batch loss 6.84743452 epoch total loss 6.53316879\n",
      "Trained batch 766 batch loss 6.47999525 epoch total loss 6.53309917\n",
      "Trained batch 767 batch loss 6.49909306 epoch total loss 6.53305483\n",
      "Trained batch 768 batch loss 7.36546135 epoch total loss 6.5341382\n",
      "Trained batch 769 batch loss 6.38560963 epoch total loss 6.53394556\n",
      "Trained batch 770 batch loss 6.80393696 epoch total loss 6.53429556\n",
      "Trained batch 771 batch loss 6.33017826 epoch total loss 6.53403091\n",
      "Trained batch 772 batch loss 6.84083462 epoch total loss 6.53442812\n",
      "Trained batch 773 batch loss 6.39015484 epoch total loss 6.53424168\n",
      "Trained batch 774 batch loss 6.37576199 epoch total loss 6.53403711\n",
      "Trained batch 775 batch loss 7.73187828 epoch total loss 6.53558302\n",
      "Trained batch 776 batch loss 6.55116177 epoch total loss 6.53560305\n",
      "Trained batch 777 batch loss 6.50640154 epoch total loss 6.53556538\n",
      "Trained batch 778 batch loss 7.01475143 epoch total loss 6.53618097\n",
      "Trained batch 779 batch loss 6.870327 epoch total loss 6.53660965\n",
      "Trained batch 780 batch loss 6.70545292 epoch total loss 6.53682661\n",
      "Trained batch 781 batch loss 6.51402187 epoch total loss 6.53679752\n",
      "Trained batch 782 batch loss 7.20755196 epoch total loss 6.53765488\n",
      "Trained batch 783 batch loss 6.53723621 epoch total loss 6.5376544\n",
      "Trained batch 784 batch loss 6.84202862 epoch total loss 6.53804255\n",
      "Trained batch 785 batch loss 7.33996773 epoch total loss 6.53906393\n",
      "Trained batch 786 batch loss 6.85695744 epoch total loss 6.53946829\n",
      "Trained batch 787 batch loss 7.34492302 epoch total loss 6.54049158\n",
      "Trained batch 788 batch loss 6.56251478 epoch total loss 6.54051924\n",
      "Trained batch 789 batch loss 6.38908386 epoch total loss 6.54032755\n",
      "Trained batch 790 batch loss 7.48205423 epoch total loss 6.54151917\n",
      "Trained batch 791 batch loss 6.47976494 epoch total loss 6.54144144\n",
      "Trained batch 792 batch loss 6.4734273 epoch total loss 6.54135609\n",
      "Trained batch 793 batch loss 6.70381165 epoch total loss 6.54156065\n",
      "Trained batch 794 batch loss 6.99496078 epoch total loss 6.5421319\n",
      "Trained batch 795 batch loss 6.42066813 epoch total loss 6.54197931\n",
      "Trained batch 796 batch loss 6.37485552 epoch total loss 6.5417695\n",
      "Trained batch 797 batch loss 6.46421146 epoch total loss 6.54167223\n",
      "Trained batch 798 batch loss 6.56769276 epoch total loss 6.54170513\n",
      "Trained batch 799 batch loss 7.01694489 epoch total loss 6.5423\n",
      "Trained batch 800 batch loss 6.73271942 epoch total loss 6.54253864\n",
      "Trained batch 801 batch loss 6.58581543 epoch total loss 6.54259253\n",
      "Trained batch 802 batch loss 6.48342419 epoch total loss 6.54251862\n",
      "Trained batch 803 batch loss 6.43628073 epoch total loss 6.54238701\n",
      "Trained batch 804 batch loss 6.84747553 epoch total loss 6.54276657\n",
      "Trained batch 805 batch loss 6.63007212 epoch total loss 6.54287481\n",
      "Trained batch 806 batch loss 6.72149277 epoch total loss 6.54309654\n",
      "Trained batch 807 batch loss 6.68567181 epoch total loss 6.54327297\n",
      "Trained batch 808 batch loss 6.32259369 epoch total loss 6.543\n",
      "Trained batch 809 batch loss 6.79373884 epoch total loss 6.54331\n",
      "Trained batch 810 batch loss 7.06670332 epoch total loss 6.54395676\n",
      "Trained batch 811 batch loss 7.35987759 epoch total loss 6.54496288\n",
      "Trained batch 812 batch loss 6.60080051 epoch total loss 6.54503155\n",
      "Trained batch 813 batch loss 6.66807 epoch total loss 6.5451827\n",
      "Trained batch 814 batch loss 6.29023838 epoch total loss 6.54486895\n",
      "Trained batch 815 batch loss 6.00184774 epoch total loss 6.5442028\n",
      "Trained batch 816 batch loss 4.91819334 epoch total loss 6.54221\n",
      "Trained batch 817 batch loss 6.57254791 epoch total loss 6.5422473\n",
      "Trained batch 818 batch loss 6.54836082 epoch total loss 6.54225492\n",
      "Trained batch 819 batch loss 6.43139458 epoch total loss 6.54211903\n",
      "Trained batch 820 batch loss 6.00620127 epoch total loss 6.54146576\n",
      "Trained batch 821 batch loss 5.80679607 epoch total loss 6.54057074\n",
      "Trained batch 822 batch loss 6.9854188 epoch total loss 6.54111195\n",
      "Trained batch 823 batch loss 6.56844187 epoch total loss 6.54114485\n",
      "Trained batch 824 batch loss 6.71888065 epoch total loss 6.54136038\n",
      "Trained batch 825 batch loss 6.37076712 epoch total loss 6.54115343\n",
      "Trained batch 826 batch loss 6.11017036 epoch total loss 6.54063177\n",
      "Trained batch 827 batch loss 6.70284653 epoch total loss 6.54082775\n",
      "Trained batch 828 batch loss 6.96398163 epoch total loss 6.54133892\n",
      "Trained batch 829 batch loss 6.94016075 epoch total loss 6.54181957\n",
      "Trained batch 830 batch loss 6.60056067 epoch total loss 6.54189062\n",
      "Trained batch 831 batch loss 6.60225 epoch total loss 6.54196262\n",
      "Trained batch 832 batch loss 5.46245384 epoch total loss 6.54066515\n",
      "Trained batch 833 batch loss 6.79877901 epoch total loss 6.54097509\n",
      "Trained batch 834 batch loss 6.69273329 epoch total loss 6.54115725\n",
      "Trained batch 835 batch loss 6.16150475 epoch total loss 6.54070282\n",
      "Trained batch 836 batch loss 5.85893822 epoch total loss 6.53988743\n",
      "Trained batch 837 batch loss 6.34867859 epoch total loss 6.53965855\n",
      "Trained batch 838 batch loss 6.72484159 epoch total loss 6.53987932\n",
      "Trained batch 839 batch loss 6.76029587 epoch total loss 6.54014206\n",
      "Trained batch 840 batch loss 7.22934246 epoch total loss 6.5409627\n",
      "Trained batch 841 batch loss 6.69151354 epoch total loss 6.54114151\n",
      "Trained batch 842 batch loss 6.60092354 epoch total loss 6.54121256\n",
      "Trained batch 843 batch loss 6.29167747 epoch total loss 6.54091644\n",
      "Trained batch 844 batch loss 7.78580761 epoch total loss 6.5423913\n",
      "Trained batch 845 batch loss 6.2459259 epoch total loss 6.54204082\n",
      "Trained batch 846 batch loss 6.64268589 epoch total loss 6.54215956\n",
      "Trained batch 847 batch loss 6.36606598 epoch total loss 6.54195166\n",
      "Trained batch 848 batch loss 6.41932631 epoch total loss 6.54180717\n",
      "Trained batch 849 batch loss 6.6177187 epoch total loss 6.54189682\n",
      "Trained batch 850 batch loss 6.59591675 epoch total loss 6.54196024\n",
      "Trained batch 851 batch loss 6.47475815 epoch total loss 6.54188108\n",
      "Trained batch 852 batch loss 6.45662498 epoch total loss 6.54178095\n",
      "Trained batch 853 batch loss 6.4095149 epoch total loss 6.54162598\n",
      "Trained batch 854 batch loss 6.4093008 epoch total loss 6.54147053\n",
      "Trained batch 855 batch loss 6.6111536 epoch total loss 6.54155254\n",
      "Trained batch 856 batch loss 6.07620525 epoch total loss 6.54100895\n",
      "Trained batch 857 batch loss 6.10712624 epoch total loss 6.54050207\n",
      "Trained batch 858 batch loss 6.31509352 epoch total loss 6.54023933\n",
      "Trained batch 859 batch loss 5.7313385 epoch total loss 6.53929806\n",
      "Trained batch 860 batch loss 6.82210064 epoch total loss 6.53962708\n",
      "Trained batch 861 batch loss 6.20321751 epoch total loss 6.53923607\n",
      "Trained batch 862 batch loss 6.31558561 epoch total loss 6.53897619\n",
      "Trained batch 863 batch loss 6.39465427 epoch total loss 6.53880882\n",
      "Trained batch 864 batch loss 6.46782923 epoch total loss 6.53872681\n",
      "Trained batch 865 batch loss 6.36782169 epoch total loss 6.53852892\n",
      "Trained batch 866 batch loss 5.54324579 epoch total loss 6.53738\n",
      "Trained batch 867 batch loss 5.80316305 epoch total loss 6.53653336\n",
      "Trained batch 868 batch loss 6.70058155 epoch total loss 6.53672218\n",
      "Trained batch 869 batch loss 5.87434 epoch total loss 6.53596\n",
      "Trained batch 870 batch loss 6.37609291 epoch total loss 6.53577662\n",
      "Trained batch 871 batch loss 6.62542248 epoch total loss 6.53587961\n",
      "Trained batch 872 batch loss 6.5797224 epoch total loss 6.53592968\n",
      "Trained batch 873 batch loss 6.72562027 epoch total loss 6.53614664\n",
      "Trained batch 874 batch loss 6.25400352 epoch total loss 6.53582382\n",
      "Trained batch 875 batch loss 6.37282276 epoch total loss 6.53563786\n",
      "Trained batch 876 batch loss 6.16102791 epoch total loss 6.53521\n",
      "Trained batch 877 batch loss 6.80967426 epoch total loss 6.53552294\n",
      "Trained batch 878 batch loss 6.57086229 epoch total loss 6.53556347\n",
      "Trained batch 879 batch loss 6.58406353 epoch total loss 6.53561831\n",
      "Trained batch 880 batch loss 6.64773846 epoch total loss 6.5357461\n",
      "Trained batch 881 batch loss 6.40730953 epoch total loss 6.5356\n",
      "Trained batch 882 batch loss 6.70472765 epoch total loss 6.53579187\n",
      "Trained batch 883 batch loss 6.38735294 epoch total loss 6.53562355\n",
      "Trained batch 884 batch loss 6.21802807 epoch total loss 6.53526449\n",
      "Trained batch 885 batch loss 6.13365 epoch total loss 6.53481102\n",
      "Trained batch 886 batch loss 6.13342953 epoch total loss 6.53435755\n",
      "Trained batch 887 batch loss 6.53458691 epoch total loss 6.53435802\n",
      "Trained batch 888 batch loss 6.23508644 epoch total loss 6.5340209\n",
      "Trained batch 889 batch loss 6.36011314 epoch total loss 6.5338254\n",
      "Trained batch 890 batch loss 6.56702709 epoch total loss 6.53386259\n",
      "Trained batch 891 batch loss 6.3217926 epoch total loss 6.53362465\n",
      "Trained batch 892 batch loss 6.07169914 epoch total loss 6.5331068\n",
      "Trained batch 893 batch loss 6.16124392 epoch total loss 6.53269\n",
      "Trained batch 894 batch loss 6.08220387 epoch total loss 6.53218603\n",
      "Trained batch 895 batch loss 6.1430006 epoch total loss 6.53175116\n",
      "Trained batch 896 batch loss 6.01957321 epoch total loss 6.53118\n",
      "Trained batch 897 batch loss 6.21148682 epoch total loss 6.53082323\n",
      "Trained batch 898 batch loss 5.91934109 epoch total loss 6.53014231\n",
      "Trained batch 899 batch loss 6.22670078 epoch total loss 6.52980471\n",
      "Trained batch 900 batch loss 6.15293741 epoch total loss 6.52938604\n",
      "Trained batch 901 batch loss 6.15713692 epoch total loss 6.52897263\n",
      "Trained batch 902 batch loss 5.96864796 epoch total loss 6.52835178\n",
      "Trained batch 903 batch loss 6.13040304 epoch total loss 6.52791119\n",
      "Trained batch 904 batch loss 6.25274801 epoch total loss 6.52760696\n",
      "Trained batch 905 batch loss 5.95212698 epoch total loss 6.52697086\n",
      "Trained batch 906 batch loss 5.87596703 epoch total loss 6.52625227\n",
      "Trained batch 907 batch loss 5.91662693 epoch total loss 6.52558\n",
      "Trained batch 908 batch loss 6.35031128 epoch total loss 6.52538681\n",
      "Trained batch 909 batch loss 6.90863609 epoch total loss 6.52580833\n",
      "Trained batch 910 batch loss 6.25841808 epoch total loss 6.5255146\n",
      "Trained batch 911 batch loss 6.32449627 epoch total loss 6.5252943\n",
      "Trained batch 912 batch loss 6.72881556 epoch total loss 6.52551746\n",
      "Trained batch 913 batch loss 6.79598427 epoch total loss 6.52581358\n",
      "Trained batch 914 batch loss 6.76698112 epoch total loss 6.52607775\n",
      "Trained batch 915 batch loss 5.62214565 epoch total loss 6.52508974\n",
      "Trained batch 916 batch loss 6.4242382 epoch total loss 6.52497959\n",
      "Trained batch 917 batch loss 7.32141304 epoch total loss 6.52584791\n",
      "Trained batch 918 batch loss 6.93765259 epoch total loss 6.52629662\n",
      "Trained batch 919 batch loss 7.17385149 epoch total loss 6.5270009\n",
      "Trained batch 920 batch loss 7.28291321 epoch total loss 6.52782249\n",
      "Trained batch 921 batch loss 6.8227787 epoch total loss 6.52814293\n",
      "Trained batch 922 batch loss 6.6726408 epoch total loss 6.5283\n",
      "Trained batch 923 batch loss 6.69357252 epoch total loss 6.52847862\n",
      "Trained batch 924 batch loss 6.67968893 epoch total loss 6.52864218\n",
      "Trained batch 925 batch loss 6.4863615 epoch total loss 6.5285964\n",
      "Trained batch 926 batch loss 6.76029873 epoch total loss 6.52884674\n",
      "Trained batch 927 batch loss 7.68945932 epoch total loss 6.53009844\n",
      "Trained batch 928 batch loss 7.60070801 epoch total loss 6.53125191\n",
      "Trained batch 929 batch loss 7.22489643 epoch total loss 6.53199911\n",
      "Trained batch 930 batch loss 6.40787601 epoch total loss 6.53186512\n",
      "Trained batch 931 batch loss 6.35661459 epoch total loss 6.53167677\n",
      "Trained batch 932 batch loss 6.53176928 epoch total loss 6.53167677\n",
      "Trained batch 933 batch loss 6.24285126 epoch total loss 6.5313673\n",
      "Trained batch 934 batch loss 5.66166878 epoch total loss 6.53043604\n",
      "Trained batch 935 batch loss 5.49785662 epoch total loss 6.52933168\n",
      "Trained batch 936 batch loss 6.78081274 epoch total loss 6.52960062\n",
      "Trained batch 937 batch loss 6.97161341 epoch total loss 6.53007221\n",
      "Trained batch 938 batch loss 6.78096676 epoch total loss 6.53033972\n",
      "Trained batch 939 batch loss 6.57916832 epoch total loss 6.53039169\n",
      "Trained batch 940 batch loss 6.56731844 epoch total loss 6.53043079\n",
      "Trained batch 941 batch loss 6.67876434 epoch total loss 6.53058863\n",
      "Trained batch 942 batch loss 6.12173223 epoch total loss 6.53015423\n",
      "Trained batch 943 batch loss 6.60486698 epoch total loss 6.53023338\n",
      "Trained batch 944 batch loss 6.41829634 epoch total loss 6.53011513\n",
      "Trained batch 945 batch loss 6.72077322 epoch total loss 6.53031683\n",
      "Trained batch 946 batch loss 6.83940125 epoch total loss 6.53064346\n",
      "Trained batch 947 batch loss 7.23561049 epoch total loss 6.53138828\n",
      "Trained batch 948 batch loss 6.99462271 epoch total loss 6.53187704\n",
      "Trained batch 949 batch loss 6.83330631 epoch total loss 6.53219461\n",
      "Trained batch 950 batch loss 6.57529879 epoch total loss 6.53224\n",
      "Trained batch 951 batch loss 6.62293816 epoch total loss 6.53233528\n",
      "Trained batch 952 batch loss 6.44533825 epoch total loss 6.53224421\n",
      "Trained batch 953 batch loss 6.45080662 epoch total loss 6.53215837\n",
      "Trained batch 954 batch loss 6.22923374 epoch total loss 6.5318408\n",
      "Trained batch 955 batch loss 6.53785229 epoch total loss 6.531847\n",
      "Trained batch 956 batch loss 7.39795542 epoch total loss 6.53275299\n",
      "Trained batch 957 batch loss 7.39866257 epoch total loss 6.53365755\n",
      "Trained batch 958 batch loss 7.34894466 epoch total loss 6.53450918\n",
      "Trained batch 959 batch loss 6.46632099 epoch total loss 6.53443766\n",
      "Trained batch 960 batch loss 6.29368 epoch total loss 6.53418684\n",
      "Trained batch 961 batch loss 5.76272821 epoch total loss 6.53338385\n",
      "Trained batch 962 batch loss 6.46178246 epoch total loss 6.53331\n",
      "Trained batch 963 batch loss 6.07736778 epoch total loss 6.53283596\n",
      "Trained batch 964 batch loss 5.93720627 epoch total loss 6.53221798\n",
      "Trained batch 965 batch loss 6.65235806 epoch total loss 6.53234243\n",
      "Trained batch 966 batch loss 5.44012451 epoch total loss 6.53121138\n",
      "Trained batch 967 batch loss 5.9482708 epoch total loss 6.53060865\n",
      "Trained batch 968 batch loss 7.38196754 epoch total loss 6.53148794\n",
      "Trained batch 969 batch loss 6.50581598 epoch total loss 6.53146172\n",
      "Trained batch 970 batch loss 7.27955532 epoch total loss 6.53223324\n",
      "Trained batch 971 batch loss 6.94931269 epoch total loss 6.53266239\n",
      "Trained batch 972 batch loss 7.27776957 epoch total loss 6.53342915\n",
      "Trained batch 973 batch loss 6.69752264 epoch total loss 6.53359795\n",
      "Trained batch 974 batch loss 6.65275288 epoch total loss 6.53372049\n",
      "Trained batch 975 batch loss 6.77351952 epoch total loss 6.53396654\n",
      "Trained batch 976 batch loss 6.35101891 epoch total loss 6.53377914\n",
      "Trained batch 977 batch loss 6.71702814 epoch total loss 6.53396606\n",
      "Trained batch 978 batch loss 6.22105122 epoch total loss 6.53364658\n",
      "Trained batch 979 batch loss 6.68126965 epoch total loss 6.53379726\n",
      "Trained batch 980 batch loss 6.6392107 epoch total loss 6.53390455\n",
      "Trained batch 981 batch loss 6.11536789 epoch total loss 6.53347778\n",
      "Trained batch 982 batch loss 6.31924629 epoch total loss 6.53326\n",
      "Trained batch 983 batch loss 5.61170197 epoch total loss 6.53232241\n",
      "Trained batch 984 batch loss 5.77588224 epoch total loss 6.53155375\n",
      "Trained batch 985 batch loss 6.29331875 epoch total loss 6.53131199\n",
      "Trained batch 986 batch loss 6.81735897 epoch total loss 6.53160191\n",
      "Trained batch 987 batch loss 6.5775423 epoch total loss 6.53164864\n",
      "Trained batch 988 batch loss 7.26023197 epoch total loss 6.5323863\n",
      "Trained batch 989 batch loss 6.61447906 epoch total loss 6.5324688\n",
      "Trained batch 990 batch loss 6.7041254 epoch total loss 6.53264236\n",
      "Trained batch 991 batch loss 5.66662312 epoch total loss 6.53176832\n",
      "Trained batch 992 batch loss 6.09268951 epoch total loss 6.53132582\n",
      "Trained batch 993 batch loss 5.97400427 epoch total loss 6.53076458\n",
      "Trained batch 994 batch loss 6.93082476 epoch total loss 6.53116703\n",
      "Trained batch 995 batch loss 6.35366631 epoch total loss 6.53098822\n",
      "Trained batch 996 batch loss 6.66891575 epoch total loss 6.53112698\n",
      "Trained batch 997 batch loss 7.20418835 epoch total loss 6.53180218\n",
      "Trained batch 998 batch loss 7.82146549 epoch total loss 6.53309393\n",
      "Trained batch 999 batch loss 7.11160278 epoch total loss 6.53367329\n",
      "Trained batch 1000 batch loss 6.21599722 epoch total loss 6.53335524\n",
      "Trained batch 1001 batch loss 7.33462286 epoch total loss 6.53415585\n",
      "Trained batch 1002 batch loss 6.81939316 epoch total loss 6.53444052\n",
      "Trained batch 1003 batch loss 6.7822423 epoch total loss 6.53468752\n",
      "Trained batch 1004 batch loss 6.87507153 epoch total loss 6.53502655\n",
      "Trained batch 1005 batch loss 7.14393044 epoch total loss 6.53563261\n",
      "Trained batch 1006 batch loss 6.12478495 epoch total loss 6.53522444\n",
      "Trained batch 1007 batch loss 5.4768343 epoch total loss 6.53417349\n",
      "Trained batch 1008 batch loss 6.25379181 epoch total loss 6.53389549\n",
      "Trained batch 1009 batch loss 4.99044228 epoch total loss 6.53236532\n",
      "Trained batch 1010 batch loss 6.22334766 epoch total loss 6.53205919\n",
      "Trained batch 1011 batch loss 6.66103649 epoch total loss 6.53218699\n",
      "Trained batch 1012 batch loss 6.02232 epoch total loss 6.53168344\n",
      "Trained batch 1013 batch loss 5.33639479 epoch total loss 6.53050327\n",
      "Trained batch 1014 batch loss 6.41840076 epoch total loss 6.53039265\n",
      "Trained batch 1015 batch loss 7.12603188 epoch total loss 6.53097963\n",
      "Trained batch 1016 batch loss 7.17488766 epoch total loss 6.53161335\n",
      "Trained batch 1017 batch loss 7.29670525 epoch total loss 6.5323658\n",
      "Trained batch 1018 batch loss 5.16522169 epoch total loss 6.53102255\n",
      "Trained batch 1019 batch loss 7.03214025 epoch total loss 6.53151464\n",
      "Trained batch 1020 batch loss 5.82361889 epoch total loss 6.53082037\n",
      "Trained batch 1021 batch loss 5.6939 epoch total loss 6.53000069\n",
      "Trained batch 1022 batch loss 7.38255835 epoch total loss 6.53083467\n",
      "Trained batch 1023 batch loss 7.08694935 epoch total loss 6.53137827\n",
      "Trained batch 1024 batch loss 6.75280476 epoch total loss 6.53159475\n",
      "Trained batch 1025 batch loss 5.39072609 epoch total loss 6.53048182\n",
      "Trained batch 1026 batch loss 6.86364555 epoch total loss 6.53080654\n",
      "Trained batch 1027 batch loss 6.64001369 epoch total loss 6.53091288\n",
      "Trained batch 1028 batch loss 6.71869 epoch total loss 6.5310955\n",
      "Trained batch 1029 batch loss 6.28638363 epoch total loss 6.53085804\n",
      "Trained batch 1030 batch loss 6.55391121 epoch total loss 6.53088045\n",
      "Trained batch 1031 batch loss 6.82024 epoch total loss 6.53116083\n",
      "Trained batch 1032 batch loss 6.87947 epoch total loss 6.53149843\n",
      "Trained batch 1033 batch loss 7.14109802 epoch total loss 6.53208876\n",
      "Trained batch 1034 batch loss 6.38764811 epoch total loss 6.53194904\n",
      "Trained batch 1035 batch loss 6.15514946 epoch total loss 6.53158474\n",
      "Trained batch 1036 batch loss 7.77293301 epoch total loss 6.53278303\n",
      "Trained batch 1037 batch loss 7.38339853 epoch total loss 6.53360319\n",
      "Trained batch 1038 batch loss 7.31893 epoch total loss 6.53436\n",
      "Trained batch 1039 batch loss 6.80713463 epoch total loss 6.53462219\n",
      "Trained batch 1040 batch loss 6.75156927 epoch total loss 6.53483105\n",
      "Trained batch 1041 batch loss 7.31944 epoch total loss 6.53558445\n",
      "Trained batch 1042 batch loss 6.68571 epoch total loss 6.53572845\n",
      "Trained batch 1043 batch loss 6.54183 epoch total loss 6.53573465\n",
      "Trained batch 1044 batch loss 6.54272509 epoch total loss 6.53574133\n",
      "Trained batch 1045 batch loss 6.48105955 epoch total loss 6.53568888\n",
      "Trained batch 1046 batch loss 6.83156586 epoch total loss 6.53597164\n",
      "Trained batch 1047 batch loss 6.59589958 epoch total loss 6.53602886\n",
      "Trained batch 1048 batch loss 6.79619122 epoch total loss 6.53627729\n",
      "Trained batch 1049 batch loss 5.00230503 epoch total loss 6.53481483\n",
      "Trained batch 1050 batch loss 6.59824944 epoch total loss 6.53487539\n",
      "Trained batch 1051 batch loss 6.45382786 epoch total loss 6.53479815\n",
      "Trained batch 1052 batch loss 6.45039749 epoch total loss 6.53471756\n",
      "Trained batch 1053 batch loss 6.89021778 epoch total loss 6.53505516\n",
      "Trained batch 1054 batch loss 6.45054388 epoch total loss 6.53497505\n",
      "Trained batch 1055 batch loss 6.47047329 epoch total loss 6.53491402\n",
      "Trained batch 1056 batch loss 5.30831861 epoch total loss 6.53375244\n",
      "Trained batch 1057 batch loss 6.20399 epoch total loss 6.53344059\n",
      "Trained batch 1058 batch loss 5.51524258 epoch total loss 6.53247786\n",
      "Trained batch 1059 batch loss 5.83124161 epoch total loss 6.53181553\n",
      "Trained batch 1060 batch loss 6.68677187 epoch total loss 6.53196192\n",
      "Trained batch 1061 batch loss 6.13832426 epoch total loss 6.53159094\n",
      "Trained batch 1062 batch loss 6.62608719 epoch total loss 6.53168\n",
      "Trained batch 1063 batch loss 6.83248 epoch total loss 6.53196287\n",
      "Trained batch 1064 batch loss 6.5315 epoch total loss 6.53196287\n",
      "Trained batch 1065 batch loss 6.88591719 epoch total loss 6.53229475\n",
      "Trained batch 1066 batch loss 6.59644747 epoch total loss 6.53235531\n",
      "Trained batch 1067 batch loss 6.47674656 epoch total loss 6.53230286\n",
      "Trained batch 1068 batch loss 6.53726721 epoch total loss 6.53230762\n",
      "Trained batch 1069 batch loss 6.78300095 epoch total loss 6.53254223\n",
      "Trained batch 1070 batch loss 6.49923515 epoch total loss 6.53251076\n",
      "Trained batch 1071 batch loss 6.19915485 epoch total loss 6.5322\n",
      "Trained batch 1072 batch loss 4.67517471 epoch total loss 6.53046751\n",
      "Trained batch 1073 batch loss 5.17429829 epoch total loss 6.52920341\n",
      "Trained batch 1074 batch loss 6.58584785 epoch total loss 6.52925634\n",
      "Trained batch 1075 batch loss 5.90117836 epoch total loss 6.52867222\n",
      "Trained batch 1076 batch loss 6.58193 epoch total loss 6.52872181\n",
      "Trained batch 1077 batch loss 6.04568 epoch total loss 6.52827358\n",
      "Trained batch 1078 batch loss 5.64257717 epoch total loss 6.52745199\n",
      "Trained batch 1079 batch loss 6.0054512 epoch total loss 6.526968\n",
      "Trained batch 1080 batch loss 5.9120245 epoch total loss 6.52639866\n",
      "Trained batch 1081 batch loss 6.11029339 epoch total loss 6.52601385\n",
      "Trained batch 1082 batch loss 5.1533227 epoch total loss 6.52474546\n",
      "Trained batch 1083 batch loss 5.08430719 epoch total loss 6.52341557\n",
      "Trained batch 1084 batch loss 4.73433304 epoch total loss 6.52176523\n",
      "Trained batch 1085 batch loss 5.17972 epoch total loss 6.52052784\n",
      "Trained batch 1086 batch loss 5.87007332 epoch total loss 6.51992893\n",
      "Trained batch 1087 batch loss 6.06966305 epoch total loss 6.51951504\n",
      "Trained batch 1088 batch loss 6.61292601 epoch total loss 6.51960087\n",
      "Trained batch 1089 batch loss 7.51051283 epoch total loss 6.52051115\n",
      "Trained batch 1090 batch loss 6.83775616 epoch total loss 6.52080202\n",
      "Trained batch 1091 batch loss 6.7138 epoch total loss 6.52097893\n",
      "Trained batch 1092 batch loss 6.36919117 epoch total loss 6.52084\n",
      "Trained batch 1093 batch loss 6.76784563 epoch total loss 6.52106619\n",
      "Trained batch 1094 batch loss 5.56891203 epoch total loss 6.52019596\n",
      "Trained batch 1095 batch loss 6.16244221 epoch total loss 6.51986933\n",
      "Trained batch 1096 batch loss 5.74837065 epoch total loss 6.51916552\n",
      "Trained batch 1097 batch loss 5.96576071 epoch total loss 6.51866102\n",
      "Trained batch 1098 batch loss 6.29147387 epoch total loss 6.51845407\n",
      "Trained batch 1099 batch loss 6.72300577 epoch total loss 6.51864052\n",
      "Trained batch 1100 batch loss 6.50570297 epoch total loss 6.5186286\n",
      "Trained batch 1101 batch loss 5.81703758 epoch total loss 6.51799154\n",
      "Trained batch 1102 batch loss 5.13557386 epoch total loss 6.51673698\n",
      "Trained batch 1103 batch loss 5.11972952 epoch total loss 6.5154705\n",
      "Trained batch 1104 batch loss 5.88660908 epoch total loss 6.51490116\n",
      "Trained batch 1105 batch loss 5.3843832 epoch total loss 6.51387787\n",
      "Trained batch 1106 batch loss 6.54355383 epoch total loss 6.51390457\n",
      "Trained batch 1107 batch loss 6.61021423 epoch total loss 6.51399183\n",
      "Trained batch 1108 batch loss 6.45738363 epoch total loss 6.51394081\n",
      "Trained batch 1109 batch loss 6.61146355 epoch total loss 6.51402855\n",
      "Trained batch 1110 batch loss 6.54165745 epoch total loss 6.51405334\n",
      "Trained batch 1111 batch loss 6.71086311 epoch total loss 6.51423025\n",
      "Trained batch 1112 batch loss 5.31916952 epoch total loss 6.51315594\n",
      "Trained batch 1113 batch loss 6.81787205 epoch total loss 6.51342964\n",
      "Trained batch 1114 batch loss 6.30014277 epoch total loss 6.51323843\n",
      "Trained batch 1115 batch loss 5.8570447 epoch total loss 6.51264954\n",
      "Trained batch 1116 batch loss 5.32791424 epoch total loss 6.51158857\n",
      "Trained batch 1117 batch loss 6.10373592 epoch total loss 6.51122284\n",
      "Trained batch 1118 batch loss 5.33560896 epoch total loss 6.51017141\n",
      "Trained batch 1119 batch loss 4.66431332 epoch total loss 6.50852203\n",
      "Trained batch 1120 batch loss 4.93964052 epoch total loss 6.50712109\n",
      "Trained batch 1121 batch loss 5.93368292 epoch total loss 6.50660944\n",
      "Trained batch 1122 batch loss 5.94262648 epoch total loss 6.50610638\n",
      "Trained batch 1123 batch loss 6.23915958 epoch total loss 6.50586891\n",
      "Trained batch 1124 batch loss 6.41054916 epoch total loss 6.50578403\n",
      "Trained batch 1125 batch loss 5.8223114 epoch total loss 6.50517654\n",
      "Trained batch 1126 batch loss 5.04602098 epoch total loss 6.5038805\n",
      "Trained batch 1127 batch loss 5.74337196 epoch total loss 6.50320578\n",
      "Trained batch 1128 batch loss 5.90222216 epoch total loss 6.50267315\n",
      "Trained batch 1129 batch loss 5.25433779 epoch total loss 6.50156736\n",
      "Trained batch 1130 batch loss 5.79362488 epoch total loss 6.5009408\n",
      "Trained batch 1131 batch loss 5.76845598 epoch total loss 6.50029325\n",
      "Trained batch 1132 batch loss 6.03133 epoch total loss 6.49987888\n",
      "Trained batch 1133 batch loss 5.30258751 epoch total loss 6.49882221\n",
      "Trained batch 1134 batch loss 6.28591442 epoch total loss 6.49863482\n",
      "Trained batch 1135 batch loss 6.36005831 epoch total loss 6.49851227\n",
      "Trained batch 1136 batch loss 5.15265036 epoch total loss 6.4973278\n",
      "Trained batch 1137 batch loss 6.49233723 epoch total loss 6.49732304\n",
      "Trained batch 1138 batch loss 5.29987526 epoch total loss 6.49627113\n",
      "Trained batch 1139 batch loss 5.31936932 epoch total loss 6.49523783\n",
      "Trained batch 1140 batch loss 5.63167667 epoch total loss 6.49448\n",
      "Trained batch 1141 batch loss 6.4389019 epoch total loss 6.4944315\n",
      "Trained batch 1142 batch loss 5.12323093 epoch total loss 6.49323082\n",
      "Trained batch 1143 batch loss 4.92581606 epoch total loss 6.49185944\n",
      "Trained batch 1144 batch loss 5.11742544 epoch total loss 6.49065781\n",
      "Trained batch 1145 batch loss 5.85400963 epoch total loss 6.49010181\n",
      "Trained batch 1146 batch loss 5.14731884 epoch total loss 6.48893\n",
      "Trained batch 1147 batch loss 6.51564407 epoch total loss 6.48895359\n",
      "Trained batch 1148 batch loss 5.87399578 epoch total loss 6.48841763\n",
      "Trained batch 1149 batch loss 6.22643852 epoch total loss 6.4881897\n",
      "Trained batch 1150 batch loss 5.35962296 epoch total loss 6.48720884\n",
      "Trained batch 1151 batch loss 5.20068932 epoch total loss 6.48609114\n",
      "Trained batch 1152 batch loss 7.0029006 epoch total loss 6.48654\n",
      "Trained batch 1153 batch loss 7.69322872 epoch total loss 6.4875865\n",
      "Trained batch 1154 batch loss 6.85307503 epoch total loss 6.48790312\n",
      "Trained batch 1155 batch loss 6.50220776 epoch total loss 6.48791552\n",
      "Trained batch 1156 batch loss 6.4692688 epoch total loss 6.4878993\n",
      "Trained batch 1157 batch loss 6.40236092 epoch total loss 6.48782539\n",
      "Trained batch 1158 batch loss 6.57050705 epoch total loss 6.48789692\n",
      "Trained batch 1159 batch loss 6.48549 epoch total loss 6.48789454\n",
      "Trained batch 1160 batch loss 6.57245064 epoch total loss 6.48796701\n",
      "Trained batch 1161 batch loss 6.29763 epoch total loss 6.48780346\n",
      "Trained batch 1162 batch loss 6.19536924 epoch total loss 6.48755169\n",
      "Trained batch 1163 batch loss 6.28752041 epoch total loss 6.48738\n",
      "Trained batch 1164 batch loss 6.19699717 epoch total loss 6.48713\n",
      "Trained batch 1165 batch loss 6.32280064 epoch total loss 6.48698902\n",
      "Trained batch 1166 batch loss 5.87809801 epoch total loss 6.48646688\n",
      "Trained batch 1167 batch loss 6.57757473 epoch total loss 6.48654509\n",
      "Trained batch 1168 batch loss 6.41956377 epoch total loss 6.48648739\n",
      "Trained batch 1169 batch loss 6.46581745 epoch total loss 6.48646975\n",
      "Trained batch 1170 batch loss 6.62756824 epoch total loss 6.48659039\n",
      "Trained batch 1171 batch loss 6.45118189 epoch total loss 6.48656\n",
      "Trained batch 1172 batch loss 6.15251827 epoch total loss 6.48627472\n",
      "Trained batch 1173 batch loss 6.13009548 epoch total loss 6.48597097\n",
      "Trained batch 1174 batch loss 6.23092222 epoch total loss 6.48575354\n",
      "Trained batch 1175 batch loss 6.55284214 epoch total loss 6.48581076\n",
      "Trained batch 1176 batch loss 5.75500441 epoch total loss 6.48518944\n",
      "Trained batch 1177 batch loss 6.56118441 epoch total loss 6.48525381\n",
      "Trained batch 1178 batch loss 6.44196367 epoch total loss 6.48521662\n",
      "Trained batch 1179 batch loss 6.30438423 epoch total loss 6.48506355\n",
      "Trained batch 1180 batch loss 6.45352316 epoch total loss 6.48503685\n",
      "Trained batch 1181 batch loss 6.31800747 epoch total loss 6.48489523\n",
      "Trained batch 1182 batch loss 6.49187899 epoch total loss 6.48490095\n",
      "Trained batch 1183 batch loss 6.46457672 epoch total loss 6.48488331\n",
      "Trained batch 1184 batch loss 6.71435165 epoch total loss 6.48507738\n",
      "Trained batch 1185 batch loss 6.6296854 epoch total loss 6.48519945\n",
      "Trained batch 1186 batch loss 6.22969151 epoch total loss 6.48498392\n",
      "Trained batch 1187 batch loss 6.16292191 epoch total loss 6.4847126\n",
      "Trained batch 1188 batch loss 6.31315041 epoch total loss 6.48456812\n",
      "Trained batch 1189 batch loss 7.26033497 epoch total loss 6.48522043\n",
      "Trained batch 1190 batch loss 6.77330494 epoch total loss 6.48546267\n",
      "Trained batch 1191 batch loss 6.9806776 epoch total loss 6.48587847\n",
      "Trained batch 1192 batch loss 7.10199642 epoch total loss 6.48639536\n",
      "Trained batch 1193 batch loss 6.84519958 epoch total loss 6.48669624\n",
      "Trained batch 1194 batch loss 6.15755367 epoch total loss 6.48642063\n",
      "Trained batch 1195 batch loss 6.67426395 epoch total loss 6.48657799\n",
      "Trained batch 1196 batch loss 6.56620312 epoch total loss 6.48664474\n",
      "Trained batch 1197 batch loss 6.88917732 epoch total loss 6.48698092\n",
      "Trained batch 1198 batch loss 6.69572973 epoch total loss 6.48715496\n",
      "Trained batch 1199 batch loss 6.39916515 epoch total loss 6.48708153\n",
      "Trained batch 1200 batch loss 6.55174 epoch total loss 6.48713541\n",
      "Trained batch 1201 batch loss 6.36999559 epoch total loss 6.48703814\n",
      "Trained batch 1202 batch loss 6.8735981 epoch total loss 6.48735952\n",
      "Trained batch 1203 batch loss 6.42995358 epoch total loss 6.48731184\n",
      "Trained batch 1204 batch loss 6.87787437 epoch total loss 6.48763657\n",
      "Trained batch 1205 batch loss 6.41330719 epoch total loss 6.48757458\n",
      "Trained batch 1206 batch loss 6.47852278 epoch total loss 6.48756695\n",
      "Trained batch 1207 batch loss 6.4362 epoch total loss 6.48752451\n",
      "Trained batch 1208 batch loss 6.37611628 epoch total loss 6.487432\n",
      "Trained batch 1209 batch loss 6.46435356 epoch total loss 6.48741293\n",
      "Trained batch 1210 batch loss 6.9036293 epoch total loss 6.48775721\n",
      "Trained batch 1211 batch loss 6.49028587 epoch total loss 6.48775911\n",
      "Trained batch 1212 batch loss 6.24378681 epoch total loss 6.48755789\n",
      "Trained batch 1213 batch loss 6.47479916 epoch total loss 6.48754692\n",
      "Trained batch 1214 batch loss 5.40711975 epoch total loss 6.48665714\n",
      "Trained batch 1215 batch loss 6.67190075 epoch total loss 6.48680973\n",
      "Trained batch 1216 batch loss 6.38921356 epoch total loss 6.48672915\n",
      "Trained batch 1217 batch loss 5.57952642 epoch total loss 6.48598385\n",
      "Trained batch 1218 batch loss 5.87027693 epoch total loss 6.4854784\n",
      "Trained batch 1219 batch loss 6.56800842 epoch total loss 6.48554564\n",
      "Trained batch 1220 batch loss 6.50702238 epoch total loss 6.48556328\n",
      "Trained batch 1221 batch loss 6.93415356 epoch total loss 6.48593044\n",
      "Trained batch 1222 batch loss 6.77096176 epoch total loss 6.48616409\n",
      "Trained batch 1223 batch loss 6.14236975 epoch total loss 6.48588276\n",
      "Trained batch 1224 batch loss 5.45097065 epoch total loss 6.4850378\n",
      "Trained batch 1225 batch loss 7.37288475 epoch total loss 6.4857626\n",
      "Trained batch 1226 batch loss 7.02823448 epoch total loss 6.4862051\n",
      "Trained batch 1227 batch loss 6.65549755 epoch total loss 6.48634291\n",
      "Trained batch 1228 batch loss 6.5829277 epoch total loss 6.48642159\n",
      "Trained batch 1229 batch loss 6.25469065 epoch total loss 6.48623323\n",
      "Trained batch 1230 batch loss 5.60033131 epoch total loss 6.48551273\n",
      "Trained batch 1231 batch loss 5.83817053 epoch total loss 6.48498726\n",
      "Trained batch 1232 batch loss 5.88231659 epoch total loss 6.48449802\n",
      "Trained batch 1233 batch loss 4.89846754 epoch total loss 6.48321152\n",
      "Trained batch 1234 batch loss 5.83781385 epoch total loss 6.48268843\n",
      "Trained batch 1235 batch loss 5.88457155 epoch total loss 6.48220444\n",
      "Trained batch 1236 batch loss 6.33891535 epoch total loss 6.48208857\n",
      "Trained batch 1237 batch loss 6.34543419 epoch total loss 6.48197794\n",
      "Trained batch 1238 batch loss 7.34639549 epoch total loss 6.48267603\n",
      "Trained batch 1239 batch loss 6.55915546 epoch total loss 6.48273754\n",
      "Trained batch 1240 batch loss 6.8041687 epoch total loss 6.48299694\n",
      "Trained batch 1241 batch loss 6.9255619 epoch total loss 6.48335361\n",
      "Trained batch 1242 batch loss 6.24526596 epoch total loss 6.48316193\n",
      "Trained batch 1243 batch loss 6.46598816 epoch total loss 6.48314762\n",
      "Trained batch 1244 batch loss 6.59758377 epoch total loss 6.48323965\n",
      "Trained batch 1245 batch loss 5.58472776 epoch total loss 6.4825182\n",
      "Trained batch 1246 batch loss 6.40426731 epoch total loss 6.48245573\n",
      "Trained batch 1247 batch loss 6.69207859 epoch total loss 6.48262358\n",
      "Trained batch 1248 batch loss 6.8281641 epoch total loss 6.4829\n",
      "Trained batch 1249 batch loss 6.29673 epoch total loss 6.48275137\n",
      "Trained batch 1250 batch loss 6.22001171 epoch total loss 6.48254156\n",
      "Trained batch 1251 batch loss 6.46428537 epoch total loss 6.48252678\n",
      "Trained batch 1252 batch loss 6.42511034 epoch total loss 6.482481\n",
      "Trained batch 1253 batch loss 6.56812763 epoch total loss 6.48254967\n",
      "Trained batch 1254 batch loss 6.00969028 epoch total loss 6.48217249\n",
      "Trained batch 1255 batch loss 6.66054249 epoch total loss 6.48231506\n",
      "Trained batch 1256 batch loss 6.81911516 epoch total loss 6.48258305\n",
      "Trained batch 1257 batch loss 6.81616783 epoch total loss 6.48284864\n",
      "Trained batch 1258 batch loss 6.33095312 epoch total loss 6.482728\n",
      "Trained batch 1259 batch loss 6.6401639 epoch total loss 6.48285294\n",
      "Trained batch 1260 batch loss 6.91964197 epoch total loss 6.4831996\n",
      "Trained batch 1261 batch loss 6.78546429 epoch total loss 6.48343945\n",
      "Trained batch 1262 batch loss 6.74351931 epoch total loss 6.48364544\n",
      "Trained batch 1263 batch loss 6.46313 epoch total loss 6.48362923\n",
      "Trained batch 1264 batch loss 6.27407646 epoch total loss 6.48346329\n",
      "Trained batch 1265 batch loss 6.40235519 epoch total loss 6.48339939\n",
      "Trained batch 1266 batch loss 6.22342157 epoch total loss 6.48319387\n",
      "Trained batch 1267 batch loss 6.50503159 epoch total loss 6.48321104\n",
      "Trained batch 1268 batch loss 6.59958839 epoch total loss 6.48330307\n",
      "Trained batch 1269 batch loss 6.07152224 epoch total loss 6.48297834\n",
      "Trained batch 1270 batch loss 6.12519836 epoch total loss 6.48269653\n",
      "Trained batch 1271 batch loss 6.3986454 epoch total loss 6.48263025\n",
      "Trained batch 1272 batch loss 6.39957237 epoch total loss 6.48256445\n",
      "Trained batch 1273 batch loss 6.30567169 epoch total loss 6.48242569\n",
      "Trained batch 1274 batch loss 6.66366053 epoch total loss 6.48256826\n",
      "Trained batch 1275 batch loss 6.55061626 epoch total loss 6.48262167\n",
      "Trained batch 1276 batch loss 6.54697895 epoch total loss 6.48267221\n",
      "Trained batch 1277 batch loss 6.30847359 epoch total loss 6.48253584\n",
      "Trained batch 1278 batch loss 6.17131519 epoch total loss 6.48229218\n",
      "Trained batch 1279 batch loss 6.06963539 epoch total loss 6.48196888\n",
      "Trained batch 1280 batch loss 5.95239449 epoch total loss 6.48155499\n",
      "Trained batch 1281 batch loss 6.4582243 epoch total loss 6.48153687\n",
      "Trained batch 1282 batch loss 6.39300823 epoch total loss 6.48146725\n",
      "Trained batch 1283 batch loss 6.07596159 epoch total loss 6.48115158\n",
      "Trained batch 1284 batch loss 6.21291351 epoch total loss 6.48094273\n",
      "Trained batch 1285 batch loss 5.3864603 epoch total loss 6.48009109\n",
      "Trained batch 1286 batch loss 6.31743717 epoch total loss 6.47996473\n",
      "Trained batch 1287 batch loss 5.8429203 epoch total loss 6.4794693\n",
      "Trained batch 1288 batch loss 5.92358828 epoch total loss 6.47903824\n",
      "Trained batch 1289 batch loss 5.9070158 epoch total loss 6.4785943\n",
      "Trained batch 1290 batch loss 5.49806 epoch total loss 6.47783422\n",
      "Trained batch 1291 batch loss 6.05098438 epoch total loss 6.4775033\n",
      "Trained batch 1292 batch loss 5.21118069 epoch total loss 6.4765234\n",
      "Trained batch 1293 batch loss 5.94242048 epoch total loss 6.47611\n",
      "Trained batch 1294 batch loss 6.15353489 epoch total loss 6.4758606\n",
      "Trained batch 1295 batch loss 5.86074257 epoch total loss 6.47538519\n",
      "Trained batch 1296 batch loss 6.56660843 epoch total loss 6.47545576\n",
      "Trained batch 1297 batch loss 6.21963882 epoch total loss 6.47525835\n",
      "Trained batch 1298 batch loss 5.88088274 epoch total loss 6.47480059\n",
      "Trained batch 1299 batch loss 6.77139711 epoch total loss 6.47502899\n",
      "Trained batch 1300 batch loss 7.13198 epoch total loss 6.47553396\n",
      "Trained batch 1301 batch loss 6.89218426 epoch total loss 6.47585487\n",
      "Trained batch 1302 batch loss 6.5484 epoch total loss 6.47591066\n",
      "Trained batch 1303 batch loss 5.9659071 epoch total loss 6.47551918\n",
      "Trained batch 1304 batch loss 5.88061333 epoch total loss 6.47506332\n",
      "Trained batch 1305 batch loss 6.7120719 epoch total loss 6.47524452\n",
      "Trained batch 1306 batch loss 6.38824081 epoch total loss 6.47517824\n",
      "Trained batch 1307 batch loss 6.01245642 epoch total loss 6.47482443\n",
      "Trained batch 1308 batch loss 6.65851593 epoch total loss 6.47496462\n",
      "Trained batch 1309 batch loss 6.74460602 epoch total loss 6.47517061\n",
      "Trained batch 1310 batch loss 6.78551197 epoch total loss 6.47540712\n",
      "Trained batch 1311 batch loss 6.70893478 epoch total loss 6.47558498\n",
      "Trained batch 1312 batch loss 6.69976807 epoch total loss 6.47575617\n",
      "Trained batch 1313 batch loss 6.46653652 epoch total loss 6.47574949\n",
      "Trained batch 1314 batch loss 5.82035208 epoch total loss 6.47525072\n",
      "Trained batch 1315 batch loss 5.29671669 epoch total loss 6.47435474\n",
      "Trained batch 1316 batch loss 4.83970404 epoch total loss 6.47311258\n",
      "Trained batch 1317 batch loss 5.17018127 epoch total loss 6.47212315\n",
      "Trained batch 1318 batch loss 6.64211178 epoch total loss 6.47225237\n",
      "Trained batch 1319 batch loss 6.63013601 epoch total loss 6.47237206\n",
      "Trained batch 1320 batch loss 5.95457935 epoch total loss 6.47197914\n",
      "Trained batch 1321 batch loss 6.97555828 epoch total loss 6.47236061\n",
      "Trained batch 1322 batch loss 6.81437349 epoch total loss 6.47261953\n",
      "Trained batch 1323 batch loss 6.32882309 epoch total loss 6.47251081\n",
      "Trained batch 1324 batch loss 6.64298105 epoch total loss 6.47263908\n",
      "Trained batch 1325 batch loss 7.16229773 epoch total loss 6.47316\n",
      "Trained batch 1326 batch loss 6.62376451 epoch total loss 6.47327328\n",
      "Trained batch 1327 batch loss 7.02361488 epoch total loss 6.47368813\n",
      "Trained batch 1328 batch loss 6.32899523 epoch total loss 6.47357893\n",
      "Trained batch 1329 batch loss 6.50037861 epoch total loss 6.47359896\n",
      "Trained batch 1330 batch loss 6.58017111 epoch total loss 6.47367907\n",
      "Trained batch 1331 batch loss 6.09846497 epoch total loss 6.47339725\n",
      "Trained batch 1332 batch loss 6.65093136 epoch total loss 6.47353077\n",
      "Trained batch 1333 batch loss 6.74722624 epoch total loss 6.47373629\n",
      "Trained batch 1334 batch loss 6.61395645 epoch total loss 6.47384167\n",
      "Trained batch 1335 batch loss 6.56277227 epoch total loss 6.47390795\n",
      "Trained batch 1336 batch loss 6.71620798 epoch total loss 6.47408915\n",
      "Trained batch 1337 batch loss 5.71089172 epoch total loss 6.47351837\n",
      "Trained batch 1338 batch loss 6.86934853 epoch total loss 6.47381401\n",
      "Trained batch 1339 batch loss 6.45230484 epoch total loss 6.4737978\n",
      "Trained batch 1340 batch loss 6.64008331 epoch total loss 6.4739213\n",
      "Trained batch 1341 batch loss 6.21929932 epoch total loss 6.47373199\n",
      "Trained batch 1342 batch loss 6.61860561 epoch total loss 6.47383928\n",
      "Trained batch 1343 batch loss 6.57803059 epoch total loss 6.47391701\n",
      "Trained batch 1344 batch loss 7.00913239 epoch total loss 6.47431517\n",
      "Trained batch 1345 batch loss 7.90263653 epoch total loss 6.47537661\n",
      "Trained batch 1346 batch loss 7.20295334 epoch total loss 6.47591734\n",
      "Trained batch 1347 batch loss 6.76711 epoch total loss 6.47613382\n",
      "Trained batch 1348 batch loss 5.23992825 epoch total loss 6.47521734\n",
      "Trained batch 1349 batch loss 6.44609547 epoch total loss 6.47519588\n",
      "Trained batch 1350 batch loss 5.23493099 epoch total loss 6.4742775\n",
      "Trained batch 1351 batch loss 5.53955078 epoch total loss 6.47358561\n",
      "Trained batch 1352 batch loss 5.62404442 epoch total loss 6.47295761\n",
      "Trained batch 1353 batch loss 5.0687356 epoch total loss 6.47191954\n",
      "Trained batch 1354 batch loss 5.37876225 epoch total loss 6.47111225\n",
      "Trained batch 1355 batch loss 5.23077393 epoch total loss 6.47019625\n",
      "Trained batch 1356 batch loss 5.13472605 epoch total loss 6.46921158\n",
      "Trained batch 1357 batch loss 4.87373543 epoch total loss 6.46803617\n",
      "Trained batch 1358 batch loss 6.15390205 epoch total loss 6.46780491\n",
      "Trained batch 1359 batch loss 6.33930874 epoch total loss 6.46771\n",
      "Trained batch 1360 batch loss 6.63994408 epoch total loss 6.46783686\n",
      "Trained batch 1361 batch loss 6.17323971 epoch total loss 6.46762\n",
      "Trained batch 1362 batch loss 6.45139694 epoch total loss 6.46760798\n",
      "Trained batch 1363 batch loss 6.53321838 epoch total loss 6.46765614\n",
      "Trained batch 1364 batch loss 6.58514452 epoch total loss 6.46774197\n",
      "Trained batch 1365 batch loss 6.68139362 epoch total loss 6.46789885\n",
      "Trained batch 1366 batch loss 6.35560799 epoch total loss 6.46781635\n",
      "Trained batch 1367 batch loss 6.56698084 epoch total loss 6.46788931\n",
      "Trained batch 1368 batch loss 6.42522573 epoch total loss 6.46785784\n",
      "Trained batch 1369 batch loss 6.54330254 epoch total loss 6.46791267\n",
      "Trained batch 1370 batch loss 6.51565647 epoch total loss 6.46794748\n",
      "Trained batch 1371 batch loss 6.62104464 epoch total loss 6.46805906\n",
      "Trained batch 1372 batch loss 6.48071575 epoch total loss 6.46806812\n",
      "Trained batch 1373 batch loss 6.42997074 epoch total loss 6.46804\n",
      "Trained batch 1374 batch loss 6.81875801 epoch total loss 6.4682951\n",
      "Trained batch 1375 batch loss 6.76727676 epoch total loss 6.46851301\n",
      "Trained batch 1376 batch loss 6.52010298 epoch total loss 6.46855068\n",
      "Trained batch 1377 batch loss 6.39979315 epoch total loss 6.4685\n",
      "Trained batch 1378 batch loss 6.54932213 epoch total loss 6.46855927\n",
      "Trained batch 1379 batch loss 6.26286125 epoch total loss 6.46841\n",
      "Trained batch 1380 batch loss 6.49481678 epoch total loss 6.46842957\n",
      "Trained batch 1381 batch loss 6.4716382 epoch total loss 6.46843195\n",
      "Trained batch 1382 batch loss 6.3971777 epoch total loss 6.46838045\n",
      "Trained batch 1383 batch loss 6.57755232 epoch total loss 6.46845913\n",
      "Trained batch 1384 batch loss 6.4778204 epoch total loss 6.46846581\n",
      "Trained batch 1385 batch loss 6.3834548 epoch total loss 6.46840429\n",
      "Trained batch 1386 batch loss 6.53424501 epoch total loss 6.46845198\n",
      "Trained batch 1387 batch loss 6.3750391 epoch total loss 6.46838474\n",
      "Trained batch 1388 batch loss 6.35509825 epoch total loss 6.4683032\n",
      "Trained batch 1389 batch loss 6.49637699 epoch total loss 6.46832323\n",
      "Trained batch 1390 batch loss 6.35362387 epoch total loss 6.46824074\n",
      "Trained batch 1391 batch loss 6.29218674 epoch total loss 6.4681139\n",
      "Trained batch 1392 batch loss 6.47820568 epoch total loss 6.46812153\n",
      "Trained batch 1393 batch loss 6.27525091 epoch total loss 6.46798325\n",
      "Trained batch 1394 batch loss 6.33643341 epoch total loss 6.46788883\n",
      "Trained batch 1395 batch loss 6.36420345 epoch total loss 6.46781492\n",
      "Trained batch 1396 batch loss 6.37023163 epoch total loss 6.46774483\n",
      "Trained batch 1397 batch loss 6.06776237 epoch total loss 6.46745825\n",
      "Trained batch 1398 batch loss 6.17838526 epoch total loss 6.46725178\n",
      "Trained batch 1399 batch loss 6.14321232 epoch total loss 6.46702\n",
      "Trained batch 1400 batch loss 6.20743179 epoch total loss 6.46683455\n",
      "Trained batch 1401 batch loss 6.35235119 epoch total loss 6.46675301\n",
      "Trained batch 1402 batch loss 6.34118891 epoch total loss 6.46666336\n",
      "Trained batch 1403 batch loss 6.21488857 epoch total loss 6.46648359\n",
      "Trained batch 1404 batch loss 5.83936787 epoch total loss 6.46603727\n",
      "Trained batch 1405 batch loss 6.47808456 epoch total loss 6.46604633\n",
      "Trained batch 1406 batch loss 6.5429616 epoch total loss 6.46610069\n",
      "Trained batch 1407 batch loss 6.51123047 epoch total loss 6.46613312\n",
      "Trained batch 1408 batch loss 5.81929159 epoch total loss 6.46567392\n",
      "Trained batch 1409 batch loss 5.64116573 epoch total loss 6.46508932\n",
      "Trained batch 1410 batch loss 6.78696442 epoch total loss 6.46531725\n",
      "Trained batch 1411 batch loss 6.49108696 epoch total loss 6.46533585\n",
      "Trained batch 1412 batch loss 6.44357967 epoch total loss 6.46532\n",
      "Trained batch 1413 batch loss 6.24682617 epoch total loss 6.46516562\n",
      "Trained batch 1414 batch loss 5.30646801 epoch total loss 6.46434641\n",
      "Trained batch 1415 batch loss 6.35918236 epoch total loss 6.4642725\n",
      "Trained batch 1416 batch loss 5.95199966 epoch total loss 6.46391058\n",
      "Trained batch 1417 batch loss 5.45178795 epoch total loss 6.46319675\n",
      "Trained batch 1418 batch loss 5.63433456 epoch total loss 6.46261263\n",
      "Trained batch 1419 batch loss 7.03478146 epoch total loss 6.46301603\n",
      "Trained batch 1420 batch loss 6.49364853 epoch total loss 6.46303701\n",
      "Trained batch 1421 batch loss 6.67185497 epoch total loss 6.46318388\n",
      "Trained batch 1422 batch loss 6.61105442 epoch total loss 6.46328831\n",
      "Trained batch 1423 batch loss 6.72206831 epoch total loss 6.46347\n",
      "Trained batch 1424 batch loss 6.34216 epoch total loss 6.46338463\n",
      "Trained batch 1425 batch loss 6.4537158 epoch total loss 6.46337795\n",
      "Trained batch 1426 batch loss 7.54198456 epoch total loss 6.46413422\n",
      "Trained batch 1427 batch loss 6.38137579 epoch total loss 6.46407652\n",
      "Trained batch 1428 batch loss 6.051723 epoch total loss 6.46378803\n",
      "Trained batch 1429 batch loss 6.22883701 epoch total loss 6.46362305\n",
      "Trained batch 1430 batch loss 7.07895851 epoch total loss 6.46405363\n",
      "Trained batch 1431 batch loss 5.94111156 epoch total loss 6.46368837\n",
      "Trained batch 1432 batch loss 6.50073242 epoch total loss 6.4637146\n",
      "Trained batch 1433 batch loss 6.39657211 epoch total loss 6.46366739\n",
      "Trained batch 1434 batch loss 6.56302834 epoch total loss 6.46373701\n",
      "Trained batch 1435 batch loss 5.50741148 epoch total loss 6.46307087\n",
      "Trained batch 1436 batch loss 5.70840263 epoch total loss 6.46254492\n",
      "Trained batch 1437 batch loss 5.41272593 epoch total loss 6.46181488\n",
      "Trained batch 1438 batch loss 5.58833122 epoch total loss 6.46120691\n",
      "Trained batch 1439 batch loss 6.54924536 epoch total loss 6.46126795\n",
      "Trained batch 1440 batch loss 6.68647242 epoch total loss 6.46142435\n",
      "Trained batch 1441 batch loss 6.75690269 epoch total loss 6.46162939\n",
      "Trained batch 1442 batch loss 6.50833654 epoch total loss 6.46166229\n",
      "Trained batch 1443 batch loss 6.61001968 epoch total loss 6.46176529\n",
      "Trained batch 1444 batch loss 6.2388134 epoch total loss 6.46161127\n",
      "Trained batch 1445 batch loss 6.22927856 epoch total loss 6.46145058\n",
      "Trained batch 1446 batch loss 6.13521624 epoch total loss 6.46122456\n",
      "Trained batch 1447 batch loss 6.12437773 epoch total loss 6.46099138\n",
      "Trained batch 1448 batch loss 5.83472443 epoch total loss 6.46055937\n",
      "Trained batch 1449 batch loss 5.29091787 epoch total loss 6.45975208\n",
      "Trained batch 1450 batch loss 5.59953213 epoch total loss 6.4591589\n",
      "Trained batch 1451 batch loss 6.34946299 epoch total loss 6.45908308\n",
      "Trained batch 1452 batch loss 6.38158417 epoch total loss 6.45903\n",
      "Trained batch 1453 batch loss 5.53246117 epoch total loss 6.45839214\n",
      "Trained batch 1454 batch loss 5.18479443 epoch total loss 6.45751619\n",
      "Trained batch 1455 batch loss 7.91971 epoch total loss 6.45852137\n",
      "Trained batch 1456 batch loss 7.4838171 epoch total loss 6.45922518\n",
      "Trained batch 1457 batch loss 6.50556755 epoch total loss 6.45925713\n",
      "Trained batch 1458 batch loss 5.85244179 epoch total loss 6.45884085\n",
      "Trained batch 1459 batch loss 5.93890858 epoch total loss 6.45848417\n",
      "Trained batch 1460 batch loss 6.01653862 epoch total loss 6.45818186\n",
      "Trained batch 1461 batch loss 6.45188046 epoch total loss 6.45817757\n",
      "Trained batch 1462 batch loss 6.46909523 epoch total loss 6.45818472\n",
      "Trained batch 1463 batch loss 6.78840113 epoch total loss 6.45841026\n",
      "Trained batch 1464 batch loss 6.7710557 epoch total loss 6.45862436\n",
      "Trained batch 1465 batch loss 7.40507221 epoch total loss 6.45927048\n",
      "Trained batch 1466 batch loss 7.51986361 epoch total loss 6.45999336\n",
      "Trained batch 1467 batch loss 7.02648163 epoch total loss 6.4603796\n",
      "Trained batch 1468 batch loss 7.01930141 epoch total loss 6.46076059\n",
      "Trained batch 1469 batch loss 6.69051313 epoch total loss 6.460917\n",
      "Trained batch 1470 batch loss 7.33611155 epoch total loss 6.46151209\n",
      "Trained batch 1471 batch loss 7.17077255 epoch total loss 6.46199417\n",
      "Trained batch 1472 batch loss 7.52726364 epoch total loss 6.46271801\n",
      "Trained batch 1473 batch loss 6.36760044 epoch total loss 6.46265316\n",
      "Trained batch 1474 batch loss 6.47197247 epoch total loss 6.46265936\n",
      "Trained batch 1475 batch loss 6.78446865 epoch total loss 6.46287727\n",
      "Trained batch 1476 batch loss 6.69044781 epoch total loss 6.46303177\n",
      "Trained batch 1477 batch loss 7.15566 epoch total loss 6.4635\n",
      "Trained batch 1478 batch loss 6.96241331 epoch total loss 6.4638381\n",
      "Trained batch 1479 batch loss 6.76471424 epoch total loss 6.46404171\n",
      "Trained batch 1480 batch loss 6.48070097 epoch total loss 6.46405268\n",
      "Trained batch 1481 batch loss 6.71462822 epoch total loss 6.46422195\n",
      "Trained batch 1482 batch loss 6.6508255 epoch total loss 6.46434736\n",
      "Trained batch 1483 batch loss 6.4760375 epoch total loss 6.46435499\n",
      "Trained batch 1484 batch loss 6.34947443 epoch total loss 6.46427774\n",
      "Trained batch 1485 batch loss 6.34367752 epoch total loss 6.46419668\n",
      "Trained batch 1486 batch loss 6.31247234 epoch total loss 6.46409464\n",
      "Trained batch 1487 batch loss 6.53337097 epoch total loss 6.46414089\n",
      "Trained batch 1488 batch loss 6.36053848 epoch total loss 6.46407127\n",
      "Trained batch 1489 batch loss 6.44511223 epoch total loss 6.46405888\n",
      "Trained batch 1490 batch loss 6.74224663 epoch total loss 6.46424532\n",
      "Trained batch 1491 batch loss 6.9059248 epoch total loss 6.46454191\n",
      "Trained batch 1492 batch loss 5.99947214 epoch total loss 6.46422958\n",
      "Trained batch 1493 batch loss 6.56839752 epoch total loss 6.46429968\n",
      "Trained batch 1494 batch loss 6.77805424 epoch total loss 6.46451\n",
      "Trained batch 1495 batch loss 6.86440659 epoch total loss 6.46477699\n",
      "Trained batch 1496 batch loss 6.51442766 epoch total loss 6.46481037\n",
      "Trained batch 1497 batch loss 6.72462273 epoch total loss 6.46498394\n",
      "Trained batch 1498 batch loss 6.71321774 epoch total loss 6.4651494\n",
      "Trained batch 1499 batch loss 6.63577795 epoch total loss 6.46526337\n",
      "Trained batch 1500 batch loss 6.4866147 epoch total loss 6.46527719\n",
      "Trained batch 1501 batch loss 6.57382536 epoch total loss 6.46535\n",
      "Trained batch 1502 batch loss 6.83163595 epoch total loss 6.46559381\n",
      "Trained batch 1503 batch loss 6.37455845 epoch total loss 6.46553373\n",
      "Trained batch 1504 batch loss 6.64098072 epoch total loss 6.46565\n",
      "Trained batch 1505 batch loss 6.49268341 epoch total loss 6.46566868\n",
      "Trained batch 1506 batch loss 6.47304535 epoch total loss 6.46567297\n",
      "Trained batch 1507 batch loss 6.47523546 epoch total loss 6.46567965\n",
      "Trained batch 1508 batch loss 6.47180271 epoch total loss 6.46568346\n",
      "Trained batch 1509 batch loss 6.64859962 epoch total loss 6.46580458\n",
      "Trained batch 1510 batch loss 6.49190426 epoch total loss 6.46582222\n",
      "Trained batch 1511 batch loss 6.5975008 epoch total loss 6.46590948\n",
      "Trained batch 1512 batch loss 6.95087433 epoch total loss 6.46623039\n",
      "Trained batch 1513 batch loss 6.46581268 epoch total loss 6.46623039\n",
      "Trained batch 1514 batch loss 6.57020378 epoch total loss 6.46629906\n",
      "Trained batch 1515 batch loss 6.81436825 epoch total loss 6.46652889\n",
      "Trained batch 1516 batch loss 6.62453079 epoch total loss 6.46663332\n",
      "Trained batch 1517 batch loss 5.36857843 epoch total loss 6.465909\n",
      "Trained batch 1518 batch loss 6.64403057 epoch total loss 6.46602631\n",
      "Trained batch 1519 batch loss 5.89652872 epoch total loss 6.46565104\n",
      "Trained batch 1520 batch loss 6.39450502 epoch total loss 6.46560431\n",
      "Trained batch 1521 batch loss 6.53995228 epoch total loss 6.46565342\n",
      "Trained batch 1522 batch loss 6.25880051 epoch total loss 6.46551752\n",
      "Trained batch 1523 batch loss 7.29616 epoch total loss 6.46606255\n",
      "Trained batch 1524 batch loss 6.92511415 epoch total loss 6.46636391\n",
      "Trained batch 1525 batch loss 5.96029377 epoch total loss 6.46603155\n",
      "Trained batch 1526 batch loss 6.79291058 epoch total loss 6.46624565\n",
      "Trained batch 1527 batch loss 6.85821152 epoch total loss 6.46650267\n",
      "Trained batch 1528 batch loss 6.90091705 epoch total loss 6.46678734\n",
      "Trained batch 1529 batch loss 7.00002193 epoch total loss 6.46713591\n",
      "Trained batch 1530 batch loss 6.88703299 epoch total loss 6.46741\n",
      "Trained batch 1531 batch loss 7.09637737 epoch total loss 6.46782112\n",
      "Trained batch 1532 batch loss 6.88212109 epoch total loss 6.46809149\n",
      "Trained batch 1533 batch loss 6.77381086 epoch total loss 6.46829081\n",
      "Trained batch 1534 batch loss 6.81855536 epoch total loss 6.46851873\n",
      "Trained batch 1535 batch loss 6.65268612 epoch total loss 6.4686389\n",
      "Trained batch 1536 batch loss 6.8482933 epoch total loss 6.4688859\n",
      "Trained batch 1537 batch loss 6.85957 epoch total loss 6.46914\n",
      "Trained batch 1538 batch loss 6.95215702 epoch total loss 6.46945429\n",
      "Trained batch 1539 batch loss 6.89456081 epoch total loss 6.46973038\n",
      "Trained batch 1540 batch loss 6.43677711 epoch total loss 6.46970892\n",
      "Trained batch 1541 batch loss 6.82072639 epoch total loss 6.46993637\n",
      "Trained batch 1542 batch loss 7.02626324 epoch total loss 6.47029734\n",
      "Trained batch 1543 batch loss 6.77431679 epoch total loss 6.47049427\n",
      "Trained batch 1544 batch loss 6.6966238 epoch total loss 6.47064066\n",
      "Trained batch 1545 batch loss 6.84755611 epoch total loss 6.47088432\n",
      "Trained batch 1546 batch loss 6.07204342 epoch total loss 6.47062683\n",
      "Trained batch 1547 batch loss 6.9074707 epoch total loss 6.47090912\n",
      "Trained batch 1548 batch loss 6.74723339 epoch total loss 6.47108746\n",
      "Trained batch 1549 batch loss 6.46294975 epoch total loss 6.47108221\n",
      "Trained batch 1550 batch loss 6.66081047 epoch total loss 6.47120476\n",
      "Trained batch 1551 batch loss 6.5214119 epoch total loss 6.47123718\n",
      "Trained batch 1552 batch loss 6.42952347 epoch total loss 6.47121048\n",
      "Trained batch 1553 batch loss 6.53702879 epoch total loss 6.47125292\n",
      "Trained batch 1554 batch loss 6.47507954 epoch total loss 6.47125483\n",
      "Trained batch 1555 batch loss 6.43066835 epoch total loss 6.4712286\n",
      "Trained batch 1556 batch loss 6.40198421 epoch total loss 6.47118473\n",
      "Trained batch 1557 batch loss 6.50422287 epoch total loss 6.47120571\n",
      "Trained batch 1558 batch loss 6.54374027 epoch total loss 6.47125244\n",
      "Trained batch 1559 batch loss 6.26240158 epoch total loss 6.47111845\n",
      "Trained batch 1560 batch loss 6.14685345 epoch total loss 6.47091055\n",
      "Trained batch 1561 batch loss 6.31132841 epoch total loss 6.47080803\n",
      "Trained batch 1562 batch loss 6.54598427 epoch total loss 6.47085619\n",
      "Trained batch 1563 batch loss 6.52627945 epoch total loss 6.47089195\n",
      "Trained batch 1564 batch loss 5.65222406 epoch total loss 6.47036839\n",
      "Trained batch 1565 batch loss 5.48341608 epoch total loss 6.46973801\n",
      "Trained batch 1566 batch loss 5.18474436 epoch total loss 6.46891689\n",
      "Trained batch 1567 batch loss 5.52557087 epoch total loss 6.46831512\n",
      "Trained batch 1568 batch loss 6.93654251 epoch total loss 6.46861362\n",
      "Trained batch 1569 batch loss 6.80153275 epoch total loss 6.46882582\n",
      "Trained batch 1570 batch loss 6.81019831 epoch total loss 6.46904373\n",
      "Trained batch 1571 batch loss 6.56257725 epoch total loss 6.46910286\n",
      "Trained batch 1572 batch loss 6.67376518 epoch total loss 6.46923351\n",
      "Trained batch 1573 batch loss 6.02885818 epoch total loss 6.46895361\n",
      "Trained batch 1574 batch loss 6.58716059 epoch total loss 6.46902847\n",
      "Trained batch 1575 batch loss 6.68066263 epoch total loss 6.46916294\n",
      "Trained batch 1576 batch loss 6.70423937 epoch total loss 6.46931219\n",
      "Trained batch 1577 batch loss 5.58606672 epoch total loss 6.46875191\n",
      "Trained batch 1578 batch loss 4.21835709 epoch total loss 6.46732616\n",
      "Trained batch 1579 batch loss 5.78521967 epoch total loss 6.46689415\n",
      "Trained batch 1580 batch loss 4.98158073 epoch total loss 6.46595383\n",
      "Trained batch 1581 batch loss 5.72956276 epoch total loss 6.46548796\n",
      "Trained batch 1582 batch loss 5.21209431 epoch total loss 6.46469545\n",
      "Trained batch 1583 batch loss 6.49590206 epoch total loss 6.46471548\n",
      "Trained batch 1584 batch loss 6.92775345 epoch total loss 6.46500778\n",
      "Trained batch 1585 batch loss 6.4511075 epoch total loss 6.4649992\n",
      "Trained batch 1586 batch loss 6.5496788 epoch total loss 6.4650526\n",
      "Trained batch 1587 batch loss 6.77502298 epoch total loss 6.46524811\n",
      "Trained batch 1588 batch loss 6.60109806 epoch total loss 6.46533394\n",
      "Trained batch 1589 batch loss 6.37045479 epoch total loss 6.46527386\n",
      "Trained batch 1590 batch loss 6.46541309 epoch total loss 6.46527433\n",
      "Trained batch 1591 batch loss 6.43743038 epoch total loss 6.46525669\n",
      "Trained batch 1592 batch loss 6.24844837 epoch total loss 6.46512032\n",
      "Trained batch 1593 batch loss 6.20113325 epoch total loss 6.46495485\n",
      "Trained batch 1594 batch loss 6.45010567 epoch total loss 6.46494532\n",
      "Trained batch 1595 batch loss 5.74372864 epoch total loss 6.46449375\n",
      "Trained batch 1596 batch loss 6.05025768 epoch total loss 6.46423388\n",
      "Trained batch 1597 batch loss 6.56991196 epoch total loss 6.4643\n",
      "Trained batch 1598 batch loss 6.48560238 epoch total loss 6.46431351\n",
      "Trained batch 1599 batch loss 6.32804 epoch total loss 6.46422815\n",
      "Trained batch 1600 batch loss 6.11068058 epoch total loss 6.4640069\n",
      "Trained batch 1601 batch loss 6.17342377 epoch total loss 6.4638257\n",
      "Trained batch 1602 batch loss 5.64175 epoch total loss 6.46331263\n",
      "Trained batch 1603 batch loss 5.75078773 epoch total loss 6.46286821\n",
      "Trained batch 1604 batch loss 5.48957062 epoch total loss 6.4622612\n",
      "Trained batch 1605 batch loss 4.86022568 epoch total loss 6.46126318\n",
      "Trained batch 1606 batch loss 4.81454325 epoch total loss 6.4602375\n",
      "Trained batch 1607 batch loss 4.679 epoch total loss 6.45912886\n",
      "Trained batch 1608 batch loss 5.11746883 epoch total loss 6.45829439\n",
      "Trained batch 1609 batch loss 4.44808817 epoch total loss 6.45704508\n",
      "Trained batch 1610 batch loss 5.04403639 epoch total loss 6.4561677\n",
      "Trained batch 1611 batch loss 7.27694035 epoch total loss 6.45667744\n",
      "Trained batch 1612 batch loss 6.33119583 epoch total loss 6.45659924\n",
      "Trained batch 1613 batch loss 7.41617393 epoch total loss 6.45719433\n",
      "Trained batch 1614 batch loss 7.29488182 epoch total loss 6.45771313\n",
      "Trained batch 1615 batch loss 7.11982632 epoch total loss 6.45812321\n",
      "Trained batch 1616 batch loss 6.92767954 epoch total loss 6.45841408\n",
      "Trained batch 1617 batch loss 5.89457703 epoch total loss 6.45806503\n",
      "Trained batch 1618 batch loss 6.9359808 epoch total loss 6.45836\n",
      "Trained batch 1619 batch loss 5.24359608 epoch total loss 6.45760965\n",
      "Trained batch 1620 batch loss 6.62174034 epoch total loss 6.45771122\n",
      "Trained batch 1621 batch loss 6.74085855 epoch total loss 6.45788622\n",
      "Trained batch 1622 batch loss 6.43186426 epoch total loss 6.45787\n",
      "Trained batch 1623 batch loss 6.63346386 epoch total loss 6.45797825\n",
      "Trained batch 1624 batch loss 6.30416155 epoch total loss 6.45788336\n",
      "Trained batch 1625 batch loss 6.30239677 epoch total loss 6.45778799\n",
      "Trained batch 1626 batch loss 6.44613934 epoch total loss 6.45778084\n",
      "Trained batch 1627 batch loss 6.46879578 epoch total loss 6.45778751\n",
      "Trained batch 1628 batch loss 6.30026722 epoch total loss 6.45769072\n",
      "Trained batch 1629 batch loss 6.31048536 epoch total loss 6.4576\n",
      "Trained batch 1630 batch loss 6.72385454 epoch total loss 6.4577632\n",
      "Trained batch 1631 batch loss 6.92675591 epoch total loss 6.45805073\n",
      "Trained batch 1632 batch loss 6.42211914 epoch total loss 6.45802879\n",
      "Trained batch 1633 batch loss 6.52656603 epoch total loss 6.45807076\n",
      "Trained batch 1634 batch loss 6.3575387 epoch total loss 6.45800924\n",
      "Trained batch 1635 batch loss 6.27004242 epoch total loss 6.45789433\n",
      "Trained batch 1636 batch loss 6.33723736 epoch total loss 6.45782042\n",
      "Trained batch 1637 batch loss 6.51292229 epoch total loss 6.45785379\n",
      "Trained batch 1638 batch loss 6.52373409 epoch total loss 6.45789385\n",
      "Trained batch 1639 batch loss 6.3605938 epoch total loss 6.45783424\n",
      "Trained batch 1640 batch loss 6.33346 epoch total loss 6.45775843\n",
      "Trained batch 1641 batch loss 6.40384293 epoch total loss 6.45772552\n",
      "Trained batch 1642 batch loss 6.16723537 epoch total loss 6.45754862\n",
      "Trained batch 1643 batch loss 6.13905954 epoch total loss 6.45735455\n",
      "Trained batch 1644 batch loss 6.13291073 epoch total loss 6.45715714\n",
      "Trained batch 1645 batch loss 6.16515541 epoch total loss 6.45697975\n",
      "Trained batch 1646 batch loss 6.05402374 epoch total loss 6.45673466\n",
      "Trained batch 1647 batch loss 6.08587 epoch total loss 6.45650959\n",
      "Trained batch 1648 batch loss 6.31908417 epoch total loss 6.45642614\n",
      "Trained batch 1649 batch loss 6.2015028 epoch total loss 6.45627165\n",
      "Trained batch 1650 batch loss 6.21092892 epoch total loss 6.45612288\n",
      "Trained batch 1651 batch loss 6.41932678 epoch total loss 6.45610046\n",
      "Trained batch 1652 batch loss 6.00550461 epoch total loss 6.45582771\n",
      "Trained batch 1653 batch loss 6.07719612 epoch total loss 6.45559883\n",
      "Trained batch 1654 batch loss 6.27248812 epoch total loss 6.45548773\n",
      "Trained batch 1655 batch loss 6.08139038 epoch total loss 6.45526171\n",
      "Trained batch 1656 batch loss 6.32393551 epoch total loss 6.45518255\n",
      "Trained batch 1657 batch loss 6.63512182 epoch total loss 6.45529079\n",
      "Trained batch 1658 batch loss 6.05537033 epoch total loss 6.45505\n",
      "Trained batch 1659 batch loss 6.21695328 epoch total loss 6.45490646\n",
      "Trained batch 1660 batch loss 6.53090477 epoch total loss 6.45495224\n",
      "Trained batch 1661 batch loss 6.11891174 epoch total loss 6.45475\n",
      "Trained batch 1662 batch loss 6.32899 epoch total loss 6.45467424\n",
      "Trained batch 1663 batch loss 6.19315672 epoch total loss 6.45451736\n",
      "Trained batch 1664 batch loss 5.99611282 epoch total loss 6.45424175\n",
      "Trained batch 1665 batch loss 6.20652676 epoch total loss 6.45409298\n",
      "Trained batch 1666 batch loss 6.00693321 epoch total loss 6.45382452\n",
      "Trained batch 1667 batch loss 5.9032526 epoch total loss 6.45349407\n",
      "Trained batch 1668 batch loss 5.97805357 epoch total loss 6.4532094\n",
      "Trained batch 1669 batch loss 5.98407459 epoch total loss 6.45292854\n",
      "Trained batch 1670 batch loss 6.22163248 epoch total loss 6.45279\n",
      "Trained batch 1671 batch loss 5.89510679 epoch total loss 6.45245647\n",
      "Trained batch 1672 batch loss 6.15852547 epoch total loss 6.45228052\n",
      "Trained batch 1673 batch loss 6.12222624 epoch total loss 6.45208311\n",
      "Trained batch 1674 batch loss 5.7721777 epoch total loss 6.45167685\n",
      "Trained batch 1675 batch loss 6.09748888 epoch total loss 6.45146561\n",
      "Trained batch 1676 batch loss 6.3190155 epoch total loss 6.45138693\n",
      "Trained batch 1677 batch loss 6.114048 epoch total loss 6.4511857\n",
      "Trained batch 1678 batch loss 6.40594196 epoch total loss 6.451159\n",
      "Trained batch 1679 batch loss 6.23141575 epoch total loss 6.45102835\n",
      "Trained batch 1680 batch loss 5.90287113 epoch total loss 6.45070219\n",
      "Trained batch 1681 batch loss 4.61929369 epoch total loss 6.44961262\n",
      "Trained batch 1682 batch loss 6.55527258 epoch total loss 6.44967556\n",
      "Trained batch 1683 batch loss 6.09145451 epoch total loss 6.44946289\n",
      "Trained batch 1684 batch loss 5.29687166 epoch total loss 6.44877863\n",
      "Trained batch 1685 batch loss 6.1930294 epoch total loss 6.448627\n",
      "Trained batch 1686 batch loss 6.75882721 epoch total loss 6.44881105\n",
      "Trained batch 1687 batch loss 6.72782 epoch total loss 6.44897604\n",
      "Trained batch 1688 batch loss 6.11072254 epoch total loss 6.44877577\n",
      "Trained batch 1689 batch loss 5.95356464 epoch total loss 6.44848204\n",
      "Trained batch 1690 batch loss 6.45591 epoch total loss 6.4484868\n",
      "Trained batch 1691 batch loss 6.47341442 epoch total loss 6.44850159\n",
      "Trained batch 1692 batch loss 5.93378544 epoch total loss 6.44819736\n",
      "Trained batch 1693 batch loss 6.23249912 epoch total loss 6.44806957\n",
      "Trained batch 1694 batch loss 5.71261549 epoch total loss 6.44763565\n",
      "Trained batch 1695 batch loss 5.74944258 epoch total loss 6.44722366\n",
      "Trained batch 1696 batch loss 6.03376532 epoch total loss 6.44698\n",
      "Trained batch 1697 batch loss 6.72842026 epoch total loss 6.44714594\n",
      "Trained batch 1698 batch loss 5.72911167 epoch total loss 6.44672346\n",
      "Trained batch 1699 batch loss 6.0289216 epoch total loss 6.44647741\n",
      "Trained batch 1700 batch loss 6.6778717 epoch total loss 6.44661379\n",
      "Trained batch 1701 batch loss 5.36230326 epoch total loss 6.44597626\n",
      "Trained batch 1702 batch loss 6.40956 epoch total loss 6.44595432\n",
      "Trained batch 1703 batch loss 6.36057186 epoch total loss 6.44590425\n",
      "Trained batch 1704 batch loss 6.72086716 epoch total loss 6.44606543\n",
      "Trained batch 1705 batch loss 6.40475655 epoch total loss 6.44604111\n",
      "Trained batch 1706 batch loss 6.73519707 epoch total loss 6.44621086\n",
      "Trained batch 1707 batch loss 6.78280401 epoch total loss 6.44640827\n",
      "Trained batch 1708 batch loss 6.66048717 epoch total loss 6.4465332\n",
      "Trained batch 1709 batch loss 6.40781116 epoch total loss 6.44651079\n",
      "Trained batch 1710 batch loss 5.65110207 epoch total loss 6.44604588\n",
      "Trained batch 1711 batch loss 5.80480909 epoch total loss 6.44567108\n",
      "Trained batch 1712 batch loss 7.01729298 epoch total loss 6.44600487\n",
      "Trained batch 1713 batch loss 6.47114706 epoch total loss 6.44601917\n",
      "Trained batch 1714 batch loss 6.81200314 epoch total loss 6.4462328\n",
      "Trained batch 1715 batch loss 6.82601452 epoch total loss 6.44645405\n",
      "Trained batch 1716 batch loss 6.34658432 epoch total loss 6.44639587\n",
      "Trained batch 1717 batch loss 6.73176861 epoch total loss 6.44656229\n",
      "Trained batch 1718 batch loss 6.34983778 epoch total loss 6.44650555\n",
      "Trained batch 1719 batch loss 5.97066498 epoch total loss 6.44622898\n",
      "Trained batch 1720 batch loss 6.03172636 epoch total loss 6.4459877\n",
      "Trained batch 1721 batch loss 5.92104387 epoch total loss 6.44568253\n",
      "Trained batch 1722 batch loss 6.32547331 epoch total loss 6.44561243\n",
      "Trained batch 1723 batch loss 6.52443457 epoch total loss 6.44565821\n",
      "Trained batch 1724 batch loss 5.7760334 epoch total loss 6.44527\n",
      "Trained batch 1725 batch loss 5.13544655 epoch total loss 6.44451094\n",
      "Trained batch 1726 batch loss 5.42943335 epoch total loss 6.443923\n",
      "Trained batch 1727 batch loss 6.43991184 epoch total loss 6.44392\n",
      "Trained batch 1728 batch loss 6.78606319 epoch total loss 6.4441185\n",
      "Trained batch 1729 batch loss 5.77462053 epoch total loss 6.44373083\n",
      "Trained batch 1730 batch loss 6.21464729 epoch total loss 6.44359875\n",
      "Trained batch 1731 batch loss 6.80529213 epoch total loss 6.44380808\n",
      "Trained batch 1732 batch loss 6.34097672 epoch total loss 6.44374847\n",
      "Trained batch 1733 batch loss 5.72054768 epoch total loss 6.44333124\n",
      "Trained batch 1734 batch loss 4.76623631 epoch total loss 6.44236422\n",
      "Trained batch 1735 batch loss 5.22066641 epoch total loss 6.44166\n",
      "Trained batch 1736 batch loss 6.60482216 epoch total loss 6.44175386\n",
      "Trained batch 1737 batch loss 6.35478115 epoch total loss 6.4417038\n",
      "Trained batch 1738 batch loss 6.05274963 epoch total loss 6.44147968\n",
      "Trained batch 1739 batch loss 6.11888885 epoch total loss 6.44129467\n",
      "Trained batch 1740 batch loss 5.64749861 epoch total loss 6.44083834\n",
      "Trained batch 1741 batch loss 5.99876213 epoch total loss 6.44058466\n",
      "Trained batch 1742 batch loss 6.00525093 epoch total loss 6.44033432\n",
      "Trained batch 1743 batch loss 6.39387321 epoch total loss 6.44030762\n",
      "Trained batch 1744 batch loss 6.53835821 epoch total loss 6.44036341\n",
      "Trained batch 1745 batch loss 6.56907272 epoch total loss 6.44043732\n",
      "Trained batch 1746 batch loss 6.74701166 epoch total loss 6.44061327\n",
      "Trained batch 1747 batch loss 7.10128689 epoch total loss 6.4409914\n",
      "Trained batch 1748 batch loss 6.26193714 epoch total loss 6.44088888\n",
      "Trained batch 1749 batch loss 6.21456432 epoch total loss 6.44075966\n",
      "Trained batch 1750 batch loss 6.4973073 epoch total loss 6.44079208\n",
      "Trained batch 1751 batch loss 6.57265472 epoch total loss 6.44086695\n",
      "Trained batch 1752 batch loss 6.2416811 epoch total loss 6.44075298\n",
      "Trained batch 1753 batch loss 6.24580574 epoch total loss 6.44064188\n",
      "Trained batch 1754 batch loss 5.78915787 epoch total loss 6.44027042\n",
      "Trained batch 1755 batch loss 6.78736115 epoch total loss 6.44046831\n",
      "Trained batch 1756 batch loss 6.90697718 epoch total loss 6.44073391\n",
      "Trained batch 1757 batch loss 6.87810421 epoch total loss 6.44098282\n",
      "Trained batch 1758 batch loss 6.68697214 epoch total loss 6.44112253\n",
      "Trained batch 1759 batch loss 6.11257696 epoch total loss 6.44093561\n",
      "Trained batch 1760 batch loss 5.56613493 epoch total loss 6.44043875\n",
      "Trained batch 1761 batch loss 6.20964241 epoch total loss 6.44030762\n",
      "Trained batch 1762 batch loss 6.39537764 epoch total loss 6.44028234\n",
      "Trained batch 1763 batch loss 6.39759588 epoch total loss 6.44025803\n",
      "Trained batch 1764 batch loss 6.43805122 epoch total loss 6.44025707\n",
      "Trained batch 1765 batch loss 6.47086525 epoch total loss 6.44027424\n",
      "Trained batch 1766 batch loss 6.30940342 epoch total loss 6.44020033\n",
      "Trained batch 1767 batch loss 6.12686443 epoch total loss 6.44002295\n",
      "Trained batch 1768 batch loss 5.81423759 epoch total loss 6.43966913\n",
      "Trained batch 1769 batch loss 6.90921593 epoch total loss 6.43993473\n",
      "Trained batch 1770 batch loss 6.54862213 epoch total loss 6.43999624\n",
      "Trained batch 1771 batch loss 6.13405275 epoch total loss 6.43982315\n",
      "Trained batch 1772 batch loss 6.18457031 epoch total loss 6.43967915\n",
      "Trained batch 1773 batch loss 6.00614834 epoch total loss 6.43943453\n",
      "Trained batch 1774 batch loss 6.0922 epoch total loss 6.43923855\n",
      "Trained batch 1775 batch loss 6.26127148 epoch total loss 6.43913841\n",
      "Trained batch 1776 batch loss 6.45352364 epoch total loss 6.43914652\n",
      "Trained batch 1777 batch loss 6.4116087 epoch total loss 6.43913031\n",
      "Trained batch 1778 batch loss 6.40760279 epoch total loss 6.43911266\n",
      "Trained batch 1779 batch loss 6.30070257 epoch total loss 6.43903494\n",
      "Trained batch 1780 batch loss 6.51138 epoch total loss 6.43907547\n",
      "Trained batch 1781 batch loss 6.6878109 epoch total loss 6.43921518\n",
      "Trained batch 1782 batch loss 6.74326324 epoch total loss 6.43938589\n",
      "Trained batch 1783 batch loss 5.77208471 epoch total loss 6.43901157\n",
      "Trained batch 1784 batch loss 6.80941105 epoch total loss 6.43921947\n",
      "Trained batch 1785 batch loss 6.70642662 epoch total loss 6.43936872\n",
      "Trained batch 1786 batch loss 6.03660107 epoch total loss 6.43914318\n",
      "Trained batch 1787 batch loss 7.18912506 epoch total loss 6.4395628\n",
      "Trained batch 1788 batch loss 6.14380074 epoch total loss 6.43939734\n",
      "Trained batch 1789 batch loss 6.68045568 epoch total loss 6.43953228\n",
      "Trained batch 1790 batch loss 6.49128056 epoch total loss 6.43956137\n",
      "Trained batch 1791 batch loss 6.50249624 epoch total loss 6.43959665\n",
      "Trained batch 1792 batch loss 6.36782742 epoch total loss 6.4395566\n",
      "Trained batch 1793 batch loss 6.41141701 epoch total loss 6.43954086\n",
      "Trained batch 1794 batch loss 5.89727879 epoch total loss 6.43923855\n",
      "Trained batch 1795 batch loss 5.30522156 epoch total loss 6.43860722\n",
      "Trained batch 1796 batch loss 5.2601552 epoch total loss 6.43795061\n",
      "Trained batch 1797 batch loss 5.19575644 epoch total loss 6.4372592\n",
      "Trained batch 1798 batch loss 4.45642757 epoch total loss 6.43615723\n",
      "Trained batch 1799 batch loss 4.72110653 epoch total loss 6.43520403\n",
      "Trained batch 1800 batch loss 5.47434664 epoch total loss 6.43467\n",
      "Trained batch 1801 batch loss 5.64779 epoch total loss 6.43423319\n",
      "Trained batch 1802 batch loss 6.12055111 epoch total loss 6.43405867\n",
      "Trained batch 1803 batch loss 4.50752354 epoch total loss 6.43299055\n",
      "Trained batch 1804 batch loss 5.59608221 epoch total loss 6.43252611\n",
      "Trained batch 1805 batch loss 6.35549259 epoch total loss 6.43248367\n",
      "Trained batch 1806 batch loss 5.13959599 epoch total loss 6.43176746\n",
      "Trained batch 1807 batch loss 5.31544447 epoch total loss 6.43115\n",
      "Trained batch 1808 batch loss 5.15550137 epoch total loss 6.43044424\n",
      "Trained batch 1809 batch loss 4.27373505 epoch total loss 6.42925167\n",
      "Trained batch 1810 batch loss 5.85078812 epoch total loss 6.42893219\n",
      "Trained batch 1811 batch loss 4.24985743 epoch total loss 6.42772913\n",
      "Trained batch 1812 batch loss 4.59926891 epoch total loss 6.42672\n",
      "Trained batch 1813 batch loss 4.98814774 epoch total loss 6.42592669\n",
      "Trained batch 1814 batch loss 4.49349689 epoch total loss 6.42486143\n",
      "Trained batch 1815 batch loss 3.9227705 epoch total loss 6.42348289\n",
      "Trained batch 1816 batch loss 4.66451 epoch total loss 6.42251396\n",
      "Trained batch 1817 batch loss 4.96234703 epoch total loss 6.42171\n",
      "Trained batch 1818 batch loss 3.96930075 epoch total loss 6.42036104\n",
      "Trained batch 1819 batch loss 5.04624653 epoch total loss 6.41960573\n",
      "Trained batch 1820 batch loss 4.97724342 epoch total loss 6.41881323\n",
      "Trained batch 1821 batch loss 7.50028706 epoch total loss 6.41940689\n",
      "Trained batch 1822 batch loss 7.03950214 epoch total loss 6.41974735\n",
      "Trained batch 1823 batch loss 5.18736 epoch total loss 6.4190712\n",
      "Trained batch 1824 batch loss 6.66322756 epoch total loss 6.41920519\n",
      "Trained batch 1825 batch loss 7.51153278 epoch total loss 6.41980362\n",
      "Trained batch 1826 batch loss 6.71575642 epoch total loss 6.41996574\n",
      "Trained batch 1827 batch loss 6.91469574 epoch total loss 6.42023659\n",
      "Trained batch 1828 batch loss 6.71427965 epoch total loss 6.42039728\n",
      "Trained batch 1829 batch loss 6.59070301 epoch total loss 6.42049074\n",
      "Trained batch 1830 batch loss 7.0252037 epoch total loss 6.42082119\n",
      "Trained batch 1831 batch loss 6.59198475 epoch total loss 6.42091465\n",
      "Trained batch 1832 batch loss 7.89858818 epoch total loss 6.42172098\n",
      "Trained batch 1833 batch loss 6.63193321 epoch total loss 6.42183542\n",
      "Trained batch 1834 batch loss 6.9566617 epoch total loss 6.42212725\n",
      "Trained batch 1835 batch loss 6.7629776 epoch total loss 6.42231321\n",
      "Trained batch 1836 batch loss 6.56176472 epoch total loss 6.42238903\n",
      "Trained batch 1837 batch loss 6.7138586 epoch total loss 6.42254734\n",
      "Trained batch 1838 batch loss 6.50878239 epoch total loss 6.42259455\n",
      "Trained batch 1839 batch loss 6.47730589 epoch total loss 6.42262411\n",
      "Trained batch 1840 batch loss 6.35612774 epoch total loss 6.42258835\n",
      "Trained batch 1841 batch loss 6.49740124 epoch total loss 6.42262888\n",
      "Trained batch 1842 batch loss 6.34947681 epoch total loss 6.4225893\n",
      "Trained batch 1843 batch loss 6.55174541 epoch total loss 6.4226594\n",
      "Trained batch 1844 batch loss 6.33925533 epoch total loss 6.42261362\n",
      "Trained batch 1845 batch loss 6.08110714 epoch total loss 6.42242861\n",
      "Trained batch 1846 batch loss 6.64108753 epoch total loss 6.42254686\n",
      "Trained batch 1847 batch loss 4.78831434 epoch total loss 6.42166185\n",
      "Trained batch 1848 batch loss 5.74754524 epoch total loss 6.42129707\n",
      "Trained batch 1849 batch loss 6.57167339 epoch total loss 6.42137814\n",
      "Trained batch 1850 batch loss 5.21031523 epoch total loss 6.42072296\n",
      "Trained batch 1851 batch loss 6.40023518 epoch total loss 6.42071199\n",
      "Trained batch 1852 batch loss 6.30104733 epoch total loss 6.42064762\n",
      "Trained batch 1853 batch loss 6.24225664 epoch total loss 6.4205513\n",
      "Trained batch 1854 batch loss 5.55645 epoch total loss 6.42008495\n",
      "Trained batch 1855 batch loss 6.35888577 epoch total loss 6.42005205\n",
      "Trained batch 1856 batch loss 6.49974632 epoch total loss 6.42009497\n",
      "Trained batch 1857 batch loss 6.11001301 epoch total loss 6.41992807\n",
      "Trained batch 1858 batch loss 6.35502338 epoch total loss 6.41989326\n",
      "Trained batch 1859 batch loss 7.00419044 epoch total loss 6.4202075\n",
      "Trained batch 1860 batch loss 6.1569 epoch total loss 6.42006636\n",
      "Trained batch 1861 batch loss 6.14699316 epoch total loss 6.41992\n",
      "Trained batch 1862 batch loss 6.37536621 epoch total loss 6.41989565\n",
      "Trained batch 1863 batch loss 6.858006 epoch total loss 6.42013121\n",
      "Trained batch 1864 batch loss 6.4191184 epoch total loss 6.42013025\n",
      "Trained batch 1865 batch loss 6.48775244 epoch total loss 6.42016649\n",
      "Trained batch 1866 batch loss 6.70918846 epoch total loss 6.42032099\n",
      "Trained batch 1867 batch loss 6.15628576 epoch total loss 6.42018\n",
      "Trained batch 1868 batch loss 6.18695402 epoch total loss 6.42005444\n",
      "Trained batch 1869 batch loss 5.60179 epoch total loss 6.4196167\n",
      "Trained batch 1870 batch loss 6.42301655 epoch total loss 6.41961861\n",
      "Trained batch 1871 batch loss 5.77808952 epoch total loss 6.41927576\n",
      "Trained batch 1872 batch loss 5.19998598 epoch total loss 6.4186244\n",
      "Trained batch 1873 batch loss 5.27932787 epoch total loss 6.41801643\n",
      "Trained batch 1874 batch loss 5.29718113 epoch total loss 6.417418\n",
      "Trained batch 1875 batch loss 5.3104887 epoch total loss 6.41682768\n",
      "Trained batch 1876 batch loss 5.50249958 epoch total loss 6.41634035\n",
      "Trained batch 1877 batch loss 5.29627895 epoch total loss 6.41574335\n",
      "Trained batch 1878 batch loss 5.18006659 epoch total loss 6.41508532\n",
      "Trained batch 1879 batch loss 5.8321 epoch total loss 6.41477489\n",
      "Trained batch 1880 batch loss 5.5715313 epoch total loss 6.41432619\n",
      "Trained batch 1881 batch loss 6.48102808 epoch total loss 6.41436195\n",
      "Trained batch 1882 batch loss 6.08467484 epoch total loss 6.41418695\n",
      "Trained batch 1883 batch loss 6.42412853 epoch total loss 6.4141922\n",
      "Trained batch 1884 batch loss 5.54763746 epoch total loss 6.41373253\n",
      "Trained batch 1885 batch loss 5.58330917 epoch total loss 6.41329145\n",
      "Trained batch 1886 batch loss 5.98671913 epoch total loss 6.41306543\n",
      "Trained batch 1887 batch loss 6.29919243 epoch total loss 6.41300488\n",
      "Trained batch 1888 batch loss 7.40124607 epoch total loss 6.41352797\n",
      "Trained batch 1889 batch loss 7.10801029 epoch total loss 6.41389608\n",
      "Trained batch 1890 batch loss 5.66454887 epoch total loss 6.41349936\n",
      "Trained batch 1891 batch loss 5.65074825 epoch total loss 6.41309595\n",
      "Trained batch 1892 batch loss 5.42502213 epoch total loss 6.41257334\n",
      "Trained batch 1893 batch loss 5.7499218 epoch total loss 6.41222334\n",
      "Trained batch 1894 batch loss 5.7763567 epoch total loss 6.41188765\n",
      "Trained batch 1895 batch loss 7.36247683 epoch total loss 6.41238928\n",
      "Trained batch 1896 batch loss 5.69685745 epoch total loss 6.4120121\n",
      "Trained batch 1897 batch loss 5.3855381 epoch total loss 6.41147089\n",
      "Trained batch 1898 batch loss 5.79109478 epoch total loss 6.41114426\n",
      "Trained batch 1899 batch loss 5.94615316 epoch total loss 6.41089916\n",
      "Trained batch 1900 batch loss 6.00307894 epoch total loss 6.41068459\n",
      "Trained batch 1901 batch loss 4.66855049 epoch total loss 6.40976858\n",
      "Trained batch 1902 batch loss 5.62065697 epoch total loss 6.40935373\n",
      "Trained batch 1903 batch loss 5.77868748 epoch total loss 6.40902233\n",
      "Trained batch 1904 batch loss 5.11244392 epoch total loss 6.40834093\n",
      "Trained batch 1905 batch loss 6.63503456 epoch total loss 6.40846\n",
      "Trained batch 1906 batch loss 6.19833422 epoch total loss 6.40834951\n",
      "Trained batch 1907 batch loss 6.34262466 epoch total loss 6.40831518\n",
      "Trained batch 1908 batch loss 6.35630703 epoch total loss 6.408288\n",
      "Trained batch 1909 batch loss 6.014328 epoch total loss 6.40808201\n",
      "Trained batch 1910 batch loss 5.36550808 epoch total loss 6.40753603\n",
      "Trained batch 1911 batch loss 4.8085103 epoch total loss 6.40669918\n",
      "Trained batch 1912 batch loss 5.06279325 epoch total loss 6.40599632\n",
      "Trained batch 1913 batch loss 3.68768859 epoch total loss 6.40457487\n",
      "Trained batch 1914 batch loss 3.753438 epoch total loss 6.40319\n",
      "Trained batch 1915 batch loss 3.63768625 epoch total loss 6.40174627\n",
      "Trained batch 1916 batch loss 5.24150658 epoch total loss 6.40114\n",
      "Trained batch 1917 batch loss 4.90401411 epoch total loss 6.40035963\n",
      "Trained batch 1918 batch loss 6.58180094 epoch total loss 6.40045404\n",
      "Trained batch 1919 batch loss 6.72684288 epoch total loss 6.40062428\n",
      "Trained batch 1920 batch loss 4.08124733 epoch total loss 6.39941597\n",
      "Trained batch 1921 batch loss 4.15821171 epoch total loss 6.39824963\n",
      "Trained batch 1922 batch loss 4.89961338 epoch total loss 6.39746952\n",
      "Trained batch 1923 batch loss 5.13549519 epoch total loss 6.39681339\n",
      "Trained batch 1924 batch loss 4.32453489 epoch total loss 6.39573622\n",
      "Trained batch 1925 batch loss 5.02187252 epoch total loss 6.39502239\n",
      "Trained batch 1926 batch loss 6.34798336 epoch total loss 6.3949976\n",
      "Trained batch 1927 batch loss 6.98832798 epoch total loss 6.39530563\n",
      "Trained batch 1928 batch loss 6.7891264 epoch total loss 6.39550972\n",
      "Trained batch 1929 batch loss 6.36701107 epoch total loss 6.39549494\n",
      "Trained batch 1930 batch loss 6.54371452 epoch total loss 6.39557219\n",
      "Trained batch 1931 batch loss 6.24211073 epoch total loss 6.39549255\n",
      "Trained batch 1932 batch loss 6.21401787 epoch total loss 6.39539862\n",
      "Trained batch 1933 batch loss 6.23644781 epoch total loss 6.39531612\n",
      "Trained batch 1934 batch loss 6.92769861 epoch total loss 6.39559174\n",
      "Trained batch 1935 batch loss 6.93341398 epoch total loss 6.39586973\n",
      "Trained batch 1936 batch loss 6.47316551 epoch total loss 6.39591\n",
      "Trained batch 1937 batch loss 5.03738308 epoch total loss 6.39520836\n",
      "Trained batch 1938 batch loss 5.81192684 epoch total loss 6.394907\n",
      "Trained batch 1939 batch loss 5.98880863 epoch total loss 6.39469814\n",
      "Trained batch 1940 batch loss 4.23587 epoch total loss 6.39358521\n",
      "Trained batch 1941 batch loss 5.76936245 epoch total loss 6.39326382\n",
      "Trained batch 1942 batch loss 5.87332773 epoch total loss 6.39299583\n",
      "Trained batch 1943 batch loss 6.34439182 epoch total loss 6.39297104\n",
      "Trained batch 1944 batch loss 6.37874031 epoch total loss 6.39296389\n",
      "Trained batch 1945 batch loss 6.2596693 epoch total loss 6.39289522\n",
      "Trained batch 1946 batch loss 6.41006756 epoch total loss 6.39290428\n",
      "Trained batch 1947 batch loss 6.22011614 epoch total loss 6.39281559\n",
      "Trained batch 1948 batch loss 6.49057579 epoch total loss 6.39286518\n",
      "Trained batch 1949 batch loss 6.3019681 epoch total loss 6.39281845\n",
      "Trained batch 1950 batch loss 6.84933853 epoch total loss 6.39305305\n",
      "Trained batch 1951 batch loss 6.59993362 epoch total loss 6.39315891\n",
      "Trained batch 1952 batch loss 6.54803944 epoch total loss 6.39323807\n",
      "Trained batch 1953 batch loss 6.59095192 epoch total loss 6.39333916\n",
      "Trained batch 1954 batch loss 6.36995745 epoch total loss 6.39332724\n",
      "Trained batch 1955 batch loss 6.05068684 epoch total loss 6.39315224\n",
      "Trained batch 1956 batch loss 6.19540262 epoch total loss 6.39305115\n",
      "Trained batch 1957 batch loss 6.29899597 epoch total loss 6.39300299\n",
      "Trained batch 1958 batch loss 6.40156603 epoch total loss 6.39300728\n",
      "Trained batch 1959 batch loss 6.28801537 epoch total loss 6.3929534\n",
      "Trained batch 1960 batch loss 5.98494148 epoch total loss 6.39274549\n",
      "Trained batch 1961 batch loss 6.12803 epoch total loss 6.39261055\n",
      "Trained batch 1962 batch loss 6.20281029 epoch total loss 6.39251375\n",
      "Trained batch 1963 batch loss 6.25187397 epoch total loss 6.39244223\n",
      "Trained batch 1964 batch loss 6.26148891 epoch total loss 6.39237595\n",
      "Trained batch 1965 batch loss 6.10653305 epoch total loss 6.39223\n",
      "Trained batch 1966 batch loss 6.6174593 epoch total loss 6.39234447\n",
      "Trained batch 1967 batch loss 6.37139893 epoch total loss 6.39233398\n",
      "Trained batch 1968 batch loss 6.77218723 epoch total loss 6.3925271\n",
      "Trained batch 1969 batch loss 5.87423038 epoch total loss 6.39226389\n",
      "Trained batch 1970 batch loss 5.15501738 epoch total loss 6.39163589\n",
      "Trained batch 1971 batch loss 5.81365967 epoch total loss 6.39134264\n",
      "Trained batch 1972 batch loss 5.99246359 epoch total loss 6.39114\n",
      "Trained batch 1973 batch loss 6.34739637 epoch total loss 6.39111805\n",
      "Trained batch 1974 batch loss 6.50059414 epoch total loss 6.39117384\n",
      "Trained batch 1975 batch loss 6.38540936 epoch total loss 6.39117098\n",
      "Trained batch 1976 batch loss 6.45952225 epoch total loss 6.39120579\n",
      "Trained batch 1977 batch loss 6.36807632 epoch total loss 6.39119387\n",
      "Trained batch 1978 batch loss 6.30302238 epoch total loss 6.39114952\n",
      "Trained batch 1979 batch loss 6.34276199 epoch total loss 6.39112473\n",
      "Trained batch 1980 batch loss 6.58009815 epoch total loss 6.39122\n",
      "Trained batch 1981 batch loss 6.25160408 epoch total loss 6.39115\n",
      "Trained batch 1982 batch loss 6.52678728 epoch total loss 6.39121819\n",
      "Trained batch 1983 batch loss 6.15399 epoch total loss 6.39109898\n",
      "Trained batch 1984 batch loss 6.5328021 epoch total loss 6.3911705\n",
      "Trained batch 1985 batch loss 5.92981148 epoch total loss 6.39093781\n",
      "Trained batch 1986 batch loss 6.31419849 epoch total loss 6.39089918\n",
      "Trained batch 1987 batch loss 6.42149258 epoch total loss 6.39091492\n",
      "Trained batch 1988 batch loss 6.31634045 epoch total loss 6.39087772\n",
      "Trained batch 1989 batch loss 5.85450649 epoch total loss 6.39060783\n",
      "Trained batch 1990 batch loss 7.1593647 epoch total loss 6.39099407\n",
      "Trained batch 1991 batch loss 7.82906437 epoch total loss 6.39171648\n",
      "Trained batch 1992 batch loss 5.76978 epoch total loss 6.39140415\n",
      "Trained batch 1993 batch loss 6.59573412 epoch total loss 6.39150667\n",
      "Trained batch 1994 batch loss 7.25014973 epoch total loss 6.39193726\n",
      "Trained batch 1995 batch loss 6.56396866 epoch total loss 6.39202356\n",
      "Trained batch 1996 batch loss 6.59321499 epoch total loss 6.39212418\n",
      "Trained batch 1997 batch loss 5.49909 epoch total loss 6.3916769\n",
      "Trained batch 1998 batch loss 5.82418776 epoch total loss 6.39139271\n",
      "Trained batch 1999 batch loss 5.91839695 epoch total loss 6.3911562\n",
      "Trained batch 2000 batch loss 5.61296654 epoch total loss 6.3907671\n",
      "Trained batch 2001 batch loss 6.09504795 epoch total loss 6.39061928\n",
      "Trained batch 2002 batch loss 6.10364914 epoch total loss 6.39047575\n",
      "Trained batch 2003 batch loss 6.46870375 epoch total loss 6.39051485\n",
      "Trained batch 2004 batch loss 6.67724705 epoch total loss 6.3906579\n",
      "Trained batch 2005 batch loss 5.25447512 epoch total loss 6.3900919\n",
      "Trained batch 2006 batch loss 5.12036085 epoch total loss 6.38945866\n",
      "Trained batch 2007 batch loss 5.22610378 epoch total loss 6.3888793\n",
      "Trained batch 2008 batch loss 6.47844076 epoch total loss 6.38892365\n",
      "Trained batch 2009 batch loss 6.16238117 epoch total loss 6.38881111\n",
      "Trained batch 2010 batch loss 6.23925686 epoch total loss 6.38873672\n",
      "Trained batch 2011 batch loss 6.33240652 epoch total loss 6.38870811\n",
      "Trained batch 2012 batch loss 5.73091221 epoch total loss 6.388381\n",
      "Trained batch 2013 batch loss 6.21836758 epoch total loss 6.38829708\n",
      "Trained batch 2014 batch loss 6.48825836 epoch total loss 6.38834667\n",
      "Trained batch 2015 batch loss 6.39074898 epoch total loss 6.38834763\n",
      "Trained batch 2016 batch loss 6.14156437 epoch total loss 6.38822508\n",
      "Trained batch 2017 batch loss 6.53054762 epoch total loss 6.38829565\n",
      "Trained batch 2018 batch loss 6.3939352 epoch total loss 6.38829851\n",
      "Trained batch 2019 batch loss 6.71727085 epoch total loss 6.38846111\n",
      "Trained batch 2020 batch loss 6.4161725 epoch total loss 6.38847446\n",
      "Trained batch 2021 batch loss 5.72392321 epoch total loss 6.38814545\n",
      "Trained batch 2022 batch loss 6.42554569 epoch total loss 6.38816404\n",
      "Trained batch 2023 batch loss 5.56061363 epoch total loss 6.38775539\n",
      "Trained batch 2024 batch loss 6.35528088 epoch total loss 6.38773918\n",
      "Trained batch 2025 batch loss 5.80896187 epoch total loss 6.38745308\n",
      "Trained batch 2026 batch loss 6.45583153 epoch total loss 6.38748693\n",
      "Trained batch 2027 batch loss 5.45465374 epoch total loss 6.38702726\n",
      "Trained batch 2028 batch loss 6.52450943 epoch total loss 6.38709497\n",
      "Trained batch 2029 batch loss 7.09849072 epoch total loss 6.38744545\n",
      "Trained batch 2030 batch loss 6.33386135 epoch total loss 6.38741922\n",
      "Trained batch 2031 batch loss 5.57093048 epoch total loss 6.38701725\n",
      "Trained batch 2032 batch loss 6.59505 epoch total loss 6.38712\n",
      "Trained batch 2033 batch loss 6.1580286 epoch total loss 6.38700676\n",
      "Trained batch 2034 batch loss 5.87749243 epoch total loss 6.3867569\n",
      "Trained batch 2035 batch loss 5.44283056 epoch total loss 6.38629246\n",
      "Trained batch 2036 batch loss 7.3163538 epoch total loss 6.38674927\n",
      "Trained batch 2037 batch loss 6.52673531 epoch total loss 6.38681793\n",
      "Trained batch 2038 batch loss 6.03820753 epoch total loss 6.38664675\n",
      "Trained batch 2039 batch loss 6.98665333 epoch total loss 6.38694096\n",
      "Trained batch 2040 batch loss 6.89044237 epoch total loss 6.38718796\n",
      "Trained batch 2041 batch loss 6.6938715 epoch total loss 6.38733816\n",
      "Trained batch 2042 batch loss 6.42988682 epoch total loss 6.38735914\n",
      "Trained batch 2043 batch loss 7.64617443 epoch total loss 6.38797522\n",
      "Trained batch 2044 batch loss 6.31068134 epoch total loss 6.38793755\n",
      "Trained batch 2045 batch loss 7.5785141 epoch total loss 6.38852\n",
      "Trained batch 2046 batch loss 7.51397419 epoch total loss 6.38906956\n",
      "Trained batch 2047 batch loss 6.63549948 epoch total loss 6.38919\n",
      "Trained batch 2048 batch loss 6.32081842 epoch total loss 6.38915682\n",
      "Trained batch 2049 batch loss 7.58572 epoch total loss 6.38974094\n",
      "Trained batch 2050 batch loss 7.2597928 epoch total loss 6.39016533\n",
      "Trained batch 2051 batch loss 7.53435802 epoch total loss 6.39072323\n",
      "Trained batch 2052 batch loss 6.72867632 epoch total loss 6.39088774\n",
      "Trained batch 2053 batch loss 6.22845125 epoch total loss 6.39080858\n",
      "Trained batch 2054 batch loss 5.35006475 epoch total loss 6.3903017\n",
      "Trained batch 2055 batch loss 7.29781914 epoch total loss 6.39074326\n",
      "Trained batch 2056 batch loss 6.63671684 epoch total loss 6.39086294\n",
      "Trained batch 2057 batch loss 6.08837795 epoch total loss 6.3907156\n",
      "Trained batch 2058 batch loss 6.35659742 epoch total loss 6.39069891\n",
      "Trained batch 2059 batch loss 5.59243536 epoch total loss 6.39031172\n",
      "Trained batch 2060 batch loss 6.57898092 epoch total loss 6.39040327\n",
      "Trained batch 2061 batch loss 6.42701149 epoch total loss 6.39042091\n",
      "Trained batch 2062 batch loss 5.36230516 epoch total loss 6.38992214\n",
      "Trained batch 2063 batch loss 5.51103687 epoch total loss 6.38949585\n",
      "Trained batch 2064 batch loss 5.55886555 epoch total loss 6.3890934\n",
      "Trained batch 2065 batch loss 5.95791149 epoch total loss 6.38888454\n",
      "Trained batch 2066 batch loss 5.39024591 epoch total loss 6.38840151\n",
      "Trained batch 2067 batch loss 5.92052794 epoch total loss 6.38817549\n",
      "Trained batch 2068 batch loss 5.98497295 epoch total loss 6.38798046\n",
      "Trained batch 2069 batch loss 6.04257154 epoch total loss 6.38781357\n",
      "Trained batch 2070 batch loss 5.64800215 epoch total loss 6.38745642\n",
      "Trained batch 2071 batch loss 5.10945225 epoch total loss 6.38683939\n",
      "Trained batch 2072 batch loss 7.25535583 epoch total loss 6.38725853\n",
      "Trained batch 2073 batch loss 5.13167238 epoch total loss 6.38665295\n",
      "Trained batch 2074 batch loss 5.92974377 epoch total loss 6.38643265\n",
      "Trained batch 2075 batch loss 4.56440449 epoch total loss 6.38555431\n",
      "Trained batch 2076 batch loss 5.74023628 epoch total loss 6.38524342\n",
      "Trained batch 2077 batch loss 4.96394682 epoch total loss 6.38455915\n",
      "Trained batch 2078 batch loss 5.08671761 epoch total loss 6.38393497\n",
      "Trained batch 2079 batch loss 7.13045502 epoch total loss 6.38429403\n",
      "Trained batch 2080 batch loss 5.37660027 epoch total loss 6.38380957\n",
      "Trained batch 2081 batch loss 6.05391312 epoch total loss 6.38365126\n",
      "Trained batch 2082 batch loss 6.71558094 epoch total loss 6.38381052\n",
      "Trained batch 2083 batch loss 6.38057137 epoch total loss 6.38380909\n",
      "Trained batch 2084 batch loss 6.43756676 epoch total loss 6.38383484\n",
      "Trained batch 2085 batch loss 6.72918797 epoch total loss 6.38400078\n",
      "Trained batch 2086 batch loss 6.18436718 epoch total loss 6.38390493\n",
      "Trained batch 2087 batch loss 6.66665363 epoch total loss 6.38404083\n",
      "Trained batch 2088 batch loss 6.47038603 epoch total loss 6.38408232\n",
      "Trained batch 2089 batch loss 6.72663403 epoch total loss 6.38424635\n",
      "Trained batch 2090 batch loss 6.44083881 epoch total loss 6.38427305\n",
      "Trained batch 2091 batch loss 6.78352928 epoch total loss 6.38446379\n",
      "Trained batch 2092 batch loss 6.6358552 epoch total loss 6.38458395\n",
      "Trained batch 2093 batch loss 6.18574 epoch total loss 6.38448906\n",
      "Trained batch 2094 batch loss 6.10528135 epoch total loss 6.38435555\n",
      "Trained batch 2095 batch loss 6.03380108 epoch total loss 6.38418865\n",
      "Trained batch 2096 batch loss 6.36675215 epoch total loss 6.38418055\n",
      "Trained batch 2097 batch loss 6.47408676 epoch total loss 6.38422298\n",
      "Trained batch 2098 batch loss 6.83800173 epoch total loss 6.38443947\n",
      "Trained batch 2099 batch loss 5.7976141 epoch total loss 6.38416\n",
      "Trained batch 2100 batch loss 7.26174736 epoch total loss 6.38457775\n",
      "Trained batch 2101 batch loss 5.9348259 epoch total loss 6.38436365\n",
      "Trained batch 2102 batch loss 5.43248224 epoch total loss 6.38391066\n",
      "Trained batch 2103 batch loss 6.09634304 epoch total loss 6.38377428\n",
      "Trained batch 2104 batch loss 6.28806686 epoch total loss 6.3837285\n",
      "Trained batch 2105 batch loss 5.72036648 epoch total loss 6.38341379\n",
      "Trained batch 2106 batch loss 6.25536919 epoch total loss 6.38335276\n",
      "Trained batch 2107 batch loss 6.43865681 epoch total loss 6.38337898\n",
      "Trained batch 2108 batch loss 6.35738087 epoch total loss 6.38336658\n",
      "Trained batch 2109 batch loss 6.80260181 epoch total loss 6.38356543\n",
      "Trained batch 2110 batch loss 6.66478634 epoch total loss 6.38369894\n",
      "Trained batch 2111 batch loss 6.36319351 epoch total loss 6.38368893\n",
      "Trained batch 2112 batch loss 6.77258301 epoch total loss 6.38387299\n",
      "Trained batch 2113 batch loss 6.52484035 epoch total loss 6.38393974\n",
      "Trained batch 2114 batch loss 6.85830688 epoch total loss 6.38416433\n",
      "Trained batch 2115 batch loss 6.75587559 epoch total loss 6.38434\n",
      "Trained batch 2116 batch loss 6.2402792 epoch total loss 6.38427162\n",
      "Trained batch 2117 batch loss 4.01505852 epoch total loss 6.38315248\n",
      "Trained batch 2118 batch loss 6.84701109 epoch total loss 6.38337135\n",
      "Trained batch 2119 batch loss 6.8914814 epoch total loss 6.3836112\n",
      "Trained batch 2120 batch loss 6.61823273 epoch total loss 6.38372183\n",
      "Trained batch 2121 batch loss 6.32661581 epoch total loss 6.38369465\n",
      "Trained batch 2122 batch loss 6.69436455 epoch total loss 6.38384104\n",
      "Trained batch 2123 batch loss 6.35740185 epoch total loss 6.38382864\n",
      "Trained batch 2124 batch loss 4.85539341 epoch total loss 6.38310909\n",
      "Trained batch 2125 batch loss 6.24209404 epoch total loss 6.38304281\n",
      "Trained batch 2126 batch loss 6.04978752 epoch total loss 6.38288593\n",
      "Trained batch 2127 batch loss 6.24350739 epoch total loss 6.38282\n",
      "Trained batch 2128 batch loss 6.27345371 epoch total loss 6.38276911\n",
      "Trained batch 2129 batch loss 6.52278328 epoch total loss 6.38283443\n",
      "Trained batch 2130 batch loss 6.28279924 epoch total loss 6.3827877\n",
      "Trained batch 2131 batch loss 6.22678328 epoch total loss 6.38271427\n",
      "Trained batch 2132 batch loss 6.39197779 epoch total loss 6.38271856\n",
      "Trained batch 2133 batch loss 6.40525532 epoch total loss 6.38272905\n",
      "Trained batch 2134 batch loss 6.21239805 epoch total loss 6.38264894\n",
      "Trained batch 2135 batch loss 6.03682756 epoch total loss 6.3824873\n",
      "Trained batch 2136 batch loss 6.49277687 epoch total loss 6.38253927\n",
      "Trained batch 2137 batch loss 6.09920788 epoch total loss 6.38240671\n",
      "Trained batch 2138 batch loss 6.42277145 epoch total loss 6.38242579\n",
      "Trained batch 2139 batch loss 6.15597248 epoch total loss 6.38232\n",
      "Trained batch 2140 batch loss 6.01351786 epoch total loss 6.38214779\n",
      "Trained batch 2141 batch loss 5.47463512 epoch total loss 6.38172388\n",
      "Trained batch 2142 batch loss 5.67288971 epoch total loss 6.38139296\n",
      "Trained batch 2143 batch loss 5.84369564 epoch total loss 6.38114214\n",
      "Trained batch 2144 batch loss 7.18405628 epoch total loss 6.38151598\n",
      "Trained batch 2145 batch loss 6.84027576 epoch total loss 6.38173\n",
      "Trained batch 2146 batch loss 6.37236118 epoch total loss 6.38172531\n",
      "Trained batch 2147 batch loss 6.22178 epoch total loss 6.38165092\n",
      "Trained batch 2148 batch loss 6.67217255 epoch total loss 6.38178587\n",
      "Trained batch 2149 batch loss 5.2586813 epoch total loss 6.38126326\n",
      "Trained batch 2150 batch loss 6.3500843 epoch total loss 6.38124847\n",
      "Trained batch 2151 batch loss 6.39129353 epoch total loss 6.38125324\n",
      "Trained batch 2152 batch loss 6.96708393 epoch total loss 6.38152552\n",
      "Trained batch 2153 batch loss 6.14527702 epoch total loss 6.38141584\n",
      "Trained batch 2154 batch loss 7.30293179 epoch total loss 6.38184357\n",
      "Trained batch 2155 batch loss 5.90176201 epoch total loss 6.38162088\n",
      "Trained batch 2156 batch loss 6.35816 epoch total loss 6.38161\n",
      "Trained batch 2157 batch loss 6.05364037 epoch total loss 6.38145781\n",
      "Trained batch 2158 batch loss 6.24514675 epoch total loss 6.38139486\n",
      "Trained batch 2159 batch loss 6.47443581 epoch total loss 6.38143778\n",
      "Trained batch 2160 batch loss 6.0553627 epoch total loss 6.3812871\n",
      "Trained batch 2161 batch loss 6.63954449 epoch total loss 6.38140678\n",
      "Trained batch 2162 batch loss 6.32653666 epoch total loss 6.38138103\n",
      "Trained batch 2163 batch loss 6.5678668 epoch total loss 6.38146687\n",
      "Trained batch 2164 batch loss 7.17601776 epoch total loss 6.38183403\n",
      "Trained batch 2165 batch loss 6.2242651 epoch total loss 6.38176155\n",
      "Trained batch 2166 batch loss 6.66101265 epoch total loss 6.3818903\n",
      "Trained batch 2167 batch loss 6.5314455 epoch total loss 6.38195944\n",
      "Trained batch 2168 batch loss 6.07555103 epoch total loss 6.38181782\n",
      "Trained batch 2169 batch loss 6.49166393 epoch total loss 6.38186836\n",
      "Trained batch 2170 batch loss 6.25205946 epoch total loss 6.38180828\n",
      "Trained batch 2171 batch loss 6.27867222 epoch total loss 6.3817606\n",
      "Trained batch 2172 batch loss 6.21705818 epoch total loss 6.38168478\n",
      "Trained batch 2173 batch loss 7.02315617 epoch total loss 6.38198042\n",
      "Trained batch 2174 batch loss 5.64142752 epoch total loss 6.38163948\n",
      "Trained batch 2175 batch loss 6.0344944 epoch total loss 6.38147974\n",
      "Trained batch 2176 batch loss 6.1946454 epoch total loss 6.38139391\n",
      "Trained batch 2177 batch loss 6.2448082 epoch total loss 6.38133144\n",
      "Trained batch 2178 batch loss 5.42759895 epoch total loss 6.38089323\n",
      "Trained batch 2179 batch loss 6.12179899 epoch total loss 6.3807745\n",
      "Trained batch 2180 batch loss 5.82790661 epoch total loss 6.3805213\n",
      "Trained batch 2181 batch loss 5.76527834 epoch total loss 6.38023901\n",
      "Trained batch 2182 batch loss 5.99303055 epoch total loss 6.38006163\n",
      "Trained batch 2183 batch loss 6.39014292 epoch total loss 6.38006687\n",
      "Trained batch 2184 batch loss 6.37964773 epoch total loss 6.38006639\n",
      "Trained batch 2185 batch loss 6.25190163 epoch total loss 6.38000774\n",
      "Trained batch 2186 batch loss 6.53457928 epoch total loss 6.38007832\n",
      "Trained batch 2187 batch loss 5.78563738 epoch total loss 6.37980652\n",
      "Trained batch 2188 batch loss 6.7190094 epoch total loss 6.37996149\n",
      "Trained batch 2189 batch loss 6.750494 epoch total loss 6.38013077\n",
      "Trained batch 2190 batch loss 6.6867485 epoch total loss 6.38027096\n",
      "Trained batch 2191 batch loss 6.24788666 epoch total loss 6.3802104\n",
      "Trained batch 2192 batch loss 6.1939106 epoch total loss 6.38012552\n",
      "Trained batch 2193 batch loss 6.82247448 epoch total loss 6.38032722\n",
      "Trained batch 2194 batch loss 5.85347366 epoch total loss 6.3800869\n",
      "Trained batch 2195 batch loss 6.63782883 epoch total loss 6.38020468\n",
      "Trained batch 2196 batch loss 6.76131201 epoch total loss 6.38037825\n",
      "Trained batch 2197 batch loss 7.01762962 epoch total loss 6.38066816\n",
      "Trained batch 2198 batch loss 6.65188265 epoch total loss 6.38079166\n",
      "Trained batch 2199 batch loss 6.981493 epoch total loss 6.38106489\n",
      "Trained batch 2200 batch loss 6.40402889 epoch total loss 6.38107538\n",
      "Trained batch 2201 batch loss 6.19930458 epoch total loss 6.38099289\n",
      "Trained batch 2202 batch loss 6.36480379 epoch total loss 6.38098574\n",
      "Trained batch 2203 batch loss 6.4497757 epoch total loss 6.38101721\n",
      "Trained batch 2204 batch loss 6.17362404 epoch total loss 6.38092327\n",
      "Trained batch 2205 batch loss 5.96188831 epoch total loss 6.38073301\n",
      "Trained batch 2206 batch loss 6.64953709 epoch total loss 6.38085508\n",
      "Trained batch 2207 batch loss 6.59076595 epoch total loss 6.38095\n",
      "Trained batch 2208 batch loss 6.65340424 epoch total loss 6.38107347\n",
      "Trained batch 2209 batch loss 6.6149478 epoch total loss 6.38117933\n",
      "Trained batch 2210 batch loss 6.58800602 epoch total loss 6.38127279\n",
      "Trained batch 2211 batch loss 7.19811583 epoch total loss 6.38164234\n",
      "Trained batch 2212 batch loss 7.55762196 epoch total loss 6.38217402\n",
      "Trained batch 2213 batch loss 7.06487942 epoch total loss 6.38248253\n",
      "Trained batch 2214 batch loss 7.27155828 epoch total loss 6.38288403\n",
      "Trained batch 2215 batch loss 6.7867837 epoch total loss 6.38306665\n",
      "Trained batch 2216 batch loss 6.70742512 epoch total loss 6.38321257\n",
      "Trained batch 2217 batch loss 6.5017128 epoch total loss 6.38326597\n",
      "Trained batch 2218 batch loss 6.46795368 epoch total loss 6.38330412\n",
      "Trained batch 2219 batch loss 6.31423759 epoch total loss 6.38327312\n",
      "Trained batch 2220 batch loss 6.78361416 epoch total loss 6.38345337\n",
      "Trained batch 2221 batch loss 6.69520283 epoch total loss 6.38359404\n",
      "Trained batch 2222 batch loss 6.55786371 epoch total loss 6.38367224\n",
      "Trained batch 2223 batch loss 5.33044195 epoch total loss 6.38319826\n",
      "Trained batch 2224 batch loss 6.04031086 epoch total loss 6.38304377\n",
      "Trained batch 2225 batch loss 6.02706957 epoch total loss 6.38288403\n",
      "Trained batch 2226 batch loss 6.78125572 epoch total loss 6.38306284\n",
      "Trained batch 2227 batch loss 5.43003273 epoch total loss 6.38263512\n",
      "Trained batch 2228 batch loss 6.89537525 epoch total loss 6.38286495\n",
      "Trained batch 2229 batch loss 6.92012405 epoch total loss 6.38310623\n",
      "Trained batch 2230 batch loss 6.40525341 epoch total loss 6.38311577\n",
      "Trained batch 2231 batch loss 5.89237356 epoch total loss 6.38289595\n",
      "Trained batch 2232 batch loss 6.72109509 epoch total loss 6.38304758\n",
      "Trained batch 2233 batch loss 6.19464779 epoch total loss 6.38296318\n",
      "Trained batch 2234 batch loss 6.01361465 epoch total loss 6.38279772\n",
      "Trained batch 2235 batch loss 6.00449753 epoch total loss 6.38262844\n",
      "Trained batch 2236 batch loss 7.17207909 epoch total loss 6.3829813\n",
      "Trained batch 2237 batch loss 5.45245934 epoch total loss 6.3825655\n",
      "Trained batch 2238 batch loss 6.43280888 epoch total loss 6.38258791\n",
      "Trained batch 2239 batch loss 6.44339371 epoch total loss 6.38261509\n",
      "Trained batch 2240 batch loss 7.06494522 epoch total loss 6.38292\n",
      "Trained batch 2241 batch loss 6.46764851 epoch total loss 6.38295746\n",
      "Trained batch 2242 batch loss 7.09861374 epoch total loss 6.38327694\n",
      "Trained batch 2243 batch loss 6.40135431 epoch total loss 6.38328505\n",
      "Trained batch 2244 batch loss 6.73486137 epoch total loss 6.38344145\n",
      "Trained batch 2245 batch loss 6.83730125 epoch total loss 6.38364315\n",
      "Trained batch 2246 batch loss 6.33888292 epoch total loss 6.3836236\n",
      "Trained batch 2247 batch loss 6.42978287 epoch total loss 6.3836441\n",
      "Trained batch 2248 batch loss 6.47581959 epoch total loss 6.38368464\n",
      "Trained batch 2249 batch loss 6.5140276 epoch total loss 6.38374281\n",
      "Trained batch 2250 batch loss 6.58390903 epoch total loss 6.3838315\n",
      "Trained batch 2251 batch loss 6.48307037 epoch total loss 6.38387585\n",
      "Trained batch 2252 batch loss 6.90784359 epoch total loss 6.38410854\n",
      "Trained batch 2253 batch loss 6.36777544 epoch total loss 6.38410139\n",
      "Trained batch 2254 batch loss 6.52035332 epoch total loss 6.38416195\n",
      "Trained batch 2255 batch loss 5.96383286 epoch total loss 6.38397551\n",
      "Trained batch 2256 batch loss 5.52481079 epoch total loss 6.38359451\n",
      "Trained batch 2257 batch loss 5.47795296 epoch total loss 6.38319349\n",
      "Trained batch 2258 batch loss 5.85653591 epoch total loss 6.38296\n",
      "Trained batch 2259 batch loss 5.4461484 epoch total loss 6.38254547\n",
      "Trained batch 2260 batch loss 5.09139156 epoch total loss 6.38197422\n",
      "Trained batch 2261 batch loss 5.53080416 epoch total loss 6.381598\n",
      "Trained batch 2262 batch loss 5.40178537 epoch total loss 6.38116455\n",
      "Trained batch 2263 batch loss 4.81223822 epoch total loss 6.38047123\n",
      "Trained batch 2264 batch loss 5.16475868 epoch total loss 6.37993479\n",
      "Trained batch 2265 batch loss 4.97409344 epoch total loss 6.37931347\n",
      "Trained batch 2266 batch loss 5.0838995 epoch total loss 6.37874222\n",
      "Trained batch 2267 batch loss 4.68833351 epoch total loss 6.37799644\n",
      "Trained batch 2268 batch loss 4.8113718 epoch total loss 6.37730598\n",
      "Trained batch 2269 batch loss 4.9602766 epoch total loss 6.37668133\n",
      "Trained batch 2270 batch loss 4.99229622 epoch total loss 6.37607098\n",
      "Trained batch 2271 batch loss 6.65544939 epoch total loss 6.376194\n",
      "Trained batch 2272 batch loss 6.76951218 epoch total loss 6.37636709\n",
      "Trained batch 2273 batch loss 6.29626 epoch total loss 6.37633181\n",
      "Trained batch 2274 batch loss 7.38361931 epoch total loss 6.37677479\n",
      "Trained batch 2275 batch loss 6.2235117 epoch total loss 6.37670755\n",
      "Trained batch 2276 batch loss 6.71992064 epoch total loss 6.37685823\n",
      "Trained batch 2277 batch loss 6.46527719 epoch total loss 6.37689686\n",
      "Trained batch 2278 batch loss 6.64330769 epoch total loss 6.37701416\n",
      "Trained batch 2279 batch loss 6.80943298 epoch total loss 6.37720394\n",
      "Trained batch 2280 batch loss 6.61076164 epoch total loss 6.37730598\n",
      "Trained batch 2281 batch loss 6.91627789 epoch total loss 6.37754202\n",
      "Trained batch 2282 batch loss 6.39992714 epoch total loss 6.37755203\n",
      "Trained batch 2283 batch loss 6.54857159 epoch total loss 6.37762737\n",
      "Trained batch 2284 batch loss 6.51034832 epoch total loss 6.37768555\n",
      "Trained batch 2285 batch loss 7.07400608 epoch total loss 6.37799025\n",
      "Trained batch 2286 batch loss 6.71970272 epoch total loss 6.37814\n",
      "Trained batch 2287 batch loss 6.2927556 epoch total loss 6.37810278\n",
      "Trained batch 2288 batch loss 6.01051712 epoch total loss 6.37794209\n",
      "Trained batch 2289 batch loss 5.60816 epoch total loss 6.37760592\n",
      "Trained batch 2290 batch loss 6.24733973 epoch total loss 6.37754869\n",
      "Trained batch 2291 batch loss 6.39916611 epoch total loss 6.37755823\n",
      "Trained batch 2292 batch loss 6.26333761 epoch total loss 6.37750864\n",
      "Trained batch 2293 batch loss 6.52655792 epoch total loss 6.37757349\n",
      "Trained batch 2294 batch loss 6.74929237 epoch total loss 6.37773561\n",
      "Trained batch 2295 batch loss 5.71512747 epoch total loss 6.37744665\n",
      "Trained batch 2296 batch loss 6.8649826 epoch total loss 6.37765932\n",
      "Trained batch 2297 batch loss 6.74533558 epoch total loss 6.37781906\n",
      "Trained batch 2298 batch loss 6.77387714 epoch total loss 6.3779912\n",
      "Trained batch 2299 batch loss 6.55758572 epoch total loss 6.3780694\n",
      "Trained batch 2300 batch loss 6.66714382 epoch total loss 6.37819529\n",
      "Trained batch 2301 batch loss 6.6133852 epoch total loss 6.37829733\n",
      "Trained batch 2302 batch loss 6.08155203 epoch total loss 6.37816858\n",
      "Trained batch 2303 batch loss 6.47046041 epoch total loss 6.37820864\n",
      "Trained batch 2304 batch loss 6.29362297 epoch total loss 6.37817192\n",
      "Trained batch 2305 batch loss 6.25011539 epoch total loss 6.37811661\n",
      "Trained batch 2306 batch loss 7.07542419 epoch total loss 6.37841892\n",
      "Trained batch 2307 batch loss 6.87440968 epoch total loss 6.3786335\n",
      "Trained batch 2308 batch loss 6.28811312 epoch total loss 6.3785944\n",
      "Trained batch 2309 batch loss 6.35356045 epoch total loss 6.37858343\n",
      "Trained batch 2310 batch loss 6.8252 epoch total loss 6.37877703\n",
      "Trained batch 2311 batch loss 7.39054441 epoch total loss 6.37921476\n",
      "Trained batch 2312 batch loss 8.22385597 epoch total loss 6.38001251\n",
      "Trained batch 2313 batch loss 6.99835348 epoch total loss 6.38027954\n",
      "Trained batch 2314 batch loss 7.26231861 epoch total loss 6.38066101\n",
      "Trained batch 2315 batch loss 6.46350861 epoch total loss 6.38069677\n",
      "Trained batch 2316 batch loss 6.5126543 epoch total loss 6.38075399\n",
      "Trained batch 2317 batch loss 6.77001286 epoch total loss 6.38092184\n",
      "Trained batch 2318 batch loss 6.38480473 epoch total loss 6.38092327\n",
      "Trained batch 2319 batch loss 6.99801588 epoch total loss 6.38118935\n",
      "Trained batch 2320 batch loss 6.45908737 epoch total loss 6.3812232\n",
      "Trained batch 2321 batch loss 7.07888699 epoch total loss 6.38152361\n",
      "Trained batch 2322 batch loss 7.05260944 epoch total loss 6.38181257\n",
      "Trained batch 2323 batch loss 6.58409405 epoch total loss 6.3819\n",
      "Trained batch 2324 batch loss 6.49952126 epoch total loss 6.38195086\n",
      "Trained batch 2325 batch loss 6.38643503 epoch total loss 6.38195276\n",
      "Trained batch 2326 batch loss 6.56694889 epoch total loss 6.38203239\n",
      "Trained batch 2327 batch loss 5.71835232 epoch total loss 6.38174725\n",
      "Trained batch 2328 batch loss 6.25601482 epoch total loss 6.38169336\n",
      "Trained batch 2329 batch loss 6.48132515 epoch total loss 6.38173628\n",
      "Trained batch 2330 batch loss 7.16436 epoch total loss 6.38207197\n",
      "Trained batch 2331 batch loss 6.77214909 epoch total loss 6.38223934\n",
      "Trained batch 2332 batch loss 6.71781111 epoch total loss 6.38238335\n",
      "Trained batch 2333 batch loss 6.65333796 epoch total loss 6.38249922\n",
      "Trained batch 2334 batch loss 6.74748087 epoch total loss 6.38265562\n",
      "Trained batch 2335 batch loss 7.06860256 epoch total loss 6.38294935\n",
      "Trained batch 2336 batch loss 7.43890285 epoch total loss 6.38340092\n",
      "Trained batch 2337 batch loss 6.52330303 epoch total loss 6.383461\n",
      "Trained batch 2338 batch loss 6.46239853 epoch total loss 6.38349438\n",
      "Trained batch 2339 batch loss 6.67765665 epoch total loss 6.38362026\n",
      "Trained batch 2340 batch loss 6.50736046 epoch total loss 6.38367367\n",
      "Trained batch 2341 batch loss 7.49177361 epoch total loss 6.38414717\n",
      "Trained batch 2342 batch loss 7.07775497 epoch total loss 6.38444328\n",
      "Trained batch 2343 batch loss 6.53256035 epoch total loss 6.38450623\n",
      "Trained batch 2344 batch loss 6.59733391 epoch total loss 6.3845973\n",
      "Trained batch 2345 batch loss 6.74952602 epoch total loss 6.38475323\n",
      "Trained batch 2346 batch loss 6.70790863 epoch total loss 6.38489103\n",
      "Trained batch 2347 batch loss 6.43005896 epoch total loss 6.38491\n",
      "Trained batch 2348 batch loss 7.02368355 epoch total loss 6.3851819\n",
      "Trained batch 2349 batch loss 6.54566574 epoch total loss 6.38525057\n",
      "Trained batch 2350 batch loss 6.46279 epoch total loss 6.38528347\n",
      "Trained batch 2351 batch loss 6.63707542 epoch total loss 6.38539028\n",
      "Trained batch 2352 batch loss 6.55733109 epoch total loss 6.38546371\n",
      "Trained batch 2353 batch loss 7.0055275 epoch total loss 6.38572741\n",
      "Trained batch 2354 batch loss 6.34152508 epoch total loss 6.38570881\n",
      "Trained batch 2355 batch loss 5.3579278 epoch total loss 6.3852725\n",
      "Trained batch 2356 batch loss 6.24019194 epoch total loss 6.38521099\n",
      "Trained batch 2357 batch loss 5.56629753 epoch total loss 6.38486338\n",
      "Trained batch 2358 batch loss 6.5643115 epoch total loss 6.38493967\n",
      "Trained batch 2359 batch loss 6.36900473 epoch total loss 6.38493299\n",
      "Trained batch 2360 batch loss 6.44734859 epoch total loss 6.38495922\n",
      "Trained batch 2361 batch loss 6.18285322 epoch total loss 6.38487339\n",
      "Trained batch 2362 batch loss 6.05841923 epoch total loss 6.38473558\n",
      "Trained batch 2363 batch loss 6.2558136 epoch total loss 6.38468075\n",
      "Trained batch 2364 batch loss 6.5467186 epoch total loss 6.38474941\n",
      "Trained batch 2365 batch loss 6.85812855 epoch total loss 6.38494968\n",
      "Trained batch 2366 batch loss 6.49413633 epoch total loss 6.38499594\n",
      "Trained batch 2367 batch loss 6.29508209 epoch total loss 6.38495779\n",
      "Trained batch 2368 batch loss 5.4466753 epoch total loss 6.38456154\n",
      "Trained batch 2369 batch loss 6.26343489 epoch total loss 6.38451052\n",
      "Trained batch 2370 batch loss 5.05917597 epoch total loss 6.38395119\n",
      "Trained batch 2371 batch loss 5.74071693 epoch total loss 6.38368\n",
      "Trained batch 2372 batch loss 5.02754 epoch total loss 6.38310814\n",
      "Trained batch 2373 batch loss 4.88163614 epoch total loss 6.38247538\n",
      "Trained batch 2374 batch loss 5.04263544 epoch total loss 6.38191128\n",
      "Trained batch 2375 batch loss 4.97618103 epoch total loss 6.38131952\n",
      "Trained batch 2376 batch loss 4.87314034 epoch total loss 6.38068485\n",
      "Trained batch 2377 batch loss 4.82830763 epoch total loss 6.38003159\n",
      "Trained batch 2378 batch loss 4.67904806 epoch total loss 6.37931633\n",
      "Trained batch 2379 batch loss 5.03184175 epoch total loss 6.37875\n",
      "Trained batch 2380 batch loss 7.07631397 epoch total loss 6.3790431\n",
      "Trained batch 2381 batch loss 6.03869247 epoch total loss 6.3789\n",
      "Trained batch 2382 batch loss 6.43576241 epoch total loss 6.37892389\n",
      "Trained batch 2383 batch loss 5.63217735 epoch total loss 6.37861\n",
      "Trained batch 2384 batch loss 6.17794847 epoch total loss 6.37852621\n",
      "Trained batch 2385 batch loss 5.5498023 epoch total loss 6.3781786\n",
      "Trained batch 2386 batch loss 6.54548836 epoch total loss 6.37824917\n",
      "Trained batch 2387 batch loss 6.55645847 epoch total loss 6.37832355\n",
      "Trained batch 2388 batch loss 5.44854832 epoch total loss 6.37793398\n",
      "Trained batch 2389 batch loss 7.71828508 epoch total loss 6.37849522\n",
      "Trained batch 2390 batch loss 6.98677254 epoch total loss 6.37875\n",
      "Trained batch 2391 batch loss 6.94394398 epoch total loss 6.37898636\n",
      "Trained batch 2392 batch loss 7.05157328 epoch total loss 6.37926769\n",
      "Trained batch 2393 batch loss 6.664011 epoch total loss 6.37938643\n",
      "Trained batch 2394 batch loss 6.31090641 epoch total loss 6.37935781\n",
      "Trained batch 2395 batch loss 6.72512 epoch total loss 6.3795023\n",
      "Trained batch 2396 batch loss 7.07285833 epoch total loss 6.37979174\n",
      "Trained batch 2397 batch loss 6.0784874 epoch total loss 6.37966585\n",
      "Trained batch 2398 batch loss 6.51788807 epoch total loss 6.37972355\n",
      "Trained batch 2399 batch loss 6.0472393 epoch total loss 6.37958479\n",
      "Trained batch 2400 batch loss 5.96747255 epoch total loss 6.37941313\n",
      "Trained batch 2401 batch loss 6.77229881 epoch total loss 6.37957716\n",
      "Trained batch 2402 batch loss 6.80137968 epoch total loss 6.37975264\n",
      "Trained batch 2403 batch loss 6.61730909 epoch total loss 6.37985134\n",
      "Trained batch 2404 batch loss 5.34915113 epoch total loss 6.37942314\n",
      "Trained batch 2405 batch loss 6.70892572 epoch total loss 6.37956\n",
      "Trained batch 2406 batch loss 6.10338926 epoch total loss 6.37944508\n",
      "Trained batch 2407 batch loss 5.70436192 epoch total loss 6.3791647\n",
      "Trained batch 2408 batch loss 6.04069567 epoch total loss 6.37902403\n",
      "Trained batch 2409 batch loss 5.46460438 epoch total loss 6.37864494\n",
      "Trained batch 2410 batch loss 5.4724884 epoch total loss 6.37826872\n",
      "Trained batch 2411 batch loss 7.21738911 epoch total loss 6.37861681\n",
      "Trained batch 2412 batch loss 6.35048199 epoch total loss 6.37860537\n",
      "Trained batch 2413 batch loss 6.9793396 epoch total loss 6.37885427\n",
      "Trained batch 2414 batch loss 6.89354658 epoch total loss 6.3790679\n",
      "Trained batch 2415 batch loss 6.77292442 epoch total loss 6.3792305\n",
      "Trained batch 2416 batch loss 7.38180733 epoch total loss 6.37964535\n",
      "Trained batch 2417 batch loss 6.42285538 epoch total loss 6.37966347\n",
      "Trained batch 2418 batch loss 6.55188 epoch total loss 6.37973452\n",
      "Trained batch 2419 batch loss 6.10684776 epoch total loss 6.37962151\n",
      "Trained batch 2420 batch loss 6.43988514 epoch total loss 6.3796463\n",
      "Trained batch 2421 batch loss 5.60269785 epoch total loss 6.37932539\n",
      "Trained batch 2422 batch loss 6.14346457 epoch total loss 6.37922812\n",
      "Trained batch 2423 batch loss 6.0254 epoch total loss 6.3790822\n",
      "Trained batch 2424 batch loss 6.67489815 epoch total loss 6.3792038\n",
      "Trained batch 2425 batch loss 6.17636776 epoch total loss 6.37912035\n",
      "Trained batch 2426 batch loss 6.29861546 epoch total loss 6.37908745\n",
      "Trained batch 2427 batch loss 5.33304358 epoch total loss 6.37865639\n",
      "Trained batch 2428 batch loss 5.78145885 epoch total loss 6.37841034\n",
      "Trained batch 2429 batch loss 5.29924583 epoch total loss 6.37796593\n",
      "Trained batch 2430 batch loss 6.44771719 epoch total loss 6.37799454\n",
      "Trained batch 2431 batch loss 6.26182222 epoch total loss 6.37794638\n",
      "Trained batch 2432 batch loss 6.14161 epoch total loss 6.37784958\n",
      "Trained batch 2433 batch loss 5.81978846 epoch total loss 6.37761974\n",
      "Trained batch 2434 batch loss 5.84169 epoch total loss 6.37739944\n",
      "Trained batch 2435 batch loss 5.8774128 epoch total loss 6.37719393\n",
      "Trained batch 2436 batch loss 6.54208469 epoch total loss 6.37726164\n",
      "Trained batch 2437 batch loss 5.86266422 epoch total loss 6.3770504\n",
      "Trained batch 2438 batch loss 7.19814777 epoch total loss 6.37738752\n",
      "Trained batch 2439 batch loss 6.94503117 epoch total loss 6.37762\n",
      "Trained batch 2440 batch loss 5.40069294 epoch total loss 6.37721968\n",
      "Trained batch 2441 batch loss 6.6544075 epoch total loss 6.37733316\n",
      "Trained batch 2442 batch loss 6.90313196 epoch total loss 6.37754869\n",
      "Trained batch 2443 batch loss 6.63545 epoch total loss 6.37765408\n",
      "Trained batch 2444 batch loss 6.80118608 epoch total loss 6.37782717\n",
      "Trained batch 2445 batch loss 6.71090746 epoch total loss 6.37796354\n",
      "Trained batch 2446 batch loss 4.7784214 epoch total loss 6.37731\n",
      "Trained batch 2447 batch loss 6.78692818 epoch total loss 6.37747717\n",
      "Trained batch 2448 batch loss 6.62999535 epoch total loss 6.37758\n",
      "Trained batch 2449 batch loss 6.22066164 epoch total loss 6.37751627\n",
      "Trained batch 2450 batch loss 6.47418642 epoch total loss 6.37755585\n",
      "Trained batch 2451 batch loss 6.72773838 epoch total loss 6.37769842\n",
      "Trained batch 2452 batch loss 6.7120676 epoch total loss 6.3778348\n",
      "Trained batch 2453 batch loss 6.27356577 epoch total loss 6.37779236\n",
      "Trained batch 2454 batch loss 6.53152847 epoch total loss 6.37785482\n",
      "Trained batch 2455 batch loss 6.47467518 epoch total loss 6.3778944\n",
      "Trained batch 2456 batch loss 7.01133442 epoch total loss 6.37815237\n",
      "Trained batch 2457 batch loss 7.15636635 epoch total loss 6.37846899\n",
      "Trained batch 2458 batch loss 7.67456579 epoch total loss 6.37899637\n",
      "Trained batch 2459 batch loss 7.21363449 epoch total loss 6.37933588\n",
      "Trained batch 2460 batch loss 7.17832 epoch total loss 6.37966108\n",
      "Trained batch 2461 batch loss 7.78502321 epoch total loss 6.38023186\n",
      "Trained batch 2462 batch loss 7.58522224 epoch total loss 6.38072157\n",
      "Trained batch 2463 batch loss 7.24059391 epoch total loss 6.38107\n",
      "Trained batch 2464 batch loss 7.05471277 epoch total loss 6.38134384\n",
      "Trained batch 2465 batch loss 7.39791393 epoch total loss 6.38175583\n",
      "Trained batch 2466 batch loss 7.05814 epoch total loss 6.38203049\n",
      "Trained batch 2467 batch loss 7.15966845 epoch total loss 6.38234568\n",
      "Trained batch 2468 batch loss 6.89240646 epoch total loss 6.38255262\n",
      "Trained batch 2469 batch loss 7.47056246 epoch total loss 6.38299322\n",
      "Trained batch 2470 batch loss 6.87158537 epoch total loss 6.38319111\n",
      "Trained batch 2471 batch loss 6.69294739 epoch total loss 6.38331699\n",
      "Trained batch 2472 batch loss 7.01647043 epoch total loss 6.38357306\n",
      "Trained batch 2473 batch loss 6.783288 epoch total loss 6.3837347\n",
      "Trained batch 2474 batch loss 6.87985897 epoch total loss 6.38393497\n",
      "Trained batch 2475 batch loss 6.81623745 epoch total loss 6.38411\n",
      "Trained batch 2476 batch loss 6.59314728 epoch total loss 6.3841939\n",
      "Trained batch 2477 batch loss 6.8500042 epoch total loss 6.38438225\n",
      "Trained batch 2478 batch loss 6.97085476 epoch total loss 6.38461876\n",
      "Trained batch 2479 batch loss 6.81166029 epoch total loss 6.3847909\n",
      "Trained batch 2480 batch loss 7.18428564 epoch total loss 6.38511324\n",
      "Trained batch 2481 batch loss 6.95987225 epoch total loss 6.38534498\n",
      "Trained batch 2482 batch loss 5.70126343 epoch total loss 6.38506937\n",
      "Trained batch 2483 batch loss 6.26022243 epoch total loss 6.38501883\n",
      "Trained batch 2484 batch loss 6.89943695 epoch total loss 6.38522577\n",
      "Trained batch 2485 batch loss 5.78955555 epoch total loss 6.3849864\n",
      "Trained batch 2486 batch loss 6.08965588 epoch total loss 6.38486767\n",
      "Trained batch 2487 batch loss 6.23972 epoch total loss 6.38480902\n",
      "Trained batch 2488 batch loss 6.69665289 epoch total loss 6.38493443\n",
      "Trained batch 2489 batch loss 6.69393826 epoch total loss 6.38505888\n",
      "Trained batch 2490 batch loss 6.77447 epoch total loss 6.38521528\n",
      "Trained batch 2491 batch loss 6.47383 epoch total loss 6.38525057\n",
      "Trained batch 2492 batch loss 6.96202278 epoch total loss 6.38548183\n",
      "Trained batch 2493 batch loss 6.80148506 epoch total loss 6.38564873\n",
      "Trained batch 2494 batch loss 7.14714479 epoch total loss 6.38595438\n",
      "Trained batch 2495 batch loss 6.9396944 epoch total loss 6.38617611\n",
      "Trained batch 2496 batch loss 6.9899683 epoch total loss 6.38641834\n",
      "Trained batch 2497 batch loss 6.46778584 epoch total loss 6.38645077\n",
      "Trained batch 2498 batch loss 6.66967154 epoch total loss 6.38656425\n",
      "Trained batch 2499 batch loss 5.59773636 epoch total loss 6.38624859\n",
      "Trained batch 2500 batch loss 6.55271339 epoch total loss 6.38631535\n",
      "Trained batch 2501 batch loss 5.0000968 epoch total loss 6.38576078\n",
      "Trained batch 2502 batch loss 5.42358398 epoch total loss 6.38537645\n",
      "Trained batch 2503 batch loss 5.67573547 epoch total loss 6.38509274\n",
      "Trained batch 2504 batch loss 4.28763771 epoch total loss 6.38425541\n",
      "Trained batch 2505 batch loss 5.1969471 epoch total loss 6.38378143\n",
      "Trained batch 2506 batch loss 5.28145027 epoch total loss 6.38334179\n",
      "Trained batch 2507 batch loss 5.20371103 epoch total loss 6.38287115\n",
      "Trained batch 2508 batch loss 4.51383114 epoch total loss 6.38212585\n",
      "Trained batch 2509 batch loss 6.56516647 epoch total loss 6.38219929\n",
      "Trained batch 2510 batch loss 5.86658669 epoch total loss 6.38199329\n",
      "Trained batch 2511 batch loss 6.78326321 epoch total loss 6.38215351\n",
      "Trained batch 2512 batch loss 6.82817841 epoch total loss 6.38233089\n",
      "Trained batch 2513 batch loss 6.50105667 epoch total loss 6.3823781\n",
      "Trained batch 2514 batch loss 6.47038507 epoch total loss 6.38241339\n",
      "Trained batch 2515 batch loss 6.51898479 epoch total loss 6.38246727\n",
      "Trained batch 2516 batch loss 6.88519478 epoch total loss 6.38266706\n",
      "Trained batch 2517 batch loss 6.65273809 epoch total loss 6.38277388\n",
      "Trained batch 2518 batch loss 6.5729394 epoch total loss 6.38284969\n",
      "Trained batch 2519 batch loss 6.68782616 epoch total loss 6.38297081\n",
      "Trained batch 2520 batch loss 6.31250477 epoch total loss 6.38294268\n",
      "Trained batch 2521 batch loss 6.45738602 epoch total loss 6.38297224\n",
      "Trained batch 2522 batch loss 6.06572914 epoch total loss 6.38284636\n",
      "Trained batch 2523 batch loss 7.13206196 epoch total loss 6.38314295\n",
      "Trained batch 2524 batch loss 6.76464748 epoch total loss 6.38329411\n",
      "Trained batch 2525 batch loss 7.00158548 epoch total loss 6.3835392\n",
      "Trained batch 2526 batch loss 7.58965874 epoch total loss 6.38401699\n",
      "Trained batch 2527 batch loss 6.68381405 epoch total loss 6.38413525\n",
      "Trained batch 2528 batch loss 7.01399374 epoch total loss 6.38438416\n",
      "Trained batch 2529 batch loss 7.22335815 epoch total loss 6.38471603\n",
      "Trained batch 2530 batch loss 7.01621532 epoch total loss 6.3849659\n",
      "Trained batch 2531 batch loss 7.55816412 epoch total loss 6.38543\n",
      "Trained batch 2532 batch loss 5.47663641 epoch total loss 6.3850708\n",
      "Trained batch 2533 batch loss 5.76723099 epoch total loss 6.38482714\n",
      "Trained batch 2534 batch loss 6.20586872 epoch total loss 6.38475657\n",
      "Trained batch 2535 batch loss 6.58619881 epoch total loss 6.38483572\n",
      "Trained batch 2536 batch loss 6.2904191 epoch total loss 6.38479853\n",
      "Trained batch 2537 batch loss 5.81826067 epoch total loss 6.38457489\n",
      "Trained batch 2538 batch loss 5.77692556 epoch total loss 6.38433599\n",
      "Trained batch 2539 batch loss 5.72708702 epoch total loss 6.38407707\n",
      "Trained batch 2540 batch loss 6.22221708 epoch total loss 6.38401365\n",
      "Trained batch 2541 batch loss 6.38523865 epoch total loss 6.38401365\n",
      "Trained batch 2542 batch loss 6.12422466 epoch total loss 6.38391161\n",
      "Trained batch 2543 batch loss 6.82290649 epoch total loss 6.38408422\n",
      "Trained batch 2544 batch loss 6.03173447 epoch total loss 6.38394547\n",
      "Trained batch 2545 batch loss 6.05597925 epoch total loss 6.38381672\n",
      "Trained batch 2546 batch loss 6.04011536 epoch total loss 6.38368177\n",
      "Trained batch 2547 batch loss 6.05641174 epoch total loss 6.38355303\n",
      "Trained batch 2548 batch loss 4.28994131 epoch total loss 6.38273144\n",
      "Trained batch 2549 batch loss 6.51988506 epoch total loss 6.38278532\n",
      "Trained batch 2550 batch loss 6.22853661 epoch total loss 6.38272476\n",
      "Trained batch 2551 batch loss 6.34686 epoch total loss 6.38271046\n",
      "Trained batch 2552 batch loss 6.40040159 epoch total loss 6.38271761\n",
      "Trained batch 2553 batch loss 7.04888153 epoch total loss 6.38297844\n",
      "Trained batch 2554 batch loss 5.74966526 epoch total loss 6.38273048\n",
      "Trained batch 2555 batch loss 6.20960093 epoch total loss 6.38266325\n",
      "Trained batch 2556 batch loss 6.46866417 epoch total loss 6.38269663\n",
      "Trained batch 2557 batch loss 6.65403271 epoch total loss 6.38280296\n",
      "Trained batch 2558 batch loss 5.93167114 epoch total loss 6.38262653\n",
      "Trained batch 2559 batch loss 6.4956851 epoch total loss 6.38267088\n",
      "Trained batch 2560 batch loss 6.14629459 epoch total loss 6.38257885\n",
      "Trained batch 2561 batch loss 6.37722397 epoch total loss 6.38257647\n",
      "Trained batch 2562 batch loss 6.584692 epoch total loss 6.38265562\n",
      "Trained batch 2563 batch loss 6.46844196 epoch total loss 6.382689\n",
      "Trained batch 2564 batch loss 6.18726826 epoch total loss 6.38261271\n",
      "Trained batch 2565 batch loss 4.40453339 epoch total loss 6.38184166\n",
      "Trained batch 2566 batch loss 6.36167717 epoch total loss 6.38183355\n",
      "Trained batch 2567 batch loss 6.5746069 epoch total loss 6.38190842\n",
      "Trained batch 2568 batch loss 6.39615202 epoch total loss 6.38191414\n",
      "Trained batch 2569 batch loss 5.11351538 epoch total loss 6.38142061\n",
      "Trained batch 2570 batch loss 6.8196497 epoch total loss 6.38159132\n",
      "Trained batch 2571 batch loss 6.2175951 epoch total loss 6.38152695\n",
      "Trained batch 2572 batch loss 5.34123135 epoch total loss 6.38112307\n",
      "Trained batch 2573 batch loss 5.34047461 epoch total loss 6.38071823\n",
      "Trained batch 2574 batch loss 6.27798891 epoch total loss 6.38067818\n",
      "Trained batch 2575 batch loss 4.90650463 epoch total loss 6.3801055\n",
      "Trained batch 2576 batch loss 5.95280266 epoch total loss 6.37993956\n",
      "Trained batch 2577 batch loss 6.74265051 epoch total loss 6.38008\n",
      "Trained batch 2578 batch loss 6.51303625 epoch total loss 6.3801322\n",
      "Trained batch 2579 batch loss 6.18731403 epoch total loss 6.38005733\n",
      "Trained batch 2580 batch loss 6.16084242 epoch total loss 6.37997198\n",
      "Trained batch 2581 batch loss 6.27838 epoch total loss 6.37993336\n",
      "Trained batch 2582 batch loss 5.8357439 epoch total loss 6.3797226\n",
      "Trained batch 2583 batch loss 6.25605679 epoch total loss 6.37967443\n",
      "Trained batch 2584 batch loss 6.23613644 epoch total loss 6.37961912\n",
      "Trained batch 2585 batch loss 5.50417519 epoch total loss 6.37928\n",
      "Trained batch 2586 batch loss 5.5793314 epoch total loss 6.3789711\n",
      "Trained batch 2587 batch loss 5.58894539 epoch total loss 6.3786664\n",
      "Trained batch 2588 batch loss 6.36876583 epoch total loss 6.37866259\n",
      "Trained batch 2589 batch loss 5.73389721 epoch total loss 6.37841368\n",
      "Trained batch 2590 batch loss 6.34746599 epoch total loss 6.37840176\n",
      "Trained batch 2591 batch loss 6.4699235 epoch total loss 6.37843752\n",
      "Trained batch 2592 batch loss 5.99708796 epoch total loss 6.37829065\n",
      "Trained batch 2593 batch loss 6.24189234 epoch total loss 6.3782382\n",
      "Trained batch 2594 batch loss 6.29385948 epoch total loss 6.3782053\n",
      "Trained batch 2595 batch loss 6.62467861 epoch total loss 6.3783\n",
      "Trained batch 2596 batch loss 6.40628529 epoch total loss 6.37831116\n",
      "Trained batch 2597 batch loss 6.2224369 epoch total loss 6.37825108\n",
      "Trained batch 2598 batch loss 6.58511925 epoch total loss 6.37833118\n",
      "Trained batch 2599 batch loss 6.68066502 epoch total loss 6.37844801\n",
      "Trained batch 2600 batch loss 6.33444118 epoch total loss 6.37843084\n",
      "Trained batch 2601 batch loss 6.53617764 epoch total loss 6.37849188\n",
      "Trained batch 2602 batch loss 6.25142288 epoch total loss 6.37844324\n",
      "Trained batch 2603 batch loss 5.4585 epoch total loss 6.37809\n",
      "Trained batch 2604 batch loss 5.6677103 epoch total loss 6.37781715\n",
      "Trained batch 2605 batch loss 6.62838745 epoch total loss 6.37791348\n",
      "Trained batch 2606 batch loss 5.5394845 epoch total loss 6.37759161\n",
      "Trained batch 2607 batch loss 6.79749393 epoch total loss 6.3777523\n",
      "Trained batch 2608 batch loss 6.71581364 epoch total loss 6.37788153\n",
      "Trained batch 2609 batch loss 6.41009951 epoch total loss 6.37789392\n",
      "Trained batch 2610 batch loss 5.55294895 epoch total loss 6.37757778\n",
      "Trained batch 2611 batch loss 6.82160568 epoch total loss 6.37774849\n",
      "Trained batch 2612 batch loss 7.01053476 epoch total loss 6.37799025\n",
      "Trained batch 2613 batch loss 6.59768 epoch total loss 6.37807417\n",
      "Trained batch 2614 batch loss 7.14671421 epoch total loss 6.37836838\n",
      "Trained batch 2615 batch loss 6.34129047 epoch total loss 6.37835407\n",
      "Trained batch 2616 batch loss 7.10862637 epoch total loss 6.37863398\n",
      "Trained batch 2617 batch loss 5.42280912 epoch total loss 6.37826824\n",
      "Trained batch 2618 batch loss 5.00650311 epoch total loss 6.37774372\n",
      "Trained batch 2619 batch loss 5.0989604 epoch total loss 6.37725592\n",
      "Trained batch 2620 batch loss 4.78033304 epoch total loss 6.37664652\n",
      "Trained batch 2621 batch loss 4.97382355 epoch total loss 6.37611198\n",
      "Trained batch 2622 batch loss 4.89339 epoch total loss 6.37554598\n",
      "Trained batch 2623 batch loss 5.81637192 epoch total loss 6.37533283\n",
      "Trained batch 2624 batch loss 4.89173126 epoch total loss 6.37476778\n",
      "Trained batch 2625 batch loss 4.94341183 epoch total loss 6.37422228\n",
      "Trained batch 2626 batch loss 4.96531534 epoch total loss 6.37368584\n",
      "Trained batch 2627 batch loss 4.93032265 epoch total loss 6.37313604\n",
      "Trained batch 2628 batch loss 4.71857166 epoch total loss 6.37250662\n",
      "Trained batch 2629 batch loss 5.34073544 epoch total loss 6.3721137\n",
      "Trained batch 2630 batch loss 4.96556759 epoch total loss 6.37157869\n",
      "Trained batch 2631 batch loss 4.77962446 epoch total loss 6.37097359\n",
      "Trained batch 2632 batch loss 5.22910118 epoch total loss 6.37053919\n",
      "Trained batch 2633 batch loss 5.76724243 epoch total loss 6.37031031\n",
      "Trained batch 2634 batch loss 5.13556 epoch total loss 6.36984158\n",
      "Trained batch 2635 batch loss 5.27172375 epoch total loss 6.36942434\n",
      "Trained batch 2636 batch loss 5.24090672 epoch total loss 6.36899614\n",
      "Trained batch 2637 batch loss 5.17432117 epoch total loss 6.36854315\n",
      "Trained batch 2638 batch loss 4.62758 epoch total loss 6.36788273\n",
      "Trained batch 2639 batch loss 4.88624191 epoch total loss 6.36732149\n",
      "Trained batch 2640 batch loss 4.81693649 epoch total loss 6.36673403\n",
      "Trained batch 2641 batch loss 4.92972088 epoch total loss 6.36619\n",
      "Trained batch 2642 batch loss 4.60474396 epoch total loss 6.36552334\n",
      "Trained batch 2643 batch loss 5.0935154 epoch total loss 6.36504221\n",
      "Trained batch 2644 batch loss 5.69438553 epoch total loss 6.36478901\n",
      "Trained batch 2645 batch loss 5.09819126 epoch total loss 6.36431\n",
      "Trained batch 2646 batch loss 4.68375874 epoch total loss 6.36367464\n",
      "Trained batch 2647 batch loss 6.55810738 epoch total loss 6.36374855\n",
      "Trained batch 2648 batch loss 6.95535851 epoch total loss 6.36397171\n",
      "Trained batch 2649 batch loss 7.13707352 epoch total loss 6.36426353\n",
      "Trained batch 2650 batch loss 7.71157885 epoch total loss 6.36477137\n",
      "Trained batch 2651 batch loss 6.26511431 epoch total loss 6.36473417\n",
      "Trained batch 2652 batch loss 6.87105083 epoch total loss 6.36492491\n",
      "Trained batch 2653 batch loss 6.98162079 epoch total loss 6.3651576\n",
      "Trained batch 2654 batch loss 5.11201715 epoch total loss 6.36468554\n",
      "Trained batch 2655 batch loss 6.03268909 epoch total loss 6.3645606\n",
      "Trained batch 2656 batch loss 6.1229372 epoch total loss 6.36446953\n",
      "Trained batch 2657 batch loss 7.140975 epoch total loss 6.36476183\n",
      "Trained batch 2658 batch loss 5.6180191 epoch total loss 6.3644805\n",
      "Trained batch 2659 batch loss 6.05163431 epoch total loss 6.36436272\n",
      "Trained batch 2660 batch loss 6.31108856 epoch total loss 6.36434221\n",
      "Trained batch 2661 batch loss 6.36531782 epoch total loss 6.36434269\n",
      "Trained batch 2662 batch loss 6.22075 epoch total loss 6.36428881\n",
      "Trained batch 2663 batch loss 6.59885359 epoch total loss 6.36437702\n",
      "Trained batch 2664 batch loss 7.97007656 epoch total loss 6.36497974\n",
      "Trained batch 2665 batch loss 6.80142498 epoch total loss 6.3651433\n",
      "Trained batch 2666 batch loss 6.81983614 epoch total loss 6.36531401\n",
      "Trained batch 2667 batch loss 7.70434475 epoch total loss 6.36581659\n",
      "Trained batch 2668 batch loss 6.53802681 epoch total loss 6.36588097\n",
      "Trained batch 2669 batch loss 6.7782383 epoch total loss 6.36603498\n",
      "Trained batch 2670 batch loss 7.45186853 epoch total loss 6.36644125\n",
      "Trained batch 2671 batch loss 7.08652592 epoch total loss 6.36671066\n",
      "Trained batch 2672 batch loss 7.03896523 epoch total loss 6.36696243\n",
      "Trained batch 2673 batch loss 7.27743292 epoch total loss 6.36730289\n",
      "Trained batch 2674 batch loss 7.00947475 epoch total loss 6.36754322\n",
      "Trained batch 2675 batch loss 7.45577431 epoch total loss 6.36795\n",
      "Trained batch 2676 batch loss 7.48028946 epoch total loss 6.36836529\n",
      "Trained batch 2677 batch loss 6.44240284 epoch total loss 6.36839342\n",
      "Trained batch 2678 batch loss 6.35049915 epoch total loss 6.36838627\n",
      "Trained batch 2679 batch loss 6.59979105 epoch total loss 6.36847305\n",
      "Trained batch 2680 batch loss 5.88176346 epoch total loss 6.3682909\n",
      "Trained batch 2681 batch loss 5.72932053 epoch total loss 6.36805201\n",
      "Trained batch 2682 batch loss 6.27820826 epoch total loss 6.36801863\n",
      "Trained batch 2683 batch loss 6.28165245 epoch total loss 6.3679862\n",
      "Trained batch 2684 batch loss 6.5182538 epoch total loss 6.36804199\n",
      "Trained batch 2685 batch loss 6.52680349 epoch total loss 6.36810112\n",
      "Trained batch 2686 batch loss 6.86444759 epoch total loss 6.36828613\n",
      "Trained batch 2687 batch loss 7.20195293 epoch total loss 6.36859608\n",
      "Trained batch 2688 batch loss 7.1703577 epoch total loss 6.3688941\n",
      "Trained batch 2689 batch loss 6.2625103 epoch total loss 6.36885452\n",
      "Trained batch 2690 batch loss 7.38169527 epoch total loss 6.36923075\n",
      "Trained batch 2691 batch loss 6.10664272 epoch total loss 6.36913347\n",
      "Trained batch 2692 batch loss 6.33584166 epoch total loss 6.36912107\n",
      "Trained batch 2693 batch loss 5.61717033 epoch total loss 6.36884165\n",
      "Trained batch 2694 batch loss 6.2868 epoch total loss 6.36881161\n",
      "Trained batch 2695 batch loss 5.88094425 epoch total loss 6.36863041\n",
      "Trained batch 2696 batch loss 7.70088196 epoch total loss 6.36912489\n",
      "Trained batch 2697 batch loss 6.83521795 epoch total loss 6.36929798\n",
      "Trained batch 2698 batch loss 7.6810956 epoch total loss 6.36978436\n",
      "Trained batch 2699 batch loss 6.75326157 epoch total loss 6.36992645\n",
      "Trained batch 2700 batch loss 6.94963741 epoch total loss 6.37014103\n",
      "Trained batch 2701 batch loss 7.08099747 epoch total loss 6.37040377\n",
      "Trained batch 2702 batch loss 5.91909409 epoch total loss 6.37023735\n",
      "Trained batch 2703 batch loss 6.7831459 epoch total loss 6.37039\n",
      "Trained batch 2704 batch loss 6.69620514 epoch total loss 6.37051\n",
      "Trained batch 2705 batch loss 6.87829542 epoch total loss 6.37069798\n",
      "Trained batch 2706 batch loss 6.51108599 epoch total loss 6.37075\n",
      "Trained batch 2707 batch loss 6.05379677 epoch total loss 6.3706336\n",
      "Trained batch 2708 batch loss 6.2345891 epoch total loss 6.37058306\n",
      "Trained batch 2709 batch loss 6.58830261 epoch total loss 6.37066317\n",
      "Trained batch 2710 batch loss 6.79470539 epoch total loss 6.37082\n",
      "Trained batch 2711 batch loss 6.40834713 epoch total loss 6.37083387\n",
      "Trained batch 2712 batch loss 5.93090963 epoch total loss 6.37067175\n",
      "Trained batch 2713 batch loss 6.50652885 epoch total loss 6.37072134\n",
      "Trained batch 2714 batch loss 6.6782074 epoch total loss 6.37083483\n",
      "Trained batch 2715 batch loss 6.98197365 epoch total loss 6.37106\n",
      "Trained batch 2716 batch loss 7.20091915 epoch total loss 6.37136555\n",
      "Trained batch 2717 batch loss 6.54479742 epoch total loss 6.37142944\n",
      "Trained batch 2718 batch loss 6.68679523 epoch total loss 6.37154579\n",
      "Trained batch 2719 batch loss 6.81360149 epoch total loss 6.37170887\n",
      "Trained batch 2720 batch loss 6.57096767 epoch total loss 6.37178183\n",
      "Trained batch 2721 batch loss 6.35803175 epoch total loss 6.37177658\n",
      "Trained batch 2722 batch loss 6.01876163 epoch total loss 6.37164688\n",
      "Trained batch 2723 batch loss 6.22829914 epoch total loss 6.37159443\n",
      "Trained batch 2724 batch loss 5.88702393 epoch total loss 6.37141657\n",
      "Trained batch 2725 batch loss 6.38701057 epoch total loss 6.37142181\n",
      "Trained batch 2726 batch loss 6.46387482 epoch total loss 6.37145615\n",
      "Trained batch 2727 batch loss 6.36268282 epoch total loss 6.37145329\n",
      "Trained batch 2728 batch loss 6.36259508 epoch total loss 6.37145042\n",
      "Trained batch 2729 batch loss 6.14919949 epoch total loss 6.37136841\n",
      "Trained batch 2730 batch loss 6.58447742 epoch total loss 6.37144661\n",
      "Trained batch 2731 batch loss 6.9635 epoch total loss 6.37166309\n",
      "Trained batch 2732 batch loss 6.82280731 epoch total loss 6.37182808\n",
      "Trained batch 2733 batch loss 6.99746227 epoch total loss 6.37205696\n",
      "Trained batch 2734 batch loss 6.52529526 epoch total loss 6.37211323\n",
      "Trained batch 2735 batch loss 6.86912155 epoch total loss 6.3722949\n",
      "Trained batch 2736 batch loss 7.00796461 epoch total loss 6.37252712\n",
      "Trained batch 2737 batch loss 7.31385946 epoch total loss 6.3728714\n",
      "Trained batch 2738 batch loss 6.80167484 epoch total loss 6.3730278\n",
      "Trained batch 2739 batch loss 6.76672363 epoch total loss 6.37317181\n",
      "Trained batch 2740 batch loss 6.85804129 epoch total loss 6.37334824\n",
      "Trained batch 2741 batch loss 6.42639828 epoch total loss 6.37336731\n",
      "Trained batch 2742 batch loss 6.66541862 epoch total loss 6.37347412\n",
      "Trained batch 2743 batch loss 6.36497402 epoch total loss 6.37347126\n",
      "Trained batch 2744 batch loss 6.23631096 epoch total loss 6.37342119\n",
      "Trained batch 2745 batch loss 6.00246096 epoch total loss 6.37328577\n",
      "Trained batch 2746 batch loss 5.69464779 epoch total loss 6.37303925\n",
      "Trained batch 2747 batch loss 5.57139587 epoch total loss 6.37274742\n",
      "Trained batch 2748 batch loss 6.36035442 epoch total loss 6.37274361\n",
      "Trained batch 2749 batch loss 6.25595093 epoch total loss 6.37270069\n",
      "Trained batch 2750 batch loss 6.79031277 epoch total loss 6.3728528\n",
      "Trained batch 2751 batch loss 6.91917372 epoch total loss 6.37305164\n",
      "Trained batch 2752 batch loss 6.57538462 epoch total loss 6.37312555\n",
      "Trained batch 2753 batch loss 6.09169722 epoch total loss 6.37302351\n",
      "Trained batch 2754 batch loss 5.5777483 epoch total loss 6.37273502\n",
      "Trained batch 2755 batch loss 6.31824 epoch total loss 6.372715\n",
      "Trained batch 2756 batch loss 6.56057596 epoch total loss 6.37278318\n",
      "Trained batch 2757 batch loss 6.19684696 epoch total loss 6.37272\n",
      "Trained batch 2758 batch loss 6.91162 epoch total loss 6.37291527\n",
      "Trained batch 2759 batch loss 6.50489569 epoch total loss 6.37296343\n",
      "Trained batch 2760 batch loss 5.97543812 epoch total loss 6.37281895\n",
      "Trained batch 2761 batch loss 6.38741255 epoch total loss 6.37282419\n",
      "Trained batch 2762 batch loss 5.95704317 epoch total loss 6.37267351\n",
      "Trained batch 2763 batch loss 5.60170364 epoch total loss 6.37239456\n",
      "Trained batch 2764 batch loss 5.33762169 epoch total loss 6.37202024\n",
      "Trained batch 2765 batch loss 5.27794313 epoch total loss 6.37162447\n",
      "Trained batch 2766 batch loss 5.33669186 epoch total loss 6.37124968\n",
      "Trained batch 2767 batch loss 5.21120071 epoch total loss 6.37083054\n",
      "Trained batch 2768 batch loss 5.64841032 epoch total loss 6.37056971\n",
      "Trained batch 2769 batch loss 5.03429508 epoch total loss 6.37008715\n",
      "Trained batch 2770 batch loss 5.30980825 epoch total loss 6.36970472\n",
      "Trained batch 2771 batch loss 6.05055332 epoch total loss 6.36959\n",
      "Trained batch 2772 batch loss 5.96675491 epoch total loss 6.36944437\n",
      "Trained batch 2773 batch loss 6.47865772 epoch total loss 6.36948347\n",
      "Trained batch 2774 batch loss 5.74570465 epoch total loss 6.36925888\n",
      "Trained batch 2775 batch loss 5.55842209 epoch total loss 6.36896658\n",
      "Trained batch 2776 batch loss 6.30605602 epoch total loss 6.36894417\n",
      "Trained batch 2777 batch loss 6.15039444 epoch total loss 6.36886549\n",
      "Trained batch 2778 batch loss 6.9119091 epoch total loss 6.36906099\n",
      "Trained batch 2779 batch loss 6.30131912 epoch total loss 6.36903667\n",
      "Trained batch 2780 batch loss 6.20434284 epoch total loss 6.36897755\n",
      "Trained batch 2781 batch loss 6.37760925 epoch total loss 6.36898041\n",
      "Trained batch 2782 batch loss 6.29640293 epoch total loss 6.36895466\n",
      "Trained batch 2783 batch loss 5.62157917 epoch total loss 6.36868572\n",
      "Trained batch 2784 batch loss 5.81362915 epoch total loss 6.36848688\n",
      "Trained batch 2785 batch loss 6.06103897 epoch total loss 6.36837626\n",
      "Trained batch 2786 batch loss 6.51382923 epoch total loss 6.36842823\n",
      "Trained batch 2787 batch loss 5.54711533 epoch total loss 6.36813354\n",
      "Trained batch 2788 batch loss 6.31303406 epoch total loss 6.36811352\n",
      "Trained batch 2789 batch loss 5.91132927 epoch total loss 6.36795\n",
      "Trained batch 2790 batch loss 5.56618404 epoch total loss 6.36766291\n",
      "Trained batch 2791 batch loss 6.05334091 epoch total loss 6.36755\n",
      "Trained batch 2792 batch loss 6.90166187 epoch total loss 6.36774158\n",
      "Trained batch 2793 batch loss 6.46671963 epoch total loss 6.36777687\n",
      "Trained batch 2794 batch loss 5.93815899 epoch total loss 6.36762285\n",
      "Trained batch 2795 batch loss 6.12109756 epoch total loss 6.36753464\n",
      "Trained batch 2796 batch loss 5.96579218 epoch total loss 6.36739063\n",
      "Trained batch 2797 batch loss 6.58482122 epoch total loss 6.36746836\n",
      "Trained batch 2798 batch loss 6.94640732 epoch total loss 6.3676753\n",
      "Trained batch 2799 batch loss 5.52160835 epoch total loss 6.36737299\n",
      "Trained batch 2800 batch loss 5.68777895 epoch total loss 6.36713028\n",
      "Trained batch 2801 batch loss 5.76100254 epoch total loss 6.36691427\n",
      "Trained batch 2802 batch loss 5.76773882 epoch total loss 6.3667\n",
      "Trained batch 2803 batch loss 6.54937363 epoch total loss 6.36676502\n",
      "Trained batch 2804 batch loss 6.46999121 epoch total loss 6.36680222\n",
      "Trained batch 2805 batch loss 6.33561325 epoch total loss 6.36679125\n",
      "Trained batch 2806 batch loss 6.5129509 epoch total loss 6.3668437\n",
      "Trained batch 2807 batch loss 6.43890238 epoch total loss 6.36686945\n",
      "Trained batch 2808 batch loss 6.71332598 epoch total loss 6.36699295\n",
      "Trained batch 2809 batch loss 6.97527695 epoch total loss 6.36720896\n",
      "Trained batch 2810 batch loss 6.89687347 epoch total loss 6.36739731\n",
      "Trained batch 2811 batch loss 7.01180935 epoch total loss 6.36762667\n",
      "Trained batch 2812 batch loss 6.92345619 epoch total loss 6.36782455\n",
      "Trained batch 2813 batch loss 6.76563168 epoch total loss 6.3679657\n",
      "Trained batch 2814 batch loss 6.87843895 epoch total loss 6.36814737\n",
      "Trained batch 2815 batch loss 6.36694574 epoch total loss 6.3681469\n",
      "Trained batch 2816 batch loss 6.66036606 epoch total loss 6.36825085\n",
      "Trained batch 2817 batch loss 6.69968796 epoch total loss 6.36836815\n",
      "Trained batch 2818 batch loss 6.88300276 epoch total loss 6.36855078\n",
      "Trained batch 2819 batch loss 6.76114416 epoch total loss 6.36869049\n",
      "Trained batch 2820 batch loss 7.04230404 epoch total loss 6.36892939\n",
      "Trained batch 2821 batch loss 6.76495934 epoch total loss 6.36907\n",
      "Trained batch 2822 batch loss 6.25882959 epoch total loss 6.36903143\n",
      "Trained batch 2823 batch loss 6.56102848 epoch total loss 6.36909914\n",
      "Trained batch 2824 batch loss 7.16212511 epoch total loss 6.36938\n",
      "Trained batch 2825 batch loss 7.01702929 epoch total loss 6.36960936\n",
      "Trained batch 2826 batch loss 6.57455254 epoch total loss 6.36968184\n",
      "Trained batch 2827 batch loss 6.97733212 epoch total loss 6.36989641\n",
      "Trained batch 2828 batch loss 7.57963943 epoch total loss 6.37032461\n",
      "Trained batch 2829 batch loss 7.25899029 epoch total loss 6.37063885\n",
      "Trained batch 2830 batch loss 7.01630688 epoch total loss 6.37086678\n",
      "Trained batch 2831 batch loss 6.79554081 epoch total loss 6.3710165\n",
      "Trained batch 2832 batch loss 6.6276741 epoch total loss 6.37110662\n",
      "Trained batch 2833 batch loss 7.19714117 epoch total loss 6.37139845\n",
      "Trained batch 2834 batch loss 6.67522144 epoch total loss 6.37150574\n",
      "Trained batch 2835 batch loss 6.66656113 epoch total loss 6.37160969\n",
      "Trained batch 2836 batch loss 6.90266895 epoch total loss 6.37179708\n",
      "Trained batch 2837 batch loss 4.96211 epoch total loss 6.3713\n",
      "Trained batch 2838 batch loss 6.40114307 epoch total loss 6.37131071\n",
      "Trained batch 2839 batch loss 5.65326405 epoch total loss 6.37105751\n",
      "Trained batch 2840 batch loss 5.76505327 epoch total loss 6.37084389\n",
      "Trained batch 2841 batch loss 4.33745527 epoch total loss 6.37012863\n",
      "Trained batch 2842 batch loss 4.8519845 epoch total loss 6.3695941\n",
      "Trained batch 2843 batch loss 5.22290516 epoch total loss 6.36919069\n",
      "Trained batch 2844 batch loss 5.63213634 epoch total loss 6.36893177\n",
      "Trained batch 2845 batch loss 5.63479519 epoch total loss 6.3686738\n",
      "Trained batch 2846 batch loss 5.21583366 epoch total loss 6.36826897\n",
      "Trained batch 2847 batch loss 5.65771341 epoch total loss 6.36801958\n",
      "Trained batch 2848 batch loss 5.63343763 epoch total loss 6.36776161\n",
      "Trained batch 2849 batch loss 6.38970852 epoch total loss 6.36776972\n",
      "Trained batch 2850 batch loss 6.41145515 epoch total loss 6.36778498\n",
      "Trained batch 2851 batch loss 6.73078537 epoch total loss 6.36791229\n",
      "Trained batch 2852 batch loss 6.44313049 epoch total loss 6.36793852\n",
      "Trained batch 2853 batch loss 6.13968658 epoch total loss 6.36785889\n",
      "Trained batch 2854 batch loss 6.6601491 epoch total loss 6.36796141\n",
      "Trained batch 2855 batch loss 6.30016136 epoch total loss 6.36793804\n",
      "Trained batch 2856 batch loss 7.11722279 epoch total loss 6.3682003\n",
      "Trained batch 2857 batch loss 6.97007 epoch total loss 6.36841106\n",
      "Trained batch 2858 batch loss 7.05979061 epoch total loss 6.3686533\n",
      "Trained batch 2859 batch loss 5.93646336 epoch total loss 6.36850166\n",
      "Trained batch 2860 batch loss 5.7242012 epoch total loss 6.3682766\n",
      "Trained batch 2861 batch loss 6.4317975 epoch total loss 6.36829901\n",
      "Trained batch 2862 batch loss 6.63805 epoch total loss 6.36839342\n",
      "Trained batch 2863 batch loss 6.49903345 epoch total loss 6.3684392\n",
      "Trained batch 2864 batch loss 6.29393339 epoch total loss 6.36841297\n",
      "Trained batch 2865 batch loss 6.64676189 epoch total loss 6.36851025\n",
      "Trained batch 2866 batch loss 6.43109894 epoch total loss 6.36853218\n",
      "Trained batch 2867 batch loss 6.32464123 epoch total loss 6.36851645\n",
      "Trained batch 2868 batch loss 4.45437431 epoch total loss 6.36784935\n",
      "Trained batch 2869 batch loss 6.45093966 epoch total loss 6.36787844\n",
      "Trained batch 2870 batch loss 6.46112204 epoch total loss 6.36791086\n",
      "Trained batch 2871 batch loss 6.24958611 epoch total loss 6.36787\n",
      "Trained batch 2872 batch loss 6.34108305 epoch total loss 6.36786079\n",
      "Trained batch 2873 batch loss 6.61198187 epoch total loss 6.36794567\n",
      "Trained batch 2874 batch loss 6.09710407 epoch total loss 6.36785126\n",
      "Trained batch 2875 batch loss 6.53111362 epoch total loss 6.36790848\n",
      "Trained batch 2876 batch loss 6.30195141 epoch total loss 6.36788559\n",
      "Trained batch 2877 batch loss 6.02859449 epoch total loss 6.36776781\n",
      "Trained batch 2878 batch loss 6.24659824 epoch total loss 6.36772585\n",
      "Trained batch 2879 batch loss 6.54486465 epoch total loss 6.36778736\n",
      "Trained batch 2880 batch loss 6.32374287 epoch total loss 6.3677721\n",
      "Trained batch 2881 batch loss 6.05216599 epoch total loss 6.36766291\n",
      "Trained batch 2882 batch loss 6.38945913 epoch total loss 6.36767\n",
      "Trained batch 2883 batch loss 6.60780764 epoch total loss 6.36775303\n",
      "Trained batch 2884 batch loss 6.46061516 epoch total loss 6.36778545\n",
      "Trained batch 2885 batch loss 6.62512 epoch total loss 6.36787462\n",
      "Trained batch 2886 batch loss 6.09640598 epoch total loss 6.36778\n",
      "Trained batch 2887 batch loss 6.33490372 epoch total loss 6.36776876\n",
      "Trained batch 2888 batch loss 6.12847853 epoch total loss 6.36768579\n",
      "Trained batch 2889 batch loss 5.96693373 epoch total loss 6.36754704\n",
      "Trained batch 2890 batch loss 6.09087181 epoch total loss 6.36745167\n",
      "Trained batch 2891 batch loss 6.21199608 epoch total loss 6.36739826\n",
      "Trained batch 2892 batch loss 6.09391928 epoch total loss 6.36730385\n",
      "Trained batch 2893 batch loss 6.11312389 epoch total loss 6.36721563\n",
      "Trained batch 2894 batch loss 5.83629799 epoch total loss 6.36703205\n",
      "Trained batch 2895 batch loss 6.90430927 epoch total loss 6.36721802\n",
      "Trained batch 2896 batch loss 6.23612976 epoch total loss 6.36717272\n",
      "Trained batch 2897 batch loss 5.88238335 epoch total loss 6.36700535\n",
      "Trained batch 2898 batch loss 6.35667896 epoch total loss 6.36700201\n",
      "Trained batch 2899 batch loss 6.31885195 epoch total loss 6.36698532\n",
      "Trained batch 2900 batch loss 6.45131874 epoch total loss 6.36701441\n",
      "Trained batch 2901 batch loss 6.22602 epoch total loss 6.36696577\n",
      "Trained batch 2902 batch loss 5.89081192 epoch total loss 6.36680174\n",
      "Trained batch 2903 batch loss 6.27691412 epoch total loss 6.36677122\n",
      "Trained batch 2904 batch loss 6.14858913 epoch total loss 6.36669588\n",
      "Trained batch 2905 batch loss 5.8667 epoch total loss 6.36652374\n",
      "Trained batch 2906 batch loss 5.92551517 epoch total loss 6.36637211\n",
      "Trained batch 2907 batch loss 6.15630245 epoch total loss 6.3663\n",
      "Trained batch 2908 batch loss 5.97980309 epoch total loss 6.36616707\n",
      "Trained batch 2909 batch loss 6.38386965 epoch total loss 6.36617374\n",
      "Trained batch 2910 batch loss 5.81625843 epoch total loss 6.36598492\n",
      "Trained batch 2911 batch loss 5.67113113 epoch total loss 6.3657465\n",
      "Trained batch 2912 batch loss 5.91513538 epoch total loss 6.365592\n",
      "Trained batch 2913 batch loss 6.40428162 epoch total loss 6.36560535\n",
      "Trained batch 2914 batch loss 5.88299322 epoch total loss 6.36543941\n",
      "Trained batch 2915 batch loss 5.97175217 epoch total loss 6.36530495\n",
      "Trained batch 2916 batch loss 6.641675 epoch total loss 6.3654\n",
      "Trained batch 2917 batch loss 5.88854218 epoch total loss 6.36523628\n",
      "Trained batch 2918 batch loss 5.9508543 epoch total loss 6.36509466\n",
      "Trained batch 2919 batch loss 6.04885 epoch total loss 6.36498594\n",
      "Trained batch 2920 batch loss 5.78315163 epoch total loss 6.3647871\n",
      "Trained batch 2921 batch loss 6.99433565 epoch total loss 6.36500216\n",
      "Trained batch 2922 batch loss 6.42869377 epoch total loss 6.36502361\n",
      "Trained batch 2923 batch loss 5.94305038 epoch total loss 6.36487961\n",
      "Trained batch 2924 batch loss 6.39535379 epoch total loss 6.36488962\n",
      "Trained batch 2925 batch loss 6.0667038 epoch total loss 6.36478758\n",
      "Trained batch 2926 batch loss 6.5307436 epoch total loss 6.36484432\n",
      "Trained batch 2927 batch loss 6.39116669 epoch total loss 6.36485338\n",
      "Trained batch 2928 batch loss 6.31473827 epoch total loss 6.36483622\n",
      "Trained batch 2929 batch loss 6.22527504 epoch total loss 6.36478806\n",
      "Trained batch 2930 batch loss 6.19573402 epoch total loss 6.36473036\n",
      "Trained batch 2931 batch loss 6.13023567 epoch total loss 6.36465073\n",
      "Trained batch 2932 batch loss 6.05261803 epoch total loss 6.36454439\n",
      "Trained batch 2933 batch loss 6.27898264 epoch total loss 6.3645153\n",
      "Trained batch 2934 batch loss 6.14790344 epoch total loss 6.36444139\n",
      "Trained batch 2935 batch loss 5.95074606 epoch total loss 6.36430073\n",
      "Trained batch 2936 batch loss 6.0504446 epoch total loss 6.36419392\n",
      "Trained batch 2937 batch loss 5.84436083 epoch total loss 6.36401653\n",
      "Trained batch 2938 batch loss 5.95451 epoch total loss 6.36387777\n",
      "Trained batch 2939 batch loss 6.1668582 epoch total loss 6.36381\n",
      "Trained batch 2940 batch loss 5.88718605 epoch total loss 6.36364794\n",
      "Trained batch 2941 batch loss 6.13001156 epoch total loss 6.36356878\n",
      "Trained batch 2942 batch loss 5.96109581 epoch total loss 6.36343193\n",
      "Trained batch 2943 batch loss 5.94631195 epoch total loss 6.36329031\n",
      "Trained batch 2944 batch loss 6.42596483 epoch total loss 6.36331177\n",
      "Trained batch 2945 batch loss 6.05853462 epoch total loss 6.36320829\n",
      "Trained batch 2946 batch loss 6.40437889 epoch total loss 6.36322212\n",
      "Trained batch 2947 batch loss 5.74793148 epoch total loss 6.36301327\n",
      "Trained batch 2948 batch loss 5.92249775 epoch total loss 6.36286402\n",
      "Trained batch 2949 batch loss 5.51620579 epoch total loss 6.36257648\n",
      "Trained batch 2950 batch loss 5.81309414 epoch total loss 6.36239\n",
      "Trained batch 2951 batch loss 6.33846903 epoch total loss 6.36238194\n",
      "Trained batch 2952 batch loss 6.14485645 epoch total loss 6.36230803\n",
      "Trained batch 2953 batch loss 6.7543478 epoch total loss 6.36244059\n",
      "Trained batch 2954 batch loss 5.59625769 epoch total loss 6.36218119\n",
      "Trained batch 2955 batch loss 6.28632641 epoch total loss 6.36215544\n",
      "Trained batch 2956 batch loss 6.39718056 epoch total loss 6.36216736\n",
      "Trained batch 2957 batch loss 6.0077219 epoch total loss 6.3620472\n",
      "Trained batch 2958 batch loss 6.66314697 epoch total loss 6.36214972\n",
      "Trained batch 2959 batch loss 6.80259895 epoch total loss 6.36229849\n",
      "Trained batch 2960 batch loss 6.25262165 epoch total loss 6.3622613\n",
      "Trained batch 2961 batch loss 6.11066437 epoch total loss 6.36217642\n",
      "Trained batch 2962 batch loss 6.03535175 epoch total loss 6.36206579\n",
      "Trained batch 2963 batch loss 5.98308468 epoch total loss 6.361938\n",
      "Trained batch 2964 batch loss 6.00809717 epoch total loss 6.36181831\n",
      "Trained batch 2965 batch loss 6.68263578 epoch total loss 6.36192703\n",
      "Trained batch 2966 batch loss 6.58175373 epoch total loss 6.36200094\n",
      "Trained batch 2967 batch loss 6.5115118 epoch total loss 6.36205149\n",
      "Trained batch 2968 batch loss 5.95665789 epoch total loss 6.36191511\n",
      "Trained batch 2969 batch loss 6.1242075 epoch total loss 6.36183548\n",
      "Trained batch 2970 batch loss 5.60813332 epoch total loss 6.36158133\n",
      "Trained batch 2971 batch loss 5.0311079 epoch total loss 6.36113358\n",
      "Trained batch 2972 batch loss 5.2594986 epoch total loss 6.36076307\n",
      "Trained batch 2973 batch loss 5.10702896 epoch total loss 6.36034155\n",
      "Trained batch 2974 batch loss 5.35006142 epoch total loss 6.36000156\n",
      "Trained batch 2975 batch loss 4.98694944 epoch total loss 6.35954\n",
      "Trained batch 2976 batch loss 4.97018385 epoch total loss 6.35907316\n",
      "Trained batch 2977 batch loss 4.77383709 epoch total loss 6.35854053\n",
      "Trained batch 2978 batch loss 5.68926048 epoch total loss 6.35831594\n",
      "Trained batch 2979 batch loss 4.92319775 epoch total loss 6.35783434\n",
      "Trained batch 2980 batch loss 5.07114601 epoch total loss 6.35740232\n",
      "Trained batch 2981 batch loss 5.1393795 epoch total loss 6.3569932\n",
      "Trained batch 2982 batch loss 4.61811733 epoch total loss 6.35641\n",
      "Trained batch 2983 batch loss 4.86130428 epoch total loss 6.35590887\n",
      "Trained batch 2984 batch loss 5.44135761 epoch total loss 6.35560226\n",
      "Trained batch 2985 batch loss 7.12567472 epoch total loss 6.35586\n",
      "Trained batch 2986 batch loss 6.52938032 epoch total loss 6.35591793\n",
      "Trained batch 2987 batch loss 6.5108633 epoch total loss 6.35597038\n",
      "Trained batch 2988 batch loss 4.5969243 epoch total loss 6.35538197\n",
      "Trained batch 2989 batch loss 6.62684441 epoch total loss 6.35547256\n",
      "Trained batch 2990 batch loss 6.59193325 epoch total loss 6.35555172\n",
      "Trained batch 2991 batch loss 7.11515522 epoch total loss 6.35580587\n",
      "Trained batch 2992 batch loss 5.57596 epoch total loss 6.35554504\n",
      "Trained batch 2993 batch loss 6.83565378 epoch total loss 6.35570574\n",
      "Trained batch 2994 batch loss 6.71155167 epoch total loss 6.35582447\n",
      "Trained batch 2995 batch loss 5.77418327 epoch total loss 6.35563\n",
      "Trained batch 2996 batch loss 5.35592461 epoch total loss 6.35529613\n",
      "Trained batch 2997 batch loss 6.67871475 epoch total loss 6.35540438\n",
      "Trained batch 2998 batch loss 5.12013292 epoch total loss 6.35499239\n",
      "Trained batch 2999 batch loss 4.48563623 epoch total loss 6.35436964\n",
      "Trained batch 3000 batch loss 5.66140795 epoch total loss 6.35413885\n",
      "Trained batch 3001 batch loss 6.20463848 epoch total loss 6.35408878\n",
      "Trained batch 3002 batch loss 7.15022087 epoch total loss 6.35435438\n",
      "Trained batch 3003 batch loss 5.25557518 epoch total loss 6.35398865\n",
      "Trained batch 3004 batch loss 4.28659296 epoch total loss 6.35330057\n",
      "Trained batch 3005 batch loss 5.70601559 epoch total loss 6.35308456\n",
      "Trained batch 3006 batch loss 6.05530834 epoch total loss 6.35298538\n",
      "Trained batch 3007 batch loss 6.31602383 epoch total loss 6.35297346\n",
      "Trained batch 3008 batch loss 6.71388435 epoch total loss 6.35309362\n",
      "Trained batch 3009 batch loss 6.35017538 epoch total loss 6.35309219\n",
      "Trained batch 3010 batch loss 6.72566319 epoch total loss 6.35321665\n",
      "Trained batch 3011 batch loss 6.04347706 epoch total loss 6.35311365\n",
      "Trained batch 3012 batch loss 6.39047527 epoch total loss 6.35312605\n",
      "Trained batch 3013 batch loss 6.81091785 epoch total loss 6.35327768\n",
      "Trained batch 3014 batch loss 4.1167841 epoch total loss 6.35253572\n",
      "Trained batch 3015 batch loss 5.76067924 epoch total loss 6.35233927\n",
      "Trained batch 3016 batch loss 4.36504555 epoch total loss 6.35168028\n",
      "Trained batch 3017 batch loss 6.93469667 epoch total loss 6.35187387\n",
      "Trained batch 3018 batch loss 5.80007172 epoch total loss 6.35169125\n",
      "Trained batch 3019 batch loss 6.45981598 epoch total loss 6.35172701\n",
      "Trained batch 3020 batch loss 6.16029787 epoch total loss 6.35166359\n",
      "Trained batch 3021 batch loss 6.36259937 epoch total loss 6.3516674\n",
      "Trained batch 3022 batch loss 6.53027153 epoch total loss 6.35172606\n",
      "Trained batch 3023 batch loss 5.79785824 epoch total loss 6.35154295\n",
      "Trained batch 3024 batch loss 6.59763575 epoch total loss 6.35162449\n",
      "Trained batch 3025 batch loss 6.31940365 epoch total loss 6.351614\n",
      "Trained batch 3026 batch loss 6.5172081 epoch total loss 6.35166883\n",
      "Trained batch 3027 batch loss 4.8536253 epoch total loss 6.35117388\n",
      "Trained batch 3028 batch loss 5.73182869 epoch total loss 6.35097\n",
      "Trained batch 3029 batch loss 4.56082964 epoch total loss 6.35037851\n",
      "Trained batch 3030 batch loss 7.05024767 epoch total loss 6.35061\n",
      "Trained batch 3031 batch loss 6.43874836 epoch total loss 6.35063887\n",
      "Trained batch 3032 batch loss 6.36141062 epoch total loss 6.35064268\n",
      "Trained batch 3033 batch loss 6.37587738 epoch total loss 6.35065079\n",
      "Trained batch 3034 batch loss 6.29934025 epoch total loss 6.35063362\n",
      "Trained batch 3035 batch loss 6.09780216 epoch total loss 6.35055\n",
      "Trained batch 3036 batch loss 6.38441372 epoch total loss 6.35056162\n",
      "Trained batch 3037 batch loss 6.32134342 epoch total loss 6.35055208\n",
      "Trained batch 3038 batch loss 6.49899626 epoch total loss 6.35060072\n",
      "Trained batch 3039 batch loss 6.44037867 epoch total loss 6.35063\n",
      "Trained batch 3040 batch loss 6.32706881 epoch total loss 6.3506217\n",
      "Trained batch 3041 batch loss 6.13472509 epoch total loss 6.35055113\n",
      "Trained batch 3042 batch loss 5.24707031 epoch total loss 6.35018873\n",
      "Trained batch 3043 batch loss 6.07582045 epoch total loss 6.35009861\n",
      "Trained batch 3044 batch loss 6.2008152 epoch total loss 6.3500495\n",
      "Trained batch 3045 batch loss 6.18854046 epoch total loss 6.34999657\n",
      "Trained batch 3046 batch loss 6.05581665 epoch total loss 6.34990072\n",
      "Trained batch 3047 batch loss 6.6701107 epoch total loss 6.35000563\n",
      "Trained batch 3048 batch loss 6.29626179 epoch total loss 6.34998798\n",
      "Trained batch 3049 batch loss 6.41797972 epoch total loss 6.3500104\n",
      "Trained batch 3050 batch loss 5.948843 epoch total loss 6.34987879\n",
      "Trained batch 3051 batch loss 6.46989155 epoch total loss 6.34991837\n",
      "Trained batch 3052 batch loss 6.02437496 epoch total loss 6.34981155\n",
      "Trained batch 3053 batch loss 6.42980576 epoch total loss 6.34983778\n",
      "Trained batch 3054 batch loss 6.02493858 epoch total loss 6.34973145\n",
      "Trained batch 3055 batch loss 6.48901558 epoch total loss 6.34977674\n",
      "Trained batch 3056 batch loss 6.12746 epoch total loss 6.34970379\n",
      "Trained batch 3057 batch loss 6.01496315 epoch total loss 6.34959459\n",
      "Trained batch 3058 batch loss 6.10325909 epoch total loss 6.34951401\n",
      "Trained batch 3059 batch loss 6.09357071 epoch total loss 6.34943056\n",
      "Trained batch 3060 batch loss 7.36155701 epoch total loss 6.34976149\n",
      "Trained batch 3061 batch loss 6.95858765 epoch total loss 6.34996033\n",
      "Trained batch 3062 batch loss 7.20215034 epoch total loss 6.3502388\n",
      "Trained batch 3063 batch loss 7.41528893 epoch total loss 6.35058689\n",
      "Trained batch 3064 batch loss 6.52011061 epoch total loss 6.3506422\n",
      "Trained batch 3065 batch loss 6.50767136 epoch total loss 6.35069323\n",
      "Trained batch 3066 batch loss 6.0953331 epoch total loss 6.35061026\n",
      "Trained batch 3067 batch loss 6.79429483 epoch total loss 6.35075521\n",
      "Trained batch 3068 batch loss 4.07086182 epoch total loss 6.35001183\n",
      "Trained batch 3069 batch loss 3.80392122 epoch total loss 6.34918213\n",
      "Trained batch 3070 batch loss 4.42296743 epoch total loss 6.34855509\n",
      "Trained batch 3071 batch loss 5.29202747 epoch total loss 6.34821129\n",
      "Trained batch 3072 batch loss 3.86977077 epoch total loss 6.34740448\n",
      "Trained batch 3073 batch loss 5.77810192 epoch total loss 6.34721899\n",
      "Trained batch 3074 batch loss 4.45386505 epoch total loss 6.34660292\n",
      "Trained batch 3075 batch loss 4.65019703 epoch total loss 6.34605122\n",
      "Trained batch 3076 batch loss 4.08992434 epoch total loss 6.34531784\n",
      "Trained batch 3077 batch loss 4.76598024 epoch total loss 6.34480429\n",
      "Trained batch 3078 batch loss 4.03898668 epoch total loss 6.34405518\n",
      "Trained batch 3079 batch loss 4.58622932 epoch total loss 6.3434844\n",
      "Trained batch 3080 batch loss 4.74495697 epoch total loss 6.34296513\n",
      "Trained batch 3081 batch loss 3.93367982 epoch total loss 6.34218311\n",
      "Trained batch 3082 batch loss 3.84542751 epoch total loss 6.34137297\n",
      "Trained batch 3083 batch loss 3.86371899 epoch total loss 6.34056902\n",
      "Trained batch 3084 batch loss 3.80289078 epoch total loss 6.339746\n",
      "Trained batch 3085 batch loss 4.52288723 epoch total loss 6.33915758\n",
      "Trained batch 3086 batch loss 5.07415 epoch total loss 6.3387475\n",
      "Trained batch 3087 batch loss 6.42811775 epoch total loss 6.33877659\n",
      "Trained batch 3088 batch loss 5.84187317 epoch total loss 6.33861542\n",
      "Trained batch 3089 batch loss 6.28236675 epoch total loss 6.3385973\n",
      "Trained batch 3090 batch loss 7.06928587 epoch total loss 6.33883381\n",
      "Trained batch 3091 batch loss 7.12814569 epoch total loss 6.33908939\n",
      "Trained batch 3092 batch loss 7.12171841 epoch total loss 6.33934212\n",
      "Trained batch 3093 batch loss 7.07333517 epoch total loss 6.33957958\n",
      "Trained batch 3094 batch loss 7.03808212 epoch total loss 6.33980513\n",
      "Trained batch 3095 batch loss 6.82237482 epoch total loss 6.33996105\n",
      "Trained batch 3096 batch loss 7.40145445 epoch total loss 6.34030437\n",
      "Trained batch 3097 batch loss 7.10946846 epoch total loss 6.34055281\n",
      "Trained batch 3098 batch loss 7.14457512 epoch total loss 6.34081221\n",
      "Trained batch 3099 batch loss 7.41426802 epoch total loss 6.34115839\n",
      "Trained batch 3100 batch loss 7.24016762 epoch total loss 6.34144831\n",
      "Trained batch 3101 batch loss 7.38742352 epoch total loss 6.34178543\n",
      "Trained batch 3102 batch loss 7.19806576 epoch total loss 6.34206152\n",
      "Trained batch 3103 batch loss 7.12208462 epoch total loss 6.34231281\n",
      "Trained batch 3104 batch loss 6.95611477 epoch total loss 6.34251118\n",
      "Trained batch 3105 batch loss 7.26967669 epoch total loss 6.34280968\n",
      "Trained batch 3106 batch loss 7.02084827 epoch total loss 6.34302807\n",
      "Trained batch 3107 batch loss 7.19600439 epoch total loss 6.34330225\n",
      "Trained batch 3108 batch loss 6.73148346 epoch total loss 6.34342766\n",
      "Trained batch 3109 batch loss 6.84786797 epoch total loss 6.34359\n",
      "Trained batch 3110 batch loss 7.01653576 epoch total loss 6.34380579\n",
      "Trained batch 3111 batch loss 6.59788752 epoch total loss 6.34388733\n",
      "Trained batch 3112 batch loss 6.66264391 epoch total loss 6.34399\n",
      "Trained batch 3113 batch loss 6.57708788 epoch total loss 6.34406424\n",
      "Trained batch 3114 batch loss 6.83615112 epoch total loss 6.34422207\n",
      "Trained batch 3115 batch loss 6.68894243 epoch total loss 6.34433317\n",
      "Trained batch 3116 batch loss 6.7736249 epoch total loss 6.34447098\n",
      "Trained batch 3117 batch loss 6.81080866 epoch total loss 6.34462\n",
      "Trained batch 3118 batch loss 6.712852 epoch total loss 6.34473848\n",
      "Trained batch 3119 batch loss 6.78330803 epoch total loss 6.34487915\n",
      "Trained batch 3120 batch loss 6.61467 epoch total loss 6.34496546\n",
      "Trained batch 3121 batch loss 6.50014305 epoch total loss 6.34501553\n",
      "Trained batch 3122 batch loss 6.67567348 epoch total loss 6.34512138\n",
      "Trained batch 3123 batch loss 6.0091 epoch total loss 6.3450141\n",
      "Trained batch 3124 batch loss 6.15226555 epoch total loss 6.34495211\n",
      "Trained batch 3125 batch loss 6.77744865 epoch total loss 6.34509039\n",
      "Trained batch 3126 batch loss 6.86706448 epoch total loss 6.34525776\n",
      "Trained batch 3127 batch loss 4.91555405 epoch total loss 6.34480047\n",
      "Trained batch 3128 batch loss 6.59550858 epoch total loss 6.34488058\n",
      "Trained batch 3129 batch loss 6.37884903 epoch total loss 6.34489155\n",
      "Trained batch 3130 batch loss 6.82108068 epoch total loss 6.34504366\n",
      "Trained batch 3131 batch loss 6.24237442 epoch total loss 6.34501076\n",
      "Trained batch 3132 batch loss 6.88924789 epoch total loss 6.34518433\n",
      "Trained batch 3133 batch loss 6.31994247 epoch total loss 6.34517622\n",
      "Trained batch 3134 batch loss 6.52582455 epoch total loss 6.34523392\n",
      "Trained batch 3135 batch loss 6.11801147 epoch total loss 6.34516096\n",
      "Trained batch 3136 batch loss 6.12871313 epoch total loss 6.3450923\n",
      "Trained batch 3137 batch loss 6.6500206 epoch total loss 6.34518957\n",
      "Trained batch 3138 batch loss 6.88551331 epoch total loss 6.34536123\n",
      "Trained batch 3139 batch loss 6.07035637 epoch total loss 6.34527397\n",
      "Trained batch 3140 batch loss 6.26981449 epoch total loss 6.34524965\n",
      "Trained batch 3141 batch loss 6.24322605 epoch total loss 6.3452177\n",
      "Trained batch 3142 batch loss 6.13068819 epoch total loss 6.34514904\n",
      "Trained batch 3143 batch loss 6.76338196 epoch total loss 6.34528255\n",
      "Trained batch 3144 batch loss 7.04271698 epoch total loss 6.34550428\n",
      "Trained batch 3145 batch loss 6.53075409 epoch total loss 6.34556341\n",
      "Trained batch 3146 batch loss 6.67847443 epoch total loss 6.34566879\n",
      "Trained batch 3147 batch loss 6.58910656 epoch total loss 6.34574652\n",
      "Trained batch 3148 batch loss 6.39491463 epoch total loss 6.34576225\n",
      "Trained batch 3149 batch loss 6.9029479 epoch total loss 6.34593868\n",
      "Trained batch 3150 batch loss 6.71127033 epoch total loss 6.34605455\n",
      "Trained batch 3151 batch loss 6.51891375 epoch total loss 6.34611\n",
      "Trained batch 3152 batch loss 6.37234592 epoch total loss 6.34611845\n",
      "Trained batch 3153 batch loss 6.67456388 epoch total loss 6.3462224\n",
      "Trained batch 3154 batch loss 7.03154 epoch total loss 6.34643936\n",
      "Trained batch 3155 batch loss 6.36127424 epoch total loss 6.34644413\n",
      "Trained batch 3156 batch loss 5.70293283 epoch total loss 6.34624052\n",
      "Trained batch 3157 batch loss 6.70352221 epoch total loss 6.34635353\n",
      "Trained batch 3158 batch loss 6.15534115 epoch total loss 6.34629297\n",
      "Trained batch 3159 batch loss 6.49575615 epoch total loss 6.34634066\n",
      "Trained batch 3160 batch loss 6.14548492 epoch total loss 6.34627676\n",
      "Trained batch 3161 batch loss 6.77790451 epoch total loss 6.34641314\n",
      "Trained batch 3162 batch loss 6.21504688 epoch total loss 6.34637165\n",
      "Trained batch 3163 batch loss 6.75695086 epoch total loss 6.34650135\n",
      "Trained batch 3164 batch loss 6.55452633 epoch total loss 6.34656715\n",
      "Trained batch 3165 batch loss 6.2873354 epoch total loss 6.34654856\n",
      "Trained batch 3166 batch loss 5.88342094 epoch total loss 6.34640217\n",
      "Trained batch 3167 batch loss 6.1630044 epoch total loss 6.34634399\n",
      "Trained batch 3168 batch loss 6.47427702 epoch total loss 6.34638453\n",
      "Trained batch 3169 batch loss 6.31387949 epoch total loss 6.34637451\n",
      "Trained batch 3170 batch loss 6.09282446 epoch total loss 6.3462944\n",
      "Trained batch 3171 batch loss 6.87588644 epoch total loss 6.3464613\n",
      "Trained batch 3172 batch loss 6.04305696 epoch total loss 6.34636545\n",
      "Trained batch 3173 batch loss 6.78288937 epoch total loss 6.34650326\n",
      "Trained batch 3174 batch loss 6.35211945 epoch total loss 6.34650517\n",
      "Trained batch 3175 batch loss 6.03159952 epoch total loss 6.34640551\n",
      "Trained batch 3176 batch loss 5.88765335 epoch total loss 6.34626102\n",
      "Trained batch 3177 batch loss 6.23405647 epoch total loss 6.34622574\n",
      "Trained batch 3178 batch loss 5.89276552 epoch total loss 6.34608316\n",
      "Trained batch 3179 batch loss 6.04865408 epoch total loss 6.34598923\n",
      "Trained batch 3180 batch loss 6.31874371 epoch total loss 6.34598064\n",
      "Trained batch 3181 batch loss 6.7991581 epoch total loss 6.34612322\n",
      "Trained batch 3182 batch loss 6.73686886 epoch total loss 6.34624577\n",
      "Trained batch 3183 batch loss 6.49253941 epoch total loss 6.34629154\n",
      "Trained batch 3184 batch loss 6.72676325 epoch total loss 6.34641123\n",
      "Trained batch 3185 batch loss 6.23686743 epoch total loss 6.34637642\n",
      "Trained batch 3186 batch loss 6.60189 epoch total loss 6.34645653\n",
      "Trained batch 3187 batch loss 6.23035479 epoch total loss 6.34642029\n",
      "Trained batch 3188 batch loss 6.48635292 epoch total loss 6.34646416\n",
      "Trained batch 3189 batch loss 6.82568932 epoch total loss 6.34661436\n",
      "Trained batch 3190 batch loss 6.48717594 epoch total loss 6.34665823\n",
      "Trained batch 3191 batch loss 6.43306 epoch total loss 6.34668541\n",
      "Trained batch 3192 batch loss 6.48007584 epoch total loss 6.34672737\n",
      "Trained batch 3193 batch loss 6.2548275 epoch total loss 6.34669828\n",
      "Trained batch 3194 batch loss 6.69799089 epoch total loss 6.34680796\n",
      "Trained batch 3195 batch loss 6.34421253 epoch total loss 6.346807\n",
      "Trained batch 3196 batch loss 5.83751583 epoch total loss 6.34664774\n",
      "Trained batch 3197 batch loss 6.4530549 epoch total loss 6.34668112\n",
      "Trained batch 3198 batch loss 7.47779179 epoch total loss 6.34703493\n",
      "Trained batch 3199 batch loss 6.18161917 epoch total loss 6.34698343\n",
      "Trained batch 3200 batch loss 6.29698563 epoch total loss 6.3469677\n",
      "Trained batch 3201 batch loss 6.47805214 epoch total loss 6.34700871\n",
      "Trained batch 3202 batch loss 6.35858154 epoch total loss 6.34701252\n",
      "Trained batch 3203 batch loss 6.31615162 epoch total loss 6.34700298\n",
      "Trained batch 3204 batch loss 6.68990803 epoch total loss 6.34711027\n",
      "Trained batch 3205 batch loss 6.3803091 epoch total loss 6.34712076\n",
      "Trained batch 3206 batch loss 6.24164629 epoch total loss 6.34708786\n",
      "Trained batch 3207 batch loss 7.87484741 epoch total loss 6.34756422\n",
      "Trained batch 3208 batch loss 6.57909536 epoch total loss 6.34763622\n",
      "Trained batch 3209 batch loss 6.38063431 epoch total loss 6.34764671\n",
      "Trained batch 3210 batch loss 6.66282892 epoch total loss 6.34774446\n",
      "Trained batch 3211 batch loss 5.689538 epoch total loss 6.34753942\n",
      "Trained batch 3212 batch loss 6.01077271 epoch total loss 6.347435\n",
      "Trained batch 3213 batch loss 6.16661882 epoch total loss 6.34737825\n",
      "Trained batch 3214 batch loss 6.09988594 epoch total loss 6.34730148\n",
      "Trained batch 3215 batch loss 6.5100975 epoch total loss 6.34735203\n",
      "Trained batch 3216 batch loss 4.90861225 epoch total loss 6.34690428\n",
      "Trained batch 3217 batch loss 5.90783834 epoch total loss 6.3467679\n",
      "Trained batch 3218 batch loss 5.8364954 epoch total loss 6.34660912\n",
      "Trained batch 3219 batch loss 5.09303045 epoch total loss 6.34622\n",
      "Trained batch 3220 batch loss 5.46569252 epoch total loss 6.34594631\n",
      "Trained batch 3221 batch loss 5.58637714 epoch total loss 6.34571028\n",
      "Trained batch 3222 batch loss 4.98654795 epoch total loss 6.34528828\n",
      "Trained batch 3223 batch loss 6.52276897 epoch total loss 6.34534359\n",
      "Trained batch 3224 batch loss 6.41440678 epoch total loss 6.34536505\n",
      "Trained batch 3225 batch loss 4.93730736 epoch total loss 6.34492874\n",
      "Trained batch 3226 batch loss 5.83468294 epoch total loss 6.34477\n",
      "Trained batch 3227 batch loss 6.72983599 epoch total loss 6.34488964\n",
      "Trained batch 3228 batch loss 5.70268059 epoch total loss 6.3446908\n",
      "Trained batch 3229 batch loss 6.25884581 epoch total loss 6.34466457\n",
      "Trained batch 3230 batch loss 6.60470772 epoch total loss 6.34474516\n",
      "Trained batch 3231 batch loss 6.83352375 epoch total loss 6.34489679\n",
      "Trained batch 3232 batch loss 6.5997591 epoch total loss 6.34497547\n",
      "Trained batch 3233 batch loss 6.75456095 epoch total loss 6.34510183\n",
      "Trained batch 3234 batch loss 6.70414305 epoch total loss 6.34521341\n",
      "Trained batch 3235 batch loss 6.62090397 epoch total loss 6.34529877\n",
      "Trained batch 3236 batch loss 6.8909874 epoch total loss 6.34546709\n",
      "Trained batch 3237 batch loss 6.79664516 epoch total loss 6.3456068\n",
      "Trained batch 3238 batch loss 6.46198893 epoch total loss 6.34564304\n",
      "Trained batch 3239 batch loss 6.71842146 epoch total loss 6.34575796\n",
      "Trained batch 3240 batch loss 6.67463064 epoch total loss 6.34585905\n",
      "Trained batch 3241 batch loss 6.81955051 epoch total loss 6.34600544\n",
      "Trained batch 3242 batch loss 6.47035646 epoch total loss 6.34604406\n",
      "Trained batch 3243 batch loss 6.33500671 epoch total loss 6.3460412\n",
      "Trained batch 3244 batch loss 6.5969286 epoch total loss 6.34611845\n",
      "Trained batch 3245 batch loss 6.10188437 epoch total loss 6.34604311\n",
      "Trained batch 3246 batch loss 6.7446413 epoch total loss 6.34616566\n",
      "Trained batch 3247 batch loss 6.77512646 epoch total loss 6.34629822\n",
      "Trained batch 3248 batch loss 6.49291611 epoch total loss 6.34634304\n",
      "Trained batch 3249 batch loss 5.83081627 epoch total loss 6.34618425\n",
      "Trained batch 3250 batch loss 6.03836 epoch total loss 6.34608936\n",
      "Trained batch 3251 batch loss 6.2530632 epoch total loss 6.34606123\n",
      "Trained batch 3252 batch loss 6.39496851 epoch total loss 6.34607601\n",
      "Trained batch 3253 batch loss 6.42457581 epoch total loss 6.3461\n",
      "Trained batch 3254 batch loss 6.03196526 epoch total loss 6.34600306\n",
      "Trained batch 3255 batch loss 4.90066624 epoch total loss 6.34555912\n",
      "Trained batch 3256 batch loss 5.5948 epoch total loss 6.34532881\n",
      "Trained batch 3257 batch loss 6.51395226 epoch total loss 6.34538031\n",
      "Trained batch 3258 batch loss 6.50971127 epoch total loss 6.34543085\n",
      "Trained batch 3259 batch loss 6.57681 epoch total loss 6.3455019\n",
      "Trained batch 3260 batch loss 6.43728828 epoch total loss 6.34553\n",
      "Trained batch 3261 batch loss 6.70038414 epoch total loss 6.34563923\n",
      "Trained batch 3262 batch loss 6.41824198 epoch total loss 6.34566116\n",
      "Trained batch 3263 batch loss 6.64534855 epoch total loss 6.34575272\n",
      "Trained batch 3264 batch loss 6.68974543 epoch total loss 6.3458581\n",
      "Trained batch 3265 batch loss 6.57189655 epoch total loss 6.34592724\n",
      "Trained batch 3266 batch loss 6.57819462 epoch total loss 6.34599876\n",
      "Trained batch 3267 batch loss 6.43821907 epoch total loss 6.34602642\n",
      "Trained batch 3268 batch loss 6.46996832 epoch total loss 6.34606457\n",
      "Trained batch 3269 batch loss 6.97156382 epoch total loss 6.34625578\n",
      "Trained batch 3270 batch loss 6.38377 epoch total loss 6.34626675\n",
      "Trained batch 3271 batch loss 6.70692253 epoch total loss 6.34637737\n",
      "Trained batch 3272 batch loss 6.47937346 epoch total loss 6.34641743\n",
      "Trained batch 3273 batch loss 6.57907581 epoch total loss 6.34648848\n",
      "Trained batch 3274 batch loss 6.92151546 epoch total loss 6.34666395\n",
      "Trained batch 3275 batch loss 6.34306145 epoch total loss 6.34666348\n",
      "Trained batch 3276 batch loss 6.64267063 epoch total loss 6.3467536\n",
      "Trained batch 3277 batch loss 6.85129452 epoch total loss 6.34690762\n",
      "Trained batch 3278 batch loss 6.21682644 epoch total loss 6.34686804\n",
      "Trained batch 3279 batch loss 6.68758059 epoch total loss 6.34697199\n",
      "Trained batch 3280 batch loss 6.52373123 epoch total loss 6.34702587\n",
      "Trained batch 3281 batch loss 6.27919674 epoch total loss 6.34700489\n",
      "Trained batch 3282 batch loss 6.40508699 epoch total loss 6.34702253\n",
      "Trained batch 3283 batch loss 6.39197922 epoch total loss 6.34703636\n",
      "Trained batch 3284 batch loss 6.45083141 epoch total loss 6.34706783\n",
      "Trained batch 3285 batch loss 6.09082603 epoch total loss 6.34699\n",
      "Trained batch 3286 batch loss 6.75114918 epoch total loss 6.34711361\n",
      "Trained batch 3287 batch loss 6.30928898 epoch total loss 6.34710169\n",
      "Trained batch 3288 batch loss 6.20849609 epoch total loss 6.34705973\n",
      "Trained batch 3289 batch loss 6.69740582 epoch total loss 6.34716654\n",
      "Trained batch 3290 batch loss 6.35715675 epoch total loss 6.3471694\n",
      "Trained batch 3291 batch loss 6.24023962 epoch total loss 6.34713697\n",
      "Trained batch 3292 batch loss 6.35045242 epoch total loss 6.34713793\n",
      "Trained batch 3293 batch loss 5.67999458 epoch total loss 6.3469348\n",
      "Trained batch 3294 batch loss 6.22393513 epoch total loss 6.34689808\n",
      "Trained batch 3295 batch loss 6.68092823 epoch total loss 6.34699965\n",
      "Trained batch 3296 batch loss 6.10606861 epoch total loss 6.34692621\n",
      "Trained batch 3297 batch loss 6.47154808 epoch total loss 6.34696388\n",
      "Trained batch 3298 batch loss 6.18691492 epoch total loss 6.34691525\n",
      "Trained batch 3299 batch loss 6.48376846 epoch total loss 6.34695721\n",
      "Trained batch 3300 batch loss 5.8034358 epoch total loss 6.34679222\n",
      "Trained batch 3301 batch loss 6.11742926 epoch total loss 6.3467226\n",
      "Trained batch 3302 batch loss 6.74407196 epoch total loss 6.34684277\n",
      "Trained batch 3303 batch loss 7.9105835 epoch total loss 6.34731627\n",
      "Trained batch 3304 batch loss 6.29671812 epoch total loss 6.34730101\n",
      "Trained batch 3305 batch loss 6.55784 epoch total loss 6.3473649\n",
      "Trained batch 3306 batch loss 6.16506958 epoch total loss 6.34731\n",
      "Trained batch 3307 batch loss 6.16354656 epoch total loss 6.34725475\n",
      "Trained batch 3308 batch loss 6.59080124 epoch total loss 6.34732819\n",
      "Trained batch 3309 batch loss 5.86767054 epoch total loss 6.34718275\n",
      "Trained batch 3310 batch loss 5.33965492 epoch total loss 6.34687853\n",
      "Trained batch 3311 batch loss 5.58323097 epoch total loss 6.34664822\n",
      "Trained batch 3312 batch loss 6.23409748 epoch total loss 6.34661436\n",
      "Trained batch 3313 batch loss 5.88293266 epoch total loss 6.34647417\n",
      "Trained batch 3314 batch loss 6.52528381 epoch total loss 6.34652805\n",
      "Trained batch 3315 batch loss 5.26413488 epoch total loss 6.34620142\n",
      "Trained batch 3316 batch loss 5.69787741 epoch total loss 6.34600592\n",
      "Trained batch 3317 batch loss 6.02902126 epoch total loss 6.34591055\n",
      "Trained batch 3318 batch loss 6.28438902 epoch total loss 6.34589195\n",
      "Trained batch 3319 batch loss 6.18697643 epoch total loss 6.34584427\n",
      "Trained batch 3320 batch loss 6.19554472 epoch total loss 6.34579897\n",
      "Trained batch 3321 batch loss 6.49923897 epoch total loss 6.34584522\n",
      "Trained batch 3322 batch loss 6.42793226 epoch total loss 6.34587\n",
      "Trained batch 3323 batch loss 6.14529037 epoch total loss 6.34580946\n",
      "Trained batch 3324 batch loss 6.10150576 epoch total loss 6.34573603\n",
      "Trained batch 3325 batch loss 6.28465509 epoch total loss 6.34571791\n",
      "Trained batch 3326 batch loss 6.24448586 epoch total loss 6.34568739\n",
      "Trained batch 3327 batch loss 5.73211 epoch total loss 6.34550285\n",
      "Trained batch 3328 batch loss 6.16270304 epoch total loss 6.34544802\n",
      "Trained batch 3329 batch loss 6.13337517 epoch total loss 6.34538412\n",
      "Trained batch 3330 batch loss 6.35793972 epoch total loss 6.34538746\n",
      "Trained batch 3331 batch loss 6.46572399 epoch total loss 6.34542322\n",
      "Trained batch 3332 batch loss 6.33589029 epoch total loss 6.34542036\n",
      "Trained batch 3333 batch loss 6.34993839 epoch total loss 6.34542179\n",
      "Trained batch 3334 batch loss 5.85680866 epoch total loss 6.3452754\n",
      "Trained batch 3335 batch loss 5.93423414 epoch total loss 6.3451519\n",
      "Trained batch 3336 batch loss 6.30611 epoch total loss 6.34514046\n",
      "Trained batch 3337 batch loss 6.24409437 epoch total loss 6.34511042\n",
      "Trained batch 3338 batch loss 6.44272232 epoch total loss 6.3451395\n",
      "Trained batch 3339 batch loss 6.37905216 epoch total loss 6.34515\n",
      "Trained batch 3340 batch loss 6.09421062 epoch total loss 6.34507465\n",
      "Trained batch 3341 batch loss 6.18323326 epoch total loss 6.34502602\n",
      "Trained batch 3342 batch loss 6.11303902 epoch total loss 6.34495687\n",
      "Trained batch 3343 batch loss 6.0170126 epoch total loss 6.34485912\n",
      "Trained batch 3344 batch loss 6.57167912 epoch total loss 6.34492683\n",
      "Trained batch 3345 batch loss 6.70300865 epoch total loss 6.34503412\n",
      "Trained batch 3346 batch loss 5.90179777 epoch total loss 6.34490156\n",
      "Trained batch 3347 batch loss 6.24482107 epoch total loss 6.34487152\n",
      "Trained batch 3348 batch loss 6.16169691 epoch total loss 6.34481716\n",
      "Trained batch 3349 batch loss 7.32795715 epoch total loss 6.34511042\n",
      "Trained batch 3350 batch loss 5.86744308 epoch total loss 6.34496784\n",
      "Trained batch 3351 batch loss 6.28540468 epoch total loss 6.34495\n",
      "Trained batch 3352 batch loss 6.15684414 epoch total loss 6.34489393\n",
      "Trained batch 3353 batch loss 6.55418873 epoch total loss 6.3449564\n",
      "Trained batch 3354 batch loss 5.86455965 epoch total loss 6.34481335\n",
      "Trained batch 3355 batch loss 6.28651047 epoch total loss 6.34479618\n",
      "Trained batch 3356 batch loss 6.42330265 epoch total loss 6.34481955\n",
      "Trained batch 3357 batch loss 5.93719 epoch total loss 6.34469843\n",
      "Trained batch 3358 batch loss 6.62097597 epoch total loss 6.34478045\n",
      "Trained batch 3359 batch loss 5.66805363 epoch total loss 6.34457922\n",
      "Trained batch 3360 batch loss 6.85870934 epoch total loss 6.34473228\n",
      "Trained batch 3361 batch loss 7.57454681 epoch total loss 6.34509802\n",
      "Trained batch 3362 batch loss 5.68366241 epoch total loss 6.34490156\n",
      "Trained batch 3363 batch loss 4.55565214 epoch total loss 6.34436893\n",
      "Trained batch 3364 batch loss 6.29551315 epoch total loss 6.34435463\n",
      "Trained batch 3365 batch loss 5.74641037 epoch total loss 6.34417677\n",
      "Trained batch 3366 batch loss 5.51116371 epoch total loss 6.34392929\n",
      "Trained batch 3367 batch loss 4.57399654 epoch total loss 6.34340382\n",
      "Trained batch 3368 batch loss 4.73679543 epoch total loss 6.3429265\n",
      "Trained batch 3369 batch loss 6.27509642 epoch total loss 6.34290648\n",
      "Trained batch 3370 batch loss 6.41914415 epoch total loss 6.34292936\n",
      "Trained batch 3371 batch loss 6.52066374 epoch total loss 6.34298229\n",
      "Trained batch 3372 batch loss 6.20277 epoch total loss 6.34294081\n",
      "Trained batch 3373 batch loss 6.16970873 epoch total loss 6.34288931\n",
      "Trained batch 3374 batch loss 6.4665966 epoch total loss 6.34292603\n",
      "Trained batch 3375 batch loss 6.22856283 epoch total loss 6.34289217\n",
      "Trained batch 3376 batch loss 6.04363155 epoch total loss 6.34280348\n",
      "Trained batch 3377 batch loss 6.28791142 epoch total loss 6.34278727\n",
      "Trained batch 3378 batch loss 6.29569674 epoch total loss 6.34277296\n",
      "Trained batch 3379 batch loss 6.30971622 epoch total loss 6.34276342\n",
      "Trained batch 3380 batch loss 6.64833307 epoch total loss 6.34285355\n",
      "Trained batch 3381 batch loss 6.68870163 epoch total loss 6.34295607\n",
      "Trained batch 3382 batch loss 6.54059315 epoch total loss 6.34301472\n",
      "Trained batch 3383 batch loss 5.6645813 epoch total loss 6.34281397\n",
      "Trained batch 3384 batch loss 6.48298502 epoch total loss 6.34285545\n",
      "Trained batch 3385 batch loss 6.14847088 epoch total loss 6.34279776\n",
      "Trained batch 3386 batch loss 6.25662899 epoch total loss 6.34277248\n",
      "Trained batch 3387 batch loss 6.3140974 epoch total loss 6.3427639\n",
      "Trained batch 3388 batch loss 6.50120163 epoch total loss 6.34281111\n",
      "Trained batch 3389 batch loss 5.91681 epoch total loss 6.34268475\n",
      "Trained batch 3390 batch loss 5.61097145 epoch total loss 6.34246922\n",
      "Trained batch 3391 batch loss 4.98514032 epoch total loss 6.34206867\n",
      "Trained batch 3392 batch loss 6.56690216 epoch total loss 6.34213495\n",
      "Trained batch 3393 batch loss 6.88449955 epoch total loss 6.34229469\n",
      "Trained batch 3394 batch loss 6.67795324 epoch total loss 6.3423934\n",
      "Trained batch 3395 batch loss 5.98127174 epoch total loss 6.34228706\n",
      "Trained batch 3396 batch loss 5.26630402 epoch total loss 6.34197\n",
      "Trained batch 3397 batch loss 7.06867552 epoch total loss 6.34218359\n",
      "Trained batch 3398 batch loss 6.36916351 epoch total loss 6.3421917\n",
      "Trained batch 3399 batch loss 6.79486465 epoch total loss 6.34232473\n",
      "Trained batch 3400 batch loss 6.39501286 epoch total loss 6.34234047\n",
      "Trained batch 3401 batch loss 6.48085499 epoch total loss 6.342381\n",
      "Trained batch 3402 batch loss 6.25185 epoch total loss 6.3423543\n",
      "Trained batch 3403 batch loss 5.66933537 epoch total loss 6.34215689\n",
      "Trained batch 3404 batch loss 5.21836472 epoch total loss 6.34182692\n",
      "Trained batch 3405 batch loss 4.35833311 epoch total loss 6.34124374\n",
      "Trained batch 3406 batch loss 3.99717093 epoch total loss 6.34055614\n",
      "Trained batch 3407 batch loss 5.80611706 epoch total loss 6.34039927\n",
      "Trained batch 3408 batch loss 5.11322451 epoch total loss 6.34003925\n",
      "Trained batch 3409 batch loss 4.87037373 epoch total loss 6.33960819\n",
      "Trained batch 3410 batch loss 5.21143436 epoch total loss 6.33927727\n",
      "Trained batch 3411 batch loss 6.14756823 epoch total loss 6.33922148\n",
      "Trained batch 3412 batch loss 6.51940632 epoch total loss 6.33927441\n",
      "Trained batch 3413 batch loss 6.39610386 epoch total loss 6.3392911\n",
      "Trained batch 3414 batch loss 6.3536768 epoch total loss 6.33929491\n",
      "Trained batch 3415 batch loss 6.3433857 epoch total loss 6.33929634\n",
      "Trained batch 3416 batch loss 6.17761087 epoch total loss 6.33924913\n",
      "Trained batch 3417 batch loss 5.70881033 epoch total loss 6.3390646\n",
      "Trained batch 3418 batch loss 6.34551239 epoch total loss 6.33906651\n",
      "Trained batch 3419 batch loss 6.08931828 epoch total loss 6.33899355\n",
      "Trained batch 3420 batch loss 6.33936405 epoch total loss 6.33899403\n",
      "Trained batch 3421 batch loss 6.2934103 epoch total loss 6.33898067\n",
      "Trained batch 3422 batch loss 6.04881 epoch total loss 6.3388958\n",
      "Trained batch 3423 batch loss 6.42919302 epoch total loss 6.33892202\n",
      "Trained batch 3424 batch loss 6.2680378 epoch total loss 6.33890152\n",
      "Trained batch 3425 batch loss 6.22622967 epoch total loss 6.33886862\n",
      "Trained batch 3426 batch loss 5.89936352 epoch total loss 6.33874\n",
      "Trained batch 3427 batch loss 6.32570362 epoch total loss 6.33873653\n",
      "Trained batch 3428 batch loss 6.21286726 epoch total loss 6.3387\n",
      "Trained batch 3429 batch loss 6.49797344 epoch total loss 6.33874607\n",
      "Trained batch 3430 batch loss 7.03462315 epoch total loss 6.3389492\n",
      "Trained batch 3431 batch loss 7.04032803 epoch total loss 6.33915377\n",
      "Trained batch 3432 batch loss 6.86418819 epoch total loss 6.33930635\n",
      "Trained batch 3433 batch loss 6.16747189 epoch total loss 6.33925676\n",
      "Trained batch 3434 batch loss 6.37295771 epoch total loss 6.3392663\n",
      "Trained batch 3435 batch loss 6.79008484 epoch total loss 6.33939791\n",
      "Trained batch 3436 batch loss 7.01286 epoch total loss 6.33959436\n",
      "Trained batch 3437 batch loss 6.85152197 epoch total loss 6.33974314\n",
      "Trained batch 3438 batch loss 6.18151093 epoch total loss 6.33969736\n",
      "Trained batch 3439 batch loss 6.89010525 epoch total loss 6.33985758\n",
      "Trained batch 3440 batch loss 6.44998741 epoch total loss 6.33988905\n",
      "Trained batch 3441 batch loss 6.3329854 epoch total loss 6.33988667\n",
      "Trained batch 3442 batch loss 6.70045757 epoch total loss 6.33999205\n",
      "Trained batch 3443 batch loss 6.80602264 epoch total loss 6.34012747\n",
      "Trained batch 3444 batch loss 6.88553047 epoch total loss 6.3402853\n",
      "Trained batch 3445 batch loss 6.83526134 epoch total loss 6.34042931\n",
      "Trained batch 3446 batch loss 6.17807674 epoch total loss 6.3403821\n",
      "Trained batch 3447 batch loss 6.35419273 epoch total loss 6.34038591\n",
      "Trained batch 3448 batch loss 6.33325148 epoch total loss 6.34038401\n",
      "Trained batch 3449 batch loss 6.4442749 epoch total loss 6.34041405\n",
      "Trained batch 3450 batch loss 6.85458279 epoch total loss 6.3405633\n",
      "Trained batch 3451 batch loss 6.28584862 epoch total loss 6.34054708\n",
      "Trained batch 3452 batch loss 6.24637556 epoch total loss 6.34052\n",
      "Trained batch 3453 batch loss 6.57892513 epoch total loss 6.34058857\n",
      "Trained batch 3454 batch loss 6.75727463 epoch total loss 6.34070969\n",
      "Trained batch 3455 batch loss 6.46540451 epoch total loss 6.34074545\n",
      "Trained batch 3456 batch loss 6.69978333 epoch total loss 6.34084892\n",
      "Trained batch 3457 batch loss 6.32418585 epoch total loss 6.34084415\n",
      "Trained batch 3458 batch loss 6.57216311 epoch total loss 6.34091139\n",
      "Trained batch 3459 batch loss 4.75809288 epoch total loss 6.34045362\n",
      "Trained batch 3460 batch loss 5.38243389 epoch total loss 6.34017658\n",
      "Trained batch 3461 batch loss 5.76798344 epoch total loss 6.34001112\n",
      "Trained batch 3462 batch loss 6.06057405 epoch total loss 6.33993053\n",
      "Trained batch 3463 batch loss 6.2565279 epoch total loss 6.33990622\n",
      "Trained batch 3464 batch loss 5.62011337 epoch total loss 6.33969831\n",
      "Trained batch 3465 batch loss 6.05857801 epoch total loss 6.33961725\n",
      "Trained batch 3466 batch loss 5.76330137 epoch total loss 6.33945084\n",
      "Trained batch 3467 batch loss 5.06144238 epoch total loss 6.33908224\n",
      "Trained batch 3468 batch loss 4.61220789 epoch total loss 6.33858395\n",
      "Trained batch 3469 batch loss 5.95268106 epoch total loss 6.33847284\n",
      "Trained batch 3470 batch loss 6.29555511 epoch total loss 6.33846045\n",
      "Trained batch 3471 batch loss 5.22377872 epoch total loss 6.33813953\n",
      "Trained batch 3472 batch loss 6.34965563 epoch total loss 6.33814287\n",
      "Trained batch 3473 batch loss 6.8591032 epoch total loss 6.3382926\n",
      "Trained batch 3474 batch loss 6.04144764 epoch total loss 6.33820724\n",
      "Trained batch 3475 batch loss 6.85455608 epoch total loss 6.33835602\n",
      "Trained batch 3476 batch loss 6.06039906 epoch total loss 6.33827591\n",
      "Trained batch 3477 batch loss 5.99538231 epoch total loss 6.33817768\n",
      "Trained batch 3478 batch loss 6.18649 epoch total loss 6.33813381\n",
      "Trained batch 3479 batch loss 6.55521965 epoch total loss 6.3381958\n",
      "Trained batch 3480 batch loss 6.15530872 epoch total loss 6.33814383\n",
      "Trained batch 3481 batch loss 7.60005665 epoch total loss 6.33850622\n",
      "Trained batch 3482 batch loss 6.20626163 epoch total loss 6.33846855\n",
      "Trained batch 3483 batch loss 6.4160552 epoch total loss 6.33849049\n",
      "Trained batch 3484 batch loss 3.87808347 epoch total loss 6.33778477\n",
      "Trained batch 3485 batch loss 6.01066494 epoch total loss 6.33769035\n",
      "Trained batch 3486 batch loss 6.25536776 epoch total loss 6.33766699\n",
      "Trained batch 3487 batch loss 6.15249729 epoch total loss 6.33761406\n",
      "Trained batch 3488 batch loss 6.00058651 epoch total loss 6.33751726\n",
      "Trained batch 3489 batch loss 5.60629 epoch total loss 6.33730745\n",
      "Trained batch 3490 batch loss 5.38691425 epoch total loss 6.33703518\n",
      "Trained batch 3491 batch loss 5.97186232 epoch total loss 6.33693075\n",
      "Trained batch 3492 batch loss 4.89684153 epoch total loss 6.33651829\n",
      "Trained batch 3493 batch loss 6.01150465 epoch total loss 6.3364253\n",
      "Trained batch 3494 batch loss 5.94531155 epoch total loss 6.33631325\n",
      "Trained batch 3495 batch loss 6.46414089 epoch total loss 6.33635\n",
      "Trained batch 3496 batch loss 6.01424217 epoch total loss 6.33625746\n",
      "Trained batch 3497 batch loss 4.58173943 epoch total loss 6.33575583\n",
      "Trained batch 3498 batch loss 5.10769939 epoch total loss 6.33540487\n",
      "Trained batch 3499 batch loss 5.8470068 epoch total loss 6.33526564\n",
      "Trained batch 3500 batch loss 5.79749584 epoch total loss 6.33511162\n",
      "Trained batch 3501 batch loss 4.63673878 epoch total loss 6.33462667\n",
      "Trained batch 3502 batch loss 6.3743906 epoch total loss 6.33463812\n",
      "Trained batch 3503 batch loss 5.85080099 epoch total loss 6.33450031\n",
      "Trained batch 3504 batch loss 5.27381611 epoch total loss 6.33419752\n",
      "Trained batch 3505 batch loss 5.05366802 epoch total loss 6.33383179\n",
      "Trained batch 3506 batch loss 4.67212772 epoch total loss 6.33335781\n",
      "Trained batch 3507 batch loss 5.77086067 epoch total loss 6.33319759\n",
      "Trained batch 3508 batch loss 5.58938646 epoch total loss 6.3329854\n",
      "Trained batch 3509 batch loss 6.03462267 epoch total loss 6.33290052\n",
      "Trained batch 3510 batch loss 5.51932287 epoch total loss 6.33266878\n",
      "Trained batch 3511 batch loss 5.61028576 epoch total loss 6.33246279\n",
      "Trained batch 3512 batch loss 5.52351427 epoch total loss 6.33223248\n",
      "Trained batch 3513 batch loss 5.95243073 epoch total loss 6.33212471\n",
      "Trained batch 3514 batch loss 5.96214771 epoch total loss 6.33202\n",
      "Trained batch 3515 batch loss 4.96150684 epoch total loss 6.33162928\n",
      "Trained batch 3516 batch loss 6.08358669 epoch total loss 6.33155918\n",
      "Trained batch 3517 batch loss 7.20841694 epoch total loss 6.33180857\n",
      "Trained batch 3518 batch loss 8.52746487 epoch total loss 6.33243275\n",
      "Trained batch 3519 batch loss 7.66808701 epoch total loss 6.33281231\n",
      "Trained batch 3520 batch loss 8.03361 epoch total loss 6.33329535\n",
      "Trained batch 3521 batch loss 7.68979216 epoch total loss 6.33368063\n",
      "Trained batch 3522 batch loss 7.90739822 epoch total loss 6.33412743\n",
      "Trained batch 3523 batch loss 7.40620375 epoch total loss 6.33443165\n",
      "Trained batch 3524 batch loss 7.60694456 epoch total loss 6.33479309\n",
      "Trained batch 3525 batch loss 7.33908939 epoch total loss 6.33507824\n",
      "Trained batch 3526 batch loss 6.83136654 epoch total loss 6.33521891\n",
      "Trained batch 3527 batch loss 7.47628546 epoch total loss 6.33554268\n",
      "Trained batch 3528 batch loss 6.13702393 epoch total loss 6.33548641\n",
      "Trained batch 3529 batch loss 6.96810389 epoch total loss 6.3356657\n",
      "Trained batch 3530 batch loss 6.43572187 epoch total loss 6.33569384\n",
      "Trained batch 3531 batch loss 6.9115243 epoch total loss 6.33585739\n",
      "Trained batch 3532 batch loss 6.89183235 epoch total loss 6.33601475\n",
      "Trained batch 3533 batch loss 6.99107075 epoch total loss 6.3362\n",
      "Trained batch 3534 batch loss 6.50172043 epoch total loss 6.33624697\n",
      "Trained batch 3535 batch loss 6.51207495 epoch total loss 6.33629656\n",
      "Trained batch 3536 batch loss 6.61751413 epoch total loss 6.33637619\n",
      "Trained batch 3537 batch loss 6.47048092 epoch total loss 6.33641386\n",
      "Trained batch 3538 batch loss 6.60786438 epoch total loss 6.33649063\n",
      "Trained batch 3539 batch loss 6.8195982 epoch total loss 6.33662748\n",
      "Trained batch 3540 batch loss 6.68011808 epoch total loss 6.33672428\n",
      "Trained batch 3541 batch loss 6.59365273 epoch total loss 6.33679676\n",
      "Trained batch 3542 batch loss 6.62611437 epoch total loss 6.33687878\n",
      "Trained batch 3543 batch loss 6.94557476 epoch total loss 6.33705044\n",
      "Trained batch 3544 batch loss 6.51968575 epoch total loss 6.33710194\n",
      "Trained batch 3545 batch loss 6.78166 epoch total loss 6.33722734\n",
      "Trained batch 3546 batch loss 6.19219255 epoch total loss 6.33718634\n",
      "Trained batch 3547 batch loss 6.43918228 epoch total loss 6.33721495\n",
      "Trained batch 3548 batch loss 6.50670719 epoch total loss 6.33726263\n",
      "Trained batch 3549 batch loss 6.66935825 epoch total loss 6.33735609\n",
      "Trained batch 3550 batch loss 6.97316074 epoch total loss 6.33753538\n",
      "Trained batch 3551 batch loss 7.20122957 epoch total loss 6.33777857\n",
      "Trained batch 3552 batch loss 6.66932678 epoch total loss 6.33787203\n",
      "Trained batch 3553 batch loss 6.39728975 epoch total loss 6.33788824\n",
      "Trained batch 3554 batch loss 7.28514957 epoch total loss 6.33815479\n",
      "Trained batch 3555 batch loss 6.99379349 epoch total loss 6.33833933\n",
      "Trained batch 3556 batch loss 6.9191885 epoch total loss 6.33850288\n",
      "Trained batch 3557 batch loss 6.145154 epoch total loss 6.33844852\n",
      "Trained batch 3558 batch loss 6.76670361 epoch total loss 6.33856916\n",
      "Trained batch 3559 batch loss 7.12627697 epoch total loss 6.33879042\n",
      "Trained batch 3560 batch loss 6.5762 epoch total loss 6.33885717\n",
      "Trained batch 3561 batch loss 6.02869606 epoch total loss 6.33877039\n",
      "Trained batch 3562 batch loss 6.36641502 epoch total loss 6.3387785\n",
      "Trained batch 3563 batch loss 5.98699713 epoch total loss 6.33867931\n",
      "Trained batch 3564 batch loss 4.877882 epoch total loss 6.33826923\n",
      "Trained batch 3565 batch loss 6.51075 epoch total loss 6.33831787\n",
      "Trained batch 3566 batch loss 6.27916574 epoch total loss 6.33830118\n",
      "Trained batch 3567 batch loss 6.6987381 epoch total loss 6.33840275\n",
      "Trained batch 3568 batch loss 6.74302959 epoch total loss 6.33851576\n",
      "Trained batch 3569 batch loss 6.96854973 epoch total loss 6.33869219\n",
      "Trained batch 3570 batch loss 6.25866032 epoch total loss 6.33867\n",
      "Trained batch 3571 batch loss 6.60644054 epoch total loss 6.33874416\n",
      "Trained batch 3572 batch loss 4.79310179 epoch total loss 6.33831167\n",
      "Trained batch 3573 batch loss 6.54754639 epoch total loss 6.33837\n",
      "Trained batch 3574 batch loss 6.49751854 epoch total loss 6.33841467\n",
      "Trained batch 3575 batch loss 6.90492 epoch total loss 6.33857298\n",
      "Trained batch 3576 batch loss 5.98536253 epoch total loss 6.33847427\n",
      "Trained batch 3577 batch loss 6.51967573 epoch total loss 6.3385253\n",
      "Trained batch 3578 batch loss 6.73663425 epoch total loss 6.3386364\n",
      "Trained batch 3579 batch loss 6.18452024 epoch total loss 6.33859301\n",
      "Trained batch 3580 batch loss 6.03757191 epoch total loss 6.33850861\n",
      "Trained batch 3581 batch loss 6.23353672 epoch total loss 6.33847952\n",
      "Trained batch 3582 batch loss 6.63397884 epoch total loss 6.33856249\n",
      "Trained batch 3583 batch loss 6.25242 epoch total loss 6.33853817\n",
      "Trained batch 3584 batch loss 6.15384674 epoch total loss 6.33848667\n",
      "Trained batch 3585 batch loss 6.33735752 epoch total loss 6.33848667\n",
      "Trained batch 3586 batch loss 6.69963551 epoch total loss 6.33858728\n",
      "Trained batch 3587 batch loss 6.10372448 epoch total loss 6.33852148\n",
      "Trained batch 3588 batch loss 6.18404818 epoch total loss 6.33847857\n",
      "Trained batch 3589 batch loss 5.75706959 epoch total loss 6.33831692\n",
      "Trained batch 3590 batch loss 6.19285393 epoch total loss 6.33827639\n",
      "Trained batch 3591 batch loss 7.21964502 epoch total loss 6.33852148\n",
      "Trained batch 3592 batch loss 6.10735512 epoch total loss 6.33845711\n",
      "Trained batch 3593 batch loss 6.26249886 epoch total loss 6.33843565\n",
      "Trained batch 3594 batch loss 6.3609457 epoch total loss 6.33844233\n",
      "Trained batch 3595 batch loss 5.91473675 epoch total loss 6.33832407\n",
      "Trained batch 3596 batch loss 6.08640385 epoch total loss 6.33825397\n",
      "Trained batch 3597 batch loss 6.51767921 epoch total loss 6.33830404\n",
      "Trained batch 3598 batch loss 6.29794025 epoch total loss 6.33829308\n",
      "Trained batch 3599 batch loss 6.00413418 epoch total loss 6.3382\n",
      "Trained batch 3600 batch loss 6.93388605 epoch total loss 6.33836555\n",
      "Trained batch 3601 batch loss 6.13519192 epoch total loss 6.33830881\n",
      "Trained batch 3602 batch loss 6.07531071 epoch total loss 6.33823586\n",
      "Trained batch 3603 batch loss 6.23823357 epoch total loss 6.3382082\n",
      "Trained batch 3604 batch loss 6.58786869 epoch total loss 6.33827734\n",
      "Trained batch 3605 batch loss 6.43375778 epoch total loss 6.33830404\n",
      "Trained batch 3606 batch loss 6.54585695 epoch total loss 6.33836126\n",
      "Trained batch 3607 batch loss 6.13870811 epoch total loss 6.33830595\n",
      "Trained batch 3608 batch loss 6.15002155 epoch total loss 6.33825397\n",
      "Trained batch 3609 batch loss 6.17362499 epoch total loss 6.3382082\n",
      "Trained batch 3610 batch loss 6.37901258 epoch total loss 6.33821964\n",
      "Trained batch 3611 batch loss 6.81648302 epoch total loss 6.3383522\n",
      "Trained batch 3612 batch loss 6.29172516 epoch total loss 6.33833885\n",
      "Trained batch 3613 batch loss 6.42819929 epoch total loss 6.33836365\n",
      "Trained batch 3614 batch loss 7.56791687 epoch total loss 6.33870411\n",
      "Trained batch 3615 batch loss 6.36223316 epoch total loss 6.33871031\n",
      "Trained batch 3616 batch loss 5.8442955 epoch total loss 6.33857346\n",
      "Trained batch 3617 batch loss 7.26543331 epoch total loss 6.33882952\n",
      "Trained batch 3618 batch loss 5.98092461 epoch total loss 6.33873081\n",
      "Trained batch 3619 batch loss 6.32413816 epoch total loss 6.33872652\n",
      "Trained batch 3620 batch loss 5.04822493 epoch total loss 6.33837032\n",
      "Trained batch 3621 batch loss 6.54260921 epoch total loss 6.33842659\n",
      "Trained batch 3622 batch loss 6.4963026 epoch total loss 6.33847046\n",
      "Trained batch 3623 batch loss 6.24844837 epoch total loss 6.33844519\n",
      "Trained batch 3624 batch loss 5.78063107 epoch total loss 6.33829165\n",
      "Trained batch 3625 batch loss 5.87274599 epoch total loss 6.33816338\n",
      "Trained batch 3626 batch loss 5.9656806 epoch total loss 6.33806038\n",
      "Trained batch 3627 batch loss 6.32747269 epoch total loss 6.33805752\n",
      "Trained batch 3628 batch loss 6.27038193 epoch total loss 6.33803844\n",
      "Trained batch 3629 batch loss 6.16715193 epoch total loss 6.33799171\n",
      "Trained batch 3630 batch loss 6.84345055 epoch total loss 6.33813095\n",
      "Trained batch 3631 batch loss 7.24100161 epoch total loss 6.33837938\n",
      "Trained batch 3632 batch loss 6.57002735 epoch total loss 6.33844328\n",
      "Trained batch 3633 batch loss 6.46812725 epoch total loss 6.33847952\n",
      "Trained batch 3634 batch loss 6.4475522 epoch total loss 6.33850908\n",
      "Trained batch 3635 batch loss 6.52376938 epoch total loss 6.33856\n",
      "Trained batch 3636 batch loss 6.34152031 epoch total loss 6.33856106\n",
      "Trained batch 3637 batch loss 5.63937 epoch total loss 6.33836842\n",
      "Trained batch 3638 batch loss 6.35219383 epoch total loss 6.33837223\n",
      "Trained batch 3639 batch loss 6.53140831 epoch total loss 6.33842516\n",
      "Trained batch 3640 batch loss 5.84907293 epoch total loss 6.33829069\n",
      "Trained batch 3641 batch loss 6.49413395 epoch total loss 6.33833361\n",
      "Trained batch 3642 batch loss 6.29535103 epoch total loss 6.33832169\n",
      "Trained batch 3643 batch loss 6.01229191 epoch total loss 6.33823204\n",
      "Trained batch 3644 batch loss 7.30651855 epoch total loss 6.33849812\n",
      "Trained batch 3645 batch loss 7.93369 epoch total loss 6.33893538\n",
      "Trained batch 3646 batch loss 7.56993961 epoch total loss 6.33927345\n",
      "Trained batch 3647 batch loss 6.92157745 epoch total loss 6.33943319\n",
      "Trained batch 3648 batch loss 7.42721272 epoch total loss 6.33973122\n",
      "Trained batch 3649 batch loss 7.34205055 epoch total loss 6.34000587\n",
      "Trained batch 3650 batch loss 6.75357819 epoch total loss 6.34011936\n",
      "Trained batch 3651 batch loss 7.1780839 epoch total loss 6.34034872\n",
      "Trained batch 3652 batch loss 6.46417904 epoch total loss 6.34038305\n",
      "Trained batch 3653 batch loss 7.71671677 epoch total loss 6.34075975\n",
      "Trained batch 3654 batch loss 7.85085392 epoch total loss 6.34117317\n",
      "Trained batch 3655 batch loss 6.47558832 epoch total loss 6.34121037\n",
      "Trained batch 3656 batch loss 6.48560143 epoch total loss 6.34125\n",
      "Trained batch 3657 batch loss 7.62654161 epoch total loss 6.34160137\n",
      "Trained batch 3658 batch loss 6.96699429 epoch total loss 6.34177208\n",
      "Trained batch 3659 batch loss 6.67737913 epoch total loss 6.34186411\n",
      "Trained batch 3660 batch loss 6.75436831 epoch total loss 6.34197664\n",
      "Trained batch 3661 batch loss 6.72672558 epoch total loss 6.34208155\n",
      "Trained batch 3662 batch loss 7.16043282 epoch total loss 6.34230518\n",
      "Trained batch 3663 batch loss 6.65744162 epoch total loss 6.34239149\n",
      "Trained batch 3664 batch loss 6.97728109 epoch total loss 6.34256458\n",
      "Trained batch 3665 batch loss 5.76171875 epoch total loss 6.3424058\n",
      "Trained batch 3666 batch loss 6.08008 epoch total loss 6.34233427\n",
      "Trained batch 3667 batch loss 5.53017616 epoch total loss 6.34211254\n",
      "Trained batch 3668 batch loss 5.72919703 epoch total loss 6.34194565\n",
      "Trained batch 3669 batch loss 5.65422535 epoch total loss 6.34175825\n",
      "Trained batch 3670 batch loss 5.69349384 epoch total loss 6.34158134\n",
      "Trained batch 3671 batch loss 6.31260109 epoch total loss 6.34157324\n",
      "Trained batch 3672 batch loss 6.12452 epoch total loss 6.34151459\n",
      "Trained batch 3673 batch loss 6.39893913 epoch total loss 6.34153\n",
      "Trained batch 3674 batch loss 6.36727667 epoch total loss 6.341537\n",
      "Trained batch 3675 batch loss 6.26579762 epoch total loss 6.34151649\n",
      "Trained batch 3676 batch loss 6.28590679 epoch total loss 6.34150076\n",
      "Trained batch 3677 batch loss 6.45333481 epoch total loss 6.34153128\n",
      "Trained batch 3678 batch loss 6.58811283 epoch total loss 6.34159803\n",
      "Trained batch 3679 batch loss 6.08567667 epoch total loss 6.34152889\n",
      "Trained batch 3680 batch loss 6.42409325 epoch total loss 6.3415513\n",
      "Trained batch 3681 batch loss 5.96823788 epoch total loss 6.34144974\n",
      "Trained batch 3682 batch loss 6.17196274 epoch total loss 6.34140396\n",
      "Trained batch 3683 batch loss 6.16251945 epoch total loss 6.34135532\n",
      "Trained batch 3684 batch loss 5.61863756 epoch total loss 6.34115887\n",
      "Trained batch 3685 batch loss 7.1246748 epoch total loss 6.34137201\n",
      "Trained batch 3686 batch loss 5.67239952 epoch total loss 6.34119034\n",
      "Trained batch 3687 batch loss 5.03483295 epoch total loss 6.34083605\n",
      "Trained batch 3688 batch loss 6.52485847 epoch total loss 6.34088612\n",
      "Trained batch 3689 batch loss 6.79311562 epoch total loss 6.34100866\n",
      "Trained batch 3690 batch loss 6.1619463 epoch total loss 6.34096\n",
      "Trained batch 3691 batch loss 6.18532324 epoch total loss 6.34091806\n",
      "Trained batch 3692 batch loss 6.75053644 epoch total loss 6.34102869\n",
      "Trained batch 3693 batch loss 5.7564 epoch total loss 6.34087038\n",
      "Trained batch 3694 batch loss 5.91684866 epoch total loss 6.34075546\n",
      "Trained batch 3695 batch loss 6.39926386 epoch total loss 6.34077072\n",
      "Trained batch 3696 batch loss 6.23644066 epoch total loss 6.34074259\n",
      "Trained batch 3697 batch loss 5.13559055 epoch total loss 6.34041643\n",
      "Trained batch 3698 batch loss 5.56136799 epoch total loss 6.34020567\n",
      "Trained batch 3699 batch loss 5.97638369 epoch total loss 6.34010744\n",
      "Trained batch 3700 batch loss 6.65633774 epoch total loss 6.34019279\n",
      "Trained batch 3701 batch loss 6.01133442 epoch total loss 6.3401041\n",
      "Trained batch 3702 batch loss 5.95932293 epoch total loss 6.34000111\n",
      "Trained batch 3703 batch loss 6.13440275 epoch total loss 6.33994532\n",
      "Trained batch 3704 batch loss 4.36777639 epoch total loss 6.33941317\n",
      "Trained batch 3705 batch loss 6.7465353 epoch total loss 6.33952284\n",
      "Trained batch 3706 batch loss 6.16587973 epoch total loss 6.33947611\n",
      "Trained batch 3707 batch loss 6.20568466 epoch total loss 6.33944\n",
      "Trained batch 3708 batch loss 6.70804167 epoch total loss 6.33953953\n",
      "Trained batch 3709 batch loss 5.98823166 epoch total loss 6.33944464\n",
      "Trained batch 3710 batch loss 5.97552967 epoch total loss 6.33934641\n",
      "Trained batch 3711 batch loss 5.58302784 epoch total loss 6.3391428\n",
      "Trained batch 3712 batch loss 5.34649372 epoch total loss 6.33887529\n",
      "Trained batch 3713 batch loss 6.00606203 epoch total loss 6.33878517\n",
      "Trained batch 3714 batch loss 6.25241947 epoch total loss 6.33876181\n",
      "Trained batch 3715 batch loss 6.36590481 epoch total loss 6.33876896\n",
      "Trained batch 3716 batch loss 5.85793877 epoch total loss 6.33863974\n",
      "Trained batch 3717 batch loss 5.73528576 epoch total loss 6.33847713\n",
      "Trained batch 3718 batch loss 6.13140249 epoch total loss 6.33842134\n",
      "Trained batch 3719 batch loss 5.24311447 epoch total loss 6.33812666\n",
      "Trained batch 3720 batch loss 5.47718287 epoch total loss 6.33789492\n",
      "Trained batch 3721 batch loss 6.80923223 epoch total loss 6.33802128\n",
      "Trained batch 3722 batch loss 6.82462931 epoch total loss 6.33815193\n",
      "Trained batch 3723 batch loss 6.9131093 epoch total loss 6.33830643\n",
      "Trained batch 3724 batch loss 7.20960617 epoch total loss 6.33854055\n",
      "Trained batch 3725 batch loss 7.061234 epoch total loss 6.33873415\n",
      "Trained batch 3726 batch loss 6.9205513 epoch total loss 6.33889\n",
      "Trained batch 3727 batch loss 6.63246059 epoch total loss 6.33896923\n",
      "Trained batch 3728 batch loss 6.44491339 epoch total loss 6.33899784\n",
      "Trained batch 3729 batch loss 6.56672525 epoch total loss 6.3390584\n",
      "Trained batch 3730 batch loss 6.69089365 epoch total loss 6.33915329\n",
      "Trained batch 3731 batch loss 6.67519283 epoch total loss 6.33924341\n",
      "Trained batch 3732 batch loss 6.97874641 epoch total loss 6.3394146\n",
      "Trained batch 3733 batch loss 6.74313784 epoch total loss 6.33952236\n",
      "Trained batch 3734 batch loss 6.5031414 epoch total loss 6.33956671\n",
      "Trained batch 3735 batch loss 6.16895103 epoch total loss 6.33952093\n",
      "Trained batch 3736 batch loss 6.49926472 epoch total loss 6.33956385\n",
      "Trained batch 3737 batch loss 6.5850687 epoch total loss 6.33963\n",
      "Trained batch 3738 batch loss 6.75159883 epoch total loss 6.33974028\n",
      "Trained batch 3739 batch loss 6.38380432 epoch total loss 6.3397522\n",
      "Trained batch 3740 batch loss 6.96944427 epoch total loss 6.33992052\n",
      "Trained batch 3741 batch loss 7.04950905 epoch total loss 6.34011\n",
      "Trained batch 3742 batch loss 5.82605886 epoch total loss 6.3399725\n",
      "Trained batch 3743 batch loss 6.42956877 epoch total loss 6.33999681\n",
      "Trained batch 3744 batch loss 6.26550961 epoch total loss 6.33997679\n",
      "Trained batch 3745 batch loss 6.25401735 epoch total loss 6.3399539\n",
      "Trained batch 3746 batch loss 6.17405701 epoch total loss 6.33990955\n",
      "Trained batch 3747 batch loss 6.42203188 epoch total loss 6.33993149\n",
      "Trained batch 3748 batch loss 6.06164074 epoch total loss 6.3398571\n",
      "Trained batch 3749 batch loss 5.69176769 epoch total loss 6.33968449\n",
      "Trained batch 3750 batch loss 6.24953842 epoch total loss 6.33966064\n",
      "Trained batch 3751 batch loss 6.25777435 epoch total loss 6.33963871\n",
      "Trained batch 3752 batch loss 6.35314083 epoch total loss 6.33964252\n",
      "Trained batch 3753 batch loss 6.20827293 epoch total loss 6.33960772\n",
      "Trained batch 3754 batch loss 6.05627632 epoch total loss 6.3395319\n",
      "Trained batch 3755 batch loss 6.36105919 epoch total loss 6.3395381\n",
      "Trained batch 3756 batch loss 6.13272762 epoch total loss 6.33948278\n",
      "Trained batch 3757 batch loss 6.06051254 epoch total loss 6.3394084\n",
      "Trained batch 3758 batch loss 6.04717541 epoch total loss 6.33933067\n",
      "Trained batch 3759 batch loss 5.68935537 epoch total loss 6.33915806\n",
      "Trained batch 3760 batch loss 5.84796953 epoch total loss 6.3390274\n",
      "Trained batch 3761 batch loss 5.13413095 epoch total loss 6.33870697\n",
      "Trained batch 3762 batch loss 6.10713768 epoch total loss 6.33864546\n",
      "Trained batch 3763 batch loss 6.03641224 epoch total loss 6.33856535\n",
      "Trained batch 3764 batch loss 6.81649876 epoch total loss 6.33869219\n",
      "Trained batch 3765 batch loss 5.75949478 epoch total loss 6.33853865\n",
      "Trained batch 3766 batch loss 6.41723633 epoch total loss 6.33855963\n",
      "Trained batch 3767 batch loss 6.52933311 epoch total loss 6.33861\n",
      "Trained batch 3768 batch loss 6.54562235 epoch total loss 6.33866501\n",
      "Trained batch 3769 batch loss 6.85770512 epoch total loss 6.33880281\n",
      "Trained batch 3770 batch loss 5.38160515 epoch total loss 6.33854866\n",
      "Trained batch 3771 batch loss 6.70764256 epoch total loss 6.33864641\n",
      "Trained batch 3772 batch loss 6.16909838 epoch total loss 6.33860159\n",
      "Trained batch 3773 batch loss 6.21728802 epoch total loss 6.33856916\n",
      "Trained batch 3774 batch loss 5.96953392 epoch total loss 6.33847141\n",
      "Trained batch 3775 batch loss 6.18977976 epoch total loss 6.33843184\n",
      "Trained batch 3776 batch loss 6.60978603 epoch total loss 6.33850336\n",
      "Trained batch 3777 batch loss 6.50595 epoch total loss 6.33854771\n",
      "Trained batch 3778 batch loss 6.41358852 epoch total loss 6.33856773\n",
      "Trained batch 3779 batch loss 6.20656967 epoch total loss 6.33853292\n",
      "Trained batch 3780 batch loss 6.25193167 epoch total loss 6.33851\n",
      "Trained batch 3781 batch loss 5.96805954 epoch total loss 6.33841228\n",
      "Trained batch 3782 batch loss 6.36670589 epoch total loss 6.33842\n",
      "Trained batch 3783 batch loss 6.89697027 epoch total loss 6.33856726\n",
      "Trained batch 3784 batch loss 5.73776817 epoch total loss 6.33840895\n",
      "Trained batch 3785 batch loss 6.53672028 epoch total loss 6.3384614\n",
      "Trained batch 3786 batch loss 6.06966114 epoch total loss 6.33839035\n",
      "Trained batch 3787 batch loss 5.86123562 epoch total loss 6.33826447\n",
      "Trained batch 3788 batch loss 6.01838112 epoch total loss 6.33818\n",
      "Trained batch 3789 batch loss 5.99194431 epoch total loss 6.33808851\n",
      "Trained batch 3790 batch loss 4.97570515 epoch total loss 6.33772945\n",
      "Trained batch 3791 batch loss 5.95657635 epoch total loss 6.33762884\n",
      "Trained batch 3792 batch loss 6.2512579 epoch total loss 6.33760643\n",
      "Trained batch 3793 batch loss 6.21467829 epoch total loss 6.33757401\n",
      "Trained batch 3794 batch loss 5.78300953 epoch total loss 6.33742762\n",
      "Trained batch 3795 batch loss 6.23075485 epoch total loss 6.33739948\n",
      "Trained batch 3796 batch loss 5.88624096 epoch total loss 6.33728075\n",
      "Trained batch 3797 batch loss 6.21710062 epoch total loss 6.33724928\n",
      "Trained batch 3798 batch loss 5.98275661 epoch total loss 6.33715582\n",
      "Trained batch 3799 batch loss 6.06149673 epoch total loss 6.33708286\n",
      "Trained batch 3800 batch loss 6.16834354 epoch total loss 6.33703852\n",
      "Trained batch 3801 batch loss 5.87871933 epoch total loss 6.33691788\n",
      "Trained batch 3802 batch loss 6.18251848 epoch total loss 6.33687687\n",
      "Trained batch 3803 batch loss 6.74356461 epoch total loss 6.33698416\n",
      "Trained batch 3804 batch loss 6.64406538 epoch total loss 6.33706522\n",
      "Trained batch 3805 batch loss 5.53116465 epoch total loss 6.33685303\n",
      "Trained batch 3806 batch loss 6.23803616 epoch total loss 6.33682728\n",
      "Trained batch 3807 batch loss 4.88524485 epoch total loss 6.33644581\n",
      "Trained batch 3808 batch loss 6.32490063 epoch total loss 6.33644247\n",
      "Trained batch 3809 batch loss 5.57714272 epoch total loss 6.33624315\n",
      "Trained batch 3810 batch loss 5.6081934 epoch total loss 6.33605194\n",
      "Trained batch 3811 batch loss 5.50545311 epoch total loss 6.33583403\n",
      "Trained batch 3812 batch loss 6.58503628 epoch total loss 6.33589935\n",
      "Trained batch 3813 batch loss 6.97868395 epoch total loss 6.33606815\n",
      "Trained batch 3814 batch loss 4.81399584 epoch total loss 6.33566904\n",
      "Trained batch 3815 batch loss 5.36663675 epoch total loss 6.33541536\n",
      "Trained batch 3816 batch loss 5.9075346 epoch total loss 6.33530331\n",
      "Trained batch 3817 batch loss 6.06906748 epoch total loss 6.33523321\n",
      "Trained batch 3818 batch loss 5.8062191 epoch total loss 6.33509493\n",
      "Trained batch 3819 batch loss 4.96884108 epoch total loss 6.3347373\n",
      "Trained batch 3820 batch loss 7.20567703 epoch total loss 6.33496523\n",
      "Trained batch 3821 batch loss 6.1355176 epoch total loss 6.33491278\n",
      "Trained batch 3822 batch loss 5.7575388 epoch total loss 6.33476162\n",
      "Trained batch 3823 batch loss 7.38367939 epoch total loss 6.3350358\n",
      "Trained batch 3824 batch loss 6.51215649 epoch total loss 6.33508205\n",
      "Trained batch 3825 batch loss 6.16832924 epoch total loss 6.33503819\n",
      "Trained batch 3826 batch loss 6.40300512 epoch total loss 6.33505583\n",
      "Trained batch 3827 batch loss 6.34644508 epoch total loss 6.33505869\n",
      "Trained batch 3828 batch loss 7.19171238 epoch total loss 6.33528233\n",
      "Trained batch 3829 batch loss 6.57563925 epoch total loss 6.33534527\n",
      "Trained batch 3830 batch loss 6.39520645 epoch total loss 6.33536053\n",
      "Trained batch 3831 batch loss 6.60306644 epoch total loss 6.33543062\n",
      "Trained batch 3832 batch loss 6.970788 epoch total loss 6.33559656\n",
      "Trained batch 3833 batch loss 6.31328 epoch total loss 6.33559036\n",
      "Trained batch 3834 batch loss 5.7778964 epoch total loss 6.33544493\n",
      "Trained batch 3835 batch loss 6.06274462 epoch total loss 6.33537388\n",
      "Trained batch 3836 batch loss 6.69025421 epoch total loss 6.33546591\n",
      "Trained batch 3837 batch loss 6.34395409 epoch total loss 6.33546829\n",
      "Trained batch 3838 batch loss 6.33027267 epoch total loss 6.33546686\n",
      "Trained batch 3839 batch loss 6.435112 epoch total loss 6.33549261\n",
      "Trained batch 3840 batch loss 6.23177719 epoch total loss 6.33546591\n",
      "Trained batch 3841 batch loss 6.18843746 epoch total loss 6.33542728\n",
      "Trained batch 3842 batch loss 6.67597389 epoch total loss 6.33551598\n",
      "Trained batch 3843 batch loss 6.44655371 epoch total loss 6.33554506\n",
      "Trained batch 3844 batch loss 6.0906167 epoch total loss 6.33548117\n",
      "Trained batch 3845 batch loss 6.65133286 epoch total loss 6.33556318\n",
      "Trained batch 3846 batch loss 6.32377434 epoch total loss 6.33556032\n",
      "Trained batch 3847 batch loss 6.40668869 epoch total loss 6.33557844\n",
      "Trained batch 3848 batch loss 6.57652807 epoch total loss 6.33564091\n",
      "Trained batch 3849 batch loss 6.41617489 epoch total loss 6.33566189\n",
      "Trained batch 3850 batch loss 6.39850235 epoch total loss 6.3356781\n",
      "Trained batch 3851 batch loss 6.50500107 epoch total loss 6.33572245\n",
      "Trained batch 3852 batch loss 6.35655308 epoch total loss 6.33572817\n",
      "Trained batch 3853 batch loss 6.21379375 epoch total loss 6.33569622\n",
      "Trained batch 3854 batch loss 6.22618198 epoch total loss 6.33566809\n",
      "Trained batch 3855 batch loss 6.85966 epoch total loss 6.33580351\n",
      "Trained batch 3856 batch loss 6.51247454 epoch total loss 6.33584929\n",
      "Trained batch 3857 batch loss 6.58170795 epoch total loss 6.33591318\n",
      "Trained batch 3858 batch loss 6.23125696 epoch total loss 6.335886\n",
      "Trained batch 3859 batch loss 6.35998583 epoch total loss 6.33589172\n",
      "Trained batch 3860 batch loss 6.28358698 epoch total loss 6.33587837\n",
      "Trained batch 3861 batch loss 5.9912672 epoch total loss 6.3357892\n",
      "Trained batch 3862 batch loss 6.47424221 epoch total loss 6.33582544\n",
      "Trained batch 3863 batch loss 6.28717899 epoch total loss 6.33581257\n",
      "Trained batch 3864 batch loss 6.21913433 epoch total loss 6.33578253\n",
      "Trained batch 3865 batch loss 6.10167456 epoch total loss 6.33572149\n",
      "Trained batch 3866 batch loss 6.63916922 epoch total loss 6.3358\n",
      "Trained batch 3867 batch loss 6.45818043 epoch total loss 6.33583212\n",
      "Trained batch 3868 batch loss 5.95324898 epoch total loss 6.33573294\n",
      "Trained batch 3869 batch loss 5.96970177 epoch total loss 6.33563805\n",
      "Trained batch 3870 batch loss 6.89521551 epoch total loss 6.33578253\n",
      "Trained batch 3871 batch loss 6.0693512 epoch total loss 6.33571386\n",
      "Trained batch 3872 batch loss 6.71649551 epoch total loss 6.33581257\n",
      "Trained batch 3873 batch loss 6.0696888 epoch total loss 6.3357439\n",
      "Trained batch 3874 batch loss 6.14327717 epoch total loss 6.33569384\n",
      "Trained batch 3875 batch loss 6.13363 epoch total loss 6.33564186\n",
      "Trained batch 3876 batch loss 6.75938 epoch total loss 6.33575106\n",
      "Trained batch 3877 batch loss 6.29367638 epoch total loss 6.33574\n",
      "Trained batch 3878 batch loss 6.35432053 epoch total loss 6.33574438\n",
      "Trained batch 3879 batch loss 5.9909935 epoch total loss 6.33565569\n",
      "Trained batch 3880 batch loss 6.22448111 epoch total loss 6.33562708\n",
      "Trained batch 3881 batch loss 7.14590359 epoch total loss 6.33583593\n",
      "Trained batch 3882 batch loss 7.00830078 epoch total loss 6.33600903\n",
      "Trained batch 3883 batch loss 6.11300564 epoch total loss 6.33595181\n",
      "Trained batch 3884 batch loss 6.98993587 epoch total loss 6.33612\n",
      "Trained batch 3885 batch loss 6.20714951 epoch total loss 6.33608675\n",
      "Trained batch 3886 batch loss 7.72813749 epoch total loss 6.33644533\n",
      "Trained batch 3887 batch loss 6.07264328 epoch total loss 6.33637714\n",
      "Trained batch 3888 batch loss 5.87757301 epoch total loss 6.33625889\n",
      "Trained batch 3889 batch loss 7.13751078 epoch total loss 6.33646488\n",
      "Trained batch 3890 batch loss 6.09179497 epoch total loss 6.33640194\n",
      "Trained batch 3891 batch loss 6.25732613 epoch total loss 6.33638191\n",
      "Trained batch 3892 batch loss 6.33512402 epoch total loss 6.33638144\n",
      "Trained batch 3893 batch loss 6.04768562 epoch total loss 6.33630705\n",
      "Trained batch 3894 batch loss 6.23817635 epoch total loss 6.33628225\n",
      "Trained batch 3895 batch loss 6.07129622 epoch total loss 6.33621407\n",
      "Trained batch 3896 batch loss 6.14434147 epoch total loss 6.33616495\n",
      "Trained batch 3897 batch loss 6.38085747 epoch total loss 6.3361764\n",
      "Trained batch 3898 batch loss 6.2550807 epoch total loss 6.33615589\n",
      "Trained batch 3899 batch loss 5.93343496 epoch total loss 6.33605289\n",
      "Trained batch 3900 batch loss 6.23033714 epoch total loss 6.33602571\n",
      "Trained batch 3901 batch loss 6.05408382 epoch total loss 6.33595371\n",
      "Trained batch 3902 batch loss 6.3345685 epoch total loss 6.33595324\n",
      "Trained batch 3903 batch loss 5.84454536 epoch total loss 6.33582687\n",
      "Trained batch 3904 batch loss 6.36021328 epoch total loss 6.33583307\n",
      "Trained batch 3905 batch loss 6.12406158 epoch total loss 6.33577871\n",
      "Trained batch 3906 batch loss 6.08123446 epoch total loss 6.33571386\n",
      "Trained batch 3907 batch loss 6.06361914 epoch total loss 6.33564472\n",
      "Trained batch 3908 batch loss 6.88142109 epoch total loss 6.33578396\n",
      "Trained batch 3909 batch loss 7.37635851 epoch total loss 6.33605051\n",
      "Trained batch 3910 batch loss 6.56235695 epoch total loss 6.33610821\n",
      "Trained batch 3911 batch loss 6.53919649 epoch total loss 6.33616\n",
      "Trained batch 3912 batch loss 6.35665464 epoch total loss 6.33616543\n",
      "Trained batch 3913 batch loss 6.32288933 epoch total loss 6.33616209\n",
      "Trained batch 3914 batch loss 5.92879486 epoch total loss 6.33605814\n",
      "Trained batch 3915 batch loss 6.55643892 epoch total loss 6.33611441\n",
      "Trained batch 3916 batch loss 5.75298691 epoch total loss 6.33596611\n",
      "Trained batch 3917 batch loss 6.42177248 epoch total loss 6.33598804\n",
      "Trained batch 3918 batch loss 6.10528564 epoch total loss 6.33592892\n",
      "Trained batch 3919 batch loss 6.53266239 epoch total loss 6.33597946\n",
      "Trained batch 3920 batch loss 6.0133419 epoch total loss 6.33589697\n",
      "Trained batch 3921 batch loss 6.05678129 epoch total loss 6.33582592\n",
      "Trained batch 3922 batch loss 6.27365685 epoch total loss 6.33581\n",
      "Trained batch 3923 batch loss 6.1153717 epoch total loss 6.33575392\n",
      "Trained batch 3924 batch loss 6.09103537 epoch total loss 6.33569145\n",
      "Trained batch 3925 batch loss 6.38789082 epoch total loss 6.33570528\n",
      "Trained batch 3926 batch loss 6.17851067 epoch total loss 6.33566475\n",
      "Trained batch 3927 batch loss 5.85666704 epoch total loss 6.33554316\n",
      "Trained batch 3928 batch loss 6.0688839 epoch total loss 6.33547497\n",
      "Trained batch 3929 batch loss 5.82613516 epoch total loss 6.33534527\n",
      "Trained batch 3930 batch loss 6.564713 epoch total loss 6.33540392\n",
      "Trained batch 3931 batch loss 6.16699028 epoch total loss 6.33536053\n",
      "Trained batch 3932 batch loss 6.30962181 epoch total loss 6.33535433\n",
      "Trained batch 3933 batch loss 6.06303453 epoch total loss 6.33528519\n",
      "Trained batch 3934 batch loss 6.2936573 epoch total loss 6.33527422\n",
      "Trained batch 3935 batch loss 6.24898148 epoch total loss 6.33525181\n",
      "Trained batch 3936 batch loss 6.51963043 epoch total loss 6.33529902\n",
      "Trained batch 3937 batch loss 6.32012796 epoch total loss 6.3352952\n",
      "Trained batch 3938 batch loss 5.99623299 epoch total loss 6.33520889\n",
      "Trained batch 3939 batch loss 6.3346529 epoch total loss 6.33520842\n",
      "Trained batch 3940 batch loss 6.2388382 epoch total loss 6.3351841\n",
      "Trained batch 3941 batch loss 6.36633968 epoch total loss 6.3351922\n",
      "Trained batch 3942 batch loss 6.07813692 epoch total loss 6.33512688\n",
      "Trained batch 3943 batch loss 6.1758914 epoch total loss 6.33508635\n",
      "Trained batch 3944 batch loss 6.02466345 epoch total loss 6.33500814\n",
      "Trained batch 3945 batch loss 5.89769554 epoch total loss 6.33489752\n",
      "Trained batch 3946 batch loss 5.97835636 epoch total loss 6.33480692\n",
      "Trained batch 3947 batch loss 5.22459936 epoch total loss 6.33452559\n",
      "Trained batch 3948 batch loss 6.18156862 epoch total loss 6.33448696\n",
      "Trained batch 3949 batch loss 6.01266384 epoch total loss 6.33440542\n",
      "Trained batch 3950 batch loss 6.16841698 epoch total loss 6.33436298\n",
      "Trained batch 3951 batch loss 6.78046465 epoch total loss 6.33447647\n",
      "Trained batch 3952 batch loss 6.10088062 epoch total loss 6.33441734\n",
      "Trained batch 3953 batch loss 5.58899307 epoch total loss 6.33422899\n",
      "Trained batch 3954 batch loss 6.47264814 epoch total loss 6.3342638\n",
      "Trained batch 3955 batch loss 6.44755554 epoch total loss 6.33429241\n",
      "Trained batch 3956 batch loss 6.3049 epoch total loss 6.33428526\n",
      "Trained batch 3957 batch loss 6.31834793 epoch total loss 6.33428097\n",
      "Trained batch 3958 batch loss 6.25020552 epoch total loss 6.33425951\n",
      "Trained batch 3959 batch loss 6.5114131 epoch total loss 6.33430433\n",
      "Trained batch 3960 batch loss 6.76428795 epoch total loss 6.33441305\n",
      "Trained batch 3961 batch loss 6.08205891 epoch total loss 6.33434916\n",
      "Trained batch 3962 batch loss 6.22952747 epoch total loss 6.33432293\n",
      "Trained batch 3963 batch loss 5.76897 epoch total loss 6.33418036\n",
      "Trained batch 3964 batch loss 6.28935099 epoch total loss 6.33416891\n",
      "Trained batch 3965 batch loss 6.31156206 epoch total loss 6.33416367\n",
      "Trained batch 3966 batch loss 6.4611249 epoch total loss 6.33419561\n",
      "Trained batch 3967 batch loss 6.18839359 epoch total loss 6.3341589\n",
      "Trained batch 3968 batch loss 6.81515741 epoch total loss 6.33427954\n",
      "Trained batch 3969 batch loss 6.50106382 epoch total loss 6.33432198\n",
      "Trained batch 3970 batch loss 6.17837143 epoch total loss 6.3342824\n",
      "Trained batch 3971 batch loss 5.92079639 epoch total loss 6.33417797\n",
      "Trained batch 3972 batch loss 6.31844807 epoch total loss 6.33417416\n",
      "Trained batch 3973 batch loss 6.35214233 epoch total loss 6.33417845\n",
      "Trained batch 3974 batch loss 5.42410564 epoch total loss 6.33394957\n",
      "Trained batch 3975 batch loss 5.95820522 epoch total loss 6.33385515\n",
      "Trained batch 3976 batch loss 6.51181746 epoch total loss 6.3339\n",
      "Trained batch 3977 batch loss 7.62307072 epoch total loss 6.33422422\n",
      "Trained batch 3978 batch loss 5.74189663 epoch total loss 6.33407497\n",
      "Trained batch 3979 batch loss 6.70933342 epoch total loss 6.33416939\n",
      "Trained batch 3980 batch loss 6.67400122 epoch total loss 6.33425474\n",
      "Trained batch 3981 batch loss 5.70799637 epoch total loss 6.33409739\n",
      "Trained batch 3982 batch loss 6.99524212 epoch total loss 6.33426332\n",
      "Trained batch 3983 batch loss 5.42789 epoch total loss 6.33403587\n",
      "Trained batch 3984 batch loss 6.08342 epoch total loss 6.33397293\n",
      "Trained batch 3985 batch loss 5.58574 epoch total loss 6.33378553\n",
      "Trained batch 3986 batch loss 6.96910858 epoch total loss 6.3339448\n",
      "Trained batch 3987 batch loss 6.66491318 epoch total loss 6.33402729\n",
      "Trained batch 3988 batch loss 6.74966764 epoch total loss 6.33413172\n",
      "Trained batch 3989 batch loss 5.2879715 epoch total loss 6.33386946\n",
      "Trained batch 3990 batch loss 6.6212492 epoch total loss 6.33394146\n",
      "Trained batch 3991 batch loss 7.3271265 epoch total loss 6.33419\n",
      "Trained batch 3992 batch loss 7.04958344 epoch total loss 6.33436871\n",
      "Trained batch 3993 batch loss 6.71121168 epoch total loss 6.33446312\n",
      "Trained batch 3994 batch loss 6.73001575 epoch total loss 6.3345623\n",
      "Trained batch 3995 batch loss 6.85694218 epoch total loss 6.33469343\n",
      "Trained batch 3996 batch loss 6.64606619 epoch total loss 6.33477116\n",
      "Trained batch 3997 batch loss 6.82199287 epoch total loss 6.33489323\n",
      "Trained batch 3998 batch loss 6.53600693 epoch total loss 6.33494329\n",
      "Trained batch 3999 batch loss 6.4292 epoch total loss 6.33496714\n",
      "Trained batch 4000 batch loss 7.00967312 epoch total loss 6.33513594\n",
      "Trained batch 4001 batch loss 6.72795963 epoch total loss 6.33523417\n",
      "Trained batch 4002 batch loss 6.94164944 epoch total loss 6.33538532\n",
      "Trained batch 4003 batch loss 6.36414194 epoch total loss 6.33539248\n",
      "Trained batch 4004 batch loss 5.3868804 epoch total loss 6.33515549\n",
      "Trained batch 4005 batch loss 5.51199532 epoch total loss 6.33495\n",
      "Trained batch 4006 batch loss 5.74581671 epoch total loss 6.3348031\n",
      "Trained batch 4007 batch loss 6.0447135 epoch total loss 6.33473063\n",
      "Trained batch 4008 batch loss 5.29265118 epoch total loss 6.33447075\n",
      "Trained batch 4009 batch loss 6.80589581 epoch total loss 6.33458853\n",
      "Trained batch 4010 batch loss 6.43815804 epoch total loss 6.33461428\n",
      "Trained batch 4011 batch loss 6.58291483 epoch total loss 6.33467579\n",
      "Trained batch 4012 batch loss 7.12394238 epoch total loss 6.33487225\n",
      "Trained batch 4013 batch loss 5.97714281 epoch total loss 6.33478308\n",
      "Trained batch 4014 batch loss 6.50381851 epoch total loss 6.33482504\n",
      "Trained batch 4015 batch loss 6.02325487 epoch total loss 6.33474779\n",
      "Trained batch 4016 batch loss 6.25753 epoch total loss 6.33472824\n",
      "Trained batch 4017 batch loss 5.63056374 epoch total loss 6.33455324\n",
      "Trained batch 4018 batch loss 6.40036 epoch total loss 6.33456945\n",
      "Trained batch 4019 batch loss 6.48834229 epoch total loss 6.33460808\n",
      "Trained batch 4020 batch loss 5.80010748 epoch total loss 6.33447504\n",
      "Trained batch 4021 batch loss 6.93747807 epoch total loss 6.33462524\n",
      "Trained batch 4022 batch loss 6.24312401 epoch total loss 6.33460188\n",
      "Trained batch 4023 batch loss 6.57745647 epoch total loss 6.33466244\n",
      "Trained batch 4024 batch loss 6.63850307 epoch total loss 6.33473825\n",
      "Trained batch 4025 batch loss 6.38607502 epoch total loss 6.33475113\n",
      "Trained batch 4026 batch loss 6.30264378 epoch total loss 6.33474302\n",
      "Trained batch 4027 batch loss 6.77089119 epoch total loss 6.33485174\n",
      "Trained batch 4028 batch loss 6.44338131 epoch total loss 6.33487844\n",
      "Trained batch 4029 batch loss 6.459373 epoch total loss 6.33490944\n",
      "Trained batch 4030 batch loss 6.03997231 epoch total loss 6.33483601\n",
      "Trained batch 4031 batch loss 6.11455584 epoch total loss 6.33478165\n",
      "Trained batch 4032 batch loss 6.56431484 epoch total loss 6.33483839\n",
      "Trained batch 4033 batch loss 6.56857157 epoch total loss 6.33489609\n",
      "Trained batch 4034 batch loss 5.96642303 epoch total loss 6.33480501\n",
      "Trained batch 4035 batch loss 6.34068823 epoch total loss 6.33480644\n",
      "Trained batch 4036 batch loss 6.7063303 epoch total loss 6.33489847\n",
      "Trained batch 4037 batch loss 6.67856884 epoch total loss 6.33498335\n",
      "Trained batch 4038 batch loss 6.06211948 epoch total loss 6.33491611\n",
      "Trained batch 4039 batch loss 6.48755646 epoch total loss 6.33495378\n",
      "Trained batch 4040 batch loss 6.16326284 epoch total loss 6.33491182\n",
      "Trained batch 4041 batch loss 6.01161861 epoch total loss 6.33483171\n",
      "Trained batch 4042 batch loss 6.67489958 epoch total loss 6.33491611\n",
      "Trained batch 4043 batch loss 6.69133282 epoch total loss 6.33500433\n",
      "Trained batch 4044 batch loss 6.16771364 epoch total loss 6.33496284\n",
      "Trained batch 4045 batch loss 6.68375874 epoch total loss 6.33504915\n",
      "Trained batch 4046 batch loss 6.50311375 epoch total loss 6.33509064\n",
      "Trained batch 4047 batch loss 5.52933073 epoch total loss 6.3348918\n",
      "Trained batch 4048 batch loss 6.542099 epoch total loss 6.33494329\n",
      "Trained batch 4049 batch loss 6.10643673 epoch total loss 6.33488655\n",
      "Trained batch 4050 batch loss 6.47779894 epoch total loss 6.33492184\n",
      "Trained batch 4051 batch loss 6.5605669 epoch total loss 6.33497763\n",
      "Trained batch 4052 batch loss 5.89637327 epoch total loss 6.33486938\n",
      "Trained batch 4053 batch loss 6.4631443 epoch total loss 6.33490086\n",
      "Trained batch 4054 batch loss 6.18555546 epoch total loss 6.33486414\n",
      "Trained batch 4055 batch loss 6.04443693 epoch total loss 6.33479261\n",
      "Trained batch 4056 batch loss 6.14728355 epoch total loss 6.33474636\n",
      "Trained batch 4057 batch loss 6.37185669 epoch total loss 6.33475494\n",
      "Trained batch 4058 batch loss 5.51084042 epoch total loss 6.33455229\n",
      "Trained batch 4059 batch loss 5.10305786 epoch total loss 6.33424902\n",
      "Trained batch 4060 batch loss 5.3663373 epoch total loss 6.3340106\n",
      "Trained batch 4061 batch loss 5.55366135 epoch total loss 6.33381844\n",
      "Trained batch 4062 batch loss 5.89875603 epoch total loss 6.33371115\n",
      "Trained batch 4063 batch loss 5.96718311 epoch total loss 6.33362103\n",
      "Trained batch 4064 batch loss 5.92288637 epoch total loss 6.33352\n",
      "Trained batch 4065 batch loss 5.49543858 epoch total loss 6.33331394\n",
      "Trained batch 4066 batch loss 5.24031878 epoch total loss 6.33304548\n",
      "Trained batch 4067 batch loss 4.92029762 epoch total loss 6.33269787\n",
      "Trained batch 4068 batch loss 4.7490654 epoch total loss 6.33230877\n",
      "Trained batch 4069 batch loss 5.28512669 epoch total loss 6.33205128\n",
      "Trained batch 4070 batch loss 5.87059975 epoch total loss 6.33193827\n",
      "Trained batch 4071 batch loss 6.86005926 epoch total loss 6.33206749\n",
      "Trained batch 4072 batch loss 6.11256123 epoch total loss 6.33201408\n",
      "Trained batch 4073 batch loss 4.89157295 epoch total loss 6.33166027\n",
      "Trained batch 4074 batch loss 5.12746906 epoch total loss 6.33136415\n",
      "Trained batch 4075 batch loss 4.9337244 epoch total loss 6.33102131\n",
      "Trained batch 4076 batch loss 4.75345135 epoch total loss 6.33063459\n",
      "Trained batch 4077 batch loss 5.35493 epoch total loss 6.33039522\n",
      "Trained batch 4078 batch loss 4.52399492 epoch total loss 6.32995224\n",
      "Trained batch 4079 batch loss 4.8206377 epoch total loss 6.32958221\n",
      "Trained batch 4080 batch loss 5.47429 epoch total loss 6.32937241\n",
      "Trained batch 4081 batch loss 5.17454624 epoch total loss 6.32908916\n",
      "Trained batch 4082 batch loss 6.18725967 epoch total loss 6.32905483\n",
      "Trained batch 4083 batch loss 5.90215111 epoch total loss 6.32895\n",
      "Trained batch 4084 batch loss 5.81924391 epoch total loss 6.328825\n",
      "Trained batch 4085 batch loss 5.59378481 epoch total loss 6.32864523\n",
      "Trained batch 4086 batch loss 6.19323826 epoch total loss 6.32861185\n",
      "Trained batch 4087 batch loss 5.65581274 epoch total loss 6.32844734\n",
      "Trained batch 4088 batch loss 5.89379835 epoch total loss 6.32834148\n",
      "Trained batch 4089 batch loss 5.59240913 epoch total loss 6.32816124\n",
      "Trained batch 4090 batch loss 5.45540905 epoch total loss 6.32794762\n",
      "Trained batch 4091 batch loss 5.23305273 epoch total loss 6.32768\n",
      "Trained batch 4092 batch loss 5.17265224 epoch total loss 6.32739782\n",
      "Trained batch 4093 batch loss 5.06729889 epoch total loss 6.32708931\n",
      "Trained batch 4094 batch loss 4.90186453 epoch total loss 6.3267417\n",
      "Trained batch 4095 batch loss 4.97281075 epoch total loss 6.32641077\n",
      "Trained batch 4096 batch loss 4.89251518 epoch total loss 6.32606077\n",
      "Trained batch 4097 batch loss 4.83549738 epoch total loss 6.32569695\n",
      "Trained batch 4098 batch loss 4.55788803 epoch total loss 6.32526588\n",
      "Trained batch 4099 batch loss 5.0406065 epoch total loss 6.3249526\n",
      "Trained batch 4100 batch loss 5.07020283 epoch total loss 6.32464647\n",
      "Trained batch 4101 batch loss 4.97024918 epoch total loss 6.3243165\n",
      "Trained batch 4102 batch loss 4.69317818 epoch total loss 6.32391882\n",
      "Trained batch 4103 batch loss 4.56288242 epoch total loss 6.32348967\n",
      "Trained batch 4104 batch loss 4.6708436 epoch total loss 6.32308674\n",
      "Trained batch 4105 batch loss 4.59857178 epoch total loss 6.32266617\n",
      "Trained batch 4106 batch loss 4.5626545 epoch total loss 6.32223749\n",
      "Trained batch 4107 batch loss 4.76595926 epoch total loss 6.32185841\n",
      "Trained batch 4108 batch loss 4.83694792 epoch total loss 6.32149744\n",
      "Trained batch 4109 batch loss 5.00840044 epoch total loss 6.32117748\n",
      "Trained batch 4110 batch loss 4.84128952 epoch total loss 6.32081747\n",
      "Trained batch 4111 batch loss 4.87369823 epoch total loss 6.32046556\n",
      "Trained batch 4112 batch loss 4.6914711 epoch total loss 6.32006931\n",
      "Trained batch 4113 batch loss 4.6779232 epoch total loss 6.31967\n",
      "Trained batch 4114 batch loss 4.65195942 epoch total loss 6.31926489\n",
      "Trained batch 4115 batch loss 4.73183584 epoch total loss 6.31887913\n",
      "Trained batch 4116 batch loss 4.62014484 epoch total loss 6.31846666\n",
      "Trained batch 4117 batch loss 4.53508043 epoch total loss 6.3180337\n",
      "Trained batch 4118 batch loss 4.12262487 epoch total loss 6.31750059\n",
      "Trained batch 4119 batch loss 4.41872883 epoch total loss 6.31703949\n",
      "Trained batch 4120 batch loss 4.95706844 epoch total loss 6.31670904\n",
      "Trained batch 4121 batch loss 4.02070713 epoch total loss 6.3161521\n",
      "Trained batch 4122 batch loss 4.81176662 epoch total loss 6.31578732\n",
      "Trained batch 4123 batch loss 5.97089672 epoch total loss 6.31570387\n",
      "Trained batch 4124 batch loss 7.27554369 epoch total loss 6.31593657\n",
      "Trained batch 4125 batch loss 7.34385872 epoch total loss 6.31618547\n",
      "Trained batch 4126 batch loss 7.06184959 epoch total loss 6.31636667\n",
      "Trained batch 4127 batch loss 7.66103268 epoch total loss 6.31669188\n",
      "Trained batch 4128 batch loss 7.30922 epoch total loss 6.3169322\n",
      "Trained batch 4129 batch loss 7.24244785 epoch total loss 6.31715631\n",
      "Trained batch 4130 batch loss 6.69170761 epoch total loss 6.31724691\n",
      "Trained batch 4131 batch loss 7.16407967 epoch total loss 6.31745195\n",
      "Trained batch 4132 batch loss 6.71874809 epoch total loss 6.31754923\n",
      "Trained batch 4133 batch loss 6.41322041 epoch total loss 6.31757259\n",
      "Trained batch 4134 batch loss 6.35040188 epoch total loss 6.31758\n",
      "Trained batch 4135 batch loss 6.56984758 epoch total loss 6.31764126\n",
      "Trained batch 4136 batch loss 6.36061573 epoch total loss 6.31765175\n",
      "Trained batch 4137 batch loss 6.8979907 epoch total loss 6.31779242\n",
      "Trained batch 4138 batch loss 6.22946453 epoch total loss 6.31777096\n",
      "Trained batch 4139 batch loss 6.48964787 epoch total loss 6.31781244\n",
      "Trained batch 4140 batch loss 6.26567888 epoch total loss 6.3178\n",
      "Trained batch 4141 batch loss 6.70856 epoch total loss 6.31789446\n",
      "Trained batch 4142 batch loss 6.40877867 epoch total loss 6.31791592\n",
      "Trained batch 4143 batch loss 6.54998922 epoch total loss 6.31797218\n",
      "Trained batch 4144 batch loss 6.92534924 epoch total loss 6.31811905\n",
      "Trained batch 4145 batch loss 6.15176821 epoch total loss 6.31807899\n",
      "Trained batch 4146 batch loss 6.1663537 epoch total loss 6.31804228\n",
      "Trained batch 4147 batch loss 6.29113531 epoch total loss 6.3180356\n",
      "Trained batch 4148 batch loss 5.23381376 epoch total loss 6.31777477\n",
      "Trained batch 4149 batch loss 6.49216938 epoch total loss 6.31781673\n",
      "Trained batch 4150 batch loss 6.5486412 epoch total loss 6.31787205\n",
      "Trained batch 4151 batch loss 6.67066288 epoch total loss 6.31795692\n",
      "Trained batch 4152 batch loss 6.9445715 epoch total loss 6.31810808\n",
      "Trained batch 4153 batch loss 5.42259884 epoch total loss 6.31789255\n",
      "Trained batch 4154 batch loss 5.64215183 epoch total loss 6.31773\n",
      "Trained batch 4155 batch loss 5.94366503 epoch total loss 6.31764\n",
      "Trained batch 4156 batch loss 4.9769268 epoch total loss 6.31731701\n",
      "Trained batch 4157 batch loss 6.01522589 epoch total loss 6.31724453\n",
      "Trained batch 4158 batch loss 6.68129253 epoch total loss 6.31733227\n",
      "Trained batch 4159 batch loss 6.28602791 epoch total loss 6.31732416\n",
      "Trained batch 4160 batch loss 6.65034199 epoch total loss 6.31740427\n",
      "Trained batch 4161 batch loss 6.23015547 epoch total loss 6.31738329\n",
      "Trained batch 4162 batch loss 7.15378284 epoch total loss 6.31758451\n",
      "Trained batch 4163 batch loss 6.62521553 epoch total loss 6.31765842\n",
      "Trained batch 4164 batch loss 6.21300697 epoch total loss 6.31763315\n",
      "Trained batch 4165 batch loss 4.98288298 epoch total loss 6.31731272\n",
      "Trained batch 4166 batch loss 5.23284435 epoch total loss 6.31705236\n",
      "Trained batch 4167 batch loss 5.05727959 epoch total loss 6.31675\n",
      "Trained batch 4168 batch loss 5.75692415 epoch total loss 6.31661558\n",
      "Trained batch 4169 batch loss 5.59004402 epoch total loss 6.31644154\n",
      "Trained batch 4170 batch loss 5.34571 epoch total loss 6.31620836\n",
      "Trained batch 4171 batch loss 5.73354721 epoch total loss 6.31606913\n",
      "Trained batch 4172 batch loss 5.66950846 epoch total loss 6.31591415\n",
      "Trained batch 4173 batch loss 5.51855707 epoch total loss 6.31572342\n",
      "Trained batch 4174 batch loss 5.33483171 epoch total loss 6.31548834\n",
      "Trained batch 4175 batch loss 4.69520092 epoch total loss 6.3151\n",
      "Trained batch 4176 batch loss 4.89801407 epoch total loss 6.31476068\n",
      "Trained batch 4177 batch loss 4.90614414 epoch total loss 6.31442356\n",
      "Trained batch 4178 batch loss 4.86861372 epoch total loss 6.31407785\n",
      "Trained batch 4179 batch loss 4.77802038 epoch total loss 6.31371\n",
      "Trained batch 4180 batch loss 4.69625568 epoch total loss 6.31332302\n",
      "Trained batch 4181 batch loss 6.18050337 epoch total loss 6.31329107\n",
      "Trained batch 4182 batch loss 6.55086803 epoch total loss 6.31334782\n",
      "Trained batch 4183 batch loss 6.99661636 epoch total loss 6.31351089\n",
      "Trained batch 4184 batch loss 6.47084332 epoch total loss 6.31354856\n",
      "Trained batch 4185 batch loss 6.11220551 epoch total loss 6.3135\n",
      "Trained batch 4186 batch loss 6.18719625 epoch total loss 6.31347\n",
      "Trained batch 4187 batch loss 5.8999157 epoch total loss 6.31337118\n",
      "Trained batch 4188 batch loss 6.64987564 epoch total loss 6.31345177\n",
      "Trained batch 4189 batch loss 6.8103981 epoch total loss 6.3135705\n",
      "Trained batch 4190 batch loss 6.59467793 epoch total loss 6.31363726\n",
      "Trained batch 4191 batch loss 6.7830925 epoch total loss 6.31374931\n",
      "Trained batch 4192 batch loss 6.78138161 epoch total loss 6.31386089\n",
      "Trained batch 4193 batch loss 7.05307579 epoch total loss 6.31403732\n",
      "Trained batch 4194 batch loss 6.90134573 epoch total loss 6.31417704\n",
      "Trained batch 4195 batch loss 6.39184713 epoch total loss 6.31419563\n",
      "Trained batch 4196 batch loss 6.2562933 epoch total loss 6.3141818\n",
      "Trained batch 4197 batch loss 5.98417282 epoch total loss 6.31410313\n",
      "Trained batch 4198 batch loss 5.94165516 epoch total loss 6.31401443\n",
      "Trained batch 4199 batch loss 6.29784966 epoch total loss 6.31401\n",
      "Trained batch 4200 batch loss 6.41293716 epoch total loss 6.31403351\n",
      "Trained batch 4201 batch loss 6.40533924 epoch total loss 6.31405544\n",
      "Trained batch 4202 batch loss 5.84968376 epoch total loss 6.31394529\n",
      "Trained batch 4203 batch loss 6.63781118 epoch total loss 6.31402254\n",
      "Trained batch 4204 batch loss 6.35713387 epoch total loss 6.31403255\n",
      "Trained batch 4205 batch loss 6.22309399 epoch total loss 6.3140111\n",
      "Trained batch 4206 batch loss 5.62362909 epoch total loss 6.31384659\n",
      "Trained batch 4207 batch loss 6.46788025 epoch total loss 6.3138833\n",
      "Trained batch 4208 batch loss 6.06463909 epoch total loss 6.31382418\n",
      "Trained batch 4209 batch loss 6.05238247 epoch total loss 6.31376219\n",
      "Trained batch 4210 batch loss 6.74598408 epoch total loss 6.31386471\n",
      "Trained batch 4211 batch loss 6.54208469 epoch total loss 6.31391907\n",
      "Trained batch 4212 batch loss 6.16072559 epoch total loss 6.31388283\n",
      "Trained batch 4213 batch loss 6.81487083 epoch total loss 6.31400156\n",
      "Trained batch 4214 batch loss 6.13546371 epoch total loss 6.31395912\n",
      "Trained batch 4215 batch loss 6.32479095 epoch total loss 6.31396151\n",
      "Trained batch 4216 batch loss 6.43608856 epoch total loss 6.31399\n",
      "Trained batch 4217 batch loss 5.93525791 epoch total loss 6.31390047\n",
      "Trained batch 4218 batch loss 6.77314281 epoch total loss 6.31400967\n",
      "Trained batch 4219 batch loss 6.3785491 epoch total loss 6.31402493\n",
      "Trained batch 4220 batch loss 6.254 epoch total loss 6.31401062\n",
      "Trained batch 4221 batch loss 6.38233089 epoch total loss 6.31402683\n",
      "Trained batch 4222 batch loss 6.35594845 epoch total loss 6.31403685\n",
      "Trained batch 4223 batch loss 6.27606297 epoch total loss 6.31402779\n",
      "Trained batch 4224 batch loss 6.33986092 epoch total loss 6.31403351\n",
      "Trained batch 4225 batch loss 6.21552658 epoch total loss 6.31401\n",
      "Trained batch 4226 batch loss 6.1779 epoch total loss 6.3139782\n",
      "Trained batch 4227 batch loss 5.36018038 epoch total loss 6.31375217\n",
      "Trained batch 4228 batch loss 5.94314575 epoch total loss 6.31366444\n",
      "Trained batch 4229 batch loss 6.25133848 epoch total loss 6.31365\n",
      "Trained batch 4230 batch loss 6.38450575 epoch total loss 6.31366682\n",
      "Trained batch 4231 batch loss 6.78779602 epoch total loss 6.31377888\n",
      "Trained batch 4232 batch loss 6.61712742 epoch total loss 6.3138504\n",
      "Trained batch 4233 batch loss 6.26030731 epoch total loss 6.31383753\n",
      "Trained batch 4234 batch loss 6.43193817 epoch total loss 6.31386566\n",
      "Trained batch 4235 batch loss 6.67265129 epoch total loss 6.31395\n",
      "Trained batch 4236 batch loss 6.49793339 epoch total loss 6.31399345\n",
      "Trained batch 4237 batch loss 6.65245628 epoch total loss 6.31407309\n",
      "Trained batch 4238 batch loss 6.1732111 epoch total loss 6.31404\n",
      "Trained batch 4239 batch loss 6.55195522 epoch total loss 6.31409645\n",
      "Trained batch 4240 batch loss 6.26092052 epoch total loss 6.31408405\n",
      "Trained batch 4241 batch loss 6.83543158 epoch total loss 6.31420708\n",
      "Trained batch 4242 batch loss 5.78925514 epoch total loss 6.31408358\n",
      "Trained batch 4243 batch loss 6.26874542 epoch total loss 6.31407309\n",
      "Trained batch 4244 batch loss 6.29995346 epoch total loss 6.31406975\n",
      "Trained batch 4245 batch loss 6.85452843 epoch total loss 6.31419754\n",
      "Trained batch 4246 batch loss 6.15987873 epoch total loss 6.31416082\n",
      "Trained batch 4247 batch loss 6.52861309 epoch total loss 6.31421185\n",
      "Trained batch 4248 batch loss 6.26945925 epoch total loss 6.31420135\n",
      "Trained batch 4249 batch loss 6.16514206 epoch total loss 6.31416607\n",
      "Trained batch 4250 batch loss 6.0902977 epoch total loss 6.31411362\n",
      "Trained batch 4251 batch loss 6.41314697 epoch total loss 6.31413698\n",
      "Trained batch 4252 batch loss 6.06462955 epoch total loss 6.31407833\n",
      "Trained batch 4253 batch loss 5.84348774 epoch total loss 6.3139677\n",
      "Trained batch 4254 batch loss 6.01192951 epoch total loss 6.31389666\n",
      "Trained batch 4255 batch loss 6.10345554 epoch total loss 6.31384706\n",
      "Trained batch 4256 batch loss 5.91904497 epoch total loss 6.31375456\n",
      "Trained batch 4257 batch loss 6.15987349 epoch total loss 6.3137188\n",
      "Trained batch 4258 batch loss 6.14207697 epoch total loss 6.31367826\n",
      "Trained batch 4259 batch loss 6.71452904 epoch total loss 6.31377268\n",
      "Trained batch 4260 batch loss 5.84899044 epoch total loss 6.31366348\n",
      "Trained batch 4261 batch loss 5.96295547 epoch total loss 6.31358147\n",
      "Trained batch 4262 batch loss 5.82977057 epoch total loss 6.31346798\n",
      "Trained batch 4263 batch loss 5.92942476 epoch total loss 6.31337786\n",
      "Trained batch 4264 batch loss 6.2023654 epoch total loss 6.31335211\n",
      "Trained batch 4265 batch loss 6.14786339 epoch total loss 6.31331348\n",
      "Trained batch 4266 batch loss 6.57931805 epoch total loss 6.31337595\n",
      "Trained batch 4267 batch loss 6.73947668 epoch total loss 6.31347609\n",
      "Trained batch 4268 batch loss 7.01183224 epoch total loss 6.31363964\n",
      "Trained batch 4269 batch loss 6.53951359 epoch total loss 6.31369209\n",
      "Trained batch 4270 batch loss 6.70329332 epoch total loss 6.31378365\n",
      "Trained batch 4271 batch loss 6.21330881 epoch total loss 6.31376\n",
      "Trained batch 4272 batch loss 6.13729477 epoch total loss 6.31371832\n",
      "Trained batch 4273 batch loss 5.88818026 epoch total loss 6.31361914\n",
      "Trained batch 4274 batch loss 6.01047421 epoch total loss 6.31354809\n",
      "Trained batch 4275 batch loss 5.83363 epoch total loss 6.31343555\n",
      "Trained batch 4276 batch loss 7.83161163 epoch total loss 6.3137908\n",
      "Trained batch 4277 batch loss 8.17230511 epoch total loss 6.3142252\n",
      "Trained batch 4278 batch loss 7.72376919 epoch total loss 6.31455517\n",
      "Trained batch 4279 batch loss 5.88674259 epoch total loss 6.31445503\n",
      "Trained batch 4280 batch loss 5.92974138 epoch total loss 6.31436491\n",
      "Trained batch 4281 batch loss 5.75228 epoch total loss 6.31423378\n",
      "Trained batch 4282 batch loss 5.92921448 epoch total loss 6.31414366\n",
      "Trained batch 4283 batch loss 5.93920946 epoch total loss 6.3140564\n",
      "Trained batch 4284 batch loss 5.70039749 epoch total loss 6.31391335\n",
      "Trained batch 4285 batch loss 7.05480385 epoch total loss 6.31408644\n",
      "Trained batch 4286 batch loss 6.45177937 epoch total loss 6.31411839\n",
      "Trained batch 4287 batch loss 6.18221521 epoch total loss 6.31408739\n",
      "Trained batch 4288 batch loss 6.16300774 epoch total loss 6.31405163\n",
      "Trained batch 4289 batch loss 5.78784943 epoch total loss 6.31392908\n",
      "Trained batch 4290 batch loss 6.02167606 epoch total loss 6.31386089\n",
      "Trained batch 4291 batch loss 5.82418251 epoch total loss 6.31374693\n",
      "Trained batch 4292 batch loss 6.5390234 epoch total loss 6.31379938\n",
      "Trained batch 4293 batch loss 6.40838623 epoch total loss 6.31382132\n",
      "Trained batch 4294 batch loss 6.03638363 epoch total loss 6.31375694\n",
      "Trained batch 4295 batch loss 6.55381298 epoch total loss 6.31381273\n",
      "Trained batch 4296 batch loss 6.27289867 epoch total loss 6.31380367\n",
      "Trained batch 4297 batch loss 6.11560869 epoch total loss 6.31375742\n",
      "Trained batch 4298 batch loss 6.26416445 epoch total loss 6.3137455\n",
      "Trained batch 4299 batch loss 5.89443254 epoch total loss 6.31364822\n",
      "Trained batch 4300 batch loss 6.30692196 epoch total loss 6.31364632\n",
      "Trained batch 4301 batch loss 5.8857708 epoch total loss 6.31354713\n",
      "Trained batch 4302 batch loss 6.00432825 epoch total loss 6.31347513\n",
      "Trained batch 4303 batch loss 6.08125353 epoch total loss 6.31342125\n",
      "Trained batch 4304 batch loss 5.90913725 epoch total loss 6.31332731\n",
      "Trained batch 4305 batch loss 6.40256882 epoch total loss 6.31334782\n",
      "Trained batch 4306 batch loss 5.96622086 epoch total loss 6.31326723\n",
      "Trained batch 4307 batch loss 6.34408045 epoch total loss 6.31327438\n",
      "Trained batch 4308 batch loss 6.316576 epoch total loss 6.31327534\n",
      "Trained batch 4309 batch loss 6.08453655 epoch total loss 6.31322193\n",
      "Trained batch 4310 batch loss 6.03770065 epoch total loss 6.31315804\n",
      "Trained batch 4311 batch loss 6.05746555 epoch total loss 6.31309843\n",
      "Trained batch 4312 batch loss 5.89771366 epoch total loss 6.31300211\n",
      "Trained batch 4313 batch loss 5.97869 epoch total loss 6.31292486\n",
      "Trained batch 4314 batch loss 6.03390551 epoch total loss 6.31286\n",
      "Trained batch 4315 batch loss 6.57441473 epoch total loss 6.31292057\n",
      "Trained batch 4316 batch loss 6.20648575 epoch total loss 6.31289577\n",
      "Trained batch 4317 batch loss 6.05159664 epoch total loss 6.31283522\n",
      "Trained batch 4318 batch loss 6.16814899 epoch total loss 6.31280184\n",
      "Trained batch 4319 batch loss 6.37457848 epoch total loss 6.31281614\n",
      "Trained batch 4320 batch loss 6.05120182 epoch total loss 6.31275558\n",
      "Trained batch 4321 batch loss 5.99211645 epoch total loss 6.3126812\n",
      "Trained batch 4322 batch loss 5.80461597 epoch total loss 6.3125639\n",
      "Trained batch 4323 batch loss 6.97403145 epoch total loss 6.31271696\n",
      "Trained batch 4324 batch loss 6.3947444 epoch total loss 6.31273556\n",
      "Trained batch 4325 batch loss 6.20380592 epoch total loss 6.31271029\n",
      "Trained batch 4326 batch loss 6.16080952 epoch total loss 6.312675\n",
      "Trained batch 4327 batch loss 8.38317108 epoch total loss 6.31315374\n",
      "Trained batch 4328 batch loss 6.9223361 epoch total loss 6.31329441\n",
      "Trained batch 4329 batch loss 6.2587719 epoch total loss 6.31328154\n",
      "Trained batch 4330 batch loss 5.78077841 epoch total loss 6.31315851\n",
      "Trained batch 4331 batch loss 6.15786171 epoch total loss 6.31312275\n",
      "Trained batch 4332 batch loss 6.01591539 epoch total loss 6.31305408\n",
      "Trained batch 4333 batch loss 6.350914 epoch total loss 6.31306314\n",
      "Trained batch 4334 batch loss 6.39111423 epoch total loss 6.31308079\n",
      "Trained batch 4335 batch loss 6.19489717 epoch total loss 6.31305361\n",
      "Trained batch 4336 batch loss 6.15636539 epoch total loss 6.31301737\n",
      "Trained batch 4337 batch loss 6.02954102 epoch total loss 6.31295204\n",
      "Trained batch 4338 batch loss 6.23766899 epoch total loss 6.31293488\n",
      "Trained batch 4339 batch loss 5.64651775 epoch total loss 6.31278133\n",
      "Trained batch 4340 batch loss 6.13911819 epoch total loss 6.31274128\n",
      "Trained batch 4341 batch loss 6.2783 epoch total loss 6.31273317\n",
      "Trained batch 4342 batch loss 6.88559675 epoch total loss 6.31286478\n",
      "Trained batch 4343 batch loss 6.10839272 epoch total loss 6.31281757\n",
      "Trained batch 4344 batch loss 6.81089878 epoch total loss 6.31293201\n",
      "Trained batch 4345 batch loss 6.26055241 epoch total loss 6.31291962\n",
      "Trained batch 4346 batch loss 6.23593235 epoch total loss 6.31290245\n",
      "Trained batch 4347 batch loss 5.69552517 epoch total loss 6.31276035\n",
      "Trained batch 4348 batch loss 5.98899364 epoch total loss 6.31268549\n",
      "Trained batch 4349 batch loss 6.53152847 epoch total loss 6.31273556\n",
      "Trained batch 4350 batch loss 6.45374393 epoch total loss 6.31276798\n",
      "Trained batch 4351 batch loss 6.26571178 epoch total loss 6.31275702\n",
      "Trained batch 4352 batch loss 4.96322775 epoch total loss 6.31244707\n",
      "Trained batch 4353 batch loss 6.34685564 epoch total loss 6.31245518\n",
      "Trained batch 4354 batch loss 6.77991581 epoch total loss 6.31256247\n",
      "Trained batch 4355 batch loss 6.18753624 epoch total loss 6.31253386\n",
      "Trained batch 4356 batch loss 5.75349236 epoch total loss 6.31240559\n",
      "Trained batch 4357 batch loss 6.12577724 epoch total loss 6.31236219\n",
      "Trained batch 4358 batch loss 6.33438683 epoch total loss 6.31236744\n",
      "Trained batch 4359 batch loss 6.53136349 epoch total loss 6.31241751\n",
      "Trained batch 4360 batch loss 6.67701149 epoch total loss 6.31250143\n",
      "Trained batch 4361 batch loss 6.08363628 epoch total loss 6.31244898\n",
      "Trained batch 4362 batch loss 6.60802317 epoch total loss 6.31251669\n",
      "Trained batch 4363 batch loss 6.46655798 epoch total loss 6.31255198\n",
      "Trained batch 4364 batch loss 5.32944 epoch total loss 6.31232691\n",
      "Trained batch 4365 batch loss 6.55676651 epoch total loss 6.3123827\n",
      "Trained batch 4366 batch loss 6.33669567 epoch total loss 6.31238794\n",
      "Trained batch 4367 batch loss 5.87559891 epoch total loss 6.31228781\n",
      "Trained batch 4368 batch loss 6.20279932 epoch total loss 6.31226301\n",
      "Trained batch 4369 batch loss 5.96441 epoch total loss 6.31218338\n",
      "Trained batch 4370 batch loss 6.33834267 epoch total loss 6.31218958\n",
      "Trained batch 4371 batch loss 5.75489616 epoch total loss 6.31206226\n",
      "Trained batch 4372 batch loss 6.51088476 epoch total loss 6.31210756\n",
      "Trained batch 4373 batch loss 6.52530956 epoch total loss 6.31215668\n",
      "Trained batch 4374 batch loss 6.53186321 epoch total loss 6.31220675\n",
      "Trained batch 4375 batch loss 6.28355503 epoch total loss 6.3122\n",
      "Trained batch 4376 batch loss 6.52901363 epoch total loss 6.31224966\n",
      "Trained batch 4377 batch loss 6.50155783 epoch total loss 6.31229305\n",
      "Trained batch 4378 batch loss 6.38825035 epoch total loss 6.31231\n",
      "Trained batch 4379 batch loss 6.7543354 epoch total loss 6.31241131\n",
      "Trained batch 4380 batch loss 6.78628922 epoch total loss 6.31251955\n",
      "Trained batch 4381 batch loss 6.35124588 epoch total loss 6.31252861\n",
      "Trained batch 4382 batch loss 6.4539938 epoch total loss 6.31256056\n",
      "Trained batch 4383 batch loss 6.07999754 epoch total loss 6.31250763\n",
      "Trained batch 4384 batch loss 6.06556559 epoch total loss 6.31245136\n",
      "Trained batch 4385 batch loss 6.14705515 epoch total loss 6.31241369\n",
      "Trained batch 4386 batch loss 5.83097744 epoch total loss 6.31230354\n",
      "Trained batch 4387 batch loss 5.50955105 epoch total loss 6.31212091\n",
      "Trained batch 4388 batch loss 6.64567852 epoch total loss 6.31219673\n",
      "Trained batch 4389 batch loss 6.11093712 epoch total loss 6.31215096\n",
      "Trained batch 4390 batch loss 5.9126358 epoch total loss 6.31206\n",
      "Trained batch 4391 batch loss 6.56680202 epoch total loss 6.31211805\n",
      "Trained batch 4392 batch loss 6.24460602 epoch total loss 6.31210232\n",
      "Trained batch 4393 batch loss 5.99507236 epoch total loss 6.31203\n",
      "Trained batch 4394 batch loss 6.13568783 epoch total loss 6.31199\n",
      "Trained batch 4395 batch loss 6.39541149 epoch total loss 6.31200838\n",
      "Trained batch 4396 batch loss 6.06541538 epoch total loss 6.31195211\n",
      "Trained batch 4397 batch loss 6.08157635 epoch total loss 6.31189966\n",
      "Trained batch 4398 batch loss 6.02565527 epoch total loss 6.31183481\n",
      "Trained batch 4399 batch loss 5.91911697 epoch total loss 6.31174564\n",
      "Trained batch 4400 batch loss 6.10799026 epoch total loss 6.31169939\n",
      "Trained batch 4401 batch loss 6.51159 epoch total loss 6.31174469\n",
      "Trained batch 4402 batch loss 6.56993628 epoch total loss 6.31180334\n",
      "Trained batch 4403 batch loss 6.62023497 epoch total loss 6.31187344\n",
      "Trained batch 4404 batch loss 5.95226669 epoch total loss 6.31179237\n",
      "Trained batch 4405 batch loss 6.67467403 epoch total loss 6.31187439\n",
      "Trained batch 4406 batch loss 6.63240337 epoch total loss 6.31194735\n",
      "Trained batch 4407 batch loss 6.36606216 epoch total loss 6.31195927\n",
      "Trained batch 4408 batch loss 6.11353827 epoch total loss 6.31191444\n",
      "Trained batch 4409 batch loss 6.08593893 epoch total loss 6.31186295\n",
      "Trained batch 4410 batch loss 6.38220072 epoch total loss 6.31187916\n",
      "Trained batch 4411 batch loss 6.85143757 epoch total loss 6.31200123\n",
      "Trained batch 4412 batch loss 5.87936163 epoch total loss 6.31190348\n",
      "Trained batch 4413 batch loss 5.31091261 epoch total loss 6.3116765\n",
      "Trained batch 4414 batch loss 5.99682426 epoch total loss 6.31160498\n",
      "Trained batch 4415 batch loss 6.37761259 epoch total loss 6.31161976\n",
      "Trained batch 4416 batch loss 6.29677248 epoch total loss 6.31161642\n",
      "Trained batch 4417 batch loss 6.43700027 epoch total loss 6.31164503\n",
      "Trained batch 4418 batch loss 6.51025772 epoch total loss 6.31169\n",
      "Trained batch 4419 batch loss 6.44304 epoch total loss 6.31171942\n",
      "Trained batch 4420 batch loss 6.21697044 epoch total loss 6.31169796\n",
      "Trained batch 4421 batch loss 6.2281 epoch total loss 6.31167936\n",
      "Trained batch 4422 batch loss 6.32646656 epoch total loss 6.31168222\n",
      "Trained batch 4423 batch loss 6.25073195 epoch total loss 6.3116684\n",
      "Trained batch 4424 batch loss 5.78819656 epoch total loss 6.31155\n",
      "Trained batch 4425 batch loss 6.52877235 epoch total loss 6.31159973\n",
      "Trained batch 4426 batch loss 6.16298389 epoch total loss 6.31156588\n",
      "Trained batch 4427 batch loss 5.87203836 epoch total loss 6.31146622\n",
      "Trained batch 4428 batch loss 6.63689804 epoch total loss 6.31153965\n",
      "Trained batch 4429 batch loss 6.47118807 epoch total loss 6.31157589\n",
      "Trained batch 4430 batch loss 6.50783253 epoch total loss 6.31161976\n",
      "Trained batch 4431 batch loss 6.58487129 epoch total loss 6.31168127\n",
      "Trained batch 4432 batch loss 6.01921892 epoch total loss 6.31161547\n",
      "Trained batch 4433 batch loss 6.56478405 epoch total loss 6.31167269\n",
      "Trained batch 4434 batch loss 6.35819 epoch total loss 6.3116827\n",
      "Trained batch 4435 batch loss 6.14858055 epoch total loss 6.31164598\n",
      "Trained batch 4436 batch loss 6.05780554 epoch total loss 6.31158924\n",
      "Trained batch 4437 batch loss 6.09018517 epoch total loss 6.31153917\n",
      "Trained batch 4438 batch loss 5.35036469 epoch total loss 6.31132221\n",
      "Trained batch 4439 batch loss 5.09049892 epoch total loss 6.31104708\n",
      "Trained batch 4440 batch loss 6.00110149 epoch total loss 6.31097746\n",
      "Trained batch 4441 batch loss 6.3411274 epoch total loss 6.31098461\n",
      "Trained batch 4442 batch loss 4.89243889 epoch total loss 6.31066513\n",
      "Trained batch 4443 batch loss 5.85095739 epoch total loss 6.31056166\n",
      "Trained batch 4444 batch loss 6.47670412 epoch total loss 6.31059933\n",
      "Trained batch 4445 batch loss 6.28133488 epoch total loss 6.31059265\n",
      "Trained batch 4446 batch loss 6.58779907 epoch total loss 6.31065512\n",
      "Trained batch 4447 batch loss 6.84689617 epoch total loss 6.31077576\n",
      "Trained batch 4448 batch loss 6.77581072 epoch total loss 6.31088\n",
      "Trained batch 4449 batch loss 6.9353261 epoch total loss 6.31102037\n",
      "Trained batch 4450 batch loss 6.79492044 epoch total loss 6.31112909\n",
      "Trained batch 4451 batch loss 6.19906902 epoch total loss 6.3111043\n",
      "Trained batch 4452 batch loss 6.45741177 epoch total loss 6.31113672\n",
      "Trained batch 4453 batch loss 6.7143364 epoch total loss 6.3112278\n",
      "Trained batch 4454 batch loss 6.88830471 epoch total loss 6.31135702\n",
      "Trained batch 4455 batch loss 6.70125198 epoch total loss 6.31144476\n",
      "Trained batch 4456 batch loss 6.93352032 epoch total loss 6.31158447\n",
      "Trained batch 4457 batch loss 7.74896908 epoch total loss 6.31190681\n",
      "Trained batch 4458 batch loss 6.66363955 epoch total loss 6.31198549\n",
      "Trained batch 4459 batch loss 6.29649878 epoch total loss 6.31198215\n",
      "Trained batch 4460 batch loss 6.81164885 epoch total loss 6.31209469\n",
      "Trained batch 4461 batch loss 6.89068174 epoch total loss 6.31222439\n",
      "Trained batch 4462 batch loss 6.80116081 epoch total loss 6.31233358\n",
      "Trained batch 4463 batch loss 6.22078085 epoch total loss 6.31231308\n",
      "Trained batch 4464 batch loss 6.21693611 epoch total loss 6.31229162\n",
      "Trained batch 4465 batch loss 6.33049 epoch total loss 6.31229591\n",
      "Trained batch 4466 batch loss 6.04927731 epoch total loss 6.31223679\n",
      "Trained batch 4467 batch loss 7.42795372 epoch total loss 6.31248665\n",
      "Trained batch 4468 batch loss 6.72737598 epoch total loss 6.31257915\n",
      "Trained batch 4469 batch loss 7.31806803 epoch total loss 6.31280422\n",
      "Trained batch 4470 batch loss 7.04670143 epoch total loss 6.31296825\n",
      "Trained batch 4471 batch loss 6.23547935 epoch total loss 6.31295109\n",
      "Trained batch 4472 batch loss 6.82019329 epoch total loss 6.31306458\n",
      "Trained batch 4473 batch loss 6.90083218 epoch total loss 6.31319618\n",
      "Trained batch 4474 batch loss 7.45182085 epoch total loss 6.31345034\n",
      "Trained batch 4475 batch loss 7.2889843 epoch total loss 6.31366825\n",
      "Trained batch 4476 batch loss 7.85711765 epoch total loss 6.31401348\n",
      "Trained batch 4477 batch loss 7.18831635 epoch total loss 6.31420851\n",
      "Trained batch 4478 batch loss 6.61952448 epoch total loss 6.3142767\n",
      "Trained batch 4479 batch loss 7.4041853 epoch total loss 6.31452\n",
      "Trained batch 4480 batch loss 7.09059238 epoch total loss 6.31469297\n",
      "Trained batch 4481 batch loss 7.46526432 epoch total loss 6.31494951\n",
      "Trained batch 4482 batch loss 6.92515659 epoch total loss 6.31508589\n",
      "Trained batch 4483 batch loss 6.52834511 epoch total loss 6.31513357\n",
      "Trained batch 4484 batch loss 7.09873199 epoch total loss 6.31530857\n",
      "Trained batch 4485 batch loss 7.04656315 epoch total loss 6.31547165\n",
      "Trained batch 4486 batch loss 6.84603834 epoch total loss 6.31559\n",
      "Trained batch 4487 batch loss 6.84817934 epoch total loss 6.31570864\n",
      "Trained batch 4488 batch loss 6.62992668 epoch total loss 6.31577873\n",
      "Trained batch 4489 batch loss 6.50357914 epoch total loss 6.31582069\n",
      "Trained batch 4490 batch loss 6.97560024 epoch total loss 6.31596756\n",
      "Trained batch 4491 batch loss 6.15646362 epoch total loss 6.31593227\n",
      "Trained batch 4492 batch loss 6.67780161 epoch total loss 6.31601286\n",
      "Trained batch 4493 batch loss 6.90028238 epoch total loss 6.31614304\n",
      "Trained batch 4494 batch loss 6.56949 epoch total loss 6.3161993\n",
      "Trained batch 4495 batch loss 6.7298665 epoch total loss 6.31629133\n",
      "Trained batch 4496 batch loss 6.54217529 epoch total loss 6.31634188\n",
      "Trained batch 4497 batch loss 7.09315729 epoch total loss 6.31651497\n",
      "Trained batch 4498 batch loss 6.89896488 epoch total loss 6.31664419\n",
      "Trained batch 4499 batch loss 6.67631 epoch total loss 6.31672382\n",
      "Trained batch 4500 batch loss 7.05870152 epoch total loss 6.31688881\n",
      "Trained batch 4501 batch loss 7.00166798 epoch total loss 6.31704092\n",
      "Trained batch 4502 batch loss 7.20306778 epoch total loss 6.31723785\n",
      "Trained batch 4503 batch loss 6.33255 epoch total loss 6.31724119\n",
      "Trained batch 4504 batch loss 5.8068924 epoch total loss 6.3171277\n",
      "Trained batch 4505 batch loss 5.82294559 epoch total loss 6.31701803\n",
      "Trained batch 4506 batch loss 6.03122759 epoch total loss 6.31695461\n",
      "Trained batch 4507 batch loss 5.52459621 epoch total loss 6.31677914\n",
      "Trained batch 4508 batch loss 6.00185537 epoch total loss 6.31670904\n",
      "Trained batch 4509 batch loss 5.40873623 epoch total loss 6.31650782\n",
      "Trained batch 4510 batch loss 6.20504236 epoch total loss 6.31648302\n",
      "Trained batch 4511 batch loss 6.32727671 epoch total loss 6.3164854\n",
      "Trained batch 4512 batch loss 5.93894482 epoch total loss 6.31640196\n",
      "Trained batch 4513 batch loss 5.57521629 epoch total loss 6.31623793\n",
      "Trained batch 4514 batch loss 7.21652889 epoch total loss 6.31643724\n",
      "Trained batch 4515 batch loss 7.00311756 epoch total loss 6.31659\n",
      "Trained batch 4516 batch loss 6.95757818 epoch total loss 6.31673145\n",
      "Trained batch 4517 batch loss 6.58230686 epoch total loss 6.31679\n",
      "Trained batch 4518 batch loss 6.61173391 epoch total loss 6.31685543\n",
      "Trained batch 4519 batch loss 6.43907356 epoch total loss 6.31688261\n",
      "Trained batch 4520 batch loss 6.36267757 epoch total loss 6.31689262\n",
      "Trained batch 4521 batch loss 6.19425201 epoch total loss 6.31686544\n",
      "Trained batch 4522 batch loss 6.10847473 epoch total loss 6.31681967\n",
      "Trained batch 4523 batch loss 5.54368448 epoch total loss 6.31664848\n",
      "Trained batch 4524 batch loss 4.88524914 epoch total loss 6.31633186\n",
      "Trained batch 4525 batch loss 6.3413744 epoch total loss 6.31633759\n",
      "Trained batch 4526 batch loss 5.8609457 epoch total loss 6.31623697\n",
      "Trained batch 4527 batch loss 6.08009291 epoch total loss 6.316185\n",
      "Trained batch 4528 batch loss 5.750247 epoch total loss 6.31606\n",
      "Trained batch 4529 batch loss 4.76969099 epoch total loss 6.31571817\n",
      "Trained batch 4530 batch loss 6.51574898 epoch total loss 6.31576252\n",
      "Trained batch 4531 batch loss 6.61097622 epoch total loss 6.31582785\n",
      "Trained batch 4532 batch loss 6.54455423 epoch total loss 6.31587839\n",
      "Trained batch 4533 batch loss 6.54925585 epoch total loss 6.31593\n",
      "Trained batch 4534 batch loss 5.73721 epoch total loss 6.3158021\n",
      "Trained batch 4535 batch loss 6.86958122 epoch total loss 6.31592369\n",
      "Trained batch 4536 batch loss 6.36897516 epoch total loss 6.31593561\n",
      "Trained batch 4537 batch loss 5.39166975 epoch total loss 6.315732\n",
      "Trained batch 4538 batch loss 6.08032322 epoch total loss 6.31568\n",
      "Trained batch 4539 batch loss 6.46923447 epoch total loss 6.31571388\n",
      "Trained batch 4540 batch loss 5.09718704 epoch total loss 6.31544542\n",
      "Trained batch 4541 batch loss 7.02504826 epoch total loss 6.31560183\n",
      "Trained batch 4542 batch loss 7.49833393 epoch total loss 6.31586218\n",
      "Trained batch 4543 batch loss 7.1859684 epoch total loss 6.31605387\n",
      "Trained batch 4544 batch loss 5.68260193 epoch total loss 6.31591415\n",
      "Trained batch 4545 batch loss 5.21896076 epoch total loss 6.31567287\n",
      "Trained batch 4546 batch loss 5.7653389 epoch total loss 6.31555176\n",
      "Trained batch 4547 batch loss 6.11220074 epoch total loss 6.31550694\n",
      "Trained batch 4548 batch loss 6.4498167 epoch total loss 6.31553602\n",
      "Trained batch 4549 batch loss 5.37832451 epoch total loss 6.31533051\n",
      "Trained batch 4550 batch loss 5.05638885 epoch total loss 6.31505346\n",
      "Trained batch 4551 batch loss 6.26986647 epoch total loss 6.31504345\n",
      "Trained batch 4552 batch loss 5.74127197 epoch total loss 6.31491804\n",
      "Trained batch 4553 batch loss 5.60873604 epoch total loss 6.31476307\n",
      "Trained batch 4554 batch loss 4.90034723 epoch total loss 6.31445217\n",
      "Trained batch 4555 batch loss 5.5726161 epoch total loss 6.31428909\n",
      "Trained batch 4556 batch loss 6.26959229 epoch total loss 6.31427956\n",
      "Trained batch 4557 batch loss 6.083745 epoch total loss 6.31422901\n",
      "Trained batch 4558 batch loss 6.53141308 epoch total loss 6.3142767\n",
      "Trained batch 4559 batch loss 5.84549713 epoch total loss 6.3141737\n",
      "Trained batch 4560 batch loss 4.56522894 epoch total loss 6.31379032\n",
      "Trained batch 4561 batch loss 4.75272703 epoch total loss 6.31344748\n",
      "Trained batch 4562 batch loss 6.46505642 epoch total loss 6.31348085\n",
      "Trained batch 4563 batch loss 6.89116526 epoch total loss 6.31360722\n",
      "Trained batch 4564 batch loss 6.90725613 epoch total loss 6.31373739\n",
      "Trained batch 4565 batch loss 6.27972937 epoch total loss 6.31373024\n",
      "Trained batch 4566 batch loss 6.40102625 epoch total loss 6.31374884\n",
      "Trained batch 4567 batch loss 6.52512741 epoch total loss 6.31379557\n",
      "Trained batch 4568 batch loss 6.27693129 epoch total loss 6.31378746\n",
      "Trained batch 4569 batch loss 6.83763218 epoch total loss 6.3139019\n",
      "Trained batch 4570 batch loss 5.45849848 epoch total loss 6.31371498\n",
      "Trained batch 4571 batch loss 5.90907097 epoch total loss 6.31362629\n",
      "Trained batch 4572 batch loss 5.19596434 epoch total loss 6.31338167\n",
      "Trained batch 4573 batch loss 5.78770065 epoch total loss 6.31326675\n",
      "Trained batch 4574 batch loss 5.73987579 epoch total loss 6.31314135\n",
      "Trained batch 4575 batch loss 6.63786602 epoch total loss 6.31321239\n",
      "Trained batch 4576 batch loss 5.51590824 epoch total loss 6.31303835\n",
      "Trained batch 4577 batch loss 6.62460232 epoch total loss 6.31310654\n",
      "Trained batch 4578 batch loss 6.30476952 epoch total loss 6.31310463\n",
      "Trained batch 4579 batch loss 6.20820522 epoch total loss 6.31308174\n",
      "Trained batch 4580 batch loss 6.6027565 epoch total loss 6.31314516\n",
      "Trained batch 4581 batch loss 6.04177475 epoch total loss 6.31308603\n",
      "Trained batch 4582 batch loss 6.14642334 epoch total loss 6.31304932\n",
      "Trained batch 4583 batch loss 6.49303961 epoch total loss 6.31308842\n",
      "Trained batch 4584 batch loss 6.47181892 epoch total loss 6.31312323\n",
      "Trained batch 4585 batch loss 5.34956837 epoch total loss 6.31291342\n",
      "Trained batch 4586 batch loss 6.63536835 epoch total loss 6.31298351\n",
      "Trained batch 4587 batch loss 7.05103111 epoch total loss 6.31314421\n",
      "Trained batch 4588 batch loss 5.88754463 epoch total loss 6.31305122\n",
      "Trained batch 4589 batch loss 4.91736126 epoch total loss 6.31274748\n",
      "Trained batch 4590 batch loss 7.2611208 epoch total loss 6.31295395\n",
      "Trained batch 4591 batch loss 6.25906849 epoch total loss 6.3129425\n",
      "Trained batch 4592 batch loss 6.24149609 epoch total loss 6.31292725\n",
      "Trained batch 4593 batch loss 5.42640209 epoch total loss 6.31273365\n",
      "Trained batch 4594 batch loss 6.73435116 epoch total loss 6.31282568\n",
      "Trained batch 4595 batch loss 6.90072918 epoch total loss 6.31295347\n",
      "Trained batch 4596 batch loss 6.65512753 epoch total loss 6.31302786\n",
      "Trained batch 4597 batch loss 6.48246288 epoch total loss 6.31306458\n",
      "Trained batch 4598 batch loss 6.40797234 epoch total loss 6.31308556\n",
      "Trained batch 4599 batch loss 6.5873208 epoch total loss 6.31314516\n",
      "Trained batch 4600 batch loss 6.53181458 epoch total loss 6.31319237\n",
      "Trained batch 4601 batch loss 6.34720325 epoch total loss 6.3132\n",
      "Trained batch 4602 batch loss 6.45502663 epoch total loss 6.31323099\n",
      "Trained batch 4603 batch loss 6.23816967 epoch total loss 6.31321478\n",
      "Trained batch 4604 batch loss 6.16683054 epoch total loss 6.31318235\n",
      "Trained batch 4605 batch loss 6.69182682 epoch total loss 6.31326485\n",
      "Trained batch 4606 batch loss 6.44473934 epoch total loss 6.31329346\n",
      "Trained batch 4607 batch loss 6.26596451 epoch total loss 6.31328297\n",
      "Trained batch 4608 batch loss 6.07448339 epoch total loss 6.31323099\n",
      "Trained batch 4609 batch loss 6.17365646 epoch total loss 6.31320095\n",
      "Trained batch 4610 batch loss 6.33444643 epoch total loss 6.31320524\n",
      "Trained batch 4611 batch loss 5.39407682 epoch total loss 6.3130064\n",
      "Trained batch 4612 batch loss 6.09952879 epoch total loss 6.31295967\n",
      "Trained batch 4613 batch loss 5.45825577 epoch total loss 6.31277466\n",
      "Trained batch 4614 batch loss 5.49999 epoch total loss 6.31259871\n",
      "Trained batch 4615 batch loss 4.82581854 epoch total loss 6.31227636\n",
      "Trained batch 4616 batch loss 5.65367508 epoch total loss 6.31213379\n",
      "Trained batch 4617 batch loss 5.57472229 epoch total loss 6.31197405\n",
      "Trained batch 4618 batch loss 5.4247942 epoch total loss 6.31178188\n",
      "Trained batch 4619 batch loss 6.59560061 epoch total loss 6.3118434\n",
      "Trained batch 4620 batch loss 6.2802844 epoch total loss 6.31183672\n",
      "Trained batch 4621 batch loss 6.10205126 epoch total loss 6.31179142\n",
      "Trained batch 4622 batch loss 6.17398643 epoch total loss 6.31176138\n",
      "Trained batch 4623 batch loss 6.43348312 epoch total loss 6.31178761\n",
      "Trained batch 4624 batch loss 6.49203062 epoch total loss 6.31182671\n",
      "Trained batch 4625 batch loss 6.1434145 epoch total loss 6.31179\n",
      "Trained batch 4626 batch loss 5.86323547 epoch total loss 6.31169319\n",
      "Trained batch 4627 batch loss 6.1864624 epoch total loss 6.31166601\n",
      "Trained batch 4628 batch loss 5.96694469 epoch total loss 6.31159115\n",
      "Trained batch 4629 batch loss 6.28366852 epoch total loss 6.31158543\n",
      "Trained batch 4630 batch loss 6.28003454 epoch total loss 6.31157827\n",
      "Trained batch 4631 batch loss 6.71850109 epoch total loss 6.31166601\n",
      "Trained batch 4632 batch loss 6.09665 epoch total loss 6.31161976\n",
      "Trained batch 4633 batch loss 8.06825066 epoch total loss 6.31199884\n",
      "Trained batch 4634 batch loss 7.05639172 epoch total loss 6.31215954\n",
      "Trained batch 4635 batch loss 5.83004236 epoch total loss 6.31205559\n",
      "Trained batch 4636 batch loss 7.19290304 epoch total loss 6.31224537\n",
      "Trained batch 4637 batch loss 6.27428675 epoch total loss 6.31223726\n",
      "Trained batch 4638 batch loss 6.93156433 epoch total loss 6.31237078\n",
      "Trained batch 4639 batch loss 5.23482609 epoch total loss 6.31213856\n",
      "Trained batch 4640 batch loss 5.28665924 epoch total loss 6.3119173\n",
      "Trained batch 4641 batch loss 5.69246244 epoch total loss 6.31178427\n",
      "Trained batch 4642 batch loss 6.65204144 epoch total loss 6.3118577\n",
      "Trained batch 4643 batch loss 6.53873491 epoch total loss 6.31190634\n",
      "Trained batch 4644 batch loss 6.14318228 epoch total loss 6.31187\n",
      "Trained batch 4645 batch loss 6.36934 epoch total loss 6.3118825\n",
      "Trained batch 4646 batch loss 6.41376829 epoch total loss 6.31190443\n",
      "Trained batch 4647 batch loss 6.46112585 epoch total loss 6.31193638\n",
      "Trained batch 4648 batch loss 6.08193302 epoch total loss 6.31188679\n",
      "Trained batch 4649 batch loss 6.5482192 epoch total loss 6.31193781\n",
      "Trained batch 4650 batch loss 6.79162693 epoch total loss 6.31204081\n",
      "Trained batch 4651 batch loss 6.76957417 epoch total loss 6.31213951\n",
      "Trained batch 4652 batch loss 6.26177692 epoch total loss 6.31212854\n",
      "Trained batch 4653 batch loss 6.44923 epoch total loss 6.31215811\n",
      "Trained batch 4654 batch loss 6.46800423 epoch total loss 6.31219149\n",
      "Trained batch 4655 batch loss 6.57152414 epoch total loss 6.31224728\n",
      "Trained batch 4656 batch loss 6.26897335 epoch total loss 6.31223822\n",
      "Trained batch 4657 batch loss 5.97663164 epoch total loss 6.31216621\n",
      "Trained batch 4658 batch loss 5.96984339 epoch total loss 6.31209278\n",
      "Trained batch 4659 batch loss 6.16523 epoch total loss 6.31206131\n",
      "Trained batch 4660 batch loss 6.17900181 epoch total loss 6.31203318\n",
      "Trained batch 4661 batch loss 6.42213058 epoch total loss 6.31205654\n",
      "Trained batch 4662 batch loss 6.43089628 epoch total loss 6.31208229\n",
      "Trained batch 4663 batch loss 6.25636578 epoch total loss 6.31207037\n",
      "Trained batch 4664 batch loss 5.75198174 epoch total loss 6.31195\n",
      "Trained batch 4665 batch loss 5.48357391 epoch total loss 6.31177282\n",
      "Trained batch 4666 batch loss 5.65054274 epoch total loss 6.3116312\n",
      "Trained batch 4667 batch loss 5.96370935 epoch total loss 6.31155634\n",
      "Trained batch 4668 batch loss 5.72070885 epoch total loss 6.3114295\n",
      "Trained batch 4669 batch loss 5.03247833 epoch total loss 6.3111558\n",
      "Trained batch 4670 batch loss 5.69272327 epoch total loss 6.31102371\n",
      "Trained batch 4671 batch loss 5.18075752 epoch total loss 6.31078196\n",
      "Trained batch 4672 batch loss 4.02892113 epoch total loss 6.31029367\n",
      "Trained batch 4673 batch loss 6.68438482 epoch total loss 6.31037331\n",
      "Trained batch 4674 batch loss 5.84538364 epoch total loss 6.31027412\n",
      "Trained batch 4675 batch loss 7.6316328 epoch total loss 6.31055641\n",
      "Trained batch 4676 batch loss 6.06290627 epoch total loss 6.31050348\n",
      "Trained batch 4677 batch loss 5.28981113 epoch total loss 6.31028509\n",
      "Trained batch 4678 batch loss 6.94884396 epoch total loss 6.31042147\n",
      "Trained batch 4679 batch loss 5.83151 epoch total loss 6.31031942\n",
      "Trained batch 4680 batch loss 7.05025482 epoch total loss 6.31047773\n",
      "Trained batch 4681 batch loss 7.06270742 epoch total loss 6.31063843\n",
      "Trained batch 4682 batch loss 7.07618713 epoch total loss 6.31080198\n",
      "Trained batch 4683 batch loss 6.8508091 epoch total loss 6.31091738\n",
      "Trained batch 4684 batch loss 6.31725645 epoch total loss 6.31091833\n",
      "Trained batch 4685 batch loss 6.75124598 epoch total loss 6.31101274\n",
      "Trained batch 4686 batch loss 6.05092859 epoch total loss 6.31095695\n",
      "Trained batch 4687 batch loss 5.99805927 epoch total loss 6.31089\n",
      "Trained batch 4688 batch loss 6.08946562 epoch total loss 6.31084299\n",
      "Trained batch 4689 batch loss 6.78533 epoch total loss 6.31094408\n",
      "Trained batch 4690 batch loss 5.8503828 epoch total loss 6.31084585\n",
      "Trained batch 4691 batch loss 6.11664104 epoch total loss 6.31080437\n",
      "Trained batch 4692 batch loss 6.15066767 epoch total loss 6.31077051\n",
      "Trained batch 4693 batch loss 7.30085278 epoch total loss 6.31098127\n",
      "Trained batch 4694 batch loss 6.60545874 epoch total loss 6.31104422\n",
      "Trained batch 4695 batch loss 6.10776758 epoch total loss 6.31100082\n",
      "Trained batch 4696 batch loss 5.07803059 epoch total loss 6.31073809\n",
      "Trained batch 4697 batch loss 7.19628954 epoch total loss 6.31092691\n",
      "Trained batch 4698 batch loss 5.70966148 epoch total loss 6.31079865\n",
      "Trained batch 4699 batch loss 5.82991314 epoch total loss 6.3106966\n",
      "Trained batch 4700 batch loss 6.53438568 epoch total loss 6.31074429\n",
      "Trained batch 4701 batch loss 6.65937901 epoch total loss 6.31081867\n",
      "Trained batch 4702 batch loss 5.64886284 epoch total loss 6.31067753\n",
      "Trained batch 4703 batch loss 6.29370785 epoch total loss 6.31067419\n",
      "Trained batch 4704 batch loss 6.1995697 epoch total loss 6.31065035\n",
      "Trained batch 4705 batch loss 5.00587082 epoch total loss 6.31037283\n",
      "Trained batch 4706 batch loss 6.48846579 epoch total loss 6.31041098\n",
      "Trained batch 4707 batch loss 6.63135672 epoch total loss 6.31047869\n",
      "Trained batch 4708 batch loss 6.05717564 epoch total loss 6.3104248\n",
      "Trained batch 4709 batch loss 6.3356638 epoch total loss 6.31043053\n",
      "Trained batch 4710 batch loss 6.58375597 epoch total loss 6.31048822\n",
      "Trained batch 4711 batch loss 6.7984066 epoch total loss 6.31059217\n",
      "Trained batch 4712 batch loss 6.22882271 epoch total loss 6.31057453\n",
      "Trained batch 4713 batch loss 6.65688419 epoch total loss 6.31064796\n",
      "Trained batch 4714 batch loss 5.88622379 epoch total loss 6.31055784\n",
      "Trained batch 4715 batch loss 6.6346035 epoch total loss 6.31062698\n",
      "Trained batch 4716 batch loss 6.71150398 epoch total loss 6.31071186\n",
      "Trained batch 4717 batch loss 5.78389 epoch total loss 6.3106\n",
      "Trained batch 4718 batch loss 6.20194244 epoch total loss 6.31057692\n",
      "Trained batch 4719 batch loss 5.08363914 epoch total loss 6.31031656\n",
      "Trained batch 4720 batch loss 6.65918112 epoch total loss 6.31039095\n",
      "Trained batch 4721 batch loss 6.14230108 epoch total loss 6.31035519\n",
      "Trained batch 4722 batch loss 6.40696812 epoch total loss 6.31037569\n",
      "Trained batch 4723 batch loss 6.1790905 epoch total loss 6.31034803\n",
      "Trained batch 4724 batch loss 6.29678917 epoch total loss 6.31034517\n",
      "Trained batch 4725 batch loss 6.1113925 epoch total loss 6.31030321\n",
      "Trained batch 4726 batch loss 5.94589138 epoch total loss 6.31022596\n",
      "Trained batch 4727 batch loss 6.25476694 epoch total loss 6.31021404\n",
      "Trained batch 4728 batch loss 5.51116657 epoch total loss 6.31004477\n",
      "Trained batch 4729 batch loss 7.22258043 epoch total loss 6.31023788\n",
      "Trained batch 4730 batch loss 6.40723324 epoch total loss 6.31025887\n",
      "Trained batch 4731 batch loss 6.09638691 epoch total loss 6.31021309\n",
      "Trained batch 4732 batch loss 6.30234957 epoch total loss 6.31021166\n",
      "Trained batch 4733 batch loss 6.55882072 epoch total loss 6.31026411\n",
      "Trained batch 4734 batch loss 6.08660936 epoch total loss 6.3102169\n",
      "Trained batch 4735 batch loss 5.81262875 epoch total loss 6.31011152\n",
      "Trained batch 4736 batch loss 5.87762642 epoch total loss 6.31002045\n",
      "Trained batch 4737 batch loss 6.48868847 epoch total loss 6.31005764\n",
      "Trained batch 4738 batch loss 5.93841219 epoch total loss 6.30997944\n",
      "Trained batch 4739 batch loss 6.4286809 epoch total loss 6.31000423\n",
      "Trained batch 4740 batch loss 6.11157 epoch total loss 6.30996227\n",
      "Trained batch 4741 batch loss 6.56523228 epoch total loss 6.31001568\n",
      "Trained batch 4742 batch loss 6.70929384 epoch total loss 6.3101\n",
      "Trained batch 4743 batch loss 5.71682739 epoch total loss 6.30997467\n",
      "Trained batch 4744 batch loss 5.66410446 epoch total loss 6.30983877\n",
      "Trained batch 4745 batch loss 5.12136841 epoch total loss 6.30958843\n",
      "Trained batch 4746 batch loss 5.93806362 epoch total loss 6.30950975\n",
      "Trained batch 4747 batch loss 6.72794628 epoch total loss 6.30959797\n",
      "Trained batch 4748 batch loss 6.56246567 epoch total loss 6.30965137\n",
      "Trained batch 4749 batch loss 6.54339266 epoch total loss 6.30970049\n",
      "Trained batch 4750 batch loss 6.48117924 epoch total loss 6.30973625\n",
      "Trained batch 4751 batch loss 5.81461763 epoch total loss 6.3096323\n",
      "Trained batch 4752 batch loss 5.36478233 epoch total loss 6.30943346\n",
      "Trained batch 4753 batch loss 6.0674 epoch total loss 6.30938292\n",
      "Trained batch 4754 batch loss 4.83258247 epoch total loss 6.30907202\n",
      "Trained batch 4755 batch loss 5.47174692 epoch total loss 6.30889606\n",
      "Trained batch 4756 batch loss 6.48747158 epoch total loss 6.30893373\n",
      "Trained batch 4757 batch loss 5.52433395 epoch total loss 6.30876875\n",
      "Trained batch 4758 batch loss 5.27903318 epoch total loss 6.30855227\n",
      "Trained batch 4759 batch loss 6.44984913 epoch total loss 6.30858183\n",
      "Trained batch 4760 batch loss 5.91743851 epoch total loss 6.3085\n",
      "Trained batch 4761 batch loss 6.08916759 epoch total loss 6.30845404\n",
      "Trained batch 4762 batch loss 6.31565666 epoch total loss 6.30845547\n",
      "Trained batch 4763 batch loss 5.91315937 epoch total loss 6.3083725\n",
      "Trained batch 4764 batch loss 6.41458273 epoch total loss 6.30839491\n",
      "Trained batch 4765 batch loss 6.68464327 epoch total loss 6.30847406\n",
      "Trained batch 4766 batch loss 6.40962458 epoch total loss 6.30849552\n",
      "Trained batch 4767 batch loss 6.5888176 epoch total loss 6.30855417\n",
      "Trained batch 4768 batch loss 5.79948711 epoch total loss 6.30844688\n",
      "Trained batch 4769 batch loss 6.49347591 epoch total loss 6.30848598\n",
      "Trained batch 4770 batch loss 6.57039928 epoch total loss 6.30854082\n",
      "Trained batch 4771 batch loss 5.3495121 epoch total loss 6.30834\n",
      "Trained batch 4772 batch loss 6.44231176 epoch total loss 6.30836773\n",
      "Trained batch 4773 batch loss 6.62547779 epoch total loss 6.30843401\n",
      "Trained batch 4774 batch loss 6.82969189 epoch total loss 6.30854321\n",
      "Trained batch 4775 batch loss 6.13340521 epoch total loss 6.30850649\n",
      "Trained batch 4776 batch loss 6.81795692 epoch total loss 6.3086133\n",
      "Trained batch 4777 batch loss 6.31596565 epoch total loss 6.30861521\n",
      "Trained batch 4778 batch loss 6.65736103 epoch total loss 6.30868816\n",
      "Trained batch 4779 batch loss 7.72124529 epoch total loss 6.3089838\n",
      "Trained batch 4780 batch loss 6.41546345 epoch total loss 6.30900621\n",
      "Trained batch 4781 batch loss 6.29035759 epoch total loss 6.3090024\n",
      "Trained batch 4782 batch loss 5.89629078 epoch total loss 6.30891609\n",
      "Trained batch 4783 batch loss 5.98702526 epoch total loss 6.30884838\n",
      "Trained batch 4784 batch loss 5.72177887 epoch total loss 6.30872583\n",
      "Trained batch 4785 batch loss 6.08479309 epoch total loss 6.3086791\n",
      "Trained batch 4786 batch loss 6.706707 epoch total loss 6.30876207\n",
      "Trained batch 4787 batch loss 5.65073824 epoch total loss 6.30862474\n",
      "Trained batch 4788 batch loss 7.08335876 epoch total loss 6.30878687\n",
      "Trained batch 4789 batch loss 6.85419941 epoch total loss 6.30890036\n",
      "Trained batch 4790 batch loss 7.36553812 epoch total loss 6.30912113\n",
      "Trained batch 4791 batch loss 7.17006493 epoch total loss 6.30930042\n",
      "Trained batch 4792 batch loss 5.82887077 epoch total loss 6.30920029\n",
      "Trained batch 4793 batch loss 6.34359264 epoch total loss 6.30920744\n",
      "Trained batch 4794 batch loss 6.0922718 epoch total loss 6.30916214\n",
      "Trained batch 4795 batch loss 7.10212708 epoch total loss 6.30932713\n",
      "Trained batch 4796 batch loss 6.37297726 epoch total loss 6.30934048\n",
      "Trained batch 4797 batch loss 6.69052458 epoch total loss 6.30942\n",
      "Trained batch 4798 batch loss 7.03454494 epoch total loss 6.30957174\n",
      "Trained batch 4799 batch loss 6.55034 epoch total loss 6.30962181\n",
      "Trained batch 4800 batch loss 6.86387062 epoch total loss 6.30973721\n",
      "Trained batch 4801 batch loss 7.24781704 epoch total loss 6.30993271\n",
      "Trained batch 4802 batch loss 6.90121555 epoch total loss 6.31005573\n",
      "Trained batch 4803 batch loss 6.75547 epoch total loss 6.31014824\n",
      "Trained batch 4804 batch loss 6.09449 epoch total loss 6.31010342\n",
      "Trained batch 4805 batch loss 4.28245592 epoch total loss 6.30968142\n",
      "Trained batch 4806 batch loss 5.35725403 epoch total loss 6.30948353\n",
      "Trained batch 4807 batch loss 6.06486559 epoch total loss 6.30943251\n",
      "Trained batch 4808 batch loss 5.004529 epoch total loss 6.30916071\n",
      "Trained batch 4809 batch loss 5.57366085 epoch total loss 6.30900812\n",
      "Trained batch 4810 batch loss 6.61354828 epoch total loss 6.30907106\n",
      "Trained batch 4811 batch loss 5.91359043 epoch total loss 6.30898905\n",
      "Trained batch 4812 batch loss 5.69187498 epoch total loss 6.30886078\n",
      "Trained batch 4813 batch loss 6.41231346 epoch total loss 6.30888224\n",
      "Trained batch 4814 batch loss 6.68170786 epoch total loss 6.30895948\n",
      "Trained batch 4815 batch loss 6.35280466 epoch total loss 6.30896902\n",
      "Trained batch 4816 batch loss 6.49712372 epoch total loss 6.30900812\n",
      "Trained batch 4817 batch loss 5.84493065 epoch total loss 6.30891228\n",
      "Trained batch 4818 batch loss 5.91068172 epoch total loss 6.30882931\n",
      "Trained batch 4819 batch loss 6.06982708 epoch total loss 6.30877972\n",
      "Trained batch 4820 batch loss 6.16682243 epoch total loss 6.30875\n",
      "Trained batch 4821 batch loss 6.08204937 epoch total loss 6.30870295\n",
      "Trained batch 4822 batch loss 6.81362534 epoch total loss 6.30880785\n",
      "Trained batch 4823 batch loss 5.9720583 epoch total loss 6.30873823\n",
      "Trained batch 4824 batch loss 6.1486187 epoch total loss 6.30870485\n",
      "Trained batch 4825 batch loss 6.07478333 epoch total loss 6.30865669\n",
      "Trained batch 4826 batch loss 6.17520857 epoch total loss 6.30862904\n",
      "Trained batch 4827 batch loss 5.86605597 epoch total loss 6.30853701\n",
      "Trained batch 4828 batch loss 6.37495518 epoch total loss 6.30855083\n",
      "Trained batch 4829 batch loss 6.23660564 epoch total loss 6.30853605\n",
      "Trained batch 4830 batch loss 5.50738716 epoch total loss 6.30837\n",
      "Trained batch 4831 batch loss 6.08020496 epoch total loss 6.30832291\n",
      "Trained batch 4832 batch loss 6.0766716 epoch total loss 6.30827475\n",
      "Trained batch 4833 batch loss 5.9688096 epoch total loss 6.30820465\n",
      "Trained batch 4834 batch loss 5.73476267 epoch total loss 6.30808592\n",
      "Trained batch 4835 batch loss 5.87266254 epoch total loss 6.3079958\n",
      "Trained batch 4836 batch loss 5.66063 epoch total loss 6.3078618\n",
      "Trained batch 4837 batch loss 6.59770489 epoch total loss 6.30792189\n",
      "Trained batch 4838 batch loss 5.33360147 epoch total loss 6.30772066\n",
      "Trained batch 4839 batch loss 5.9843688 epoch total loss 6.3076539\n",
      "Trained batch 4840 batch loss 6.29344845 epoch total loss 6.30765057\n",
      "Trained batch 4841 batch loss 4.62615395 epoch total loss 6.30730343\n",
      "Trained batch 4842 batch loss 6.28645039 epoch total loss 6.30729914\n",
      "Trained batch 4843 batch loss 5.52270222 epoch total loss 6.30713749\n",
      "Trained batch 4844 batch loss 6.20035696 epoch total loss 6.30711555\n",
      "Trained batch 4845 batch loss 5.71762228 epoch total loss 6.30699396\n",
      "Trained batch 4846 batch loss 5.1649766 epoch total loss 6.30675793\n",
      "Trained batch 4847 batch loss 6.95619678 epoch total loss 6.30689192\n",
      "Trained batch 4848 batch loss 6.41265678 epoch total loss 6.30691385\n",
      "Trained batch 4849 batch loss 6.31946182 epoch total loss 6.30691671\n",
      "Trained batch 4850 batch loss 5.84139442 epoch total loss 6.30682087\n",
      "Trained batch 4851 batch loss 7.35717201 epoch total loss 6.30703735\n",
      "Trained batch 4852 batch loss 6.32689667 epoch total loss 6.30704117\n",
      "Trained batch 4853 batch loss 6.49695158 epoch total loss 6.30708027\n",
      "Trained batch 4854 batch loss 6.10670376 epoch total loss 6.30703878\n",
      "Trained batch 4855 batch loss 5.22589254 epoch total loss 6.30681658\n",
      "Trained batch 4856 batch loss 6.56682539 epoch total loss 6.30687\n",
      "Trained batch 4857 batch loss 6.35445929 epoch total loss 6.30687952\n",
      "Trained batch 4858 batch loss 6.70663834 epoch total loss 6.30696201\n",
      "Trained batch 4859 batch loss 5.88911057 epoch total loss 6.30687571\n",
      "Trained batch 4860 batch loss 6.04346085 epoch total loss 6.30682135\n",
      "Trained batch 4861 batch loss 6.9490366 epoch total loss 6.30695343\n",
      "Trained batch 4862 batch loss 6.48519 epoch total loss 6.30699\n",
      "Trained batch 4863 batch loss 6.05718327 epoch total loss 6.30693865\n",
      "Trained batch 4864 batch loss 6.46658373 epoch total loss 6.30697155\n",
      "Trained batch 4865 batch loss 5.20484209 epoch total loss 6.30674505\n",
      "Trained batch 4866 batch loss 6.32044125 epoch total loss 6.30674791\n",
      "Trained batch 4867 batch loss 5.95664835 epoch total loss 6.30667591\n",
      "Trained batch 4868 batch loss 6.45446348 epoch total loss 6.30670643\n",
      "Trained batch 4869 batch loss 6.11560202 epoch total loss 6.30666733\n",
      "Trained batch 4870 batch loss 6.13132858 epoch total loss 6.30663109\n",
      "Trained batch 4871 batch loss 6.66815186 epoch total loss 6.306705\n",
      "Trained batch 4872 batch loss 6.14018869 epoch total loss 6.30667114\n",
      "Trained batch 4873 batch loss 5.35058594 epoch total loss 6.30647516\n",
      "Trained batch 4874 batch loss 5.78629684 epoch total loss 6.30636835\n",
      "Trained batch 4875 batch loss 5.88042545 epoch total loss 6.30628109\n",
      "Trained batch 4876 batch loss 5.60064602 epoch total loss 6.30613661\n",
      "Trained batch 4877 batch loss 5.83445 epoch total loss 6.30604\n",
      "Trained batch 4878 batch loss 5.57093287 epoch total loss 6.30588913\n",
      "Trained batch 4879 batch loss 5.65511703 epoch total loss 6.30575562\n",
      "Trained batch 4880 batch loss 5.87402439 epoch total loss 6.3056674\n",
      "Trained batch 4881 batch loss 6.27297115 epoch total loss 6.30566072\n",
      "Trained batch 4882 batch loss 5.63131475 epoch total loss 6.30552244\n",
      "Trained batch 4883 batch loss 6.41310024 epoch total loss 6.30554485\n",
      "Trained batch 4884 batch loss 6.11252642 epoch total loss 6.30550528\n",
      "Trained batch 4885 batch loss 6.36613083 epoch total loss 6.30551767\n",
      "Trained batch 4886 batch loss 5.98572206 epoch total loss 6.30545235\n",
      "Trained batch 4887 batch loss 5.9126482 epoch total loss 6.30537176\n",
      "Trained batch 4888 batch loss 5.62646151 epoch total loss 6.305233\n",
      "Trained batch 4889 batch loss 5.84689617 epoch total loss 6.30513954\n",
      "Trained batch 4890 batch loss 5.88491917 epoch total loss 6.30505323\n",
      "Trained batch 4891 batch loss 4.74394941 epoch total loss 6.30473423\n",
      "Trained batch 4892 batch loss 3.82104731 epoch total loss 6.3042264\n",
      "Trained batch 4893 batch loss 6.31622505 epoch total loss 6.30422878\n",
      "Trained batch 4894 batch loss 7.19532871 epoch total loss 6.30441093\n",
      "Trained batch 4895 batch loss 5.77851582 epoch total loss 6.30430365\n",
      "Trained batch 4896 batch loss 5.96167755 epoch total loss 6.30423355\n",
      "Trained batch 4897 batch loss 6.58315372 epoch total loss 6.30429077\n",
      "Trained batch 4898 batch loss 6.72092342 epoch total loss 6.30437565\n",
      "Trained batch 4899 batch loss 7.20307159 epoch total loss 6.30455923\n",
      "Trained batch 4900 batch loss 6.46948385 epoch total loss 6.30459261\n",
      "Trained batch 4901 batch loss 6.69616699 epoch total loss 6.30467224\n",
      "Trained batch 4902 batch loss 6.44364262 epoch total loss 6.30470085\n",
      "Trained batch 4903 batch loss 6.94004297 epoch total loss 6.30483\n",
      "Trained batch 4904 batch loss 6.04157782 epoch total loss 6.30477619\n",
      "Trained batch 4905 batch loss 6.38177204 epoch total loss 6.30479193\n",
      "Trained batch 4906 batch loss 5.9648447 epoch total loss 6.30472231\n",
      "Trained batch 4907 batch loss 7.17532158 epoch total loss 6.3049\n",
      "Trained batch 4908 batch loss 7.31009293 epoch total loss 6.30510473\n",
      "Trained batch 4909 batch loss 7.33368254 epoch total loss 6.30531454\n",
      "Trained batch 4910 batch loss 7.42589474 epoch total loss 6.30554295\n",
      "Trained batch 4911 batch loss 6.74149799 epoch total loss 6.30563164\n",
      "Trained batch 4912 batch loss 7.02182388 epoch total loss 6.30577755\n",
      "Trained batch 4913 batch loss 7.90325689 epoch total loss 6.30610228\n",
      "Trained batch 4914 batch loss 6.60137606 epoch total loss 6.30616236\n",
      "Trained batch 4915 batch loss 7.91160154 epoch total loss 6.30648899\n",
      "Trained batch 4916 batch loss 6.40625 epoch total loss 6.30650949\n",
      "Trained batch 4917 batch loss 6.72055864 epoch total loss 6.30659389\n",
      "Trained batch 4918 batch loss 5.24074268 epoch total loss 6.30637693\n",
      "Trained batch 4919 batch loss 6.55682755 epoch total loss 6.30642796\n",
      "Trained batch 4920 batch loss 5.61851597 epoch total loss 6.30628824\n",
      "Trained batch 4921 batch loss 7.18493176 epoch total loss 6.30646658\n",
      "Trained batch 4922 batch loss 6.01404858 epoch total loss 6.30640745\n",
      "Trained batch 4923 batch loss 6.78579044 epoch total loss 6.30650473\n",
      "Trained batch 4924 batch loss 6.45792294 epoch total loss 6.30653524\n",
      "Trained batch 4925 batch loss 7.48493195 epoch total loss 6.30677414\n",
      "Trained batch 4926 batch loss 6.53970289 epoch total loss 6.30682135\n",
      "Trained batch 4927 batch loss 6.8551321 epoch total loss 6.30693293\n",
      "Trained batch 4928 batch loss 7.25536156 epoch total loss 6.30712557\n",
      "Trained batch 4929 batch loss 6.79905558 epoch total loss 6.30722523\n",
      "Trained batch 4930 batch loss 6.81680584 epoch total loss 6.30732822\n",
      "Trained batch 4931 batch loss 6.98156309 epoch total loss 6.30746508\n",
      "Trained batch 4932 batch loss 5.87349606 epoch total loss 6.30737734\n",
      "Trained batch 4933 batch loss 6.93950748 epoch total loss 6.30750513\n",
      "Trained batch 4934 batch loss 6.24961472 epoch total loss 6.30749369\n",
      "Trained batch 4935 batch loss 6.13780117 epoch total loss 6.30745935\n",
      "Trained batch 4936 batch loss 6.36916733 epoch total loss 6.30747175\n",
      "Trained batch 4937 batch loss 5.94887543 epoch total loss 6.30739927\n",
      "Trained batch 4938 batch loss 5.72235346 epoch total loss 6.30728102\n",
      "Trained batch 4939 batch loss 5.6433 epoch total loss 6.30714655\n",
      "Trained batch 4940 batch loss 6.04765 epoch total loss 6.30709362\n",
      "Trained batch 4941 batch loss 5.78528452 epoch total loss 6.30698824\n",
      "Trained batch 4942 batch loss 6.73016453 epoch total loss 6.30707359\n",
      "Trained batch 4943 batch loss 7.07179 epoch total loss 6.30722857\n",
      "Trained batch 4944 batch loss 7.32044029 epoch total loss 6.30743361\n",
      "Trained batch 4945 batch loss 6.73400784 epoch total loss 6.30752\n",
      "Trained batch 4946 batch loss 6.63576603 epoch total loss 6.30758619\n",
      "Trained batch 4947 batch loss 7.2220993 epoch total loss 6.30777121\n",
      "Trained batch 4948 batch loss 6.5692482 epoch total loss 6.30782413\n",
      "Trained batch 4949 batch loss 6.65693188 epoch total loss 6.30789423\n",
      "Trained batch 4950 batch loss 5.50184536 epoch total loss 6.30773163\n",
      "Trained batch 4951 batch loss 5.49233055 epoch total loss 6.30756712\n",
      "Trained batch 4952 batch loss 6.264925 epoch total loss 6.30755854\n",
      "Trained batch 4953 batch loss 6.8534174 epoch total loss 6.30766869\n",
      "Trained batch 4954 batch loss 6.84600306 epoch total loss 6.3077774\n",
      "Trained batch 4955 batch loss 6.57129383 epoch total loss 6.30783081\n",
      "Trained batch 4956 batch loss 5.60288715 epoch total loss 6.30768871\n",
      "Trained batch 4957 batch loss 6.61628151 epoch total loss 6.30775118\n",
      "Trained batch 4958 batch loss 6.43063259 epoch total loss 6.3077755\n",
      "Trained batch 4959 batch loss 6.13685942 epoch total loss 6.30774117\n",
      "Trained batch 4960 batch loss 6.22613239 epoch total loss 6.30772448\n",
      "Trained batch 4961 batch loss 6.19394588 epoch total loss 6.30770159\n",
      "Trained batch 4962 batch loss 5.40210247 epoch total loss 6.30751896\n",
      "Trained batch 4963 batch loss 5.69608116 epoch total loss 6.30739594\n",
      "Trained batch 4964 batch loss 5.92692089 epoch total loss 6.30731916\n",
      "Trained batch 4965 batch loss 6.50508 epoch total loss 6.30735922\n",
      "Trained batch 4966 batch loss 6.59198189 epoch total loss 6.30741644\n",
      "Trained batch 4967 batch loss 6.67860317 epoch total loss 6.3074913\n",
      "Trained batch 4968 batch loss 6.77909279 epoch total loss 6.30758619\n",
      "Trained batch 4969 batch loss 6.57184505 epoch total loss 6.3076396\n",
      "Trained batch 4970 batch loss 6.17609882 epoch total loss 6.3076129\n",
      "Trained batch 4971 batch loss 6.11978054 epoch total loss 6.30757475\n",
      "Trained batch 4972 batch loss 6.1021862 epoch total loss 6.30753374\n",
      "Trained batch 4973 batch loss 6.28360748 epoch total loss 6.3075285\n",
      "Trained batch 4974 batch loss 5.39722443 epoch total loss 6.30734539\n",
      "Trained batch 4975 batch loss 5.65716362 epoch total loss 6.30721474\n",
      "Trained batch 4976 batch loss 6.62407589 epoch total loss 6.30727863\n",
      "Trained batch 4977 batch loss 6.38288689 epoch total loss 6.30729342\n",
      "Trained batch 4978 batch loss 6.57300472 epoch total loss 6.30734682\n",
      "Trained batch 4979 batch loss 5.94117928 epoch total loss 6.30727339\n",
      "Trained batch 4980 batch loss 5.80029774 epoch total loss 6.30717182\n",
      "Trained batch 4981 batch loss 6.4031949 epoch total loss 6.3071909\n",
      "Trained batch 4982 batch loss 6.07094383 epoch total loss 6.30714321\n",
      "Trained batch 4983 batch loss 6.39826107 epoch total loss 6.30716133\n",
      "Trained batch 4984 batch loss 6.03141642 epoch total loss 6.30710602\n",
      "Trained batch 4985 batch loss 6.16922235 epoch total loss 6.30707884\n",
      "Trained batch 4986 batch loss 6.45339537 epoch total loss 6.30710793\n",
      "Trained batch 4987 batch loss 6.47154617 epoch total loss 6.30714083\n",
      "Trained batch 4988 batch loss 6.52611589 epoch total loss 6.3071847\n",
      "Trained batch 4989 batch loss 5.85293198 epoch total loss 6.30709362\n",
      "Trained batch 4990 batch loss 7.41088581 epoch total loss 6.3073144\n",
      "Trained batch 4991 batch loss 7.87375 epoch total loss 6.30762815\n",
      "Trained batch 4992 batch loss 7.90314198 epoch total loss 6.30794764\n",
      "Trained batch 4993 batch loss 6.57231808 epoch total loss 6.30800056\n",
      "Trained batch 4994 batch loss 6.26925135 epoch total loss 6.30799294\n",
      "Trained batch 4995 batch loss 6.71879101 epoch total loss 6.30807543\n",
      "Trained batch 4996 batch loss 6.64377 epoch total loss 6.30814266\n",
      "Trained batch 4997 batch loss 6.45361614 epoch total loss 6.30817175\n",
      "Trained batch 4998 batch loss 6.55107594 epoch total loss 6.30822039\n",
      "Trained batch 4999 batch loss 6.6996851 epoch total loss 6.30829859\n",
      "Trained batch 5000 batch loss 6.53875637 epoch total loss 6.30834436\n",
      "Trained batch 5001 batch loss 6.39482117 epoch total loss 6.30836153\n",
      "Trained batch 5002 batch loss 6.58771229 epoch total loss 6.3084178\n",
      "Trained batch 5003 batch loss 6.43687105 epoch total loss 6.30844355\n",
      "Trained batch 5004 batch loss 5.97931 epoch total loss 6.30837774\n",
      "Trained batch 5005 batch loss 6.39250278 epoch total loss 6.30839443\n",
      "Trained batch 5006 batch loss 5.75470066 epoch total loss 6.30828381\n",
      "Trained batch 5007 batch loss 6.45997095 epoch total loss 6.30831385\n",
      "Trained batch 5008 batch loss 5.42984295 epoch total loss 6.30813885\n",
      "Trained batch 5009 batch loss 5.01683617 epoch total loss 6.30788088\n",
      "Trained batch 5010 batch loss 5.26523304 epoch total loss 6.30767298\n",
      "Trained batch 5011 batch loss 5.07718801 epoch total loss 6.30742741\n",
      "Trained batch 5012 batch loss 5.82063913 epoch total loss 6.30733061\n",
      "Trained batch 5013 batch loss 5.95575237 epoch total loss 6.30726\n",
      "Trained batch 5014 batch loss 6.63445 epoch total loss 6.30732536\n",
      "Trained batch 5015 batch loss 6.37417603 epoch total loss 6.30733871\n",
      "Trained batch 5016 batch loss 5.88414526 epoch total loss 6.30725479\n",
      "Trained batch 5017 batch loss 5.55604362 epoch total loss 6.30710506\n",
      "Trained batch 5018 batch loss 5.38548565 epoch total loss 6.30692148\n",
      "Trained batch 5019 batch loss 5.74164438 epoch total loss 6.30680895\n",
      "Trained batch 5020 batch loss 6.4474268 epoch total loss 6.30683661\n",
      "Trained batch 5021 batch loss 6.3985095 epoch total loss 6.30685472\n",
      "Trained batch 5022 batch loss 6.72769403 epoch total loss 6.30693913\n",
      "Trained batch 5023 batch loss 6.30051374 epoch total loss 6.30693769\n",
      "Trained batch 5024 batch loss 6.4091444 epoch total loss 6.30695772\n",
      "Trained batch 5025 batch loss 6.36979771 epoch total loss 6.30697\n",
      "Trained batch 5026 batch loss 6.25918484 epoch total loss 6.30696106\n",
      "Trained batch 5027 batch loss 6.13267756 epoch total loss 6.30692625\n",
      "Trained batch 5028 batch loss 5.94657373 epoch total loss 6.30685472\n",
      "Trained batch 5029 batch loss 5.26689672 epoch total loss 6.30664778\n",
      "Trained batch 5030 batch loss 5.42888069 epoch total loss 6.30647373\n",
      "Trained batch 5031 batch loss 5.33441925 epoch total loss 6.30628\n",
      "Trained batch 5032 batch loss 6.42635155 epoch total loss 6.30630398\n",
      "Trained batch 5033 batch loss 6.25337791 epoch total loss 6.30629349\n",
      "Trained batch 5034 batch loss 6.47959471 epoch total loss 6.3063283\n",
      "Trained batch 5035 batch loss 6.11709595 epoch total loss 6.30629063\n",
      "Trained batch 5036 batch loss 6.54416227 epoch total loss 6.30633831\n",
      "Trained batch 5037 batch loss 6.28076935 epoch total loss 6.30633307\n",
      "Trained batch 5038 batch loss 6.1928196 epoch total loss 6.30631065\n",
      "Trained batch 5039 batch loss 6.14300299 epoch total loss 6.30627823\n",
      "Trained batch 5040 batch loss 6.18367577 epoch total loss 6.30625391\n",
      "Trained batch 5041 batch loss 6.0333395 epoch total loss 6.30619955\n",
      "Trained batch 5042 batch loss 6.06611252 epoch total loss 6.30615234\n",
      "Trained batch 5043 batch loss 5.87623405 epoch total loss 6.30606699\n",
      "Trained batch 5044 batch loss 6.18723965 epoch total loss 6.30604362\n",
      "Trained batch 5045 batch loss 6.23553181 epoch total loss 6.30603\n",
      "Trained batch 5046 batch loss 6.42392731 epoch total loss 6.30605316\n",
      "Trained batch 5047 batch loss 6.10206652 epoch total loss 6.30601263\n",
      "Trained batch 5048 batch loss 5.93830681 epoch total loss 6.30593967\n",
      "Trained batch 5049 batch loss 6.77275896 epoch total loss 6.30603218\n",
      "Trained batch 5050 batch loss 6.442729 epoch total loss 6.30605936\n",
      "Trained batch 5051 batch loss 6.83912277 epoch total loss 6.30616522\n",
      "Trained batch 5052 batch loss 6.26632595 epoch total loss 6.30615711\n",
      "Trained batch 5053 batch loss 6.14648247 epoch total loss 6.30612516\n",
      "Trained batch 5054 batch loss 6.29093266 epoch total loss 6.3061223\n",
      "Trained batch 5055 batch loss 6.21598339 epoch total loss 6.30610466\n",
      "Trained batch 5056 batch loss 6.10069847 epoch total loss 6.30606413\n",
      "Trained batch 5057 batch loss 5.28773546 epoch total loss 6.3058629\n",
      "Trained batch 5058 batch loss 4.82380772 epoch total loss 6.30556965\n",
      "Trained batch 5059 batch loss 5.08679724 epoch total loss 6.30532885\n",
      "Trained batch 5060 batch loss 5.03330708 epoch total loss 6.30507755\n",
      "Trained batch 5061 batch loss 4.81158829 epoch total loss 6.30478239\n",
      "Trained batch 5062 batch loss 5.85551453 epoch total loss 6.3046937\n",
      "Trained batch 5063 batch loss 6.27912426 epoch total loss 6.30468845\n",
      "Trained batch 5064 batch loss 4.64767075 epoch total loss 6.30436182\n",
      "Trained batch 5065 batch loss 5.29274 epoch total loss 6.30416203\n",
      "Trained batch 5066 batch loss 4.95743275 epoch total loss 6.30389595\n",
      "Trained batch 5067 batch loss 5.46141148 epoch total loss 6.30372953\n",
      "Trained batch 5068 batch loss 4.21941662 epoch total loss 6.30331802\n",
      "Trained batch 5069 batch loss 4.73931694 epoch total loss 6.30301\n",
      "Trained batch 5070 batch loss 4.92143583 epoch total loss 6.30273724\n",
      "Trained batch 5071 batch loss 4.54355335 epoch total loss 6.30239058\n",
      "Trained batch 5072 batch loss 5.01956081 epoch total loss 6.30213737\n",
      "Trained batch 5073 batch loss 6.43343687 epoch total loss 6.3021636\n",
      "Trained batch 5074 batch loss 5.79140663 epoch total loss 6.30206251\n",
      "Trained batch 5075 batch loss 5.31596756 epoch total loss 6.30186844\n",
      "Trained batch 5076 batch loss 5.52977848 epoch total loss 6.30171633\n",
      "Trained batch 5077 batch loss 5.68620205 epoch total loss 6.30159473\n",
      "Trained batch 5078 batch loss 6.66110086 epoch total loss 6.30166531\n",
      "Trained batch 5079 batch loss 6.99125481 epoch total loss 6.30180168\n",
      "Trained batch 5080 batch loss 5.97914648 epoch total loss 6.30173779\n",
      "Trained batch 5081 batch loss 6.26997757 epoch total loss 6.30173159\n",
      "Trained batch 5082 batch loss 6.25393057 epoch total loss 6.30172205\n",
      "Trained batch 5083 batch loss 5.82506084 epoch total loss 6.30162811\n",
      "Trained batch 5084 batch loss 5.06759453 epoch total loss 6.3013854\n",
      "Trained batch 5085 batch loss 4.84149742 epoch total loss 6.30109835\n",
      "Trained batch 5086 batch loss 4.97551 epoch total loss 6.30083752\n",
      "Trained batch 5087 batch loss 4.69389057 epoch total loss 6.30052185\n",
      "Trained batch 5088 batch loss 7.68067074 epoch total loss 6.30079317\n",
      "Trained batch 5089 batch loss 7.27758312 epoch total loss 6.30098486\n",
      "Trained batch 5090 batch loss 5.3527422 epoch total loss 6.30079889\n",
      "Trained batch 5091 batch loss 6.86495972 epoch total loss 6.30091\n",
      "Trained batch 5092 batch loss 6.71492 epoch total loss 6.30099106\n",
      "Trained batch 5093 batch loss 6.77126 epoch total loss 6.30108356\n",
      "Trained batch 5094 batch loss 6.31311893 epoch total loss 6.30108547\n",
      "Trained batch 5095 batch loss 6.02792454 epoch total loss 6.30103207\n",
      "Trained batch 5096 batch loss 6.51556587 epoch total loss 6.30107403\n",
      "Trained batch 5097 batch loss 6.55748081 epoch total loss 6.3011241\n",
      "Trained batch 5098 batch loss 6.4017477 epoch total loss 6.30114412\n",
      "Trained batch 5099 batch loss 5.77535486 epoch total loss 6.30104113\n",
      "Trained batch 5100 batch loss 6.04186535 epoch total loss 6.30099\n",
      "Trained batch 5101 batch loss 6.33558559 epoch total loss 6.30099678\n",
      "Trained batch 5102 batch loss 5.84750032 epoch total loss 6.30090809\n",
      "Trained batch 5103 batch loss 5.71281433 epoch total loss 6.30079269\n",
      "Trained batch 5104 batch loss 6.42340088 epoch total loss 6.30081701\n",
      "Trained batch 5105 batch loss 6.43286228 epoch total loss 6.30084276\n",
      "Trained batch 5106 batch loss 6.52382278 epoch total loss 6.30088663\n",
      "Trained batch 5107 batch loss 6.83431435 epoch total loss 6.30099106\n",
      "Trained batch 5108 batch loss 6.26719 epoch total loss 6.30098438\n",
      "Trained batch 5109 batch loss 6.54871464 epoch total loss 6.30103302\n",
      "Trained batch 5110 batch loss 6.29299545 epoch total loss 6.30103111\n",
      "Trained batch 5111 batch loss 6.21546745 epoch total loss 6.30101442\n",
      "Trained batch 5112 batch loss 6.18466568 epoch total loss 6.30099154\n",
      "Trained batch 5113 batch loss 5.8279047 epoch total loss 6.30089951\n",
      "Trained batch 5114 batch loss 5.24720812 epoch total loss 6.30069351\n",
      "Trained batch 5115 batch loss 6.07240391 epoch total loss 6.30064869\n",
      "Trained batch 5116 batch loss 5.85854816 epoch total loss 6.30056238\n",
      "Trained batch 5117 batch loss 6.0991 epoch total loss 6.30052328\n",
      "Trained batch 5118 batch loss 6.09853935 epoch total loss 6.3004837\n",
      "Trained batch 5119 batch loss 6.24667168 epoch total loss 6.30047274\n",
      "Trained batch 5120 batch loss 6.2002039 epoch total loss 6.30045366\n",
      "Trained batch 5121 batch loss 5.86718369 epoch total loss 6.30036879\n",
      "Trained batch 5122 batch loss 6.49620628 epoch total loss 6.30040741\n",
      "Trained batch 5123 batch loss 7.22437668 epoch total loss 6.30058765\n",
      "Trained batch 5124 batch loss 6.59521294 epoch total loss 6.30064535\n",
      "Trained batch 5125 batch loss 6.44412947 epoch total loss 6.30067301\n",
      "Trained batch 5126 batch loss 6.8014183 epoch total loss 6.30077076\n",
      "Trained batch 5127 batch loss 6.7972455 epoch total loss 6.30086756\n",
      "Trained batch 5128 batch loss 6.39929676 epoch total loss 6.30088615\n",
      "Trained batch 5129 batch loss 6.19365215 epoch total loss 6.30086517\n",
      "Trained batch 5130 batch loss 6.49384594 epoch total loss 6.30090332\n",
      "Trained batch 5131 batch loss 6.45402241 epoch total loss 6.30093288\n",
      "Trained batch 5132 batch loss 6.24590206 epoch total loss 6.30092192\n",
      "Trained batch 5133 batch loss 6.1700139 epoch total loss 6.30089664\n",
      "Trained batch 5134 batch loss 6.03304672 epoch total loss 6.30084419\n",
      "Trained batch 5135 batch loss 5.96304512 epoch total loss 6.30077839\n",
      "Trained batch 5136 batch loss 6.27619553 epoch total loss 6.30077362\n",
      "Trained batch 5137 batch loss 6.44917297 epoch total loss 6.30080271\n",
      "Trained batch 5138 batch loss 6.70207214 epoch total loss 6.30088043\n",
      "Trained batch 5139 batch loss 6.57190609 epoch total loss 6.30093336\n",
      "Trained batch 5140 batch loss 6.97146559 epoch total loss 6.30106354\n",
      "Trained batch 5141 batch loss 6.30640125 epoch total loss 6.30106449\n",
      "Trained batch 5142 batch loss 6.31039715 epoch total loss 6.3010664\n",
      "Trained batch 5143 batch loss 6.25877476 epoch total loss 6.30105829\n",
      "Trained batch 5144 batch loss 6.24938679 epoch total loss 6.30104828\n",
      "Trained batch 5145 batch loss 6.40117931 epoch total loss 6.30106735\n",
      "Trained batch 5146 batch loss 6.37922573 epoch total loss 6.30108261\n",
      "Trained batch 5147 batch loss 5.24918079 epoch total loss 6.30087852\n",
      "Trained batch 5148 batch loss 5.9145546 epoch total loss 6.30080318\n",
      "Trained batch 5149 batch loss 5.31774187 epoch total loss 6.30061245\n",
      "Trained batch 5150 batch loss 6.09363556 epoch total loss 6.3005724\n",
      "Trained batch 5151 batch loss 6.20717335 epoch total loss 6.30055428\n",
      "Trained batch 5152 batch loss 6.42360258 epoch total loss 6.30057812\n",
      "Trained batch 5153 batch loss 6.17169285 epoch total loss 6.30055285\n",
      "Trained batch 5154 batch loss 6.44391537 epoch total loss 6.30058098\n",
      "Trained batch 5155 batch loss 6.3671751 epoch total loss 6.30059385\n",
      "Trained batch 5156 batch loss 6.42543602 epoch total loss 6.30061817\n",
      "Trained batch 5157 batch loss 6.11421585 epoch total loss 6.30058146\n",
      "Trained batch 5158 batch loss 6.1410017 epoch total loss 6.30055046\n",
      "Trained batch 5159 batch loss 5.47552681 epoch total loss 6.30039072\n",
      "Trained batch 5160 batch loss 6.51231 epoch total loss 6.30043173\n",
      "Trained batch 5161 batch loss 5.98928 epoch total loss 6.30037117\n",
      "Trained batch 5162 batch loss 6.05783 epoch total loss 6.30032444\n",
      "Trained batch 5163 batch loss 6.15704918 epoch total loss 6.30029678\n",
      "Trained batch 5164 batch loss 5.50138426 epoch total loss 6.30014229\n",
      "Trained batch 5165 batch loss 6.24798059 epoch total loss 6.3001318\n",
      "Trained batch 5166 batch loss 6.15148163 epoch total loss 6.30010319\n",
      "Trained batch 5167 batch loss 5.94116783 epoch total loss 6.30003405\n",
      "Trained batch 5168 batch loss 6.22363186 epoch total loss 6.30001879\n",
      "Trained batch 5169 batch loss 6.54239273 epoch total loss 6.30006599\n",
      "Trained batch 5170 batch loss 5.87427139 epoch total loss 6.29998398\n",
      "Trained batch 5171 batch loss 4.70163679 epoch total loss 6.29967451\n",
      "Trained batch 5172 batch loss 5.69267941 epoch total loss 6.29955721\n",
      "Trained batch 5173 batch loss 6.14225388 epoch total loss 6.29952717\n",
      "Trained batch 5174 batch loss 5.92813206 epoch total loss 6.29945517\n",
      "Trained batch 5175 batch loss 5.10856915 epoch total loss 6.29922533\n",
      "Trained batch 5176 batch loss 5.30948544 epoch total loss 6.29903364\n",
      "Trained batch 5177 batch loss 6.00590801 epoch total loss 6.29897738\n",
      "Trained batch 5178 batch loss 6.18987703 epoch total loss 6.29895592\n",
      "Trained batch 5179 batch loss 6.27752876 epoch total loss 6.29895163\n",
      "Trained batch 5180 batch loss 5.60831642 epoch total loss 6.29881811\n",
      "Trained batch 5181 batch loss 6.13260365 epoch total loss 6.29878616\n",
      "Trained batch 5182 batch loss 6.06319237 epoch total loss 6.29874086\n",
      "Trained batch 5183 batch loss 5.5274229 epoch total loss 6.29859161\n",
      "Trained batch 5184 batch loss 6.62598515 epoch total loss 6.29865503\n",
      "Trained batch 5185 batch loss 6.88022852 epoch total loss 6.29876757\n",
      "Trained batch 5186 batch loss 6.48151255 epoch total loss 6.29880285\n",
      "Trained batch 5187 batch loss 6.53368378 epoch total loss 6.29884815\n",
      "Trained batch 5188 batch loss 7.12475586 epoch total loss 6.29900742\n",
      "Trained batch 5189 batch loss 6.03286362 epoch total loss 6.29895592\n",
      "Trained batch 5190 batch loss 6.07529783 epoch total loss 6.298913\n",
      "Trained batch 5191 batch loss 6.34860611 epoch total loss 6.29892254\n",
      "Trained batch 5192 batch loss 5.60714149 epoch total loss 6.2987895\n",
      "Trained batch 5193 batch loss 4.97990084 epoch total loss 6.29853535\n",
      "Trained batch 5194 batch loss 4.95616341 epoch total loss 6.29827738\n",
      "Trained batch 5195 batch loss 5.04607677 epoch total loss 6.29803658\n",
      "Trained batch 5196 batch loss 5.31248951 epoch total loss 6.29784679\n",
      "Trained batch 5197 batch loss 5.82848 epoch total loss 6.2977562\n",
      "Trained batch 5198 batch loss 6.25690079 epoch total loss 6.29774857\n",
      "Trained batch 5199 batch loss 5.87162971 epoch total loss 6.29766655\n",
      "Trained batch 5200 batch loss 4.94489336 epoch total loss 6.29740667\n",
      "Trained batch 5201 batch loss 5.51741028 epoch total loss 6.29725647\n",
      "Trained batch 5202 batch loss 6.37906933 epoch total loss 6.29727221\n",
      "Trained batch 5203 batch loss 6.15895796 epoch total loss 6.2972455\n",
      "Trained batch 5204 batch loss 5.12560272 epoch total loss 6.29702044\n",
      "Trained batch 5205 batch loss 5.17780828 epoch total loss 6.29680586\n",
      "Trained batch 5206 batch loss 6.543787 epoch total loss 6.29685307\n",
      "Trained batch 5207 batch loss 6.30125618 epoch total loss 6.29685402\n",
      "Trained batch 5208 batch loss 6.60005093 epoch total loss 6.29691267\n",
      "Trained batch 5209 batch loss 5.79324627 epoch total loss 6.29681587\n",
      "Trained batch 5210 batch loss 6.96295738 epoch total loss 6.29694414\n",
      "Trained batch 5211 batch loss 6.03398609 epoch total loss 6.2968936\n",
      "Trained batch 5212 batch loss 5.79158401 epoch total loss 6.29679728\n",
      "Trained batch 5213 batch loss 6.55475903 epoch total loss 6.29684639\n",
      "Trained batch 5214 batch loss 6.757833 epoch total loss 6.29693508\n",
      "Trained batch 5215 batch loss 6.54176712 epoch total loss 6.29698229\n",
      "Trained batch 5216 batch loss 6.82553434 epoch total loss 6.29708338\n",
      "Trained batch 5217 batch loss 6.71440029 epoch total loss 6.29716349\n",
      "Trained batch 5218 batch loss 5.73317 epoch total loss 6.29705524\n",
      "Trained batch 5219 batch loss 6.4731884 epoch total loss 6.2970891\n",
      "Trained batch 5220 batch loss 6.85469532 epoch total loss 6.29719591\n",
      "Trained batch 5221 batch loss 6.43840599 epoch total loss 6.29722309\n",
      "Trained batch 5222 batch loss 6.91857529 epoch total loss 6.29734182\n",
      "Trained batch 5223 batch loss 6.34122944 epoch total loss 6.29735\n",
      "Trained batch 5224 batch loss 6.73054218 epoch total loss 6.2974329\n",
      "Trained batch 5225 batch loss 6.37080956 epoch total loss 6.29744673\n",
      "Trained batch 5226 batch loss 6.32178307 epoch total loss 6.2974515\n",
      "Trained batch 5227 batch loss 6.37291956 epoch total loss 6.29746532\n",
      "Trained batch 5228 batch loss 6.44133568 epoch total loss 6.29749298\n",
      "Trained batch 5229 batch loss 6.55615854 epoch total loss 6.2975421\n",
      "Trained batch 5230 batch loss 6.02058506 epoch total loss 6.29748917\n",
      "Trained batch 5231 batch loss 6.19828415 epoch total loss 6.29747\n",
      "Trained batch 5232 batch loss 5.95499802 epoch total loss 6.29740429\n",
      "Trained batch 5233 batch loss 6.29325104 epoch total loss 6.29740334\n",
      "Trained batch 5234 batch loss 6.26955462 epoch total loss 6.29739809\n",
      "Trained batch 5235 batch loss 6.3134079 epoch total loss 6.29740095\n",
      "Trained batch 5236 batch loss 6.08799553 epoch total loss 6.29736137\n",
      "Trained batch 5237 batch loss 6.31780434 epoch total loss 6.29736519\n",
      "Trained batch 5238 batch loss 6.11571741 epoch total loss 6.29733086\n",
      "Trained batch 5239 batch loss 6.63161659 epoch total loss 6.29739475\n",
      "Trained batch 5240 batch loss 6.40637541 epoch total loss 6.29741526\n",
      "Trained batch 5241 batch loss 7.08847523 epoch total loss 6.29756689\n",
      "Trained batch 5242 batch loss 7.53315258 epoch total loss 6.29780197\n",
      "Trained batch 5243 batch loss 7.95775509 epoch total loss 6.29811859\n",
      "Trained batch 5244 batch loss 7.70010662 epoch total loss 6.29838562\n",
      "Trained batch 5245 batch loss 7.24023914 epoch total loss 6.29856539\n",
      "Trained batch 5246 batch loss 7.40082645 epoch total loss 6.29877615\n",
      "Trained batch 5247 batch loss 7.31557846 epoch total loss 6.29896975\n",
      "Trained batch 5248 batch loss 6.99732113 epoch total loss 6.29910278\n",
      "Trained batch 5249 batch loss 6.57535172 epoch total loss 6.29915524\n",
      "Trained batch 5250 batch loss 6.57903099 epoch total loss 6.29920816\n",
      "Trained batch 5251 batch loss 6.6338129 epoch total loss 6.29927206\n",
      "Trained batch 5252 batch loss 6.16271973 epoch total loss 6.29924631\n",
      "Trained batch 5253 batch loss 6.62307358 epoch total loss 6.2993083\n",
      "Trained batch 5254 batch loss 6.59271622 epoch total loss 6.29936409\n",
      "Trained batch 5255 batch loss 6.57075787 epoch total loss 6.29941559\n",
      "Trained batch 5256 batch loss 6.69493246 epoch total loss 6.29949093\n",
      "Trained batch 5257 batch loss 6.18628693 epoch total loss 6.29947\n",
      "Trained batch 5258 batch loss 6.35194969 epoch total loss 6.29947948\n",
      "Trained batch 5259 batch loss 5.02879906 epoch total loss 6.29923773\n",
      "Trained batch 5260 batch loss 4.23904753 epoch total loss 6.29884577\n",
      "Trained batch 5261 batch loss 5.69171286 epoch total loss 6.29873037\n",
      "Trained batch 5262 batch loss 5.75186777 epoch total loss 6.29862642\n",
      "Trained batch 5263 batch loss 5.74571037 epoch total loss 6.29852104\n",
      "Trained batch 5264 batch loss 6.70266962 epoch total loss 6.29859829\n",
      "Trained batch 5265 batch loss 5.38843679 epoch total loss 6.29842472\n",
      "Trained batch 5266 batch loss 6.62255192 epoch total loss 6.29848623\n",
      "Trained batch 5267 batch loss 5.8246851 epoch total loss 6.29839611\n",
      "Trained batch 5268 batch loss 6.29969788 epoch total loss 6.29839659\n",
      "Trained batch 5269 batch loss 5.88666248 epoch total loss 6.29831839\n",
      "Trained batch 5270 batch loss 6.83292866 epoch total loss 6.29841948\n",
      "Trained batch 5271 batch loss 6.05846357 epoch total loss 6.29837418\n",
      "Trained batch 5272 batch loss 5.17538309 epoch total loss 6.29816151\n",
      "Trained batch 5273 batch loss 6.1321764 epoch total loss 6.29813\n",
      "Trained batch 5274 batch loss 6.09651518 epoch total loss 6.29809189\n",
      "Trained batch 5275 batch loss 6.2821722 epoch total loss 6.29808855\n",
      "Trained batch 5276 batch loss 5.38065577 epoch total loss 6.29791451\n",
      "Trained batch 5277 batch loss 5.32154036 epoch total loss 6.29772902\n",
      "Trained batch 5278 batch loss 4.78666782 epoch total loss 6.29744244\n",
      "Trained batch 5279 batch loss 4.75656414 epoch total loss 6.29715109\n",
      "Trained batch 5280 batch loss 4.79573202 epoch total loss 6.29686689\n",
      "Trained batch 5281 batch loss 4.92985296 epoch total loss 6.29660797\n",
      "Trained batch 5282 batch loss 5.00741291 epoch total loss 6.29636383\n",
      "Trained batch 5283 batch loss 4.86482811 epoch total loss 6.29609251\n",
      "Trained batch 5284 batch loss 4.75072384 epoch total loss 6.2958\n",
      "Trained batch 5285 batch loss 4.80180264 epoch total loss 6.29551744\n",
      "Trained batch 5286 batch loss 4.89374447 epoch total loss 6.29525232\n",
      "Trained batch 5287 batch loss 4.75550556 epoch total loss 6.2949605\n",
      "Trained batch 5288 batch loss 4.8256321 epoch total loss 6.2946825\n",
      "Trained batch 5289 batch loss 5.99619675 epoch total loss 6.29462624\n",
      "Trained batch 5290 batch loss 6.62149906 epoch total loss 6.29468775\n",
      "Trained batch 5291 batch loss 5.67054033 epoch total loss 6.29457\n",
      "Trained batch 5292 batch loss 6.5731945 epoch total loss 6.2946229\n",
      "Trained batch 5293 batch loss 6.16189432 epoch total loss 6.29459763\n",
      "Trained batch 5294 batch loss 6.06717205 epoch total loss 6.29455423\n",
      "Trained batch 5295 batch loss 5.56235552 epoch total loss 6.29441595\n",
      "Trained batch 5296 batch loss 5.89408302 epoch total loss 6.29434061\n",
      "Trained batch 5297 batch loss 6.73481941 epoch total loss 6.29442358\n",
      "Trained batch 5298 batch loss 6.69198132 epoch total loss 6.29449844\n",
      "Trained batch 5299 batch loss 6.52000618 epoch total loss 6.29454136\n",
      "Trained batch 5300 batch loss 6.33803654 epoch total loss 6.29454947\n",
      "Trained batch 5301 batch loss 5.91929817 epoch total loss 6.29447842\n",
      "Trained batch 5302 batch loss 5.18566799 epoch total loss 6.29427\n",
      "Trained batch 5303 batch loss 5.32792854 epoch total loss 6.29408789\n",
      "Trained batch 5304 batch loss 5.08901787 epoch total loss 6.29386044\n",
      "Trained batch 5305 batch loss 4.04256201 epoch total loss 6.29343653\n",
      "Trained batch 5306 batch loss 4.0518856 epoch total loss 6.29301357\n",
      "Trained batch 5307 batch loss 4.34335899 epoch total loss 6.29264641\n",
      "Trained batch 5308 batch loss 4.4486866 epoch total loss 6.29229927\n",
      "Trained batch 5309 batch loss 4.82322693 epoch total loss 6.29202271\n",
      "Trained batch 5310 batch loss 4.74479532 epoch total loss 6.29173136\n",
      "Trained batch 5311 batch loss 4.89362621 epoch total loss 6.29146814\n",
      "Trained batch 5312 batch loss 4.73655844 epoch total loss 6.29117584\n",
      "Trained batch 5313 batch loss 4.60684109 epoch total loss 6.29085875\n",
      "Trained batch 5314 batch loss 4.74437571 epoch total loss 6.29056787\n",
      "Trained batch 5315 batch loss 4.80464268 epoch total loss 6.29028845\n",
      "Trained batch 5316 batch loss 4.89593029 epoch total loss 6.29002571\n",
      "Trained batch 5317 batch loss 4.74440479 epoch total loss 6.28973532\n",
      "Trained batch 5318 batch loss 4.84209633 epoch total loss 6.28946352\n",
      "Trained batch 5319 batch loss 4.6923666 epoch total loss 6.28916311\n",
      "Trained batch 5320 batch loss 4.86462 epoch total loss 6.28889513\n",
      "Trained batch 5321 batch loss 4.60551882 epoch total loss 6.28857851\n",
      "Trained batch 5322 batch loss 4.91661549 epoch total loss 6.28832102\n",
      "Trained batch 5323 batch loss 4.98919439 epoch total loss 6.28807688\n",
      "Trained batch 5324 batch loss 5.0913763 epoch total loss 6.28785181\n",
      "Trained batch 5325 batch loss 5.30477238 epoch total loss 6.28766727\n",
      "Trained batch 5326 batch loss 4.60249615 epoch total loss 6.28735065\n",
      "Trained batch 5327 batch loss 5.330091 epoch total loss 6.28717136\n",
      "Trained batch 5328 batch loss 4.62435961 epoch total loss 6.28685951\n",
      "Trained batch 5329 batch loss 5.35875273 epoch total loss 6.28668547\n",
      "Trained batch 5330 batch loss 6.05098152 epoch total loss 6.28664112\n",
      "Trained batch 5331 batch loss 6.04871464 epoch total loss 6.2865963\n",
      "Trained batch 5332 batch loss 6.08056545 epoch total loss 6.28655767\n",
      "Trained batch 5333 batch loss 6.73968887 epoch total loss 6.28664255\n",
      "Trained batch 5334 batch loss 5.29325867 epoch total loss 6.28645611\n",
      "Trained batch 5335 batch loss 5.26568747 epoch total loss 6.2862649\n",
      "Trained batch 5336 batch loss 4.76061964 epoch total loss 6.28597927\n",
      "Trained batch 5337 batch loss 5.65384436 epoch total loss 6.28586054\n",
      "Trained batch 5338 batch loss 6.15032673 epoch total loss 6.28583479\n",
      "Trained batch 5339 batch loss 6.15867186 epoch total loss 6.28581095\n",
      "Trained batch 5340 batch loss 6.60336208 epoch total loss 6.28587\n",
      "Trained batch 5341 batch loss 6.1072464 epoch total loss 6.28583622\n",
      "Trained batch 5342 batch loss 6.00671768 epoch total loss 6.28578424\n",
      "Trained batch 5343 batch loss 5.93482208 epoch total loss 6.28571844\n",
      "Trained batch 5344 batch loss 6.03681469 epoch total loss 6.28567171\n",
      "Trained batch 5345 batch loss 5.63717842 epoch total loss 6.28555\n",
      "Trained batch 5346 batch loss 6.37353086 epoch total loss 6.28556681\n",
      "Trained batch 5347 batch loss 5.81488371 epoch total loss 6.28547907\n",
      "Trained batch 5348 batch loss 5.79006863 epoch total loss 6.28538656\n",
      "Trained batch 5349 batch loss 6.59120846 epoch total loss 6.28544331\n",
      "Trained batch 5350 batch loss 6.53503513 epoch total loss 6.28549\n",
      "Trained batch 5351 batch loss 6.40842056 epoch total loss 6.2855134\n",
      "Trained batch 5352 batch loss 6.59375668 epoch total loss 6.28557062\n",
      "Trained batch 5353 batch loss 6.47938728 epoch total loss 6.28560734\n",
      "Trained batch 5354 batch loss 6.51251602 epoch total loss 6.2856493\n",
      "Trained batch 5355 batch loss 6.35047579 epoch total loss 6.2856617\n",
      "Trained batch 5356 batch loss 6.2747159 epoch total loss 6.28565931\n",
      "Trained batch 5357 batch loss 6.6434288 epoch total loss 6.28572655\n",
      "Trained batch 5358 batch loss 6.84004593 epoch total loss 6.28583\n",
      "Trained batch 5359 batch loss 7.22549438 epoch total loss 6.2860055\n",
      "Trained batch 5360 batch loss 6.32130909 epoch total loss 6.2860117\n",
      "Trained batch 5361 batch loss 7.01383972 epoch total loss 6.28614807\n",
      "Trained batch 5362 batch loss 6.81217194 epoch total loss 6.2862463\n",
      "Trained batch 5363 batch loss 6.93960476 epoch total loss 6.28636837\n",
      "Trained batch 5364 batch loss 7.60160589 epoch total loss 6.28661346\n",
      "Trained batch 5365 batch loss 6.77942 epoch total loss 6.28670549\n",
      "Trained batch 5366 batch loss 6.46248055 epoch total loss 6.28673792\n",
      "Trained batch 5367 batch loss 7.37716579 epoch total loss 6.28694153\n",
      "Trained batch 5368 batch loss 7.19679928 epoch total loss 6.28711081\n",
      "Trained batch 5369 batch loss 6.78395176 epoch total loss 6.28720379\n",
      "Trained batch 5370 batch loss 6.97340488 epoch total loss 6.2873311\n",
      "Trained batch 5371 batch loss 7.12615 epoch total loss 6.28748703\n",
      "Trained batch 5372 batch loss 7.10422754 epoch total loss 6.28763962\n",
      "Trained batch 5373 batch loss 5.35012 epoch total loss 6.2874651\n",
      "Trained batch 5374 batch loss 6.40439796 epoch total loss 6.28748751\n",
      "Trained batch 5375 batch loss 6.73941135 epoch total loss 6.28757143\n",
      "Trained batch 5376 batch loss 6.48930883 epoch total loss 6.28760862\n",
      "Trained batch 5377 batch loss 6.72204924 epoch total loss 6.28768969\n",
      "Trained batch 5378 batch loss 6.30359268 epoch total loss 6.28769255\n",
      "Trained batch 5379 batch loss 6.98038387 epoch total loss 6.28782129\n",
      "Trained batch 5380 batch loss 7.17567587 epoch total loss 6.28798628\n",
      "Trained batch 5381 batch loss 6.97106552 epoch total loss 6.28811359\n",
      "Trained batch 5382 batch loss 6.68967 epoch total loss 6.28818846\n",
      "Trained batch 5383 batch loss 6.53738403 epoch total loss 6.28823519\n",
      "Trained batch 5384 batch loss 6.52525282 epoch total loss 6.28827906\n",
      "Trained batch 5385 batch loss 6.31092596 epoch total loss 6.28828335\n",
      "Trained batch 5386 batch loss 6.55652142 epoch total loss 6.28833294\n",
      "Trained batch 5387 batch loss 5.7725029 epoch total loss 6.28823709\n",
      "Trained batch 5388 batch loss 6.69637966 epoch total loss 6.28831291\n",
      "Trained batch 5389 batch loss 7.09396 epoch total loss 6.28846216\n",
      "Trained batch 5390 batch loss 7.34261322 epoch total loss 6.28865814\n",
      "Trained batch 5391 batch loss 7.32005739 epoch total loss 6.28884935\n",
      "Trained batch 5392 batch loss 7.07706642 epoch total loss 6.28899574\n",
      "Trained batch 5393 batch loss 6.79606819 epoch total loss 6.28909\n",
      "Trained batch 5394 batch loss 7.52959204 epoch total loss 6.28932047\n",
      "Trained batch 5395 batch loss 6.93672752 epoch total loss 6.28944063\n",
      "Trained batch 5396 batch loss 6.64090157 epoch total loss 6.28950548\n",
      "Trained batch 5397 batch loss 6.59525871 epoch total loss 6.28956175\n",
      "Trained batch 5398 batch loss 6.71084738 epoch total loss 6.28964\n",
      "Trained batch 5399 batch loss 6.40033245 epoch total loss 6.28966\n",
      "Trained batch 5400 batch loss 6.39507961 epoch total loss 6.28967953\n",
      "Trained batch 5401 batch loss 6.57024336 epoch total loss 6.2897315\n",
      "Trained batch 5402 batch loss 6.47273064 epoch total loss 6.28976536\n",
      "Trained batch 5403 batch loss 6.10949707 epoch total loss 6.28973198\n",
      "Trained batch 5404 batch loss 6.72293615 epoch total loss 6.28981209\n",
      "Trained batch 5405 batch loss 6.88771105 epoch total loss 6.28992271\n",
      "Trained batch 5406 batch loss 6.76466751 epoch total loss 6.29001045\n",
      "Trained batch 5407 batch loss 6.50067282 epoch total loss 6.29004955\n",
      "Trained batch 5408 batch loss 6.25829411 epoch total loss 6.29004335\n",
      "Trained batch 5409 batch loss 6.22768879 epoch total loss 6.29003143\n",
      "Trained batch 5410 batch loss 5.80188656 epoch total loss 6.28994131\n",
      "Trained batch 5411 batch loss 4.93884563 epoch total loss 6.28969145\n",
      "Trained batch 5412 batch loss 5.38544178 epoch total loss 6.28952456\n",
      "Trained batch 5413 batch loss 5.15842628 epoch total loss 6.2893157\n",
      "Trained batch 5414 batch loss 5.11511135 epoch total loss 6.28909874\n",
      "Trained batch 5415 batch loss 5.62924194 epoch total loss 6.28897667\n",
      "Trained batch 5416 batch loss 6.73831892 epoch total loss 6.28905964\n",
      "Trained batch 5417 batch loss 6.81348705 epoch total loss 6.28915644\n",
      "Trained batch 5418 batch loss 6.8267107 epoch total loss 6.28925562\n",
      "Trained batch 5419 batch loss 6.89470243 epoch total loss 6.2893672\n",
      "Trained batch 5420 batch loss 6.4343071 epoch total loss 6.2893939\n",
      "Trained batch 5421 batch loss 6.49935627 epoch total loss 6.289433\n",
      "Trained batch 5422 batch loss 5.86666822 epoch total loss 6.2893548\n",
      "Trained batch 5423 batch loss 5.73069572 epoch total loss 6.2892518\n",
      "Trained batch 5424 batch loss 5.39832973 epoch total loss 6.28908777\n",
      "Trained batch 5425 batch loss 5.61647463 epoch total loss 6.28896379\n",
      "Trained batch 5426 batch loss 5.57067394 epoch total loss 6.28883123\n",
      "Trained batch 5427 batch loss 6.14909172 epoch total loss 6.28880548\n",
      "Trained batch 5428 batch loss 6.492589 epoch total loss 6.28884315\n",
      "Trained batch 5429 batch loss 6.36959553 epoch total loss 6.28885794\n",
      "Trained batch 5430 batch loss 6.70553541 epoch total loss 6.28893518\n",
      "Trained batch 5431 batch loss 6.64711475 epoch total loss 6.28900146\n",
      "Trained batch 5432 batch loss 6.57793903 epoch total loss 6.28905439\n",
      "Trained batch 5433 batch loss 7.27076483 epoch total loss 6.28923512\n",
      "Trained batch 5434 batch loss 6.56984 epoch total loss 6.28928661\n",
      "Trained batch 5435 batch loss 6.70949 epoch total loss 6.28936434\n",
      "Trained batch 5436 batch loss 5.7450695 epoch total loss 6.2892642\n",
      "Trained batch 5437 batch loss 6.21324921 epoch total loss 6.28925085\n",
      "Trained batch 5438 batch loss 6.05457735 epoch total loss 6.28920746\n",
      "Trained batch 5439 batch loss 5.67108154 epoch total loss 6.28909397\n",
      "Trained batch 5440 batch loss 7.76435375 epoch total loss 6.28936529\n",
      "Trained batch 5441 batch loss 6.63241673 epoch total loss 6.28942871\n",
      "Trained batch 5442 batch loss 6.4450655 epoch total loss 6.28945732\n",
      "Trained batch 5443 batch loss 6.53980637 epoch total loss 6.2895031\n",
      "Trained batch 5444 batch loss 6.18868446 epoch total loss 6.2894845\n",
      "Trained batch 5445 batch loss 6.3362155 epoch total loss 6.28949308\n",
      "Trained batch 5446 batch loss 6.30892849 epoch total loss 6.28949642\n",
      "Trained batch 5447 batch loss 6.24980354 epoch total loss 6.28948927\n",
      "Trained batch 5448 batch loss 5.82434559 epoch total loss 6.28940392\n",
      "Trained batch 5449 batch loss 5.89374065 epoch total loss 6.28933144\n",
      "Trained batch 5450 batch loss 6.41753 epoch total loss 6.2893548\n",
      "Trained batch 5451 batch loss 5.99190378 epoch total loss 6.28930044\n",
      "Trained batch 5452 batch loss 6.05888939 epoch total loss 6.289258\n",
      "Trained batch 5453 batch loss 6.91800976 epoch total loss 6.2893734\n",
      "Trained batch 5454 batch loss 6.88333797 epoch total loss 6.28948212\n",
      "Trained batch 5455 batch loss 6.76542711 epoch total loss 6.28956938\n",
      "Trained batch 5456 batch loss 6.6127243 epoch total loss 6.28962898\n",
      "Trained batch 5457 batch loss 6.30747271 epoch total loss 6.28963232\n",
      "Trained batch 5458 batch loss 6.70101643 epoch total loss 6.28970718\n",
      "Trained batch 5459 batch loss 6.67242479 epoch total loss 6.28977728\n",
      "Trained batch 5460 batch loss 7.34079266 epoch total loss 6.28996944\n",
      "Trained batch 5461 batch loss 6.56470823 epoch total loss 6.29002047\n",
      "Trained batch 5462 batch loss 5.7056427 epoch total loss 6.28991365\n",
      "Trained batch 5463 batch loss 5.1529007 epoch total loss 6.28970528\n",
      "Trained batch 5464 batch loss 4.9171524 epoch total loss 6.28945446\n",
      "Trained batch 5465 batch loss 5.4724865 epoch total loss 6.28930473\n",
      "Trained batch 5466 batch loss 5.52695274 epoch total loss 6.2891655\n",
      "Trained batch 5467 batch loss 5.86960697 epoch total loss 6.28908873\n",
      "Trained batch 5468 batch loss 5.72280693 epoch total loss 6.28898525\n",
      "Trained batch 5469 batch loss 6.57103729 epoch total loss 6.28903675\n",
      "Trained batch 5470 batch loss 6.59804344 epoch total loss 6.28909302\n",
      "Trained batch 5471 batch loss 5.49006 epoch total loss 6.28894663\n",
      "Trained batch 5472 batch loss 5.0218277 epoch total loss 6.28871536\n",
      "Trained batch 5473 batch loss 5.65287876 epoch total loss 6.28859949\n",
      "Trained batch 5474 batch loss 6.96746826 epoch total loss 6.28872347\n",
      "Trained batch 5475 batch loss 6.50442028 epoch total loss 6.28876305\n",
      "Trained batch 5476 batch loss 5.99912071 epoch total loss 6.28871\n",
      "Trained batch 5477 batch loss 7.01679182 epoch total loss 6.28884268\n",
      "Trained batch 5478 batch loss 6.79644108 epoch total loss 6.28893566\n",
      "Trained batch 5479 batch loss 6.50055122 epoch total loss 6.28897429\n",
      "Trained batch 5480 batch loss 5.86228848 epoch total loss 6.28889656\n",
      "Trained batch 5481 batch loss 7.05534363 epoch total loss 6.28903627\n",
      "Trained batch 5482 batch loss 6.46514177 epoch total loss 6.28906822\n",
      "Trained batch 5483 batch loss 6.52002621 epoch total loss 6.28911\n",
      "Trained batch 5484 batch loss 6.63369942 epoch total loss 6.28917313\n",
      "Trained batch 5485 batch loss 6.32963371 epoch total loss 6.28918\n",
      "Trained batch 5486 batch loss 6.05470753 epoch total loss 6.28913736\n",
      "Trained batch 5487 batch loss 6.3661232 epoch total loss 6.28915167\n",
      "Trained batch 5488 batch loss 6.79228973 epoch total loss 6.28924322\n",
      "Trained batch 5489 batch loss 6.67276049 epoch total loss 6.28931284\n",
      "Trained batch 5490 batch loss 6.56345177 epoch total loss 6.28936291\n",
      "Trained batch 5491 batch loss 5.93874884 epoch total loss 6.28929853\n",
      "Trained batch 5492 batch loss 6.43125343 epoch total loss 6.28932428\n",
      "Trained batch 5493 batch loss 6.16930723 epoch total loss 6.28930235\n",
      "Trained batch 5494 batch loss 6.02977705 epoch total loss 6.28925514\n",
      "Trained batch 5495 batch loss 6.01916265 epoch total loss 6.28920603\n",
      "Trained batch 5496 batch loss 6.3193264 epoch total loss 6.28921175\n",
      "Trained batch 5497 batch loss 5.80102253 epoch total loss 6.28912306\n",
      "Trained batch 5498 batch loss 6.26858044 epoch total loss 6.28911924\n",
      "Trained batch 5499 batch loss 6.77254963 epoch total loss 6.28920746\n",
      "Trained batch 5500 batch loss 6.34561539 epoch total loss 6.28921747\n",
      "Trained batch 5501 batch loss 6.69769907 epoch total loss 6.28929186\n",
      "Trained batch 5502 batch loss 6.86580944 epoch total loss 6.28939676\n",
      "Trained batch 5503 batch loss 6.81768227 epoch total loss 6.28949261\n",
      "Trained batch 5504 batch loss 6.14172602 epoch total loss 6.28946543\n",
      "Trained batch 5505 batch loss 6.22889805 epoch total loss 6.28945494\n",
      "Trained batch 5506 batch loss 6.32762623 epoch total loss 6.28946209\n",
      "Trained batch 5507 batch loss 6.3967495 epoch total loss 6.28948164\n",
      "Trained batch 5508 batch loss 6.29975414 epoch total loss 6.28948355\n",
      "Trained batch 5509 batch loss 6.589468 epoch total loss 6.28953838\n",
      "Trained batch 5510 batch loss 6.29114628 epoch total loss 6.28953886\n",
      "Trained batch 5511 batch loss 6.30011 epoch total loss 6.28954077\n",
      "Trained batch 5512 batch loss 5.94713545 epoch total loss 6.2894783\n",
      "Trained batch 5513 batch loss 6.20078802 epoch total loss 6.28946209\n",
      "Trained batch 5514 batch loss 6.02549171 epoch total loss 6.28941441\n",
      "Trained batch 5515 batch loss 6.0064106 epoch total loss 6.28936338\n",
      "Trained batch 5516 batch loss 6.48400211 epoch total loss 6.28939867\n",
      "Trained batch 5517 batch loss 6.0730257 epoch total loss 6.28936\n",
      "Trained batch 5518 batch loss 6.2162447 epoch total loss 6.28934622\n",
      "Trained batch 5519 batch loss 6.09032869 epoch total loss 6.28931046\n",
      "Trained batch 5520 batch loss 6.13714314 epoch total loss 6.2892828\n",
      "Trained batch 5521 batch loss 5.98536682 epoch total loss 6.28922749\n",
      "Trained batch 5522 batch loss 6.35747719 epoch total loss 6.28924\n",
      "Trained batch 5523 batch loss 5.85654306 epoch total loss 6.28916168\n",
      "Trained batch 5524 batch loss 5.5371232 epoch total loss 6.28902578\n",
      "Trained batch 5525 batch loss 5.421422 epoch total loss 6.2888689\n",
      "Trained batch 5526 batch loss 6.09035492 epoch total loss 6.28883266\n",
      "Trained batch 5527 batch loss 6.11464882 epoch total loss 6.28880119\n",
      "Trained batch 5528 batch loss 6.3895092 epoch total loss 6.28881931\n",
      "Trained batch 5529 batch loss 6.47540712 epoch total loss 6.28885317\n",
      "Trained batch 5530 batch loss 6.29487705 epoch total loss 6.28885412\n",
      "Trained batch 5531 batch loss 6.10171604 epoch total loss 6.28882027\n",
      "Trained batch 5532 batch loss 6.40420055 epoch total loss 6.28884077\n",
      "Trained batch 5533 batch loss 6.11787558 epoch total loss 6.28881\n",
      "Trained batch 5534 batch loss 5.82023096 epoch total loss 6.2887249\n",
      "Trained batch 5535 batch loss 5.89871883 epoch total loss 6.2886548\n",
      "Trained batch 5536 batch loss 6.21736526 epoch total loss 6.28864193\n",
      "Trained batch 5537 batch loss 5.96551418 epoch total loss 6.28858328\n",
      "Trained batch 5538 batch loss 5.81819344 epoch total loss 6.2884984\n",
      "Trained batch 5539 batch loss 5.82037258 epoch total loss 6.28841352\n",
      "Trained batch 5540 batch loss 5.90097237 epoch total loss 6.28834391\n",
      "Trained batch 5541 batch loss 6.31277084 epoch total loss 6.2883482\n",
      "Trained batch 5542 batch loss 5.61487722 epoch total loss 6.2882266\n",
      "Trained batch 5543 batch loss 5.99409199 epoch total loss 6.2881732\n",
      "Trained batch 5544 batch loss 6.75744915 epoch total loss 6.28825808\n",
      "Trained batch 5545 batch loss 6.90037632 epoch total loss 6.28836775\n",
      "Trained batch 5546 batch loss 6.60412884 epoch total loss 6.28842497\n",
      "Trained batch 5547 batch loss 6.51402 epoch total loss 6.28846598\n",
      "Trained batch 5548 batch loss 6.23631811 epoch total loss 6.28845644\n",
      "Trained batch 5549 batch loss 6.26926661 epoch total loss 6.2884531\n",
      "Trained batch 5550 batch loss 6.22855616 epoch total loss 6.28844261\n",
      "Trained batch 5551 batch loss 6.09763145 epoch total loss 6.28840828\n",
      "Trained batch 5552 batch loss 6.70041132 epoch total loss 6.28848219\n",
      "Trained batch 5553 batch loss 6.69667244 epoch total loss 6.28855515\n",
      "Trained batch 5554 batch loss 6.57540846 epoch total loss 6.28860664\n",
      "Trained batch 5555 batch loss 6.70654631 epoch total loss 6.28868198\n",
      "Trained batch 5556 batch loss 6.64835358 epoch total loss 6.28874683\n",
      "Trained batch 5557 batch loss 6.34569168 epoch total loss 6.28875685\n",
      "Trained batch 5558 batch loss 6.79629517 epoch total loss 6.28884792\n",
      "Trained batch 5559 batch loss 6.10762119 epoch total loss 6.28881598\n",
      "Trained batch 5560 batch loss 6.78992367 epoch total loss 6.28890562\n",
      "Trained batch 5561 batch loss 7.02148676 epoch total loss 6.2890377\n",
      "Trained batch 5562 batch loss 6.70125771 epoch total loss 6.28911257\n",
      "Trained batch 5563 batch loss 6.48584843 epoch total loss 6.28914738\n",
      "Trained batch 5564 batch loss 6.857759 epoch total loss 6.28925\n",
      "Trained batch 5565 batch loss 6.32894135 epoch total loss 6.28925705\n",
      "Trained batch 5566 batch loss 6.43024254 epoch total loss 6.28928232\n",
      "Trained batch 5567 batch loss 6.52901268 epoch total loss 6.28932476\n",
      "Trained batch 5568 batch loss 5.45469761 epoch total loss 6.28917456\n",
      "Trained batch 5569 batch loss 5.39465332 epoch total loss 6.28901434\n",
      "Trained batch 5570 batch loss 4.96080971 epoch total loss 6.28877544\n",
      "Trained batch 5571 batch loss 5.17857647 epoch total loss 6.2885766\n",
      "Trained batch 5572 batch loss 4.9506917 epoch total loss 6.28833628\n",
      "Trained batch 5573 batch loss 4.62200642 epoch total loss 6.28803682\n",
      "Trained batch 5574 batch loss 5.30352688 epoch total loss 6.28786039\n",
      "Trained batch 5575 batch loss 5.40427542 epoch total loss 6.28770161\n",
      "Trained batch 5576 batch loss 4.73465157 epoch total loss 6.28742313\n",
      "Trained batch 5577 batch loss 5.59956455 epoch total loss 6.28729963\n",
      "Trained batch 5578 batch loss 5.31308842 epoch total loss 6.28712463\n",
      "Trained batch 5579 batch loss 5.84108162 epoch total loss 6.28704453\n",
      "Trained batch 5580 batch loss 4.78559113 epoch total loss 6.28677559\n",
      "Trained batch 5581 batch loss 3.90361714 epoch total loss 6.28634834\n",
      "Trained batch 5582 batch loss 4.63959217 epoch total loss 6.28605318\n",
      "Trained batch 5583 batch loss 4.89473 epoch total loss 6.28580427\n",
      "Trained batch 5584 batch loss 4.79591894 epoch total loss 6.28553772\n",
      "Trained batch 5585 batch loss 4.85509205 epoch total loss 6.28528166\n",
      "Trained batch 5586 batch loss 4.58990383 epoch total loss 6.28497791\n",
      "Trained batch 5587 batch loss 6.20477581 epoch total loss 6.28496313\n",
      "Trained batch 5588 batch loss 6.58908939 epoch total loss 6.28501797\n",
      "Trained batch 5589 batch loss 6.26817369 epoch total loss 6.28501511\n",
      "Trained batch 5590 batch loss 5.46751213 epoch total loss 6.28486919\n",
      "Trained batch 5591 batch loss 6.38676596 epoch total loss 6.28488731\n",
      "Trained batch 5592 batch loss 6.87700272 epoch total loss 6.28499365\n",
      "Trained batch 5593 batch loss 7.36114025 epoch total loss 6.28518581\n",
      "Trained batch 5594 batch loss 6.01803923 epoch total loss 6.28513813\n",
      "Trained batch 5595 batch loss 6.06320333 epoch total loss 6.28509808\n",
      "Trained batch 5596 batch loss 6.58401489 epoch total loss 6.28515196\n",
      "Trained batch 5597 batch loss 6.42505932 epoch total loss 6.28517723\n",
      "Trained batch 5598 batch loss 6.64370251 epoch total loss 6.2852416\n",
      "Trained batch 5599 batch loss 6.36067438 epoch total loss 6.28525448\n",
      "Trained batch 5600 batch loss 5.87932 epoch total loss 6.285182\n",
      "Trained batch 5601 batch loss 6.8585 epoch total loss 6.28528452\n",
      "Trained batch 5602 batch loss 6.36273575 epoch total loss 6.28529835\n",
      "Trained batch 5603 batch loss 6.43205118 epoch total loss 6.28532505\n",
      "Trained batch 5604 batch loss 6.98921251 epoch total loss 6.28545046\n",
      "Trained batch 5605 batch loss 6.60063267 epoch total loss 6.28550673\n",
      "Trained batch 5606 batch loss 6.42069 epoch total loss 6.28553104\n",
      "Trained batch 5607 batch loss 5.81742334 epoch total loss 6.2854476\n",
      "Trained batch 5608 batch loss 6.41995907 epoch total loss 6.28547192\n",
      "Trained batch 5609 batch loss 6.81895685 epoch total loss 6.28556728\n",
      "Trained batch 5610 batch loss 5.29425144 epoch total loss 6.28539038\n",
      "Trained batch 5611 batch loss 6.33274174 epoch total loss 6.28539848\n",
      "Trained batch 5612 batch loss 6.25045824 epoch total loss 6.28539228\n",
      "Trained batch 5613 batch loss 5.93828917 epoch total loss 6.2853303\n",
      "Trained batch 5614 batch loss 6.34538698 epoch total loss 6.28534079\n",
      "Trained batch 5615 batch loss 6.27088833 epoch total loss 6.28533792\n",
      "Trained batch 5616 batch loss 6.42364788 epoch total loss 6.28536224\n",
      "Trained batch 5617 batch loss 4.87154198 epoch total loss 6.28511047\n",
      "Trained batch 5618 batch loss 6.07574606 epoch total loss 6.2850728\n",
      "Trained batch 5619 batch loss 6.45565128 epoch total loss 6.28510332\n",
      "Trained batch 5620 batch loss 5.98930931 epoch total loss 6.28505039\n",
      "Trained batch 5621 batch loss 6.55841589 epoch total loss 6.28509903\n",
      "Trained batch 5622 batch loss 6.28825903 epoch total loss 6.2851\n",
      "Trained batch 5623 batch loss 6.1318655 epoch total loss 6.2850728\n",
      "Trained batch 5624 batch loss 4.93169975 epoch total loss 6.28483248\n",
      "Trained batch 5625 batch loss 6.30680323 epoch total loss 6.28483677\n",
      "Trained batch 5626 batch loss 6.16289234 epoch total loss 6.28481531\n",
      "Trained batch 5627 batch loss 6.24427176 epoch total loss 6.28480864\n",
      "Trained batch 5628 batch loss 6.41280556 epoch total loss 6.28483152\n",
      "Trained batch 5629 batch loss 6.27395535 epoch total loss 6.28482962\n",
      "Trained batch 5630 batch loss 6.72561073 epoch total loss 6.28490782\n",
      "Trained batch 5631 batch loss 6.38507414 epoch total loss 6.28492594\n",
      "Trained batch 5632 batch loss 6.28839302 epoch total loss 6.28492689\n",
      "Trained batch 5633 batch loss 6.21574736 epoch total loss 6.28491402\n",
      "Trained batch 5634 batch loss 6.20114613 epoch total loss 6.28489923\n",
      "Trained batch 5635 batch loss 6.53254604 epoch total loss 6.28494263\n",
      "Trained batch 5636 batch loss 6.15805912 epoch total loss 6.28491974\n",
      "Trained batch 5637 batch loss 6.36332 epoch total loss 6.28493404\n",
      "Trained batch 5638 batch loss 6.02040863 epoch total loss 6.28488684\n",
      "Trained batch 5639 batch loss 5.91798973 epoch total loss 6.28482151\n",
      "Trained batch 5640 batch loss 6.27372 epoch total loss 6.2848196\n",
      "Trained batch 5641 batch loss 6.20787954 epoch total loss 6.28480577\n",
      "Trained batch 5642 batch loss 6.08576202 epoch total loss 6.28477049\n",
      "Trained batch 5643 batch loss 6.20824814 epoch total loss 6.28475666\n",
      "Trained batch 5644 batch loss 6.10752678 epoch total loss 6.28472567\n",
      "Trained batch 5645 batch loss 6.57351208 epoch total loss 6.28477716\n",
      "Trained batch 5646 batch loss 4.78550625 epoch total loss 6.28451157\n",
      "Trained batch 5647 batch loss 6.18309975 epoch total loss 6.28449345\n",
      "Trained batch 5648 batch loss 5.69345188 epoch total loss 6.28438902\n",
      "Trained batch 5649 batch loss 7.22959232 epoch total loss 6.28455687\n",
      "Trained batch 5650 batch loss 6.51124477 epoch total loss 6.28459692\n",
      "Trained batch 5651 batch loss 5.44497871 epoch total loss 6.28444862\n",
      "Trained batch 5652 batch loss 4.07670975 epoch total loss 6.28405809\n",
      "Trained batch 5653 batch loss 6.04878426 epoch total loss 6.28401613\n",
      "Trained batch 5654 batch loss 6.84099054 epoch total loss 6.28411436\n",
      "Trained batch 5655 batch loss 5.57290936 epoch total loss 6.28398895\n",
      "Trained batch 5656 batch loss 6.29752541 epoch total loss 6.28399134\n",
      "Trained batch 5657 batch loss 4.97263718 epoch total loss 6.28375912\n",
      "Trained batch 5658 batch loss 5.71281242 epoch total loss 6.28365803\n",
      "Trained batch 5659 batch loss 5.84837627 epoch total loss 6.28358126\n",
      "Trained batch 5660 batch loss 6.74244881 epoch total loss 6.28366184\n",
      "Trained batch 5661 batch loss 5.42260361 epoch total loss 6.28350973\n",
      "Trained batch 5662 batch loss 6.12533855 epoch total loss 6.28348207\n",
      "Trained batch 5663 batch loss 6.05158138 epoch total loss 6.28344059\n",
      "Trained batch 5664 batch loss 6.01591635 epoch total loss 6.28339338\n",
      "Trained batch 5665 batch loss 5.65250397 epoch total loss 6.28328228\n",
      "Trained batch 5666 batch loss 6.16257 epoch total loss 6.28326082\n",
      "Trained batch 5667 batch loss 5.87489 epoch total loss 6.28318882\n",
      "Trained batch 5668 batch loss 6.03142118 epoch total loss 6.28314447\n",
      "Trained batch 5669 batch loss 6.30233574 epoch total loss 6.28314781\n",
      "Trained batch 5670 batch loss 5.98790312 epoch total loss 6.28309584\n",
      "Trained batch 5671 batch loss 6.30890322 epoch total loss 6.2831\n",
      "Trained batch 5672 batch loss 6.14101505 epoch total loss 6.28307486\n",
      "Trained batch 5673 batch loss 6.05377722 epoch total loss 6.2830348\n",
      "Trained batch 5674 batch loss 6.71117687 epoch total loss 6.28311\n",
      "Trained batch 5675 batch loss 5.92218876 epoch total loss 6.28304672\n",
      "Trained batch 5676 batch loss 5.96373272 epoch total loss 6.28299046\n",
      "Trained batch 5677 batch loss 5.75553417 epoch total loss 6.28289747\n",
      "Trained batch 5678 batch loss 5.75660133 epoch total loss 6.28280497\n",
      "Trained batch 5679 batch loss 6.28881359 epoch total loss 6.28280592\n",
      "Trained batch 5680 batch loss 5.85246086 epoch total loss 6.28273\n",
      "Trained batch 5681 batch loss 5.22044468 epoch total loss 6.28254271\n",
      "Trained batch 5682 batch loss 5.9548111 epoch total loss 6.28248453\n",
      "Trained batch 5683 batch loss 6.34894753 epoch total loss 6.28249598\n",
      "Trained batch 5684 batch loss 6.30521202 epoch total loss 6.28250027\n",
      "Trained batch 5685 batch loss 5.77832127 epoch total loss 6.2824111\n",
      "Trained batch 5686 batch loss 6.06513882 epoch total loss 6.28237343\n",
      "Trained batch 5687 batch loss 6.44029045 epoch total loss 6.28240108\n",
      "Trained batch 5688 batch loss 5.9274826 epoch total loss 6.28233862\n",
      "Trained batch 5689 batch loss 5.92568684 epoch total loss 6.28227568\n",
      "Trained batch 5690 batch loss 6.33367252 epoch total loss 6.28228474\n",
      "Trained batch 5691 batch loss 6.13465405 epoch total loss 6.28225851\n",
      "Trained batch 5692 batch loss 6.2638278 epoch total loss 6.28225517\n",
      "Trained batch 5693 batch loss 6.19227505 epoch total loss 6.28223944\n",
      "Trained batch 5694 batch loss 6.23624659 epoch total loss 6.28223085\n",
      "Trained batch 5695 batch loss 6.57970142 epoch total loss 6.28228283\n",
      "Trained batch 5696 batch loss 6.41615295 epoch total loss 6.28230667\n",
      "Trained batch 5697 batch loss 6.2636652 epoch total loss 6.28230333\n",
      "Trained batch 5698 batch loss 6.41425323 epoch total loss 6.28232622\n",
      "Trained batch 5699 batch loss 6.00478601 epoch total loss 6.28227758\n",
      "Trained batch 5700 batch loss 6.14774227 epoch total loss 6.28225374\n",
      "Trained batch 5701 batch loss 6.25617409 epoch total loss 6.28224945\n",
      "Trained batch 5702 batch loss 6.2274518 epoch total loss 6.28224\n",
      "Trained batch 5703 batch loss 5.6769 epoch total loss 6.28213358\n",
      "Trained batch 5704 batch loss 6.09748173 epoch total loss 6.28210115\n",
      "Trained batch 5705 batch loss 5.91612053 epoch total loss 6.28203726\n",
      "Trained batch 5706 batch loss 6.08035803 epoch total loss 6.28200245\n",
      "Trained batch 5707 batch loss 6.33594799 epoch total loss 6.28201199\n",
      "Trained batch 5708 batch loss 6.04332829 epoch total loss 6.28197\n",
      "Trained batch 5709 batch loss 6.34908724 epoch total loss 6.28198147\n",
      "Trained batch 5710 batch loss 5.72027874 epoch total loss 6.28188276\n",
      "Trained batch 5711 batch loss 6.37617445 epoch total loss 6.28189898\n",
      "Trained batch 5712 batch loss 5.91777658 epoch total loss 6.28183556\n",
      "Trained batch 5713 batch loss 5.94642687 epoch total loss 6.28177643\n",
      "Trained batch 5714 batch loss 6.15371704 epoch total loss 6.28175402\n",
      "Trained batch 5715 batch loss 6.24129677 epoch total loss 6.28174686\n",
      "Trained batch 5716 batch loss 5.79801083 epoch total loss 6.28166199\n",
      "Trained batch 5717 batch loss 5.80685425 epoch total loss 6.28157949\n",
      "Trained batch 5718 batch loss 6.42449236 epoch total loss 6.28160477\n",
      "Trained batch 5719 batch loss 6.53666687 epoch total loss 6.28164911\n",
      "Trained batch 5720 batch loss 6.17026663 epoch total loss 6.28162956\n",
      "Trained batch 5721 batch loss 6.16432 epoch total loss 6.28160906\n",
      "Trained batch 5722 batch loss 5.93980694 epoch total loss 6.28155\n",
      "Trained batch 5723 batch loss 5.99522924 epoch total loss 6.2815\n",
      "Trained batch 5724 batch loss 5.97861433 epoch total loss 6.28144741\n",
      "Trained batch 5725 batch loss 6.39471149 epoch total loss 6.28146696\n",
      "Trained batch 5726 batch loss 6.26054287 epoch total loss 6.28146362\n",
      "Trained batch 5727 batch loss 4.93595314 epoch total loss 6.28122902\n",
      "Trained batch 5728 batch loss 4.76521349 epoch total loss 6.28096437\n",
      "Trained batch 5729 batch loss 5.12391472 epoch total loss 6.28076267\n",
      "Trained batch 5730 batch loss 5.25429726 epoch total loss 6.28058338\n",
      "Trained batch 5731 batch loss 5.51239 epoch total loss 6.28044891\n",
      "Trained batch 5732 batch loss 5.7940774 epoch total loss 6.28036404\n",
      "Trained batch 5733 batch loss 6.87309647 epoch total loss 6.28046799\n",
      "Trained batch 5734 batch loss 5.79359627 epoch total loss 6.28038263\n",
      "Trained batch 5735 batch loss 6.4467206 epoch total loss 6.28041172\n",
      "Trained batch 5736 batch loss 5.94167852 epoch total loss 6.28035259\n",
      "Trained batch 5737 batch loss 5.92906761 epoch total loss 6.28029108\n",
      "Trained batch 5738 batch loss 5.9893074 epoch total loss 6.28024054\n",
      "Trained batch 5739 batch loss 5.75175095 epoch total loss 6.28014803\n",
      "Trained batch 5740 batch loss 5.99441957 epoch total loss 6.28009844\n",
      "Trained batch 5741 batch loss 6.70293045 epoch total loss 6.28017235\n",
      "Trained batch 5742 batch loss 6.48693609 epoch total loss 6.28020859\n",
      "Trained batch 5743 batch loss 6.89223194 epoch total loss 6.28031492\n",
      "Trained batch 5744 batch loss 6.57604885 epoch total loss 6.28036594\n",
      "Trained batch 5745 batch loss 8.2922287 epoch total loss 6.28071642\n",
      "Trained batch 5746 batch loss 7.07533836 epoch total loss 6.28085423\n",
      "Trained batch 5747 batch loss 5.78707218 epoch total loss 6.28076792\n",
      "Trained batch 5748 batch loss 6.99804926 epoch total loss 6.28089333\n",
      "Trained batch 5749 batch loss 6.63830614 epoch total loss 6.28095531\n",
      "Trained batch 5750 batch loss 6.32273865 epoch total loss 6.28096247\n",
      "Trained batch 5751 batch loss 6.25663233 epoch total loss 6.28095865\n",
      "Trained batch 5752 batch loss 6.40219116 epoch total loss 6.28097963\n",
      "Trained batch 5753 batch loss 6.34659195 epoch total loss 6.28099108\n",
      "Trained batch 5754 batch loss 6.40382767 epoch total loss 6.28101254\n",
      "Trained batch 5755 batch loss 6.02021694 epoch total loss 6.28096676\n",
      "Trained batch 5756 batch loss 4.90780163 epoch total loss 6.28072834\n",
      "Trained batch 5757 batch loss 5.5742712 epoch total loss 6.28060532\n",
      "Trained batch 5758 batch loss 5.11355877 epoch total loss 6.28040266\n",
      "Trained batch 5759 batch loss 5.59925652 epoch total loss 6.28028393\n",
      "Trained batch 5760 batch loss 5.55679035 epoch total loss 6.280159\n",
      "Trained batch 5761 batch loss 5.68357182 epoch total loss 6.28005505\n",
      "Trained batch 5762 batch loss 6.45397 epoch total loss 6.28008509\n",
      "Trained batch 5763 batch loss 6.76711845 epoch total loss 6.28016949\n",
      "Trained batch 5764 batch loss 6.5605 epoch total loss 6.28021765\n",
      "Trained batch 5765 batch loss 6.71929 epoch total loss 6.28029394\n",
      "Trained batch 5766 batch loss 6.29857397 epoch total loss 6.2802968\n",
      "Trained batch 5767 batch loss 6.90039921 epoch total loss 6.28040457\n",
      "Trained batch 5768 batch loss 6.35207891 epoch total loss 6.28041697\n",
      "Trained batch 5769 batch loss 6.72915173 epoch total loss 6.28049517\n",
      "Trained batch 5770 batch loss 6.40378284 epoch total loss 6.28051615\n",
      "Trained batch 5771 batch loss 5.21801424 epoch total loss 6.28033209\n",
      "Trained batch 5772 batch loss 6.55005312 epoch total loss 6.28037882\n",
      "Trained batch 5773 batch loss 6.32483292 epoch total loss 6.28038645\n",
      "Trained batch 5774 batch loss 6.29610729 epoch total loss 6.28038931\n",
      "Trained batch 5775 batch loss 6.07757664 epoch total loss 6.2803545\n",
      "Trained batch 5776 batch loss 6.05245876 epoch total loss 6.28031492\n",
      "Trained batch 5777 batch loss 6.8936367 epoch total loss 6.28042078\n",
      "Trained batch 5778 batch loss 6.96253204 epoch total loss 6.28053856\n",
      "Trained batch 5779 batch loss 6.83509922 epoch total loss 6.28063488\n",
      "Trained batch 5780 batch loss 6.36778545 epoch total loss 6.28064966\n",
      "Trained batch 5781 batch loss 5.91545486 epoch total loss 6.28058624\n",
      "Trained batch 5782 batch loss 6.07257128 epoch total loss 6.28055096\n",
      "Trained batch 5783 batch loss 5.61899185 epoch total loss 6.28043604\n",
      "Trained batch 5784 batch loss 5.91793203 epoch total loss 6.28037357\n",
      "Trained batch 5785 batch loss 6.02251434 epoch total loss 6.28032875\n",
      "Trained batch 5786 batch loss 6.36616039 epoch total loss 6.28034401\n",
      "Trained batch 5787 batch loss 7.39186859 epoch total loss 6.2805357\n",
      "Trained batch 5788 batch loss 5.33778334 epoch total loss 6.28037262\n",
      "Trained batch 5789 batch loss 6.253932 epoch total loss 6.28036785\n",
      "Trained batch 5790 batch loss 6.22786951 epoch total loss 6.28035879\n",
      "Trained batch 5791 batch loss 5.99535561 epoch total loss 6.28030968\n",
      "Trained batch 5792 batch loss 6.13798237 epoch total loss 6.28028488\n",
      "Trained batch 5793 batch loss 5.82218933 epoch total loss 6.28020573\n",
      "Trained batch 5794 batch loss 6.03035545 epoch total loss 6.28016233\n",
      "Trained batch 5795 batch loss 6.07860374 epoch total loss 6.28012753\n",
      "Trained batch 5796 batch loss 6.28538799 epoch total loss 6.28012848\n",
      "Trained batch 5797 batch loss 6.12421656 epoch total loss 6.28010178\n",
      "Trained batch 5798 batch loss 5.99669552 epoch total loss 6.28005266\n",
      "Trained batch 5799 batch loss 6.00862169 epoch total loss 6.28000593\n",
      "Trained batch 5800 batch loss 6.3569417 epoch total loss 6.28001881\n",
      "Trained batch 5801 batch loss 6.49685097 epoch total loss 6.280056\n",
      "Trained batch 5802 batch loss 5.91554213 epoch total loss 6.27999306\n",
      "Trained batch 5803 batch loss 6.46133804 epoch total loss 6.28002405\n",
      "Trained batch 5804 batch loss 5.86029816 epoch total loss 6.27995157\n",
      "Trained batch 5805 batch loss 6.47925377 epoch total loss 6.27998638\n",
      "Trained batch 5806 batch loss 6.19503832 epoch total loss 6.2799716\n",
      "Trained batch 5807 batch loss 5.99897623 epoch total loss 6.27992344\n",
      "Trained batch 5808 batch loss 6.02063417 epoch total loss 6.27987862\n",
      "Trained batch 5809 batch loss 4.8642807 epoch total loss 6.27963495\n",
      "Trained batch 5810 batch loss 6.29671574 epoch total loss 6.27963781\n",
      "Trained batch 5811 batch loss 5.69462395 epoch total loss 6.2795372\n",
      "Trained batch 5812 batch loss 4.9954071 epoch total loss 6.27931643\n",
      "Trained batch 5813 batch loss 5.08519 epoch total loss 6.27911091\n",
      "Trained batch 5814 batch loss 5.87614727 epoch total loss 6.27904177\n",
      "Trained batch 5815 batch loss 4.6314 epoch total loss 6.27875853\n",
      "Trained batch 5816 batch loss 4.56288338 epoch total loss 6.27846336\n",
      "Trained batch 5817 batch loss 5.69038677 epoch total loss 6.27836227\n",
      "Trained batch 5818 batch loss 6.52893066 epoch total loss 6.27840519\n",
      "Trained batch 5819 batch loss 5.94197702 epoch total loss 6.27834749\n",
      "Trained batch 5820 batch loss 5.28501701 epoch total loss 6.27817678\n",
      "Trained batch 5821 batch loss 7.32854176 epoch total loss 6.27835703\n",
      "Trained batch 5822 batch loss 7.15838242 epoch total loss 6.27850866\n",
      "Trained batch 5823 batch loss 5.66508293 epoch total loss 6.27840281\n",
      "Trained batch 5824 batch loss 6.6693 epoch total loss 6.27847\n",
      "Trained batch 5825 batch loss 6.3738184 epoch total loss 6.27848625\n",
      "Trained batch 5826 batch loss 6.59699965 epoch total loss 6.27854109\n",
      "Trained batch 5827 batch loss 6.67222595 epoch total loss 6.2786088\n",
      "Trained batch 5828 batch loss 6.30041456 epoch total loss 6.27861261\n",
      "Trained batch 5829 batch loss 6.59375191 epoch total loss 6.2786665\n",
      "Trained batch 5830 batch loss 6.52204704 epoch total loss 6.27870846\n",
      "Trained batch 5831 batch loss 6.57932043 epoch total loss 6.27876\n",
      "Trained batch 5832 batch loss 6.58538437 epoch total loss 6.27881241\n",
      "Trained batch 5833 batch loss 5.87554932 epoch total loss 6.27874327\n",
      "Trained batch 5834 batch loss 5.94280195 epoch total loss 6.27868557\n",
      "Trained batch 5835 batch loss 5.48022938 epoch total loss 6.27854872\n",
      "Trained batch 5836 batch loss 6.1922369 epoch total loss 6.27853394\n",
      "Trained batch 5837 batch loss 6.0809288 epoch total loss 6.2785\n",
      "Trained batch 5838 batch loss 5.93936348 epoch total loss 6.27844191\n",
      "Trained batch 5839 batch loss 6.46559381 epoch total loss 6.27847385\n",
      "Trained batch 5840 batch loss 7.2979188 epoch total loss 6.2786479\n",
      "Trained batch 5841 batch loss 7.37470818 epoch total loss 6.27883577\n",
      "Trained batch 5842 batch loss 6.63370228 epoch total loss 6.27889633\n",
      "Trained batch 5843 batch loss 5.69776869 epoch total loss 6.27879715\n",
      "Trained batch 5844 batch loss 5.97991467 epoch total loss 6.27874613\n",
      "Trained batch 5845 batch loss 6.41860294 epoch total loss 6.27877\n",
      "Trained batch 5846 batch loss 7.03298759 epoch total loss 6.27889872\n",
      "Trained batch 5847 batch loss 5.7613039 epoch total loss 6.27881\n",
      "Trained batch 5848 batch loss 5.98870611 epoch total loss 6.27876043\n",
      "Trained batch 5849 batch loss 5.76367092 epoch total loss 6.27867222\n",
      "Trained batch 5850 batch loss 6.45153332 epoch total loss 6.27870178\n",
      "Trained batch 5851 batch loss 6.06124783 epoch total loss 6.27866507\n",
      "Trained batch 5852 batch loss 6.57281399 epoch total loss 6.27871561\n",
      "Trained batch 5853 batch loss 6.19294834 epoch total loss 6.27870035\n",
      "Trained batch 5854 batch loss 6.50886536 epoch total loss 6.27874\n",
      "Trained batch 5855 batch loss 5.9421649 epoch total loss 6.27868223\n",
      "Trained batch 5856 batch loss 6.30794621 epoch total loss 6.278687\n",
      "Trained batch 5857 batch loss 6.41649771 epoch total loss 6.27871084\n",
      "Trained batch 5858 batch loss 6.4252243 epoch total loss 6.27873611\n",
      "Trained batch 5859 batch loss 5.69757414 epoch total loss 6.27863693\n",
      "Trained batch 5860 batch loss 5.86867332 epoch total loss 6.27856684\n",
      "Trained batch 5861 batch loss 6.70056152 epoch total loss 6.27863884\n",
      "Trained batch 5862 batch loss 5.6739645 epoch total loss 6.27853584\n",
      "Trained batch 5863 batch loss 5.00583696 epoch total loss 6.27831841\n",
      "Trained batch 5864 batch loss 5.77128649 epoch total loss 6.27823162\n",
      "Trained batch 5865 batch loss 5.36018229 epoch total loss 6.27807522\n",
      "Trained batch 5866 batch loss 6.8645463 epoch total loss 6.27817488\n",
      "Trained batch 5867 batch loss 7.31670713 epoch total loss 6.27835178\n",
      "Trained batch 5868 batch loss 6.44710159 epoch total loss 6.27838039\n",
      "Trained batch 5869 batch loss 6.7684269 epoch total loss 6.27846384\n",
      "Trained batch 5870 batch loss 6.06053734 epoch total loss 6.27842665\n",
      "Trained batch 5871 batch loss 6.18893 epoch total loss 6.27841091\n",
      "Trained batch 5872 batch loss 6.27426434 epoch total loss 6.27841\n",
      "Trained batch 5873 batch loss 4.93110371 epoch total loss 6.2781806\n",
      "Trained batch 5874 batch loss 4.35315466 epoch total loss 6.27785254\n",
      "Trained batch 5875 batch loss 6.46993399 epoch total loss 6.27788496\n",
      "Trained batch 5876 batch loss 6.05749655 epoch total loss 6.27784777\n",
      "Trained batch 5877 batch loss 6.04155064 epoch total loss 6.27780771\n",
      "Trained batch 5878 batch loss 7.84598398 epoch total loss 6.27807474\n",
      "Trained batch 5879 batch loss 5.69616413 epoch total loss 6.27797556\n",
      "Trained batch 5880 batch loss 6.98285866 epoch total loss 6.27809572\n",
      "Trained batch 5881 batch loss 6.72784472 epoch total loss 6.27817202\n",
      "Trained batch 5882 batch loss 6.58275938 epoch total loss 6.27822351\n",
      "Trained batch 5883 batch loss 6.6181469 epoch total loss 6.27828121\n",
      "Trained batch 5884 batch loss 6.5945611 epoch total loss 6.27833509\n",
      "Trained batch 5885 batch loss 6.7087841 epoch total loss 6.27840757\n",
      "Trained batch 5886 batch loss 5.66936 epoch total loss 6.2783041\n",
      "Trained batch 5887 batch loss 6.27059698 epoch total loss 6.27830267\n",
      "Trained batch 5888 batch loss 7.50955582 epoch total loss 6.27851152\n",
      "Trained batch 5889 batch loss 5.96242046 epoch total loss 6.27845764\n",
      "Trained batch 5890 batch loss 5.89341545 epoch total loss 6.27839231\n",
      "Trained batch 5891 batch loss 6.44579554 epoch total loss 6.27842045\n",
      "Trained batch 5892 batch loss 6.38148785 epoch total loss 6.27843809\n",
      "Trained batch 5893 batch loss 6.45130444 epoch total loss 6.27846813\n",
      "Trained batch 5894 batch loss 6.66325569 epoch total loss 6.27853346\n",
      "Trained batch 5895 batch loss 6.33374643 epoch total loss 6.27854252\n",
      "Trained batch 5896 batch loss 7.17065859 epoch total loss 6.27869415\n",
      "Trained batch 5897 batch loss 4.91069078 epoch total loss 6.27846193\n",
      "Trained batch 5898 batch loss 4.72674751 epoch total loss 6.27819872\n",
      "Trained batch 5899 batch loss 4.71817398 epoch total loss 6.27793455\n",
      "Trained batch 5900 batch loss 4.98888254 epoch total loss 6.27771568\n",
      "Trained batch 5901 batch loss 6.05988789 epoch total loss 6.27767849\n",
      "Trained batch 5902 batch loss 5.66537952 epoch total loss 6.27757454\n",
      "Trained batch 5903 batch loss 5.76973677 epoch total loss 6.27748871\n",
      "Trained batch 5904 batch loss 5.82072353 epoch total loss 6.27741146\n",
      "Trained batch 5905 batch loss 5.51938677 epoch total loss 6.27728271\n",
      "Trained batch 5906 batch loss 6.0879612 epoch total loss 6.27725124\n",
      "Trained batch 5907 batch loss 5.75025272 epoch total loss 6.27716208\n",
      "Trained batch 5908 batch loss 5.85577774 epoch total loss 6.27709055\n",
      "Trained batch 5909 batch loss 6.20030403 epoch total loss 6.2770772\n",
      "Trained batch 5910 batch loss 5.97610474 epoch total loss 6.27702665\n",
      "Trained batch 5911 batch loss 6.2797246 epoch total loss 6.27702713\n",
      "Trained batch 5912 batch loss 6.01217079 epoch total loss 6.27698231\n",
      "Trained batch 5913 batch loss 6.41795111 epoch total loss 6.27700615\n",
      "Trained batch 5914 batch loss 6.5427351 epoch total loss 6.27705097\n",
      "Trained batch 5915 batch loss 6.434021 epoch total loss 6.27707767\n",
      "Trained batch 5916 batch loss 6.44273138 epoch total loss 6.27710533\n",
      "Trained batch 5917 batch loss 6.19684839 epoch total loss 6.2770915\n",
      "Trained batch 5918 batch loss 6.24590874 epoch total loss 6.27708626\n",
      "Trained batch 5919 batch loss 6.42209673 epoch total loss 6.27711058\n",
      "Trained batch 5920 batch loss 5.79493046 epoch total loss 6.27702951\n",
      "Trained batch 5921 batch loss 6.16639233 epoch total loss 6.27701139\n",
      "Trained batch 5922 batch loss 6.40823746 epoch total loss 6.27703381\n",
      "Trained batch 5923 batch loss 6.38504124 epoch total loss 6.2770524\n",
      "Trained batch 5924 batch loss 6.04533482 epoch total loss 6.2770133\n",
      "Trained batch 5925 batch loss 6.10050964 epoch total loss 6.27698374\n",
      "Trained batch 5926 batch loss 5.97412062 epoch total loss 6.27693224\n",
      "Trained batch 5927 batch loss 6.26390123 epoch total loss 6.27693033\n",
      "Trained batch 5928 batch loss 5.89219713 epoch total loss 6.27686548\n",
      "Trained batch 5929 batch loss 5.16689873 epoch total loss 6.27667809\n",
      "Trained batch 5930 batch loss 5.86692619 epoch total loss 6.27660942\n",
      "Trained batch 5931 batch loss 6.75697803 epoch total loss 6.27669048\n",
      "Trained batch 5932 batch loss 6.84185314 epoch total loss 6.27678585\n",
      "Trained batch 5933 batch loss 6.90226746 epoch total loss 6.27689123\n",
      "Trained batch 5934 batch loss 7.11659861 epoch total loss 6.27703285\n",
      "Trained batch 5935 batch loss 6.81571102 epoch total loss 6.27712393\n",
      "Trained batch 5936 batch loss 6.84369564 epoch total loss 6.2772193\n",
      "Trained batch 5937 batch loss 6.42113113 epoch total loss 6.27724361\n",
      "Trained batch 5938 batch loss 6.52680302 epoch total loss 6.27728605\n",
      "Trained batch 5939 batch loss 6.33867264 epoch total loss 6.27729654\n",
      "Trained batch 5940 batch loss 6.29292774 epoch total loss 6.27729893\n",
      "Trained batch 5941 batch loss 6.55786705 epoch total loss 6.27734661\n",
      "Trained batch 5942 batch loss 6.1683979 epoch total loss 6.27732801\n",
      "Trained batch 5943 batch loss 6.26199627 epoch total loss 6.27732515\n",
      "Trained batch 5944 batch loss 6.22053528 epoch total loss 6.27731562\n",
      "Trained batch 5945 batch loss 6.60955286 epoch total loss 6.27737141\n",
      "Trained batch 5946 batch loss 6.26319408 epoch total loss 6.27736855\n",
      "Trained batch 5947 batch loss 6.46032763 epoch total loss 6.27739954\n",
      "Trained batch 5948 batch loss 6.24637604 epoch total loss 6.27739429\n",
      "Trained batch 5949 batch loss 6.51600695 epoch total loss 6.27743435\n",
      "Trained batch 5950 batch loss 6.4441824 epoch total loss 6.27746248\n",
      "Trained batch 5951 batch loss 6.18684769 epoch total loss 6.27744722\n",
      "Trained batch 5952 batch loss 6.23771667 epoch total loss 6.27744102\n",
      "Trained batch 5953 batch loss 5.80416489 epoch total loss 6.27736139\n",
      "Trained batch 5954 batch loss 6.32252693 epoch total loss 6.2773695\n",
      "Trained batch 5955 batch loss 5.93093491 epoch total loss 6.27731085\n",
      "Trained batch 5956 batch loss 6.41002655 epoch total loss 6.27733326\n",
      "Trained batch 5957 batch loss 5.93327475 epoch total loss 6.27727556\n",
      "Trained batch 5958 batch loss 6.13343906 epoch total loss 6.27725124\n",
      "Trained batch 5959 batch loss 5.74339533 epoch total loss 6.2771616\n",
      "Trained batch 5960 batch loss 5.89708042 epoch total loss 6.27709818\n",
      "Trained batch 5961 batch loss 6.40598679 epoch total loss 6.27711964\n",
      "Trained batch 5962 batch loss 6.02486897 epoch total loss 6.2770772\n",
      "Trained batch 5963 batch loss 5.60626078 epoch total loss 6.27696466\n",
      "Trained batch 5964 batch loss 5.44031858 epoch total loss 6.27682447\n",
      "Trained batch 5965 batch loss 6.2366991 epoch total loss 6.2768178\n",
      "Trained batch 5966 batch loss 5.90071392 epoch total loss 6.27675533\n",
      "Trained batch 5967 batch loss 5.91675282 epoch total loss 6.27669477\n",
      "Trained batch 5968 batch loss 6.04396725 epoch total loss 6.27665567\n",
      "Trained batch 5969 batch loss 5.36783409 epoch total loss 6.27650356\n",
      "Trained batch 5970 batch loss 6.41830635 epoch total loss 6.2765274\n",
      "Trained batch 5971 batch loss 6.03213644 epoch total loss 6.27648592\n",
      "Trained batch 5972 batch loss 6.8344593 epoch total loss 6.27658\n",
      "Trained batch 5973 batch loss 6.29717064 epoch total loss 6.27658319\n",
      "Trained batch 5974 batch loss 6.62342882 epoch total loss 6.27664137\n",
      "Trained batch 5975 batch loss 5.3409996 epoch total loss 6.27648449\n",
      "Trained batch 5976 batch loss 6.01246357 epoch total loss 6.27644062\n",
      "Trained batch 5977 batch loss 5.81449127 epoch total loss 6.27636337\n",
      "Trained batch 5978 batch loss 5.61658096 epoch total loss 6.27625322\n",
      "Trained batch 5979 batch loss 6.605093 epoch total loss 6.27630806\n",
      "Trained batch 5980 batch loss 6.08961296 epoch total loss 6.27627707\n",
      "Trained batch 5981 batch loss 6.34788895 epoch total loss 6.27628899\n",
      "Trained batch 5982 batch loss 5.70235729 epoch total loss 6.27619314\n",
      "Trained batch 5983 batch loss 6.0575738 epoch total loss 6.2761569\n",
      "Trained batch 5984 batch loss 5.61271715 epoch total loss 6.2760458\n",
      "Trained batch 5985 batch loss 6.17229557 epoch total loss 6.27602863\n",
      "Trained batch 5986 batch loss 6.17653275 epoch total loss 6.27601194\n",
      "Trained batch 5987 batch loss 6.82331467 epoch total loss 6.2761035\n",
      "Trained batch 5988 batch loss 6.67980576 epoch total loss 6.27617073\n",
      "Trained batch 5989 batch loss 6.96946955 epoch total loss 6.2762866\n",
      "Trained batch 5990 batch loss 6.20651913 epoch total loss 6.27627468\n",
      "Trained batch 5991 batch loss 7.51693058 epoch total loss 6.27648163\n",
      "Trained batch 5992 batch loss 6.43249035 epoch total loss 6.27650785\n",
      "Trained batch 5993 batch loss 6.59278965 epoch total loss 6.27656078\n",
      "Trained batch 5994 batch loss 6.25338459 epoch total loss 6.27655697\n",
      "Trained batch 5995 batch loss 6.67902517 epoch total loss 6.2766242\n",
      "Trained batch 5996 batch loss 6.53257942 epoch total loss 6.27666664\n",
      "Trained batch 5997 batch loss 5.92038 epoch total loss 6.27660751\n",
      "Trained batch 5998 batch loss 6.20776653 epoch total loss 6.27659607\n",
      "Trained batch 5999 batch loss 6.4987483 epoch total loss 6.27663326\n",
      "Trained batch 6000 batch loss 6.70383263 epoch total loss 6.27670431\n",
      "Trained batch 6001 batch loss 6.65032101 epoch total loss 6.2767663\n",
      "Trained batch 6002 batch loss 6.57559061 epoch total loss 6.27681589\n",
      "Trained batch 6003 batch loss 6.7537756 epoch total loss 6.27689552\n",
      "Trained batch 6004 batch loss 6.78498745 epoch total loss 6.27698\n",
      "Trained batch 6005 batch loss 7.68037415 epoch total loss 6.27721357\n",
      "Trained batch 6006 batch loss 7.38499546 epoch total loss 6.27739859\n",
      "Trained batch 6007 batch loss 6.36769772 epoch total loss 6.27741337\n",
      "Trained batch 6008 batch loss 6.490942 epoch total loss 6.27744913\n",
      "Trained batch 6009 batch loss 6.50669098 epoch total loss 6.27748728\n",
      "Trained batch 6010 batch loss 7.26234531 epoch total loss 6.27765131\n",
      "Trained batch 6011 batch loss 7.09351206 epoch total loss 6.27778673\n",
      "Trained batch 6012 batch loss 7.72298622 epoch total loss 6.27802706\n",
      "Trained batch 6013 batch loss 6.29016638 epoch total loss 6.27802896\n",
      "Trained batch 6014 batch loss 6.6064868 epoch total loss 6.27808332\n",
      "Trained batch 6015 batch loss 6.3885479 epoch total loss 6.27810144\n",
      "Trained batch 6016 batch loss 5.94496536 epoch total loss 6.27804613\n",
      "Trained batch 6017 batch loss 6.54497242 epoch total loss 6.27809095\n",
      "Trained batch 6018 batch loss 6.23240185 epoch total loss 6.27808285\n",
      "Trained batch 6019 batch loss 6.76029968 epoch total loss 6.27816343\n",
      "Trained batch 6020 batch loss 6.8141737 epoch total loss 6.27825212\n",
      "Trained batch 6021 batch loss 6.70454359 epoch total loss 6.2783227\n",
      "Trained batch 6022 batch loss 6.4164362 epoch total loss 6.27834606\n",
      "Trained batch 6023 batch loss 7.53935623 epoch total loss 6.27855539\n",
      "Trained batch 6024 batch loss 6.66117477 epoch total loss 6.27861881\n",
      "Trained batch 6025 batch loss 6.53091145 epoch total loss 6.2786603\n",
      "Trained batch 6026 batch loss 6.77512741 epoch total loss 6.27874279\n",
      "Trained batch 6027 batch loss 6.97271729 epoch total loss 6.27885771\n",
      "Trained batch 6028 batch loss 6.56687689 epoch total loss 6.27890539\n",
      "Trained batch 6029 batch loss 6.73417377 epoch total loss 6.27898121\n",
      "Trained batch 6030 batch loss 6.86522961 epoch total loss 6.27907801\n",
      "Trained batch 6031 batch loss 6.59255886 epoch total loss 6.27913\n",
      "Trained batch 6032 batch loss 6.78338528 epoch total loss 6.27921391\n",
      "Trained batch 6033 batch loss 5.7773695 epoch total loss 6.27913094\n",
      "Trained batch 6034 batch loss 6.56736422 epoch total loss 6.27917862\n",
      "Trained batch 6035 batch loss 6.09754944 epoch total loss 6.2791481\n",
      "Trained batch 6036 batch loss 7.63902378 epoch total loss 6.27937412\n",
      "Trained batch 6037 batch loss 6.23993397 epoch total loss 6.27936697\n",
      "Trained batch 6038 batch loss 7.25319099 epoch total loss 6.27952862\n",
      "Trained batch 6039 batch loss 6.61012459 epoch total loss 6.27958298\n",
      "Trained batch 6040 batch loss 6.91432476 epoch total loss 6.27968836\n",
      "Trained batch 6041 batch loss 7.12461758 epoch total loss 6.27982807\n",
      "Trained batch 6042 batch loss 7.01367569 epoch total loss 6.27994967\n",
      "Trained batch 6043 batch loss 6.98182583 epoch total loss 6.28006601\n",
      "Trained batch 6044 batch loss 7.22151184 epoch total loss 6.28022194\n",
      "Trained batch 6045 batch loss 6.82163143 epoch total loss 6.28031111\n",
      "Trained batch 6046 batch loss 7.20168877 epoch total loss 6.2804637\n",
      "Trained batch 6047 batch loss 6.7252779 epoch total loss 6.28053761\n",
      "Trained batch 6048 batch loss 6.61707115 epoch total loss 6.28059292\n",
      "Trained batch 6049 batch loss 6.70106888 epoch total loss 6.28066254\n",
      "Trained batch 6050 batch loss 6.75871658 epoch total loss 6.28074121\n",
      "Trained batch 6051 batch loss 6.70642805 epoch total loss 6.28081179\n",
      "Trained batch 6052 batch loss 6.79876518 epoch total loss 6.28089714\n",
      "Trained batch 6053 batch loss 6.56088591 epoch total loss 6.28094339\n",
      "Trained batch 6054 batch loss 6.80513859 epoch total loss 6.28103\n",
      "Trained batch 6055 batch loss 6.55936718 epoch total loss 6.28107595\n",
      "Trained batch 6056 batch loss 6.94882965 epoch total loss 6.2811861\n",
      "Trained batch 6057 batch loss 6.69501829 epoch total loss 6.28125429\n",
      "Trained batch 6058 batch loss 6.12136841 epoch total loss 6.28122807\n",
      "Trained batch 6059 batch loss 6.64234257 epoch total loss 6.28128719\n",
      "Trained batch 6060 batch loss 6.38267088 epoch total loss 6.28130436\n",
      "Trained batch 6061 batch loss 6.17364311 epoch total loss 6.28128624\n",
      "Trained batch 6062 batch loss 6.21379519 epoch total loss 6.28127527\n",
      "Trained batch 6063 batch loss 6.18445158 epoch total loss 6.28125906\n",
      "Trained batch 6064 batch loss 6.15829563 epoch total loss 6.28123903\n",
      "Trained batch 6065 batch loss 6.38967752 epoch total loss 6.28125715\n",
      "Trained batch 6066 batch loss 5.99578762 epoch total loss 6.28121\n",
      "Trained batch 6067 batch loss 6.06813431 epoch total loss 6.28117466\n",
      "Trained batch 6068 batch loss 6.30468273 epoch total loss 6.28117847\n",
      "Trained batch 6069 batch loss 4.86953163 epoch total loss 6.28094625\n",
      "Trained batch 6070 batch loss 4.85030317 epoch total loss 6.2807107\n",
      "Trained batch 6071 batch loss 5.96049643 epoch total loss 6.28065825\n",
      "Trained batch 6072 batch loss 6.36136961 epoch total loss 6.2806716\n",
      "Trained batch 6073 batch loss 6.17630816 epoch total loss 6.28065443\n",
      "Trained batch 6074 batch loss 6.05804729 epoch total loss 6.28061771\n",
      "Trained batch 6075 batch loss 6.65619946 epoch total loss 6.2806797\n",
      "Trained batch 6076 batch loss 6.74539948 epoch total loss 6.28075647\n",
      "Trained batch 6077 batch loss 6.70593786 epoch total loss 6.28082657\n",
      "Trained batch 6078 batch loss 6.69358397 epoch total loss 6.28089476\n",
      "Trained batch 6079 batch loss 6.20503521 epoch total loss 6.28088188\n",
      "Trained batch 6080 batch loss 5.37893105 epoch total loss 6.28073359\n",
      "Trained batch 6081 batch loss 5.82920456 epoch total loss 6.2806592\n",
      "Trained batch 6082 batch loss 5.62485313 epoch total loss 6.28055143\n",
      "Trained batch 6083 batch loss 6.52004623 epoch total loss 6.28059053\n",
      "Trained batch 6084 batch loss 6.10753345 epoch total loss 6.2805624\n",
      "Trained batch 6085 batch loss 6.46794224 epoch total loss 6.2805934\n",
      "Trained batch 6086 batch loss 6.25958776 epoch total loss 6.28058958\n",
      "Trained batch 6087 batch loss 6.10756254 epoch total loss 6.28056145\n",
      "Trained batch 6088 batch loss 6.05718327 epoch total loss 6.28052473\n",
      "Trained batch 6089 batch loss 6.36875916 epoch total loss 6.28053904\n",
      "Trained batch 6090 batch loss 7.16700697 epoch total loss 6.28068495\n",
      "Trained batch 6091 batch loss 5.65184212 epoch total loss 6.28058195\n",
      "Trained batch 6092 batch loss 6.39684391 epoch total loss 6.28060102\n",
      "Trained batch 6093 batch loss 7.22462511 epoch total loss 6.28075647\n",
      "Trained batch 6094 batch loss 7.80932379 epoch total loss 6.28100729\n",
      "Trained batch 6095 batch loss 7.00253582 epoch total loss 6.28112555\n",
      "Trained batch 6096 batch loss 7.45447111 epoch total loss 6.28131771\n",
      "Trained batch 6097 batch loss 6.33495188 epoch total loss 6.28132677\n",
      "Trained batch 6098 batch loss 6.87514925 epoch total loss 6.28142405\n",
      "Trained batch 6099 batch loss 6.74449539 epoch total loss 6.28150034\n",
      "Trained batch 6100 batch loss 6.42735863 epoch total loss 6.28152418\n",
      "Trained batch 6101 batch loss 6.96869564 epoch total loss 6.28163671\n",
      "Trained batch 6102 batch loss 4.8799181 epoch total loss 6.28140688\n",
      "Trained batch 6103 batch loss 6.40129948 epoch total loss 6.28142643\n",
      "Trained batch 6104 batch loss 6.18813181 epoch total loss 6.28141117\n",
      "Trained batch 6105 batch loss 6.38240242 epoch total loss 6.28142786\n",
      "Trained batch 6106 batch loss 6.5243721 epoch total loss 6.28146744\n",
      "Trained batch 6107 batch loss 6.58263779 epoch total loss 6.28151655\n",
      "Trained batch 6108 batch loss 6.59699345 epoch total loss 6.28156853\n",
      "Trained batch 6109 batch loss 6.74066353 epoch total loss 6.28164387\n",
      "Trained batch 6110 batch loss 6.62587357 epoch total loss 6.2817\n",
      "Trained batch 6111 batch loss 5.95518398 epoch total loss 6.28164673\n",
      "Trained batch 6112 batch loss 5.94652367 epoch total loss 6.28159189\n",
      "Trained batch 6113 batch loss 6.30756855 epoch total loss 6.28159618\n",
      "Trained batch 6114 batch loss 6.38142252 epoch total loss 6.28161287\n",
      "Trained batch 6115 batch loss 6.71880865 epoch total loss 6.2816844\n",
      "Trained batch 6116 batch loss 6.79876471 epoch total loss 6.2817688\n",
      "Trained batch 6117 batch loss 6.70948315 epoch total loss 6.28183889\n",
      "Trained batch 6118 batch loss 6.31436062 epoch total loss 6.28184366\n",
      "Trained batch 6119 batch loss 7.01998234 epoch total loss 6.2819643\n",
      "Trained batch 6120 batch loss 6.50284481 epoch total loss 6.28200054\n",
      "Trained batch 6121 batch loss 7.06270742 epoch total loss 6.28212833\n",
      "Trained batch 6122 batch loss 6.6966238 epoch total loss 6.28219557\n",
      "Trained batch 6123 batch loss 6.60650063 epoch total loss 6.2822485\n",
      "Trained batch 6124 batch loss 6.62068272 epoch total loss 6.28230381\n",
      "Trained batch 6125 batch loss 6.08901644 epoch total loss 6.28227234\n",
      "Trained batch 6126 batch loss 6.46896505 epoch total loss 6.28230286\n",
      "Trained batch 6127 batch loss 5.92786074 epoch total loss 6.28224516\n",
      "Trained batch 6128 batch loss 6.07769489 epoch total loss 6.28221178\n",
      "Trained batch 6129 batch loss 6.34576511 epoch total loss 6.28222275\n",
      "Trained batch 6130 batch loss 6.54078817 epoch total loss 6.28226471\n",
      "Trained batch 6131 batch loss 6.71129036 epoch total loss 6.28233433\n",
      "Trained batch 6132 batch loss 6.31076479 epoch total loss 6.2823391\n",
      "Trained batch 6133 batch loss 5.71050644 epoch total loss 6.28224611\n",
      "Trained batch 6134 batch loss 5.43854141 epoch total loss 6.28210831\n",
      "Trained batch 6135 batch loss 5.335042 epoch total loss 6.28195429\n",
      "Trained batch 6136 batch loss 5.86035824 epoch total loss 6.28188515\n",
      "Trained batch 6137 batch loss 5.79201365 epoch total loss 6.28180552\n",
      "Trained batch 6138 batch loss 6.40535355 epoch total loss 6.28182602\n",
      "Trained batch 6139 batch loss 6.0420866 epoch total loss 6.28178692\n",
      "Trained batch 6140 batch loss 6.40492725 epoch total loss 6.28180742\n",
      "Trained batch 6141 batch loss 6.50578928 epoch total loss 6.28184366\n",
      "Trained batch 6142 batch loss 6.69612694 epoch total loss 6.2819109\n",
      "Trained batch 6143 batch loss 6.6635952 epoch total loss 6.28197289\n",
      "Trained batch 6144 batch loss 6.70232058 epoch total loss 6.28204155\n",
      "Trained batch 6145 batch loss 6.60764265 epoch total loss 6.28209496\n",
      "Trained batch 6146 batch loss 6.20147085 epoch total loss 6.28208208\n",
      "Trained batch 6147 batch loss 6.40116215 epoch total loss 6.28210163\n",
      "Trained batch 6148 batch loss 6.66368 epoch total loss 6.28216362\n",
      "Trained batch 6149 batch loss 6.61554 epoch total loss 6.28221798\n",
      "Trained batch 6150 batch loss 6.70539 epoch total loss 6.28228712\n",
      "Trained batch 6151 batch loss 7.18033409 epoch total loss 6.28243303\n",
      "Trained batch 6152 batch loss 6.79458141 epoch total loss 6.282516\n",
      "Trained batch 6153 batch loss 6.32320261 epoch total loss 6.28252268\n",
      "Trained batch 6154 batch loss 6.84463024 epoch total loss 6.28261423\n",
      "Trained batch 6155 batch loss 6.66961861 epoch total loss 6.2826767\n",
      "Trained batch 6156 batch loss 7.05824 epoch total loss 6.28280258\n",
      "Trained batch 6157 batch loss 6.46503353 epoch total loss 6.28283215\n",
      "Trained batch 6158 batch loss 6.6427412 epoch total loss 6.2828908\n",
      "Trained batch 6159 batch loss 6.72969484 epoch total loss 6.28296375\n",
      "Trained batch 6160 batch loss 7.19777107 epoch total loss 6.28311253\n",
      "Trained batch 6161 batch loss 6.60106325 epoch total loss 6.28316402\n",
      "Trained batch 6162 batch loss 6.25855207 epoch total loss 6.28316\n",
      "Trained batch 6163 batch loss 6.37571287 epoch total loss 6.28317499\n",
      "Trained batch 6164 batch loss 7.18182468 epoch total loss 6.2833209\n",
      "Trained batch 6165 batch loss 6.40026093 epoch total loss 6.2833395\n",
      "Trained batch 6166 batch loss 6.58038044 epoch total loss 6.28338814\n",
      "Trained batch 6167 batch loss 5.736588 epoch total loss 6.2833\n",
      "Trained batch 6168 batch loss 6.00169563 epoch total loss 6.28325367\n",
      "Trained batch 6169 batch loss 7.54827738 epoch total loss 6.28345871\n",
      "Trained batch 6170 batch loss 6.29626036 epoch total loss 6.28346062\n",
      "Trained batch 6171 batch loss 5.6706686 epoch total loss 6.28336191\n",
      "Trained batch 6172 batch loss 6.42290306 epoch total loss 6.28338432\n",
      "Trained batch 6173 batch loss 5.57429314 epoch total loss 6.28326941\n",
      "Trained batch 6174 batch loss 6.54247093 epoch total loss 6.28331137\n",
      "Trained batch 6175 batch loss 6.37526321 epoch total loss 6.28332615\n",
      "Trained batch 6176 batch loss 6.17841053 epoch total loss 6.28330946\n",
      "Trained batch 6177 batch loss 6.48622036 epoch total loss 6.28334188\n",
      "Trained batch 6178 batch loss 6.34938908 epoch total loss 6.28335238\n",
      "Trained batch 6179 batch loss 6.16016197 epoch total loss 6.28333235\n",
      "Trained batch 6180 batch loss 6.39310265 epoch total loss 6.28335047\n",
      "Trained batch 6181 batch loss 6.62159872 epoch total loss 6.28340483\n",
      "Trained batch 6182 batch loss 6.52354813 epoch total loss 6.28344393\n",
      "Trained batch 6183 batch loss 6.41807127 epoch total loss 6.28346586\n",
      "Trained batch 6184 batch loss 7.04904 epoch total loss 6.28359\n",
      "Trained batch 6185 batch loss 6.69883585 epoch total loss 6.28365707\n",
      "Trained batch 6186 batch loss 6.72811127 epoch total loss 6.2837286\n",
      "Trained batch 6187 batch loss 6.61506557 epoch total loss 6.28378201\n",
      "Trained batch 6188 batch loss 6.46317053 epoch total loss 6.28381109\n",
      "Trained batch 6189 batch loss 6.65761662 epoch total loss 6.28387117\n",
      "Trained batch 6190 batch loss 6.29924107 epoch total loss 6.28387403\n",
      "Trained batch 6191 batch loss 6.56821346 epoch total loss 6.28392\n",
      "Trained batch 6192 batch loss 6.69702 epoch total loss 6.28398609\n",
      "Trained batch 6193 batch loss 6.3818469 epoch total loss 6.28400183\n",
      "Trained batch 6194 batch loss 5.99737453 epoch total loss 6.28395557\n",
      "Trained batch 6195 batch loss 5.9749651 epoch total loss 6.28390598\n",
      "Trained batch 6196 batch loss 5.91488838 epoch total loss 6.28384638\n",
      "Trained batch 6197 batch loss 4.45596313 epoch total loss 6.28355122\n",
      "Trained batch 6198 batch loss 6.50877094 epoch total loss 6.28358746\n",
      "Trained batch 6199 batch loss 6.2662282 epoch total loss 6.28358459\n",
      "Trained batch 6200 batch loss 6.21028328 epoch total loss 6.28357315\n",
      "Trained batch 6201 batch loss 6.39328957 epoch total loss 6.28359079\n",
      "Trained batch 6202 batch loss 7.01501656 epoch total loss 6.28370905\n",
      "Trained batch 6203 batch loss 6.41858 epoch total loss 6.28373051\n",
      "Trained batch 6204 batch loss 6.22729874 epoch total loss 6.28372145\n",
      "Trained batch 6205 batch loss 6.00173759 epoch total loss 6.28367567\n",
      "Trained batch 6206 batch loss 6.45142937 epoch total loss 6.28370285\n",
      "Trained batch 6207 batch loss 6.31053734 epoch total loss 6.28370714\n",
      "Trained batch 6208 batch loss 6.31200266 epoch total loss 6.28371143\n",
      "Trained batch 6209 batch loss 6.28251743 epoch total loss 6.28371096\n",
      "Trained batch 6210 batch loss 6.39981461 epoch total loss 6.28372955\n",
      "Trained batch 6211 batch loss 5.90439415 epoch total loss 6.28366899\n",
      "Trained batch 6212 batch loss 5.46926403 epoch total loss 6.28353786\n",
      "Trained batch 6213 batch loss 5.69944668 epoch total loss 6.28344345\n",
      "Trained batch 6214 batch loss 5.46761894 epoch total loss 6.28331232\n",
      "Trained batch 6215 batch loss 6.52717447 epoch total loss 6.2833519\n",
      "Trained batch 6216 batch loss 6.65403175 epoch total loss 6.28341103\n",
      "Trained batch 6217 batch loss 6.28777695 epoch total loss 6.28341198\n",
      "Trained batch 6218 batch loss 6.7342329 epoch total loss 6.28348446\n",
      "Trained batch 6219 batch loss 7.52418089 epoch total loss 6.28368378\n",
      "Trained batch 6220 batch loss 6.33753395 epoch total loss 6.28369236\n",
      "Trained batch 6221 batch loss 6.43528557 epoch total loss 6.28371668\n",
      "Trained batch 6222 batch loss 5.9678793 epoch total loss 6.28366566\n",
      "Trained batch 6223 batch loss 6.39975119 epoch total loss 6.28368425\n",
      "Trained batch 6224 batch loss 6.10726929 epoch total loss 6.28365564\n",
      "Trained batch 6225 batch loss 6.18026447 epoch total loss 6.28363895\n",
      "Trained batch 6226 batch loss 6.59741735 epoch total loss 6.2836895\n",
      "Trained batch 6227 batch loss 7.22258472 epoch total loss 6.28384\n",
      "Trained batch 6228 batch loss 6.44616318 epoch total loss 6.28386593\n",
      "Trained batch 6229 batch loss 6.20948792 epoch total loss 6.28385448\n",
      "Trained batch 6230 batch loss 6.22234821 epoch total loss 6.28384447\n",
      "Trained batch 6231 batch loss 5.02973127 epoch total loss 6.28364372\n",
      "Trained batch 6232 batch loss 5.81338406 epoch total loss 6.28356791\n",
      "Trained batch 6233 batch loss 5.27291 epoch total loss 6.28340578\n",
      "Trained batch 6234 batch loss 4.96042395 epoch total loss 6.28319359\n",
      "Trained batch 6235 batch loss 5.00700569 epoch total loss 6.28298903\n",
      "Trained batch 6236 batch loss 4.99161482 epoch total loss 6.28278208\n",
      "Trained batch 6237 batch loss 5.19280577 epoch total loss 6.28260708\n",
      "Trained batch 6238 batch loss 5.03958225 epoch total loss 6.28240776\n",
      "Trained batch 6239 batch loss 4.87509346 epoch total loss 6.28218222\n",
      "Trained batch 6240 batch loss 5.78494406 epoch total loss 6.28210258\n",
      "Trained batch 6241 batch loss 6.33009624 epoch total loss 6.28211069\n",
      "Trained batch 6242 batch loss 5.65576649 epoch total loss 6.28201056\n",
      "Trained batch 6243 batch loss 4.97870684 epoch total loss 6.2818017\n",
      "Trained batch 6244 batch loss 4.64102697 epoch total loss 6.28153896\n",
      "Trained batch 6245 batch loss 4.73579502 epoch total loss 6.28129148\n",
      "Trained batch 6246 batch loss 4.61928272 epoch total loss 6.28102541\n",
      "Trained batch 6247 batch loss 4.82682753 epoch total loss 6.28079271\n",
      "Trained batch 6248 batch loss 4.94005299 epoch total loss 6.28057861\n",
      "Trained batch 6249 batch loss 5.5999527 epoch total loss 6.28047\n",
      "Trained batch 6250 batch loss 6.28653574 epoch total loss 6.28047085\n",
      "Trained batch 6251 batch loss 5.41651726 epoch total loss 6.28033257\n",
      "Trained batch 6252 batch loss 7.86359882 epoch total loss 6.28058577\n",
      "Trained batch 6253 batch loss 5.38550568 epoch total loss 6.28044271\n",
      "Trained batch 6254 batch loss 6.19160318 epoch total loss 6.28042889\n",
      "Trained batch 6255 batch loss 6.76383305 epoch total loss 6.28050613\n",
      "Trained batch 6256 batch loss 6.61855888 epoch total loss 6.28056\n",
      "Trained batch 6257 batch loss 6.42577 epoch total loss 6.28058338\n",
      "Trained batch 6258 batch loss 6.05363607 epoch total loss 6.28054714\n",
      "Trained batch 6259 batch loss 5.9346981 epoch total loss 6.28049183\n",
      "Trained batch 6260 batch loss 6.26666403 epoch total loss 6.28048944\n",
      "Trained batch 6261 batch loss 6.3398056 epoch total loss 6.28049898\n",
      "Trained batch 6262 batch loss 6.14330912 epoch total loss 6.28047705\n",
      "Trained batch 6263 batch loss 6.01705456 epoch total loss 6.28043461\n",
      "Trained batch 6264 batch loss 5.75295544 epoch total loss 6.28035069\n",
      "Trained batch 6265 batch loss 5.84284115 epoch total loss 6.28028107\n",
      "Trained batch 6266 batch loss 5.56099749 epoch total loss 6.28016663\n",
      "Trained batch 6267 batch loss 5.96209431 epoch total loss 6.2801156\n",
      "Trained batch 6268 batch loss 5.88037968 epoch total loss 6.28005171\n",
      "Trained batch 6269 batch loss 5.92682648 epoch total loss 6.27999496\n",
      "Trained batch 6270 batch loss 5.87088156 epoch total loss 6.27992964\n",
      "Trained batch 6271 batch loss 5.84120798 epoch total loss 6.27985954\n",
      "Trained batch 6272 batch loss 5.69527149 epoch total loss 6.27976656\n",
      "Trained batch 6273 batch loss 5.6763072 epoch total loss 6.27967024\n",
      "Trained batch 6274 batch loss 6.70634842 epoch total loss 6.27973843\n",
      "Trained batch 6275 batch loss 6.31844139 epoch total loss 6.27974463\n",
      "Trained batch 6276 batch loss 6.77504444 epoch total loss 6.2798233\n",
      "Trained batch 6277 batch loss 6.92776299 epoch total loss 6.27992678\n",
      "Trained batch 6278 batch loss 6.48977804 epoch total loss 6.27996\n",
      "Trained batch 6279 batch loss 6.59385967 epoch total loss 6.28001\n",
      "Trained batch 6280 batch loss 7.368 epoch total loss 6.28018332\n",
      "Trained batch 6281 batch loss 6.00546885 epoch total loss 6.28013945\n",
      "Trained batch 6282 batch loss 6.60082817 epoch total loss 6.28019047\n",
      "Trained batch 6283 batch loss 6.53145885 epoch total loss 6.28023052\n",
      "Trained batch 6284 batch loss 5.9680748 epoch total loss 6.28018093\n",
      "Trained batch 6285 batch loss 6.32597494 epoch total loss 6.28018761\n",
      "Trained batch 6286 batch loss 6.25586462 epoch total loss 6.28018427\n",
      "Trained batch 6287 batch loss 6.47073126 epoch total loss 6.28021479\n",
      "Trained batch 6288 batch loss 6.3793087 epoch total loss 6.28023052\n",
      "Trained batch 6289 batch loss 6.37398291 epoch total loss 6.28024578\n",
      "Trained batch 6290 batch loss 6.18715668 epoch total loss 6.280231\n",
      "Trained batch 6291 batch loss 6.22499847 epoch total loss 6.28022242\n",
      "Trained batch 6292 batch loss 6.47861242 epoch total loss 6.28025436\n",
      "Trained batch 6293 batch loss 6.24309587 epoch total loss 6.28024817\n",
      "Trained batch 6294 batch loss 5.91448212 epoch total loss 6.28019\n",
      "Trained batch 6295 batch loss 6.24214268 epoch total loss 6.28018379\n",
      "Trained batch 6296 batch loss 6.42649698 epoch total loss 6.28020716\n",
      "Trained batch 6297 batch loss 6.16267729 epoch total loss 6.28018856\n",
      "Trained batch 6298 batch loss 6.72871923 epoch total loss 6.28026\n",
      "Trained batch 6299 batch loss 6.25683117 epoch total loss 6.28025675\n",
      "Trained batch 6300 batch loss 6.16657543 epoch total loss 6.28023863\n",
      "Trained batch 6301 batch loss 6.41075277 epoch total loss 6.28025913\n",
      "Trained batch 6302 batch loss 6.58531 epoch total loss 6.28030777\n",
      "Trained batch 6303 batch loss 6.14694929 epoch total loss 6.28028679\n",
      "Trained batch 6304 batch loss 6.37782526 epoch total loss 6.28030252\n",
      "Trained batch 6305 batch loss 6.44437122 epoch total loss 6.28032875\n",
      "Trained batch 6306 batch loss 5.93946934 epoch total loss 6.28027487\n",
      "Trained batch 6307 batch loss 6.39025354 epoch total loss 6.28029251\n",
      "Trained batch 6308 batch loss 6.08430386 epoch total loss 6.28026152\n",
      "Trained batch 6309 batch loss 6.07717848 epoch total loss 6.28022957\n",
      "Trained batch 6310 batch loss 6.21222448 epoch total loss 6.2802186\n",
      "Trained batch 6311 batch loss 5.90926743 epoch total loss 6.28016\n",
      "Trained batch 6312 batch loss 5.49405241 epoch total loss 6.28003502\n",
      "Trained batch 6313 batch loss 6.20425701 epoch total loss 6.2800231\n",
      "Trained batch 6314 batch loss 6.26691437 epoch total loss 6.28002071\n",
      "Trained batch 6315 batch loss 6.2062192 epoch total loss 6.28000927\n",
      "Trained batch 6316 batch loss 6.36918879 epoch total loss 6.28002357\n",
      "Trained batch 6317 batch loss 5.88813877 epoch total loss 6.27996111\n",
      "Trained batch 6318 batch loss 5.55987644 epoch total loss 6.27984715\n",
      "Trained batch 6319 batch loss 6.18627357 epoch total loss 6.27983236\n",
      "Trained batch 6320 batch loss 6.46586132 epoch total loss 6.27986193\n",
      "Trained batch 6321 batch loss 6.78990412 epoch total loss 6.27994251\n",
      "Trained batch 6322 batch loss 5.99206495 epoch total loss 6.27989674\n",
      "Trained batch 6323 batch loss 6.0790453 epoch total loss 6.27986479\n",
      "Trained batch 6324 batch loss 6.47623253 epoch total loss 6.27989626\n",
      "Trained batch 6325 batch loss 6.68589 epoch total loss 6.27996063\n",
      "Trained batch 6326 batch loss 6.73620701 epoch total loss 6.28003216\n",
      "Trained batch 6327 batch loss 6.62940884 epoch total loss 6.28008747\n",
      "Trained batch 6328 batch loss 6.48500443 epoch total loss 6.28012\n",
      "Trained batch 6329 batch loss 6.75797129 epoch total loss 6.28019524\n",
      "Trained batch 6330 batch loss 7.0289731 epoch total loss 6.28031301\n",
      "Trained batch 6331 batch loss 6.6929841 epoch total loss 6.28037834\n",
      "Trained batch 6332 batch loss 6.19530106 epoch total loss 6.28036451\n",
      "Trained batch 6333 batch loss 6.16675472 epoch total loss 6.28034687\n",
      "Trained batch 6334 batch loss 6.53980923 epoch total loss 6.28038788\n",
      "Trained batch 6335 batch loss 6.3589735 epoch total loss 6.28040028\n",
      "Trained batch 6336 batch loss 6.16029549 epoch total loss 6.2803812\n",
      "Trained batch 6337 batch loss 5.85653 epoch total loss 6.28031445\n",
      "Trained batch 6338 batch loss 6.21951 epoch total loss 6.28030443\n",
      "Trained batch 6339 batch loss 6.5340929 epoch total loss 6.28034496\n",
      "Trained batch 6340 batch loss 6.00340271 epoch total loss 6.28030109\n",
      "Trained batch 6341 batch loss 5.61838341 epoch total loss 6.28019667\n",
      "Trained batch 6342 batch loss 6.2085 epoch total loss 6.28018522\n",
      "Trained batch 6343 batch loss 5.65196466 epoch total loss 6.28008604\n",
      "Trained batch 6344 batch loss 6.43577385 epoch total loss 6.28011084\n",
      "Trained batch 6345 batch loss 6.23058844 epoch total loss 6.28010321\n",
      "Trained batch 6346 batch loss 6.10309505 epoch total loss 6.28007507\n",
      "Trained batch 6347 batch loss 6.33121347 epoch total loss 6.28008318\n",
      "Trained batch 6348 batch loss 6.08496857 epoch total loss 6.28005266\n",
      "Trained batch 6349 batch loss 5.67105436 epoch total loss 6.27995682\n",
      "Trained batch 6350 batch loss 5.12266111 epoch total loss 6.27977419\n",
      "Trained batch 6351 batch loss 6.26493073 epoch total loss 6.2797718\n",
      "Trained batch 6352 batch loss 6.53008127 epoch total loss 6.27981138\n",
      "Trained batch 6353 batch loss 6.77210617 epoch total loss 6.27988911\n",
      "Trained batch 6354 batch loss 6.6130414 epoch total loss 6.27994156\n",
      "Trained batch 6355 batch loss 6.50910139 epoch total loss 6.2799778\n",
      "Trained batch 6356 batch loss 6.13790131 epoch total loss 6.27995491\n",
      "Trained batch 6357 batch loss 5.65270567 epoch total loss 6.2798562\n",
      "Trained batch 6358 batch loss 5.75745 epoch total loss 6.27977419\n",
      "Trained batch 6359 batch loss 6.32334614 epoch total loss 6.27978134\n",
      "Trained batch 6360 batch loss 5.82029676 epoch total loss 6.27970886\n",
      "Trained batch 6361 batch loss 6.37385416 epoch total loss 6.27972412\n",
      "Trained batch 6362 batch loss 6.301898 epoch total loss 6.27972746\n",
      "Trained batch 6363 batch loss 5.77957201 epoch total loss 6.27964878\n",
      "Trained batch 6364 batch loss 5.52139282 epoch total loss 6.27952957\n",
      "Trained batch 6365 batch loss 6.60729694 epoch total loss 6.27958059\n",
      "Trained batch 6366 batch loss 5.1368885 epoch total loss 6.2794013\n",
      "Trained batch 6367 batch loss 4.69490051 epoch total loss 6.27915239\n",
      "Trained batch 6368 batch loss 4.40347 epoch total loss 6.27885771\n",
      "Trained batch 6369 batch loss 4.91064 epoch total loss 6.27864265\n",
      "Trained batch 6370 batch loss 4.83025169 epoch total loss 6.27841568\n",
      "Trained batch 6371 batch loss 5.72375107 epoch total loss 6.27832842\n",
      "Trained batch 6372 batch loss 5.71317482 epoch total loss 6.27824\n",
      "Trained batch 6373 batch loss 6.34419537 epoch total loss 6.27825\n",
      "Trained batch 6374 batch loss 6.5793438 epoch total loss 6.27829742\n",
      "Trained batch 6375 batch loss 4.25673294 epoch total loss 6.27798033\n",
      "Trained batch 6376 batch loss 4.15273 epoch total loss 6.27764702\n",
      "Trained batch 6377 batch loss 6.19721413 epoch total loss 6.27763414\n",
      "Trained batch 6378 batch loss 6.18000078 epoch total loss 6.27761889\n",
      "Trained batch 6379 batch loss 6.71741772 epoch total loss 6.27768803\n",
      "Trained batch 6380 batch loss 6.6075964 epoch total loss 6.27774\n",
      "Trained batch 6381 batch loss 7.29643965 epoch total loss 6.27789974\n",
      "Trained batch 6382 batch loss 6.05809593 epoch total loss 6.27786541\n",
      "Trained batch 6383 batch loss 6.6650753 epoch total loss 6.27792597\n",
      "Trained batch 6384 batch loss 6.49198389 epoch total loss 6.27795935\n",
      "Trained batch 6385 batch loss 6.29140186 epoch total loss 6.27796173\n",
      "Trained batch 6386 batch loss 5.76427269 epoch total loss 6.27788162\n",
      "Trained batch 6387 batch loss 6.3839941 epoch total loss 6.27789783\n",
      "Trained batch 6388 batch loss 7.19995975 epoch total loss 6.27804232\n",
      "Trained batch 6389 batch loss 6.47997046 epoch total loss 6.27807379\n",
      "Trained batch 6390 batch loss 6.19995356 epoch total loss 6.27806139\n",
      "Trained batch 6391 batch loss 6.12262726 epoch total loss 6.27803707\n",
      "Trained batch 6392 batch loss 6.30766869 epoch total loss 6.27804184\n",
      "Trained batch 6393 batch loss 6.30292034 epoch total loss 6.27804565\n",
      "Trained batch 6394 batch loss 6.08512306 epoch total loss 6.27801561\n",
      "Trained batch 6395 batch loss 6.26007843 epoch total loss 6.27801323\n",
      "Trained batch 6396 batch loss 6.32761478 epoch total loss 6.27802086\n",
      "Trained batch 6397 batch loss 6.41439629 epoch total loss 6.27804232\n",
      "Trained batch 6398 batch loss 6.66160488 epoch total loss 6.27810192\n",
      "Trained batch 6399 batch loss 6.35332108 epoch total loss 6.27811337\n",
      "Trained batch 6400 batch loss 6.61602879 epoch total loss 6.27816629\n",
      "Trained batch 6401 batch loss 5.91953087 epoch total loss 6.27811\n",
      "Trained batch 6402 batch loss 6.92972183 epoch total loss 6.27821207\n",
      "Trained batch 6403 batch loss 5.78965855 epoch total loss 6.27813578\n",
      "Trained batch 6404 batch loss 5.42833614 epoch total loss 6.27800322\n",
      "Trained batch 6405 batch loss 5.82364273 epoch total loss 6.27793217\n",
      "Trained batch 6406 batch loss 5.96976 epoch total loss 6.27788401\n",
      "Trained batch 6407 batch loss 5.82030964 epoch total loss 6.27781248\n",
      "Trained batch 6408 batch loss 5.85012627 epoch total loss 6.2777462\n",
      "Trained batch 6409 batch loss 6.00569725 epoch total loss 6.27770329\n",
      "Trained batch 6410 batch loss 4.75106478 epoch total loss 6.27746487\n",
      "Trained batch 6411 batch loss 5.83764172 epoch total loss 6.2773962\n",
      "Trained batch 6412 batch loss 5.77402163 epoch total loss 6.27731752\n",
      "Trained batch 6413 batch loss 5.40272713 epoch total loss 6.27718115\n",
      "Trained batch 6414 batch loss 6.13012791 epoch total loss 6.27715778\n",
      "Trained batch 6415 batch loss 4.85606956 epoch total loss 6.27693653\n",
      "Trained batch 6416 batch loss 5.43032646 epoch total loss 6.27680445\n",
      "Trained batch 6417 batch loss 7.06110239 epoch total loss 6.27692699\n",
      "Trained batch 6418 batch loss 5.92107868 epoch total loss 6.27687168\n",
      "Trained batch 6419 batch loss 4.9088 epoch total loss 6.27665854\n",
      "Trained batch 6420 batch loss 6.1894865 epoch total loss 6.27664518\n",
      "Trained batch 6421 batch loss 6.73314857 epoch total loss 6.27671671\n",
      "Trained batch 6422 batch loss 6.26865816 epoch total loss 6.27671528\n",
      "Trained batch 6423 batch loss 7.06948662 epoch total loss 6.27683878\n",
      "Trained batch 6424 batch loss 6.55047655 epoch total loss 6.27688169\n",
      "Trained batch 6425 batch loss 6.39223957 epoch total loss 6.27689934\n",
      "Trained batch 6426 batch loss 6.36531734 epoch total loss 6.27691317\n",
      "Trained batch 6427 batch loss 6.39154911 epoch total loss 6.27693129\n",
      "Trained batch 6428 batch loss 6.54589367 epoch total loss 6.27697325\n",
      "Trained batch 6429 batch loss 6.08861542 epoch total loss 6.27694416\n",
      "Trained batch 6430 batch loss 6.06378365 epoch total loss 6.27691078\n",
      "Trained batch 6431 batch loss 6.32586765 epoch total loss 6.27691793\n",
      "Trained batch 6432 batch loss 6.11864 epoch total loss 6.27689314\n",
      "Trained batch 6433 batch loss 6.41181803 epoch total loss 6.27691364\n",
      "Trained batch 6434 batch loss 5.95724249 epoch total loss 6.27686405\n",
      "Trained batch 6435 batch loss 6.09849 epoch total loss 6.2768364\n",
      "Trained batch 6436 batch loss 6.33632088 epoch total loss 6.27684546\n",
      "Trained batch 6437 batch loss 6.3501873 epoch total loss 6.2768569\n",
      "Trained batch 6438 batch loss 5.84862947 epoch total loss 6.27679062\n",
      "Trained batch 6439 batch loss 5.97986603 epoch total loss 6.27674437\n",
      "Trained batch 6440 batch loss 6.13624144 epoch total loss 6.27672243\n",
      "Trained batch 6441 batch loss 5.61526489 epoch total loss 6.27662039\n",
      "Trained batch 6442 batch loss 6.5059371 epoch total loss 6.27665615\n",
      "Trained batch 6443 batch loss 6.26335621 epoch total loss 6.27665377\n",
      "Trained batch 6444 batch loss 5.91627121 epoch total loss 6.27659798\n",
      "Trained batch 6445 batch loss 5.79141378 epoch total loss 6.27652311\n",
      "Trained batch 6446 batch loss 4.54095221 epoch total loss 6.2762537\n",
      "Trained batch 6447 batch loss 6.14957428 epoch total loss 6.27623367\n",
      "Trained batch 6448 batch loss 6.14958572 epoch total loss 6.27621412\n",
      "Trained batch 6449 batch loss 5.45133829 epoch total loss 6.27608633\n",
      "Trained batch 6450 batch loss 5.73558855 epoch total loss 6.27600241\n",
      "Trained batch 6451 batch loss 4.81594372 epoch total loss 6.27577591\n",
      "Trained batch 6452 batch loss 5.84862661 epoch total loss 6.27570963\n",
      "Trained batch 6453 batch loss 5.99623966 epoch total loss 6.27566624\n",
      "Trained batch 6454 batch loss 5.69715881 epoch total loss 6.27557659\n",
      "Trained batch 6455 batch loss 6.2264657 epoch total loss 6.27556896\n",
      "Trained batch 6456 batch loss 6.46113491 epoch total loss 6.27559757\n",
      "Trained batch 6457 batch loss 6.15356207 epoch total loss 6.2755785\n",
      "Trained batch 6458 batch loss 5.87434816 epoch total loss 6.27551651\n",
      "Trained batch 6459 batch loss 6.18554449 epoch total loss 6.2755022\n",
      "Trained batch 6460 batch loss 5.61494446 epoch total loss 6.27539968\n",
      "Trained batch 6461 batch loss 5.72199154 epoch total loss 6.27531433\n",
      "Trained batch 6462 batch loss 5.76461029 epoch total loss 6.27523518\n",
      "Trained batch 6463 batch loss 5.55730534 epoch total loss 6.27512455\n",
      "Trained batch 6464 batch loss 6.12108183 epoch total loss 6.27510071\n",
      "Trained batch 6465 batch loss 5.62715054 epoch total loss 6.27500057\n",
      "Trained batch 6466 batch loss 6.35830832 epoch total loss 6.27501345\n",
      "Trained batch 6467 batch loss 5.56473875 epoch total loss 6.27490425\n",
      "Trained batch 6468 batch loss 6.48042583 epoch total loss 6.27493572\n",
      "Trained batch 6469 batch loss 6.51853371 epoch total loss 6.27497387\n",
      "Trained batch 6470 batch loss 6.01105213 epoch total loss 6.27493286\n",
      "Trained batch 6471 batch loss 6.13502312 epoch total loss 6.2749114\n",
      "Trained batch 6472 batch loss 5.61581564 epoch total loss 6.27481\n",
      "Trained batch 6473 batch loss 6.03799 epoch total loss 6.2747736\n",
      "Trained batch 6474 batch loss 6.12744331 epoch total loss 6.27475119\n",
      "Trained batch 6475 batch loss 5.79723501 epoch total loss 6.27467728\n",
      "Trained batch 6476 batch loss 4.4334383 epoch total loss 6.27439308\n",
      "Trained batch 6477 batch loss 6.73001575 epoch total loss 6.27446318\n",
      "Trained batch 6478 batch loss 5.78317 epoch total loss 6.27438736\n",
      "Trained batch 6479 batch loss 5.85957623 epoch total loss 6.27432299\n",
      "Trained batch 6480 batch loss 5.417696 epoch total loss 6.2741909\n",
      "Trained batch 6481 batch loss 5.43954 epoch total loss 6.27406263\n",
      "Trained batch 6482 batch loss 5.80056381 epoch total loss 6.27398968\n",
      "Trained batch 6483 batch loss 5.19316483 epoch total loss 6.27382231\n",
      "Trained batch 6484 batch loss 5.97106504 epoch total loss 6.27377605\n",
      "Trained batch 6485 batch loss 6.21939659 epoch total loss 6.27376747\n",
      "Trained batch 6486 batch loss 5.99250126 epoch total loss 6.27372408\n",
      "Trained batch 6487 batch loss 6.74333191 epoch total loss 6.27379656\n",
      "Trained batch 6488 batch loss 6.89972734 epoch total loss 6.27389288\n",
      "Trained batch 6489 batch loss 7.14591551 epoch total loss 6.27402687\n",
      "Trained batch 6490 batch loss 6.51674795 epoch total loss 6.27406406\n",
      "Trained batch 6491 batch loss 6.13281155 epoch total loss 6.27404213\n",
      "Trained batch 6492 batch loss 6.92488909 epoch total loss 6.27414274\n",
      "Trained batch 6493 batch loss 6.36184835 epoch total loss 6.27415657\n",
      "Trained batch 6494 batch loss 6.68198 epoch total loss 6.27421951\n",
      "Trained batch 6495 batch loss 6.58615398 epoch total loss 6.27426767\n",
      "Trained batch 6496 batch loss 6.21880341 epoch total loss 6.27425909\n",
      "Trained batch 6497 batch loss 6.82530212 epoch total loss 6.27434349\n",
      "Trained batch 6498 batch loss 2.27067256 epoch total loss 6.27372742\n",
      "Trained batch 6499 batch loss 6.62300777 epoch total loss 6.27378082\n",
      "Trained batch 6500 batch loss 5.62685966 epoch total loss 6.27368069\n",
      "Trained batch 6501 batch loss 6.33067083 epoch total loss 6.27368975\n",
      "Trained batch 6502 batch loss 5.94152355 epoch total loss 6.27363873\n",
      "Trained batch 6503 batch loss 5.92367077 epoch total loss 6.27358484\n",
      "Trained batch 6504 batch loss 6.53356266 epoch total loss 6.2736249\n",
      "Trained batch 6505 batch loss 6.23300362 epoch total loss 6.2736187\n",
      "Trained batch 6506 batch loss 6.6971879 epoch total loss 6.27368355\n",
      "Trained batch 6507 batch loss 6.41073513 epoch total loss 6.27370453\n",
      "Trained batch 6508 batch loss 5.99927187 epoch total loss 6.27366257\n",
      "Trained batch 6509 batch loss 6.35231781 epoch total loss 6.27367449\n",
      "Trained batch 6510 batch loss 6.10409546 epoch total loss 6.27364874\n",
      "Trained batch 6511 batch loss 6.70756149 epoch total loss 6.2737155\n",
      "Trained batch 6512 batch loss 6.74545097 epoch total loss 6.27378798\n",
      "Trained batch 6513 batch loss 6.75708055 epoch total loss 6.27386236\n",
      "Trained batch 6514 batch loss 5.30153465 epoch total loss 6.27371264\n",
      "Trained batch 6515 batch loss 6.42016077 epoch total loss 6.27373552\n",
      "Trained batch 6516 batch loss 6.19336414 epoch total loss 6.2737236\n",
      "Trained batch 6517 batch loss 6.32928085 epoch total loss 6.27373171\n",
      "Trained batch 6518 batch loss 6.01099968 epoch total loss 6.27369165\n",
      "Trained batch 6519 batch loss 6.3373313 epoch total loss 6.27370119\n",
      "Trained batch 6520 batch loss 6.23499966 epoch total loss 6.27369499\n",
      "Trained batch 6521 batch loss 6.48264408 epoch total loss 6.27372742\n",
      "Trained batch 6522 batch loss 6.33570862 epoch total loss 6.27373695\n",
      "Trained batch 6523 batch loss 6.36692619 epoch total loss 6.27375126\n",
      "Trained batch 6524 batch loss 6.28253365 epoch total loss 6.27375221\n",
      "Trained batch 6525 batch loss 6.42968512 epoch total loss 6.27377653\n",
      "Trained batch 6526 batch loss 6.49530363 epoch total loss 6.27381039\n",
      "Trained batch 6527 batch loss 6.17829227 epoch total loss 6.27379608\n",
      "Trained batch 6528 batch loss 6.62457085 epoch total loss 6.27385\n",
      "Trained batch 6529 batch loss 7.00908566 epoch total loss 6.27396202\n",
      "Trained batch 6530 batch loss 6.52588 epoch total loss 6.27400112\n",
      "Trained batch 6531 batch loss 6.19744158 epoch total loss 6.27398968\n",
      "Trained batch 6532 batch loss 6.85024929 epoch total loss 6.27407789\n",
      "Trained batch 6533 batch loss 6.87182617 epoch total loss 6.27416945\n",
      "Trained batch 6534 batch loss 6.98624516 epoch total loss 6.27427816\n",
      "Trained batch 6535 batch loss 6.9057169 epoch total loss 6.27437496\n",
      "Trained batch 6536 batch loss 6.59442663 epoch total loss 6.2744236\n",
      "Trained batch 6537 batch loss 6.3079381 epoch total loss 6.27442884\n",
      "Trained batch 6538 batch loss 4.60732794 epoch total loss 6.27417374\n",
      "Trained batch 6539 batch loss 6.66926718 epoch total loss 6.27423382\n",
      "Trained batch 6540 batch loss 6.47855854 epoch total loss 6.27426529\n",
      "Trained batch 6541 batch loss 7.071702 epoch total loss 6.27438688\n",
      "Trained batch 6542 batch loss 6.87854 epoch total loss 6.27447939\n",
      "Trained batch 6543 batch loss 6.99932766 epoch total loss 6.27459049\n",
      "Trained batch 6544 batch loss 6.88112926 epoch total loss 6.27468348\n",
      "Trained batch 6545 batch loss 7.02564144 epoch total loss 6.27479839\n",
      "Trained batch 6546 batch loss 5.76078892 epoch total loss 6.27471972\n",
      "Trained batch 6547 batch loss 6.27662 epoch total loss 6.27472\n",
      "Trained batch 6548 batch loss 6.24280071 epoch total loss 6.27471542\n",
      "Trained batch 6549 batch loss 6.32522964 epoch total loss 6.27472305\n",
      "Trained batch 6550 batch loss 5.80303383 epoch total loss 6.27465105\n",
      "Trained batch 6551 batch loss 4.14944696 epoch total loss 6.27432632\n",
      "Trained batch 6552 batch loss 6.97068691 epoch total loss 6.27443266\n",
      "Trained batch 6553 batch loss 5.53943825 epoch total loss 6.27432\n",
      "Trained batch 6554 batch loss 5.11748409 epoch total loss 6.2741437\n",
      "Trained batch 6555 batch loss 5.05630207 epoch total loss 6.27395773\n",
      "Trained batch 6556 batch loss 4.25742531 epoch total loss 6.27365\n",
      "Trained batch 6557 batch loss 5.67064905 epoch total loss 6.27355862\n",
      "Trained batch 6558 batch loss 5.3481226 epoch total loss 6.27341747\n",
      "Trained batch 6559 batch loss 5.10470963 epoch total loss 6.27323914\n",
      "Trained batch 6560 batch loss 5.0587635 epoch total loss 6.27305412\n",
      "Trained batch 6561 batch loss 4.76266336 epoch total loss 6.27282381\n",
      "Trained batch 6562 batch loss 4.61559725 epoch total loss 6.27257156\n",
      "Trained batch 6563 batch loss 4.73888969 epoch total loss 6.27233744\n",
      "Trained batch 6564 batch loss 5.21828938 epoch total loss 6.27217722\n",
      "Trained batch 6565 batch loss 3.69148874 epoch total loss 6.27178383\n",
      "Trained batch 6566 batch loss 7.08542728 epoch total loss 6.27190781\n",
      "Trained batch 6567 batch loss 6.89410734 epoch total loss 6.2720027\n",
      "Trained batch 6568 batch loss 6.13496494 epoch total loss 6.27198219\n",
      "Trained batch 6569 batch loss 6.66830873 epoch total loss 6.27204227\n",
      "Trained batch 6570 batch loss 6.99759865 epoch total loss 6.27215242\n",
      "Trained batch 6571 batch loss 7.04100895 epoch total loss 6.27226925\n",
      "Trained batch 6572 batch loss 6.6030035 epoch total loss 6.27231932\n",
      "Trained batch 6573 batch loss 6.65107822 epoch total loss 6.27237749\n",
      "Trained batch 6574 batch loss 6.77805424 epoch total loss 6.27245426\n",
      "Trained batch 6575 batch loss 6.38844681 epoch total loss 6.27247143\n",
      "Trained batch 6576 batch loss 6.75234842 epoch total loss 6.27254486\n",
      "Trained batch 6577 batch loss 6.63216 epoch total loss 6.2725997\n",
      "Trained batch 6578 batch loss 6.4291048 epoch total loss 6.27262354\n",
      "Trained batch 6579 batch loss 7.14537334 epoch total loss 6.2727561\n",
      "Trained batch 6580 batch loss 6.8663063 epoch total loss 6.27284622\n",
      "Trained batch 6581 batch loss 6.31993389 epoch total loss 6.27285337\n",
      "Trained batch 6582 batch loss 6.48326874 epoch total loss 6.2728858\n",
      "Trained batch 6583 batch loss 6.3875227 epoch total loss 6.27290297\n",
      "Trained batch 6584 batch loss 6.69850159 epoch total loss 6.27296782\n",
      "Trained batch 6585 batch loss 6.75270605 epoch total loss 6.27304077\n",
      "Trained batch 6586 batch loss 4.96363163 epoch total loss 6.27284193\n",
      "Trained batch 6587 batch loss 6.79441929 epoch total loss 6.27292109\n",
      "Trained batch 6588 batch loss 6.03252411 epoch total loss 6.27288437\n",
      "Trained batch 6589 batch loss 6.12707567 epoch total loss 6.27286243\n",
      "Trained batch 6590 batch loss 6.16561317 epoch total loss 6.27284575\n",
      "Trained batch 6591 batch loss 6.10389042 epoch total loss 6.27282047\n",
      "Trained batch 6592 batch loss 6.38033915 epoch total loss 6.27283669\n",
      "Trained batch 6593 batch loss 6.49949312 epoch total loss 6.27287102\n",
      "Trained batch 6594 batch loss 6.39104843 epoch total loss 6.27288914\n",
      "Trained batch 6595 batch loss 6.13717747 epoch total loss 6.27286816\n",
      "Trained batch 6596 batch loss 6.33558559 epoch total loss 6.27287769\n",
      "Trained batch 6597 batch loss 6.30172348 epoch total loss 6.27288198\n",
      "Trained batch 6598 batch loss 6.42224216 epoch total loss 6.27290487\n",
      "Trained batch 6599 batch loss 6.28468037 epoch total loss 6.2729063\n",
      "Trained batch 6600 batch loss 6.16674376 epoch total loss 6.27289057\n",
      "Trained batch 6601 batch loss 5.98716354 epoch total loss 6.27284765\n",
      "Trained batch 6602 batch loss 6.49077606 epoch total loss 6.27288055\n",
      "Trained batch 6603 batch loss 6.19786 epoch total loss 6.27286959\n",
      "Trained batch 6604 batch loss 5.88402748 epoch total loss 6.27281046\n",
      "Trained batch 6605 batch loss 6.46905422 epoch total loss 6.27284\n",
      "Trained batch 6606 batch loss 6.49290371 epoch total loss 6.2728734\n",
      "Trained batch 6607 batch loss 6.22384501 epoch total loss 6.27286577\n",
      "Trained batch 6608 batch loss 6.15470076 epoch total loss 6.27284813\n",
      "Trained batch 6609 batch loss 5.80822659 epoch total loss 6.27277803\n",
      "Trained batch 6610 batch loss 6.13249922 epoch total loss 6.27275658\n",
      "Trained batch 6611 batch loss 6.27376461 epoch total loss 6.27275705\n",
      "Trained batch 6612 batch loss 5.88719177 epoch total loss 6.2726984\n",
      "Trained batch 6613 batch loss 6.19269037 epoch total loss 6.272686\n",
      "Trained batch 6614 batch loss 4.76630688 epoch total loss 6.27245808\n",
      "Trained batch 6615 batch loss 4.92135572 epoch total loss 6.27225399\n",
      "Trained batch 6616 batch loss 4.96907663 epoch total loss 6.27205706\n",
      "Trained batch 6617 batch loss 4.99548674 epoch total loss 6.27186441\n",
      "Trained batch 6618 batch loss 4.87125587 epoch total loss 6.2716527\n",
      "Trained batch 6619 batch loss 4.82446384 epoch total loss 6.27143383\n",
      "Trained batch 6620 batch loss 4.78345394 epoch total loss 6.27120924\n",
      "Trained batch 6621 batch loss 4.75256443 epoch total loss 6.27098036\n",
      "Trained batch 6622 batch loss 5.19449 epoch total loss 6.27081776\n",
      "Trained batch 6623 batch loss 3.64767075 epoch total loss 6.27042198\n",
      "Trained batch 6624 batch loss 4.94055176 epoch total loss 6.27022123\n",
      "Trained batch 6625 batch loss 4.15883827 epoch total loss 6.26990271\n",
      "Trained batch 6626 batch loss 4.73754835 epoch total loss 6.26967144\n",
      "Trained batch 6627 batch loss 4.86703682 epoch total loss 6.26945972\n",
      "Trained batch 6628 batch loss 4.64204454 epoch total loss 6.26921415\n",
      "Trained batch 6629 batch loss 4.8038311 epoch total loss 6.26899338\n",
      "Trained batch 6630 batch loss 3.93003654 epoch total loss 6.26864052\n",
      "Trained batch 6631 batch loss 4.52971268 epoch total loss 6.26837826\n",
      "Trained batch 6632 batch loss 4.57982635 epoch total loss 6.26812363\n",
      "Trained batch 6633 batch loss 6.00721121 epoch total loss 6.26808453\n",
      "Trained batch 6634 batch loss 5.81792688 epoch total loss 6.26801634\n",
      "Trained batch 6635 batch loss 6.66767406 epoch total loss 6.26807642\n",
      "Trained batch 6636 batch loss 5.77022743 epoch total loss 6.26800156\n",
      "Trained batch 6637 batch loss 6.19753551 epoch total loss 6.26799107\n",
      "Trained batch 6638 batch loss 6.64916897 epoch total loss 6.26804829\n",
      "Trained batch 6639 batch loss 6.08188438 epoch total loss 6.26802\n",
      "Trained batch 6640 batch loss 6.34752893 epoch total loss 6.26803207\n",
      "Trained batch 6641 batch loss 6.93414211 epoch total loss 6.26813269\n",
      "Trained batch 6642 batch loss 6.32240582 epoch total loss 6.26814079\n",
      "Trained batch 6643 batch loss 6.19405508 epoch total loss 6.26813\n",
      "Trained batch 6644 batch loss 5.49687529 epoch total loss 6.26801395\n",
      "Trained batch 6645 batch loss 6.86956406 epoch total loss 6.26810455\n",
      "Trained batch 6646 batch loss 6.51242447 epoch total loss 6.26814127\n",
      "Trained batch 6647 batch loss 6.66229439 epoch total loss 6.26820087\n",
      "Trained batch 6648 batch loss 6.54281092 epoch total loss 6.26824188\n",
      "Trained batch 6649 batch loss 6.27527952 epoch total loss 6.26824284\n",
      "Trained batch 6650 batch loss 7.08144283 epoch total loss 6.26836538\n",
      "Trained batch 6651 batch loss 6.65516138 epoch total loss 6.26842356\n",
      "Trained batch 6652 batch loss 7.21971321 epoch total loss 6.26856661\n",
      "Trained batch 6653 batch loss 8.04618263 epoch total loss 6.26883364\n",
      "Trained batch 6654 batch loss 7.44541645 epoch total loss 6.26901054\n",
      "Trained batch 6655 batch loss 7.57714081 epoch total loss 6.26920748\n",
      "Trained batch 6656 batch loss 8.01291943 epoch total loss 6.26946926\n",
      "Trained batch 6657 batch loss 7.61438417 epoch total loss 6.26967096\n",
      "Trained batch 6658 batch loss 7.47899723 epoch total loss 6.26985264\n",
      "Trained batch 6659 batch loss 7.28279591 epoch total loss 6.27000475\n",
      "Trained batch 6660 batch loss 7.61278772 epoch total loss 6.27020645\n",
      "Trained batch 6661 batch loss 5.78970909 epoch total loss 6.27013397\n",
      "Trained batch 6662 batch loss 5.45767355 epoch total loss 6.2700119\n",
      "Trained batch 6663 batch loss 5.00580215 epoch total loss 6.26982212\n",
      "Trained batch 6664 batch loss 5.26148367 epoch total loss 6.26967096\n",
      "Trained batch 6665 batch loss 5.00868797 epoch total loss 6.26948166\n",
      "Trained batch 6666 batch loss 6.37415028 epoch total loss 6.26949739\n",
      "Trained batch 6667 batch loss 5.08661842 epoch total loss 6.26931953\n",
      "Trained batch 6668 batch loss 6.35298157 epoch total loss 6.26933193\n",
      "Trained batch 6669 batch loss 6.96714211 epoch total loss 6.26943684\n",
      "Trained batch 6670 batch loss 5.51112318 epoch total loss 6.26932335\n",
      "Trained batch 6671 batch loss 7.13259888 epoch total loss 6.26945257\n",
      "Trained batch 6672 batch loss 5.34938335 epoch total loss 6.26931477\n",
      "Trained batch 6673 batch loss 4.74398518 epoch total loss 6.26908588\n",
      "Trained batch 6674 batch loss 5.15098095 epoch total loss 6.26891851\n",
      "Trained batch 6675 batch loss 5.10300732 epoch total loss 6.26874352\n",
      "Trained batch 6676 batch loss 5.49934292 epoch total loss 6.2686286\n",
      "Trained batch 6677 batch loss 4.93856144 epoch total loss 6.26842928\n",
      "Trained batch 6678 batch loss 7.47137165 epoch total loss 6.26860952\n",
      "Trained batch 6679 batch loss 6.82331419 epoch total loss 6.26869249\n",
      "Trained batch 6680 batch loss 6.56691074 epoch total loss 6.26873732\n",
      "Trained batch 6681 batch loss 6.57047653 epoch total loss 6.26878214\n",
      "Trained batch 6682 batch loss 6.04537201 epoch total loss 6.26874924\n",
      "Trained batch 6683 batch loss 6.55155277 epoch total loss 6.2687912\n",
      "Trained batch 6684 batch loss 6.35617495 epoch total loss 6.26880407\n",
      "Trained batch 6685 batch loss 6.07487154 epoch total loss 6.26877499\n",
      "Trained batch 6686 batch loss 6.46911621 epoch total loss 6.26880503\n",
      "Trained batch 6687 batch loss 6.65802288 epoch total loss 6.2688632\n",
      "Trained batch 6688 batch loss 6.29675579 epoch total loss 6.26886702\n",
      "Trained batch 6689 batch loss 6.45243597 epoch total loss 6.26889467\n",
      "Trained batch 6690 batch loss 6.36782169 epoch total loss 6.26890945\n",
      "Trained batch 6691 batch loss 6.431705 epoch total loss 6.26893425\n",
      "Trained batch 6692 batch loss 5.9733777 epoch total loss 6.26889\n",
      "Trained batch 6693 batch loss 6.30152035 epoch total loss 6.26889467\n",
      "Trained batch 6694 batch loss 6.589 epoch total loss 6.26894236\n",
      "Trained batch 6695 batch loss 6.24332 epoch total loss 6.26893854\n",
      "Trained batch 6696 batch loss 6.59290504 epoch total loss 6.26898718\n",
      "Trained batch 6697 batch loss 5.99204445 epoch total loss 6.26894569\n",
      "Trained batch 6698 batch loss 6.49995661 epoch total loss 6.26898\n",
      "Trained batch 6699 batch loss 6.44284534 epoch total loss 6.26900578\n",
      "Trained batch 6700 batch loss 6.05056095 epoch total loss 6.26897335\n",
      "Trained batch 6701 batch loss 6.31849289 epoch total loss 6.26898098\n",
      "Trained batch 6702 batch loss 5.94161177 epoch total loss 6.26893187\n",
      "Trained batch 6703 batch loss 5.82793617 epoch total loss 6.26886654\n",
      "Trained batch 6704 batch loss 6.16044903 epoch total loss 6.26885033\n",
      "Trained batch 6705 batch loss 6.29315281 epoch total loss 6.26885366\n",
      "Trained batch 6706 batch loss 6.21418953 epoch total loss 6.26884556\n",
      "Trained batch 6707 batch loss 6.21277809 epoch total loss 6.26883698\n",
      "Trained batch 6708 batch loss 6.0629 epoch total loss 6.26880646\n",
      "Trained batch 6709 batch loss 5.6595993 epoch total loss 6.26871538\n",
      "Trained batch 6710 batch loss 6.21991777 epoch total loss 6.26870823\n",
      "Trained batch 6711 batch loss 5.61930275 epoch total loss 6.26861143\n",
      "Trained batch 6712 batch loss 6.21882105 epoch total loss 6.26860428\n",
      "Trained batch 6713 batch loss 6.45863628 epoch total loss 6.26863241\n",
      "Trained batch 6714 batch loss 5.99768448 epoch total loss 6.2685914\n",
      "Trained batch 6715 batch loss 6.22936535 epoch total loss 6.26858616\n",
      "Trained batch 6716 batch loss 6.13380671 epoch total loss 6.26856565\n",
      "Trained batch 6717 batch loss 6.43695641 epoch total loss 6.26859093\n",
      "Trained batch 6718 batch loss 6.2336359 epoch total loss 6.26858568\n",
      "Trained batch 6719 batch loss 6.88333225 epoch total loss 6.26867723\n",
      "Trained batch 6720 batch loss 6.71764851 epoch total loss 6.26874399\n",
      "Trained batch 6721 batch loss 6.37572575 epoch total loss 6.26876\n",
      "Trained batch 6722 batch loss 6.73678398 epoch total loss 6.26883\n",
      "Trained batch 6723 batch loss 6.24164772 epoch total loss 6.26882601\n",
      "Trained batch 6724 batch loss 6.93038559 epoch total loss 6.26892424\n",
      "Trained batch 6725 batch loss 6.17513752 epoch total loss 6.26891041\n",
      "Trained batch 6726 batch loss 6.25423145 epoch total loss 6.26890802\n",
      "Trained batch 6727 batch loss 6.39300919 epoch total loss 6.26892662\n",
      "Trained batch 6728 batch loss 5.70407343 epoch total loss 6.2688427\n",
      "Trained batch 6729 batch loss 6.01143026 epoch total loss 6.26880455\n",
      "Trained batch 6730 batch loss 6.0949707 epoch total loss 6.26877832\n",
      "Trained batch 6731 batch loss 5.47776794 epoch total loss 6.26866055\n",
      "Trained batch 6732 batch loss 6.16500092 epoch total loss 6.26864529\n",
      "Trained batch 6733 batch loss 6.12642193 epoch total loss 6.26862383\n",
      "Trained batch 6734 batch loss 6.40900517 epoch total loss 6.26864481\n",
      "Trained batch 6735 batch loss 6.31297445 epoch total loss 6.26865149\n",
      "Trained batch 6736 batch loss 6.0484314 epoch total loss 6.26861858\n",
      "Trained batch 6737 batch loss 6.60797882 epoch total loss 6.26866913\n",
      "Trained batch 6738 batch loss 6.02497864 epoch total loss 6.26863289\n",
      "Trained batch 6739 batch loss 6.51025867 epoch total loss 6.26866865\n",
      "Trained batch 6740 batch loss 6.20724201 epoch total loss 6.26865959\n",
      "Trained batch 6741 batch loss 5.78917408 epoch total loss 6.26858854\n",
      "Trained batch 6742 batch loss 6.46076 epoch total loss 6.26861715\n",
      "Trained batch 6743 batch loss 6.21142673 epoch total loss 6.26860857\n",
      "Trained batch 6744 batch loss 6.11431694 epoch total loss 6.26858521\n",
      "Trained batch 6745 batch loss 6.40941238 epoch total loss 6.26860619\n",
      "Trained batch 6746 batch loss 6.05370522 epoch total loss 6.26857471\n",
      "Trained batch 6747 batch loss 6.12750244 epoch total loss 6.26855373\n",
      "Trained batch 6748 batch loss 5.16478682 epoch total loss 6.26839\n",
      "Trained batch 6749 batch loss 4.23831749 epoch total loss 6.26808929\n",
      "Trained batch 6750 batch loss 5.89194 epoch total loss 6.2680335\n",
      "Trained batch 6751 batch loss 5.52767181 epoch total loss 6.26792383\n",
      "Trained batch 6752 batch loss 6.6281271 epoch total loss 6.26797724\n",
      "Trained batch 6753 batch loss 6.04608536 epoch total loss 6.26794434\n",
      "Trained batch 6754 batch loss 6.11765194 epoch total loss 6.2679224\n",
      "Trained batch 6755 batch loss 6.25223255 epoch total loss 6.26792\n",
      "Trained batch 6756 batch loss 5.45888948 epoch total loss 6.26780033\n",
      "Trained batch 6757 batch loss 6.59674454 epoch total loss 6.26784897\n",
      "Trained batch 6758 batch loss 6.94162 epoch total loss 6.26794863\n",
      "Trained batch 6759 batch loss 6.45736408 epoch total loss 6.26797676\n",
      "Trained batch 6760 batch loss 6.6682539 epoch total loss 6.26803589\n",
      "Trained batch 6761 batch loss 5.90670109 epoch total loss 6.26798248\n",
      "Trained batch 6762 batch loss 6.08949327 epoch total loss 6.26795578\n",
      "Trained batch 6763 batch loss 7.22448063 epoch total loss 6.26809692\n",
      "Trained batch 6764 batch loss 6.20601559 epoch total loss 6.26808786\n",
      "Trained batch 6765 batch loss 6.41213465 epoch total loss 6.26811\n",
      "Trained batch 6766 batch loss 6.87669182 epoch total loss 6.26819944\n",
      "Trained batch 6767 batch loss 6.6810627 epoch total loss 6.26826\n",
      "Trained batch 6768 batch loss 7.09097147 epoch total loss 6.2683816\n",
      "Trained batch 6769 batch loss 6.30712795 epoch total loss 6.26838732\n",
      "Trained batch 6770 batch loss 6.43261385 epoch total loss 6.26841211\n",
      "Trained batch 6771 batch loss 6.47042561 epoch total loss 6.26844168\n",
      "Trained batch 6772 batch loss 5.62061 epoch total loss 6.26834583\n",
      "Trained batch 6773 batch loss 5.33184338 epoch total loss 6.26820755\n",
      "Trained batch 6774 batch loss 6.33893585 epoch total loss 6.26821804\n",
      "Trained batch 6775 batch loss 6.09639311 epoch total loss 6.26819324\n",
      "Trained batch 6776 batch loss 6.11313915 epoch total loss 6.26817036\n",
      "Trained batch 6777 batch loss 6.08661222 epoch total loss 6.26814318\n",
      "Trained batch 6778 batch loss 7.38627148 epoch total loss 6.26830816\n",
      "Trained batch 6779 batch loss 6.59352922 epoch total loss 6.26835632\n",
      "Trained batch 6780 batch loss 3.93597937 epoch total loss 6.26801252\n",
      "Trained batch 6781 batch loss 3.99530411 epoch total loss 6.26767731\n",
      "Trained batch 6782 batch loss 4.03395653 epoch total loss 6.26734829\n",
      "Trained batch 6783 batch loss 3.90664816 epoch total loss 6.267\n",
      "Trained batch 6784 batch loss 5.16763401 epoch total loss 6.26683807\n",
      "Trained batch 6785 batch loss 4.75948381 epoch total loss 6.26661587\n",
      "Trained batch 6786 batch loss 5.55114651 epoch total loss 6.26651049\n",
      "Trained batch 6787 batch loss 4.60987377 epoch total loss 6.26626635\n",
      "Trained batch 6788 batch loss 6.69434261 epoch total loss 6.26632929\n",
      "Trained batch 6789 batch loss 5.89818859 epoch total loss 6.26627541\n",
      "Trained batch 6790 batch loss 6.60948944 epoch total loss 6.26632547\n",
      "Trained batch 6791 batch loss 6.49709034 epoch total loss 6.26635933\n",
      "Trained batch 6792 batch loss 6.3741107 epoch total loss 6.26637554\n",
      "Trained batch 6793 batch loss 6.91735363 epoch total loss 6.26647139\n",
      "Trained batch 6794 batch loss 5.51582527 epoch total loss 6.26636076\n",
      "Trained batch 6795 batch loss 5.3354044 epoch total loss 6.26622391\n",
      "Trained batch 6796 batch loss 4.74530315 epoch total loss 6.26600027\n",
      "Trained batch 6797 batch loss 4.70611048 epoch total loss 6.26577091\n",
      "Trained batch 6798 batch loss 6.07036448 epoch total loss 6.2657423\n",
      "Trained batch 6799 batch loss 6.15977478 epoch total loss 6.26572657\n",
      "Trained batch 6800 batch loss 5.79899216 epoch total loss 6.26565838\n",
      "Trained batch 6801 batch loss 5.87199497 epoch total loss 6.2656\n",
      "Trained batch 6802 batch loss 6.32151937 epoch total loss 6.26560831\n",
      "Trained batch 6803 batch loss 6.51782846 epoch total loss 6.2656455\n",
      "Trained batch 6804 batch loss 5.42771149 epoch total loss 6.265522\n",
      "Trained batch 6805 batch loss 5.88271189 epoch total loss 6.26546621\n",
      "Trained batch 6806 batch loss 4.27069187 epoch total loss 6.26517296\n",
      "Trained batch 6807 batch loss 6.54676437 epoch total loss 6.26521397\n",
      "Trained batch 6808 batch loss 7.25629473 epoch total loss 6.26536\n",
      "Trained batch 6809 batch loss 6.9489212 epoch total loss 6.26546049\n",
      "Trained batch 6810 batch loss 6.57628202 epoch total loss 6.26550627\n",
      "Trained batch 6811 batch loss 5.36859751 epoch total loss 6.26537418\n",
      "Trained batch 6812 batch loss 6.82887 epoch total loss 6.26545715\n",
      "Trained batch 6813 batch loss 6.35392046 epoch total loss 6.26547\n",
      "Trained batch 6814 batch loss 6.19108772 epoch total loss 6.26545954\n",
      "Trained batch 6815 batch loss 6.51139879 epoch total loss 6.2654953\n",
      "Trained batch 6816 batch loss 6.48088551 epoch total loss 6.26552677\n",
      "Trained batch 6817 batch loss 5.31236839 epoch total loss 6.26538706\n",
      "Trained batch 6818 batch loss 5.65555286 epoch total loss 6.26529789\n",
      "Trained batch 6819 batch loss 5.73984432 epoch total loss 6.26522064\n",
      "Trained batch 6820 batch loss 5.49095488 epoch total loss 6.26510715\n",
      "Trained batch 6821 batch loss 4.58902073 epoch total loss 6.26486158\n",
      "Trained batch 6822 batch loss 5.98832798 epoch total loss 6.26482105\n",
      "Trained batch 6823 batch loss 6.28224134 epoch total loss 6.26482344\n",
      "Trained batch 6824 batch loss 5.86316681 epoch total loss 6.26476479\n",
      "Trained batch 6825 batch loss 5.69864464 epoch total loss 6.26468182\n",
      "Trained batch 6826 batch loss 5.10075474 epoch total loss 6.26451159\n",
      "Trained batch 6827 batch loss 6.51900244 epoch total loss 6.26454878\n",
      "Trained batch 6828 batch loss 6.21244097 epoch total loss 6.26454067\n",
      "Trained batch 6829 batch loss 5.65690851 epoch total loss 6.26445198\n",
      "Trained batch 6830 batch loss 5.94280338 epoch total loss 6.2644043\n",
      "Trained batch 6831 batch loss 5.5725832 epoch total loss 6.26430368\n",
      "Trained batch 6832 batch loss 6.52679825 epoch total loss 6.26434183\n",
      "Trained batch 6833 batch loss 6.10518217 epoch total loss 6.26431894\n",
      "Trained batch 6834 batch loss 5.85934067 epoch total loss 6.26425934\n",
      "Trained batch 6835 batch loss 6.17314291 epoch total loss 6.26424599\n",
      "Trained batch 6836 batch loss 6.8512373 epoch total loss 6.26433182\n",
      "Trained batch 6837 batch loss 5.89879608 epoch total loss 6.26427841\n",
      "Trained batch 6838 batch loss 6.22635698 epoch total loss 6.26427269\n",
      "Trained batch 6839 batch loss 6.42032909 epoch total loss 6.26429605\n",
      "Trained batch 6840 batch loss 6.49294 epoch total loss 6.26432943\n",
      "Trained batch 6841 batch loss 5.90768719 epoch total loss 6.26427698\n",
      "Trained batch 6842 batch loss 4.72767925 epoch total loss 6.26405191\n",
      "Trained batch 6843 batch loss 5.6464386 epoch total loss 6.26396179\n",
      "Trained batch 6844 batch loss 6.13468885 epoch total loss 6.26394224\n",
      "Trained batch 6845 batch loss 5.83193398 epoch total loss 6.2638793\n",
      "Trained batch 6846 batch loss 6.99192476 epoch total loss 6.26398563\n",
      "Trained batch 6847 batch loss 6.71449852 epoch total loss 6.26405144\n",
      "Trained batch 6848 batch loss 6.1558814 epoch total loss 6.2640357\n",
      "Trained batch 6849 batch loss 6.95587444 epoch total loss 6.26413679\n",
      "Trained batch 6850 batch loss 6.11134911 epoch total loss 6.26411486\n",
      "Trained batch 6851 batch loss 6.30002308 epoch total loss 6.26412\n",
      "Trained batch 6852 batch loss 6.34171343 epoch total loss 6.26413155\n",
      "Trained batch 6853 batch loss 6.53464508 epoch total loss 6.26417112\n",
      "Trained batch 6854 batch loss 6.694664 epoch total loss 6.26423359\n",
      "Trained batch 6855 batch loss 6.48847 epoch total loss 6.26426649\n",
      "Trained batch 6856 batch loss 7.04880524 epoch total loss 6.26438046\n",
      "Trained batch 6857 batch loss 6.13604546 epoch total loss 6.26436186\n",
      "Trained batch 6858 batch loss 5.79503918 epoch total loss 6.26429367\n",
      "Trained batch 6859 batch loss 5.4525919 epoch total loss 6.26417542\n",
      "Trained batch 6860 batch loss 4.40846 epoch total loss 6.26390553\n",
      "Trained batch 6861 batch loss 4.79009771 epoch total loss 6.26369047\n",
      "Trained batch 6862 batch loss 5.19996452 epoch total loss 6.26353502\n",
      "Trained batch 6863 batch loss 5.10845089 epoch total loss 6.26336718\n",
      "Trained batch 6864 batch loss 4.94475317 epoch total loss 6.26317501\n",
      "Trained batch 6865 batch loss 4.49967289 epoch total loss 6.262918\n",
      "Trained batch 6866 batch loss 4.91316891 epoch total loss 6.26272154\n",
      "Trained batch 6867 batch loss 5.20368958 epoch total loss 6.26256752\n",
      "Trained batch 6868 batch loss 4.91563845 epoch total loss 6.26237106\n",
      "Trained batch 6869 batch loss 5.17490053 epoch total loss 6.26221275\n",
      "Trained batch 6870 batch loss 5.32943153 epoch total loss 6.26207685\n",
      "Trained batch 6871 batch loss 4.75355911 epoch total loss 6.26185751\n",
      "Trained batch 6872 batch loss 4.71622705 epoch total loss 6.26163244\n",
      "Trained batch 6873 batch loss 4.0622983 epoch total loss 6.26131248\n",
      "Trained batch 6874 batch loss 4.21211576 epoch total loss 6.26101398\n",
      "Trained batch 6875 batch loss 4.68529844 epoch total loss 6.26078463\n",
      "Trained batch 6876 batch loss 4.61197948 epoch total loss 6.26054525\n",
      "Trained batch 6877 batch loss 5.53828 epoch total loss 6.26044035\n",
      "Trained batch 6878 batch loss 5.99600315 epoch total loss 6.26040173\n",
      "Trained batch 6879 batch loss 7.37308502 epoch total loss 6.26056385\n",
      "Trained batch 6880 batch loss 6.05030155 epoch total loss 6.26053333\n",
      "Trained batch 6881 batch loss 5.95802498 epoch total loss 6.26048899\n",
      "Trained batch 6882 batch loss 4.66408777 epoch total loss 6.26025724\n",
      "Trained batch 6883 batch loss 4.82121372 epoch total loss 6.26004791\n",
      "Trained batch 6884 batch loss 6.2775 epoch total loss 6.2600503\n",
      "Trained batch 6885 batch loss 6.31268883 epoch total loss 6.26005793\n",
      "Trained batch 6886 batch loss 4.97801685 epoch total loss 6.25987148\n",
      "Trained batch 6887 batch loss 6.88958168 epoch total loss 6.25996351\n",
      "Trained batch 6888 batch loss 6.60933971 epoch total loss 6.26001406\n",
      "Trained batch 6889 batch loss 5.84688377 epoch total loss 6.25995398\n",
      "Trained batch 6890 batch loss 5.70602417 epoch total loss 6.25987387\n",
      "Trained batch 6891 batch loss 5.89152575 epoch total loss 6.25982046\n",
      "Trained batch 6892 batch loss 5.99867487 epoch total loss 6.25978279\n",
      "Trained batch 6893 batch loss 5.39064074 epoch total loss 6.25965643\n",
      "Trained batch 6894 batch loss 6.17288828 epoch total loss 6.25964403\n",
      "Trained batch 6895 batch loss 5.55580044 epoch total loss 6.25954151\n",
      "Trained batch 6896 batch loss 5.8599658 epoch total loss 6.25948334\n",
      "Trained batch 6897 batch loss 5.30600739 epoch total loss 6.25934505\n",
      "Trained batch 6898 batch loss 6.15846443 epoch total loss 6.25933075\n",
      "Trained batch 6899 batch loss 6.11181068 epoch total loss 6.25931\n",
      "Trained batch 6900 batch loss 5.60911369 epoch total loss 6.25921535\n",
      "Trained batch 6901 batch loss 5.98600388 epoch total loss 6.2591753\n",
      "Trained batch 6902 batch loss 5.85528183 epoch total loss 6.25911713\n",
      "Trained batch 6903 batch loss 6.29231215 epoch total loss 6.25912189\n",
      "Trained batch 6904 batch loss 5.84692097 epoch total loss 6.25906229\n",
      "Trained batch 6905 batch loss 6.51769352 epoch total loss 6.2591\n",
      "Trained batch 6906 batch loss 5.86462402 epoch total loss 6.25904274\n",
      "Trained batch 6907 batch loss 6.13617611 epoch total loss 6.2590251\n",
      "Trained batch 6908 batch loss 6.11096573 epoch total loss 6.25900316\n",
      "Trained batch 6909 batch loss 6.00465 epoch total loss 6.25896645\n",
      "Trained batch 6910 batch loss 7.29069614 epoch total loss 6.2591157\n",
      "Trained batch 6911 batch loss 5.59540319 epoch total loss 6.25901937\n",
      "Trained batch 6912 batch loss 6.00957394 epoch total loss 6.25898314\n",
      "Trained batch 6913 batch loss 6.05789948 epoch total loss 6.25895405\n",
      "Trained batch 6914 batch loss 6.23783588 epoch total loss 6.25895071\n",
      "Trained batch 6915 batch loss 5.42399883 epoch total loss 6.25883055\n",
      "Trained batch 6916 batch loss 6.32448816 epoch total loss 6.25884\n",
      "Trained batch 6917 batch loss 6.19080162 epoch total loss 6.25883\n",
      "Trained batch 6918 batch loss 7.14055061 epoch total loss 6.25895739\n",
      "Trained batch 6919 batch loss 6.413239 epoch total loss 6.25898\n",
      "Trained batch 6920 batch loss 5.76496458 epoch total loss 6.25890875\n",
      "Trained batch 6921 batch loss 5.16260195 epoch total loss 6.25875044\n",
      "Trained batch 6922 batch loss 5.25179863 epoch total loss 6.258605\n",
      "Trained batch 6923 batch loss 6.59003639 epoch total loss 6.25865269\n",
      "Trained batch 6924 batch loss 6.43726969 epoch total loss 6.25867844\n",
      "Trained batch 6925 batch loss 6.19334507 epoch total loss 6.2586689\n",
      "Trained batch 6926 batch loss 6.32069826 epoch total loss 6.25867748\n",
      "Trained batch 6927 batch loss 6.39221191 epoch total loss 6.25869656\n",
      "Trained batch 6928 batch loss 6.56701469 epoch total loss 6.2587409\n",
      "Trained batch 6929 batch loss 6.57900286 epoch total loss 6.25878716\n",
      "Trained batch 6930 batch loss 6.31237221 epoch total loss 6.25879478\n",
      "Trained batch 6931 batch loss 5.84418392 epoch total loss 6.25873518\n",
      "Trained batch 6932 batch loss 5.34095383 epoch total loss 6.25860262\n",
      "Trained batch 6933 batch loss 6.3743906 epoch total loss 6.25861931\n",
      "Trained batch 6934 batch loss 5.44964886 epoch total loss 6.25850248\n",
      "Trained batch 6935 batch loss 7.03218365 epoch total loss 6.25861406\n",
      "Trained batch 6936 batch loss 6.06250668 epoch total loss 6.25858593\n",
      "Trained batch 6937 batch loss 5.9553113 epoch total loss 6.25854206\n",
      "Trained batch 6938 batch loss 5.98348951 epoch total loss 6.25850296\n",
      "Trained batch 6939 batch loss 5.71432114 epoch total loss 6.25842428\n",
      "Trained batch 6940 batch loss 5.58635521 epoch total loss 6.25832748\n",
      "Trained batch 6941 batch loss 5.06869793 epoch total loss 6.2581563\n",
      "Trained batch 6942 batch loss 7.4345274 epoch total loss 6.25832558\n",
      "Trained batch 6943 batch loss 7.79343081 epoch total loss 6.25854683\n",
      "Trained batch 6944 batch loss 8.04634857 epoch total loss 6.25880432\n",
      "Trained batch 6945 batch loss 7.21157837 epoch total loss 6.25894117\n",
      "Trained batch 6946 batch loss 7.50206041 epoch total loss 6.25912046\n",
      "Trained batch 6947 batch loss 6.7599597 epoch total loss 6.25919294\n",
      "Trained batch 6948 batch loss 7.4659729 epoch total loss 6.25936651\n",
      "Trained batch 6949 batch loss 7.15998411 epoch total loss 6.25949621\n",
      "Trained batch 6950 batch loss 7.01701546 epoch total loss 6.25960493\n",
      "Trained batch 6951 batch loss 6.60403061 epoch total loss 6.25965452\n",
      "Trained batch 6952 batch loss 7.17453909 epoch total loss 6.25978661\n",
      "Trained batch 6953 batch loss 6.51733971 epoch total loss 6.25982332\n",
      "Trained batch 6954 batch loss 7.0077877 epoch total loss 6.25993061\n",
      "Trained batch 6955 batch loss 7.07306385 epoch total loss 6.26004791\n",
      "Trained batch 6956 batch loss 7.03985071 epoch total loss 6.26016\n",
      "Trained batch 6957 batch loss 6.74507141 epoch total loss 6.26022959\n",
      "Trained batch 6958 batch loss 7.15411234 epoch total loss 6.26035786\n",
      "Trained batch 6959 batch loss 6.87042427 epoch total loss 6.26044559\n",
      "Trained batch 6960 batch loss 6.92043304 epoch total loss 6.26054049\n",
      "Trained batch 6961 batch loss 6.76201963 epoch total loss 6.26061249\n",
      "Trained batch 6962 batch loss 6.75369453 epoch total loss 6.26068354\n",
      "Trained batch 6963 batch loss 6.46771288 epoch total loss 6.26071358\n",
      "Trained batch 6964 batch loss 6.13145399 epoch total loss 6.26069498\n",
      "Trained batch 6965 batch loss 5.88534641 epoch total loss 6.26064157\n",
      "Trained batch 6966 batch loss 5.00174046 epoch total loss 6.26046038\n",
      "Trained batch 6967 batch loss 6.68796635 epoch total loss 6.26052189\n",
      "Trained batch 6968 batch loss 6.00175381 epoch total loss 6.26048422\n",
      "Trained batch 6969 batch loss 6.6684742 epoch total loss 6.26054287\n",
      "Trained batch 6970 batch loss 6.74992657 epoch total loss 6.26061296\n",
      "Trained batch 6971 batch loss 6.39634418 epoch total loss 6.26063204\n",
      "Trained batch 6972 batch loss 5.96883392 epoch total loss 6.26059055\n",
      "Trained batch 6973 batch loss 6.50109386 epoch total loss 6.26062489\n",
      "Trained batch 6974 batch loss 5.8539 epoch total loss 6.26056671\n",
      "Trained batch 6975 batch loss 6.44831514 epoch total loss 6.26059341\n",
      "Trained batch 6976 batch loss 6.09150314 epoch total loss 6.2605691\n",
      "Trained batch 6977 batch loss 5.99583721 epoch total loss 6.26053143\n",
      "Trained batch 6978 batch loss 6.50937843 epoch total loss 6.26056671\n",
      "Trained batch 6979 batch loss 6.52182388 epoch total loss 6.26060438\n",
      "Trained batch 6980 batch loss 6.21529293 epoch total loss 6.26059771\n",
      "Trained batch 6981 batch loss 6.36856699 epoch total loss 6.26061296\n",
      "Trained batch 6982 batch loss 6.46832943 epoch total loss 6.26064301\n",
      "Trained batch 6983 batch loss 6.06158161 epoch total loss 6.2606144\n",
      "Trained batch 6984 batch loss 6.16976547 epoch total loss 6.26060104\n",
      "Trained batch 6985 batch loss 6.17465973 epoch total loss 6.26058912\n",
      "Trained batch 6986 batch loss 6.24958658 epoch total loss 6.26058769\n",
      "Trained batch 6987 batch loss 6.18980122 epoch total loss 6.26057768\n",
      "Trained batch 6988 batch loss 6.48547602 epoch total loss 6.26060963\n",
      "Trained batch 6989 batch loss 6.23157692 epoch total loss 6.26060534\n",
      "Trained batch 6990 batch loss 6.24002552 epoch total loss 6.260602\n",
      "Trained batch 6991 batch loss 6.19888115 epoch total loss 6.26059341\n",
      "Trained batch 6992 batch loss 5.44148445 epoch total loss 6.26047611\n",
      "Trained batch 6993 batch loss 6.0973959 epoch total loss 6.26045275\n",
      "Trained batch 6994 batch loss 6.61686707 epoch total loss 6.26050377\n",
      "Trained batch 6995 batch loss 6.06004381 epoch total loss 6.26047516\n",
      "Trained batch 6996 batch loss 6.59528732 epoch total loss 6.26052284\n",
      "Trained batch 6997 batch loss 6.33452225 epoch total loss 6.26053333\n",
      "Trained batch 6998 batch loss 6.82842112 epoch total loss 6.26061487\n",
      "Trained batch 6999 batch loss 6.72035122 epoch total loss 6.26068\n",
      "Trained batch 7000 batch loss 7.20219088 epoch total loss 6.26081467\n",
      "Trained batch 7001 batch loss 6.72407675 epoch total loss 6.26088047\n",
      "Trained batch 7002 batch loss 6.70948505 epoch total loss 6.26094484\n",
      "Trained batch 7003 batch loss 7.12711239 epoch total loss 6.26106882\n",
      "Trained batch 7004 batch loss 6.90570116 epoch total loss 6.26116085\n",
      "Trained batch 7005 batch loss 6.81081533 epoch total loss 6.26123953\n",
      "Trained batch 7006 batch loss 6.92318535 epoch total loss 6.26133394\n",
      "Trained batch 7007 batch loss 5.96679544 epoch total loss 6.2612915\n",
      "Trained batch 7008 batch loss 5.95218039 epoch total loss 6.26124763\n",
      "Trained batch 7009 batch loss 6.85064936 epoch total loss 6.26133204\n",
      "Trained batch 7010 batch loss 5.93619919 epoch total loss 6.26128578\n",
      "Trained batch 7011 batch loss 6.11433887 epoch total loss 6.2612648\n",
      "Trained batch 7012 batch loss 6.11957455 epoch total loss 6.26124477\n",
      "Trained batch 7013 batch loss 5.22513437 epoch total loss 6.26109695\n",
      "Trained batch 7014 batch loss 4.82750225 epoch total loss 6.26089287\n",
      "Trained batch 7015 batch loss 5.03865433 epoch total loss 6.26071882\n",
      "Trained batch 7016 batch loss 6.7594552 epoch total loss 6.26078939\n",
      "Trained batch 7017 batch loss 6.08044577 epoch total loss 6.26076412\n",
      "Trained batch 7018 batch loss 4.6605978 epoch total loss 6.26053619\n",
      "Trained batch 7019 batch loss 5.09232903 epoch total loss 6.26037\n",
      "Trained batch 7020 batch loss 6.91126919 epoch total loss 6.26046228\n",
      "Trained batch 7021 batch loss 4.76743698 epoch total loss 6.26024961\n",
      "Trained batch 7022 batch loss 6.20021677 epoch total loss 6.26024055\n",
      "Trained batch 7023 batch loss 7.3567524 epoch total loss 6.26039648\n",
      "Trained batch 7024 batch loss 7.12508869 epoch total loss 6.2605195\n",
      "Trained batch 7025 batch loss 5.5846262 epoch total loss 6.26042366\n",
      "Trained batch 7026 batch loss 6.12224913 epoch total loss 6.26040411\n",
      "Trained batch 7027 batch loss 7.00670815 epoch total loss 6.26051044\n",
      "Trained batch 7028 batch loss 6.65136766 epoch total loss 6.26056576\n",
      "Trained batch 7029 batch loss 6.72081232 epoch total loss 6.26063156\n",
      "Trained batch 7030 batch loss 6.66060257 epoch total loss 6.26068878\n",
      "Trained batch 7031 batch loss 6.46087742 epoch total loss 6.26071692\n",
      "Trained batch 7032 batch loss 6.39456081 epoch total loss 6.26073599\n",
      "Trained batch 7033 batch loss 6.57065058 epoch total loss 6.26078\n",
      "Trained batch 7034 batch loss 6.14724541 epoch total loss 6.26076412\n",
      "Trained batch 7035 batch loss 6.33806896 epoch total loss 6.26077557\n",
      "Trained batch 7036 batch loss 6.24209881 epoch total loss 6.26077271\n",
      "Trained batch 7037 batch loss 6.24297428 epoch total loss 6.26077\n",
      "Trained batch 7038 batch loss 6.59543419 epoch total loss 6.26081753\n",
      "Trained batch 7039 batch loss 6.3416872 epoch total loss 6.2608285\n",
      "Trained batch 7040 batch loss 6.24028 epoch total loss 6.26082611\n",
      "Trained batch 7041 batch loss 6.4311161 epoch total loss 6.26085\n",
      "Trained batch 7042 batch loss 6.24151468 epoch total loss 6.26084709\n",
      "Trained batch 7043 batch loss 6.59251499 epoch total loss 6.26089478\n",
      "Trained batch 7044 batch loss 5.43560934 epoch total loss 6.26077747\n",
      "Trained batch 7045 batch loss 5.2739315 epoch total loss 6.26063776\n",
      "Trained batch 7046 batch loss 5.71141863 epoch total loss 6.26055956\n",
      "Trained batch 7047 batch loss 6.28521156 epoch total loss 6.2605629\n",
      "Trained batch 7048 batch loss 6.03372049 epoch total loss 6.26053095\n",
      "Trained batch 7049 batch loss 6.05331707 epoch total loss 6.26050186\n",
      "Trained batch 7050 batch loss 6.00420189 epoch total loss 6.26046562\n",
      "Trained batch 7051 batch loss 6.42359352 epoch total loss 6.26048851\n",
      "Trained batch 7052 batch loss 6.45244789 epoch total loss 6.26051569\n",
      "Trained batch 7053 batch loss 6.46594143 epoch total loss 6.26054478\n",
      "Trained batch 7054 batch loss 5.03566742 epoch total loss 6.26037073\n",
      "Trained batch 7055 batch loss 4.90830898 epoch total loss 6.26017952\n",
      "Trained batch 7056 batch loss 6.19860268 epoch total loss 6.26017094\n",
      "Trained batch 7057 batch loss 5.36638927 epoch total loss 6.2600441\n",
      "Trained batch 7058 batch loss 5.23310852 epoch total loss 6.25989914\n",
      "Trained batch 7059 batch loss 5.91248131 epoch total loss 6.25985\n",
      "Trained batch 7060 batch loss 5.92336845 epoch total loss 6.25980234\n",
      "Trained batch 7061 batch loss 6.55926895 epoch total loss 6.2598443\n",
      "Trained batch 7062 batch loss 6.31110477 epoch total loss 6.25985193\n",
      "Trained batch 7063 batch loss 6.19028854 epoch total loss 6.2598424\n",
      "Trained batch 7064 batch loss 6.14577913 epoch total loss 6.25982571\n",
      "Trained batch 7065 batch loss 4.99899149 epoch total loss 6.25964737\n",
      "Trained batch 7066 batch loss 5.35200214 epoch total loss 6.2595191\n",
      "Trained batch 7067 batch loss 6.49345 epoch total loss 6.259552\n",
      "Trained batch 7068 batch loss 6.22469044 epoch total loss 6.25954723\n",
      "Trained batch 7069 batch loss 6.39620972 epoch total loss 6.25956631\n",
      "Trained batch 7070 batch loss 4.62587166 epoch total loss 6.25933504\n",
      "Trained batch 7071 batch loss 6.21172523 epoch total loss 6.25932837\n",
      "Trained batch 7072 batch loss 6.23026752 epoch total loss 6.25932407\n",
      "Trained batch 7073 batch loss 6.43840027 epoch total loss 6.25934935\n",
      "Trained batch 7074 batch loss 6.95723391 epoch total loss 6.25944805\n",
      "Trained batch 7075 batch loss 5.78338242 epoch total loss 6.25938129\n",
      "Trained batch 7076 batch loss 6.12989521 epoch total loss 6.2593627\n",
      "Trained batch 7077 batch loss 5.94286966 epoch total loss 6.25931787\n",
      "Trained batch 7078 batch loss 6.09630966 epoch total loss 6.25929499\n",
      "Trained batch 7079 batch loss 6.26690531 epoch total loss 6.25929594\n",
      "Trained batch 7080 batch loss 6.54012871 epoch total loss 6.25933504\n",
      "Trained batch 7081 batch loss 6.17987633 epoch total loss 6.25932407\n",
      "Trained batch 7082 batch loss 6.420012 epoch total loss 6.25934696\n",
      "Trained batch 7083 batch loss 6.08016729 epoch total loss 6.25932217\n",
      "Trained batch 7084 batch loss 6.27477646 epoch total loss 6.25932407\n",
      "Trained batch 7085 batch loss 6.03656673 epoch total loss 6.25929213\n",
      "Trained batch 7086 batch loss 6.01055 epoch total loss 6.25925732\n",
      "Trained batch 7087 batch loss 6.44730091 epoch total loss 6.25928402\n",
      "Trained batch 7088 batch loss 5.38166523 epoch total loss 6.25916052\n",
      "Trained batch 7089 batch loss 6.38028049 epoch total loss 6.25917721\n",
      "Trained batch 7090 batch loss 6.36395121 epoch total loss 6.25919199\n",
      "Trained batch 7091 batch loss 6.07279062 epoch total loss 6.25916576\n",
      "Trained batch 7092 batch loss 5.31452227 epoch total loss 6.2590332\n",
      "Trained batch 7093 batch loss 6.18737125 epoch total loss 6.25902319\n",
      "Trained batch 7094 batch loss 7.29473591 epoch total loss 6.25916862\n",
      "Trained batch 7095 batch loss 7.10784531 epoch total loss 6.25928879\n",
      "Trained batch 7096 batch loss 6.00758553 epoch total loss 6.25925303\n",
      "Trained batch 7097 batch loss 6.05172777 epoch total loss 6.25922394\n",
      "Trained batch 7098 batch loss 5.68706608 epoch total loss 6.25914335\n",
      "Trained batch 7099 batch loss 4.91715097 epoch total loss 6.25895405\n",
      "Trained batch 7100 batch loss 4.92288971 epoch total loss 6.25876617\n",
      "Trained batch 7101 batch loss 5.82397652 epoch total loss 6.25870466\n",
      "Trained batch 7102 batch loss 5.47926235 epoch total loss 6.25859499\n",
      "Trained batch 7103 batch loss 5.86802578 epoch total loss 6.25854\n",
      "Trained batch 7104 batch loss 6.28209782 epoch total loss 6.25854349\n",
      "Trained batch 7105 batch loss 6.05630207 epoch total loss 6.2585144\n",
      "Trained batch 7106 batch loss 6.54741478 epoch total loss 6.25855494\n",
      "Trained batch 7107 batch loss 5.74739313 epoch total loss 6.25848293\n",
      "Trained batch 7108 batch loss 5.86325932 epoch total loss 6.25842762\n",
      "Trained batch 7109 batch loss 6.05893898 epoch total loss 6.25839949\n",
      "Trained batch 7110 batch loss 6.13425922 epoch total loss 6.25838184\n",
      "Trained batch 7111 batch loss 6.50390339 epoch total loss 6.25841618\n",
      "Trained batch 7112 batch loss 6.32146406 epoch total loss 6.25842476\n",
      "Trained batch 7113 batch loss 6.28507376 epoch total loss 6.25842857\n",
      "Trained batch 7114 batch loss 5.97708416 epoch total loss 6.258389\n",
      "Trained batch 7115 batch loss 6.00177097 epoch total loss 6.25835276\n",
      "Trained batch 7116 batch loss 5.87185907 epoch total loss 6.2582984\n",
      "Trained batch 7117 batch loss 6.12179756 epoch total loss 6.25827885\n",
      "Trained batch 7118 batch loss 5.79001236 epoch total loss 6.25821304\n",
      "Trained batch 7119 batch loss 5.93024063 epoch total loss 6.25816679\n",
      "Trained batch 7120 batch loss 6.08322811 epoch total loss 6.25814199\n",
      "Trained batch 7121 batch loss 6.45074654 epoch total loss 6.25816917\n",
      "Trained batch 7122 batch loss 5.97914314 epoch total loss 6.25813\n",
      "Trained batch 7123 batch loss 5.66868544 epoch total loss 6.2580471\n",
      "Trained batch 7124 batch loss 5.82843733 epoch total loss 6.25798702\n",
      "Trained batch 7125 batch loss 6.23778915 epoch total loss 6.25798416\n",
      "Trained batch 7126 batch loss 6.25302792 epoch total loss 6.25798368\n",
      "Trained batch 7127 batch loss 5.03332233 epoch total loss 6.25781202\n",
      "Trained batch 7128 batch loss 4.76379776 epoch total loss 6.25760269\n",
      "Trained batch 7129 batch loss 5.44467783 epoch total loss 6.25748873\n",
      "Trained batch 7130 batch loss 5.65488958 epoch total loss 6.25740433\n",
      "Trained batch 7131 batch loss 6.86435795 epoch total loss 6.2574892\n",
      "Trained batch 7132 batch loss 5.12274647 epoch total loss 6.25733\n",
      "Trained batch 7133 batch loss 6.27383709 epoch total loss 6.25733232\n",
      "Trained batch 7134 batch loss 6.00054789 epoch total loss 6.25729609\n",
      "Trained batch 7135 batch loss 5.5773716 epoch total loss 6.25720119\n",
      "Trained batch 7136 batch loss 6.09047794 epoch total loss 6.25717735\n",
      "Trained batch 7137 batch loss 6.35105705 epoch total loss 6.2571907\n",
      "Trained batch 7138 batch loss 5.42397976 epoch total loss 6.25707436\n",
      "Trained batch 7139 batch loss 6.28541756 epoch total loss 6.25707817\n",
      "Trained batch 7140 batch loss 5.95124722 epoch total loss 6.25703573\n",
      "Trained batch 7141 batch loss 5.69405413 epoch total loss 6.25695705\n",
      "Trained batch 7142 batch loss 6.0007534 epoch total loss 6.25692081\n",
      "Trained batch 7143 batch loss 6.08284473 epoch total loss 6.2568965\n",
      "Trained batch 7144 batch loss 6.25255108 epoch total loss 6.25689602\n",
      "Trained batch 7145 batch loss 5.79610157 epoch total loss 6.25683165\n",
      "Trained batch 7146 batch loss 6.18101788 epoch total loss 6.25682068\n",
      "Trained batch 7147 batch loss 6.31475639 epoch total loss 6.25682926\n",
      "Trained batch 7148 batch loss 6.25133514 epoch total loss 6.25682831\n",
      "Trained batch 7149 batch loss 5.83245039 epoch total loss 6.2567687\n",
      "Trained batch 7150 batch loss 6.03132725 epoch total loss 6.25673723\n",
      "Trained batch 7151 batch loss 5.06673813 epoch total loss 6.25657082\n",
      "Trained batch 7152 batch loss 5.93084049 epoch total loss 6.25652504\n",
      "Trained batch 7153 batch loss 6.46113062 epoch total loss 6.25655365\n",
      "Trained batch 7154 batch loss 6.23935127 epoch total loss 6.25655127\n",
      "Trained batch 7155 batch loss 6.18873215 epoch total loss 6.25654173\n",
      "Trained batch 7156 batch loss 6.48127317 epoch total loss 6.25657272\n",
      "Trained batch 7157 batch loss 6.50133944 epoch total loss 6.25660706\n",
      "Trained batch 7158 batch loss 6.37842655 epoch total loss 6.25662374\n",
      "Trained batch 7159 batch loss 6.50071 epoch total loss 6.25665808\n",
      "Trained batch 7160 batch loss 6.27965641 epoch total loss 6.25666142\n",
      "Trained batch 7161 batch loss 6.02973366 epoch total loss 6.25663\n",
      "Trained batch 7162 batch loss 6.23111534 epoch total loss 6.25662613\n",
      "Trained batch 7163 batch loss 5.44100285 epoch total loss 6.25651264\n",
      "Trained batch 7164 batch loss 6.18970156 epoch total loss 6.25650311\n",
      "Trained batch 7165 batch loss 6.21996975 epoch total loss 6.25649786\n",
      "Trained batch 7166 batch loss 6.09004402 epoch total loss 6.25647497\n",
      "Trained batch 7167 batch loss 6.42965508 epoch total loss 6.25649881\n",
      "Trained batch 7168 batch loss 5.8082 epoch total loss 6.25643635\n",
      "Trained batch 7169 batch loss 5.74495602 epoch total loss 6.2563653\n",
      "Trained batch 7170 batch loss 6.17084694 epoch total loss 6.25635338\n",
      "Trained batch 7171 batch loss 6.05238628 epoch total loss 6.25632477\n",
      "Trained batch 7172 batch loss 6.28866 epoch total loss 6.25632954\n",
      "Trained batch 7173 batch loss 5.74510574 epoch total loss 6.25625849\n",
      "Trained batch 7174 batch loss 5.80212879 epoch total loss 6.25619459\n",
      "Trained batch 7175 batch loss 5.67893648 epoch total loss 6.25611448\n",
      "Trained batch 7176 batch loss 6.11892509 epoch total loss 6.25609493\n",
      "Trained batch 7177 batch loss 5.44583607 epoch total loss 6.25598192\n",
      "Trained batch 7178 batch loss 6.05693817 epoch total loss 6.25595474\n",
      "Trained batch 7179 batch loss 6.24732971 epoch total loss 6.25595331\n",
      "Trained batch 7180 batch loss 6.12943554 epoch total loss 6.25593567\n",
      "Trained batch 7181 batch loss 6.93519688 epoch total loss 6.25603\n",
      "Trained batch 7182 batch loss 6.43318558 epoch total loss 6.2560544\n",
      "Trained batch 7183 batch loss 6.30467701 epoch total loss 6.25606155\n",
      "Trained batch 7184 batch loss 6.15682745 epoch total loss 6.25604773\n",
      "Trained batch 7185 batch loss 6.17977333 epoch total loss 6.25603676\n",
      "Trained batch 7186 batch loss 6.35004711 epoch total loss 6.25605\n",
      "Trained batch 7187 batch loss 5.96110439 epoch total loss 6.2560091\n",
      "Trained batch 7188 batch loss 4.37541866 epoch total loss 6.25574732\n",
      "Trained batch 7189 batch loss 7.07388973 epoch total loss 6.25586128\n",
      "Trained batch 7190 batch loss 6.00777817 epoch total loss 6.25582695\n",
      "Trained batch 7191 batch loss 6.24483299 epoch total loss 6.25582552\n",
      "Trained batch 7192 batch loss 6.45003223 epoch total loss 6.25585222\n",
      "Trained batch 7193 batch loss 6.20343208 epoch total loss 6.25584507\n",
      "Trained batch 7194 batch loss 5.85126972 epoch total loss 6.2557888\n",
      "Trained batch 7195 batch loss 6.12664223 epoch total loss 6.25577068\n",
      "Trained batch 7196 batch loss 5.91719627 epoch total loss 6.25572348\n",
      "Trained batch 7197 batch loss 5.92396545 epoch total loss 6.2556777\n",
      "Trained batch 7198 batch loss 6.51121712 epoch total loss 6.25571346\n",
      "Trained batch 7199 batch loss 6.05171442 epoch total loss 6.25568485\n",
      "Trained batch 7200 batch loss 6.02073622 epoch total loss 6.25565195\n",
      "Trained batch 7201 batch loss 7.60402679 epoch total loss 6.25583935\n",
      "Trained batch 7202 batch loss 6.20820236 epoch total loss 6.25583267\n",
      "Trained batch 7203 batch loss 5.75923634 epoch total loss 6.25576353\n",
      "Trained batch 7204 batch loss 6.16845083 epoch total loss 6.25575161\n",
      "Trained batch 7205 batch loss 6.46906757 epoch total loss 6.25578117\n",
      "Trained batch 7206 batch loss 6.27675438 epoch total loss 6.25578403\n",
      "Trained batch 7207 batch loss 6.22123766 epoch total loss 6.25577927\n",
      "Trained batch 7208 batch loss 6.72642756 epoch total loss 6.25584459\n",
      "Trained batch 7209 batch loss 6.62543297 epoch total loss 6.25589609\n",
      "Trained batch 7210 batch loss 6.52043962 epoch total loss 6.25593233\n",
      "Trained batch 7211 batch loss 5.89579964 epoch total loss 6.25588226\n",
      "Trained batch 7212 batch loss 5.89332 epoch total loss 6.2558322\n",
      "Trained batch 7213 batch loss 6.0531435 epoch total loss 6.25580454\n",
      "Trained batch 7214 batch loss 5.84572077 epoch total loss 6.2557478\n",
      "Trained batch 7215 batch loss 6.30402851 epoch total loss 6.25575447\n",
      "Trained batch 7216 batch loss 6.09129429 epoch total loss 6.25573158\n",
      "Trained batch 7217 batch loss 6.1071682 epoch total loss 6.2557106\n",
      "Trained batch 7218 batch loss 6.28587914 epoch total loss 6.25571489\n",
      "Trained batch 7219 batch loss 5.97167492 epoch total loss 6.25567579\n",
      "Trained batch 7220 batch loss 6.35673475 epoch total loss 6.25568962\n",
      "Trained batch 7221 batch loss 5.99821663 epoch total loss 6.25565386\n",
      "Trained batch 7222 batch loss 6.26201105 epoch total loss 6.25565481\n",
      "Trained batch 7223 batch loss 5.85186195 epoch total loss 6.25559902\n",
      "Trained batch 7224 batch loss 6.96244526 epoch total loss 6.25569677\n",
      "Trained batch 7225 batch loss 5.68971205 epoch total loss 6.25561857\n",
      "Trained batch 7226 batch loss 5.80062723 epoch total loss 6.25555563\n",
      "Trained batch 7227 batch loss 5.83772087 epoch total loss 6.25549746\n",
      "Trained batch 7228 batch loss 5.38172579 epoch total loss 6.25537682\n",
      "Trained batch 7229 batch loss 4.41810417 epoch total loss 6.25512266\n",
      "Trained batch 7230 batch loss 4.96285 epoch total loss 6.25494337\n",
      "Trained batch 7231 batch loss 4.02424669 epoch total loss 6.25463486\n",
      "Trained batch 7232 batch loss 5.0809288 epoch total loss 6.25447273\n",
      "Trained batch 7233 batch loss 4.80333138 epoch total loss 6.25427246\n",
      "Trained batch 7234 batch loss 4.3151226 epoch total loss 6.25400448\n",
      "Trained batch 7235 batch loss 4.95250893 epoch total loss 6.25382471\n",
      "Trained batch 7236 batch loss 5.36561441 epoch total loss 6.25370216\n",
      "Trained batch 7237 batch loss 5.484056 epoch total loss 6.25359583\n",
      "Trained batch 7238 batch loss 5.65715694 epoch total loss 6.25351334\n",
      "Trained batch 7239 batch loss 4.86594868 epoch total loss 6.25332165\n",
      "Trained batch 7240 batch loss 5.19311428 epoch total loss 6.25317526\n",
      "Trained batch 7241 batch loss 4.77985096 epoch total loss 6.25297213\n",
      "Trained batch 7242 batch loss 4.96695566 epoch total loss 6.25279474\n",
      "Trained batch 7243 batch loss 5.51574707 epoch total loss 6.2526927\n",
      "Trained batch 7244 batch loss 4.88504 epoch total loss 6.25250435\n",
      "Trained batch 7245 batch loss 5.8614 epoch total loss 6.25245047\n",
      "Trained batch 7246 batch loss 6.18312 epoch total loss 6.25244093\n",
      "Trained batch 7247 batch loss 6.02945614 epoch total loss 6.25241041\n",
      "Trained batch 7248 batch loss 5.93504477 epoch total loss 6.25236654\n",
      "Trained batch 7249 batch loss 5.27453375 epoch total loss 6.2522316\n",
      "Trained batch 7250 batch loss 4.68515587 epoch total loss 6.25201511\n",
      "Trained batch 7251 batch loss 4.99712276 epoch total loss 6.25184202\n",
      "Trained batch 7252 batch loss 6.83868361 epoch total loss 6.25192308\n",
      "Trained batch 7253 batch loss 5.26209927 epoch total loss 6.25178623\n",
      "Trained batch 7254 batch loss 6.33764219 epoch total loss 6.25179815\n",
      "Trained batch 7255 batch loss 6.14462376 epoch total loss 6.25178337\n",
      "Trained batch 7256 batch loss 7.02186394 epoch total loss 6.25188971\n",
      "Trained batch 7257 batch loss 6.40625286 epoch total loss 6.25191069\n",
      "Trained batch 7258 batch loss 5.41075897 epoch total loss 6.25179482\n",
      "Trained batch 7259 batch loss 6.38728189 epoch total loss 6.25181341\n",
      "Trained batch 7260 batch loss 6.5384264 epoch total loss 6.25185299\n",
      "Trained batch 7261 batch loss 6.64778233 epoch total loss 6.25190783\n",
      "Trained batch 7262 batch loss 6.49585533 epoch total loss 6.2519412\n",
      "Trained batch 7263 batch loss 6.0309782 epoch total loss 6.25191069\n",
      "Trained batch 7264 batch loss 5.53890419 epoch total loss 6.25181293\n",
      "Trained batch 7265 batch loss 5.54640198 epoch total loss 6.25171566\n",
      "Trained batch 7266 batch loss 5.2939167 epoch total loss 6.25158358\n",
      "Trained batch 7267 batch loss 5.90228653 epoch total loss 6.25153589\n",
      "Trained batch 7268 batch loss 6.07822895 epoch total loss 6.25151205\n",
      "Trained batch 7269 batch loss 4.51489925 epoch total loss 6.25127316\n",
      "Trained batch 7270 batch loss 6.27321482 epoch total loss 6.25127602\n",
      "Trained batch 7271 batch loss 6.31237602 epoch total loss 6.2512846\n",
      "Trained batch 7272 batch loss 5.66762352 epoch total loss 6.25120449\n",
      "Trained batch 7273 batch loss 5.20749283 epoch total loss 6.25106096\n",
      "Trained batch 7274 batch loss 6.08740234 epoch total loss 6.25103807\n",
      "Trained batch 7275 batch loss 5.33264065 epoch total loss 6.25091171\n",
      "Trained batch 7276 batch loss 5.76166821 epoch total loss 6.25084448\n",
      "Trained batch 7277 batch loss 6.20317745 epoch total loss 6.2508378\n",
      "Trained batch 7278 batch loss 5.61340332 epoch total loss 6.25075054\n",
      "Trained batch 7279 batch loss 6.01081467 epoch total loss 6.25071764\n",
      "Trained batch 7280 batch loss 4.51400185 epoch total loss 6.25047922\n",
      "Trained batch 7281 batch loss 5.77035809 epoch total loss 6.25041294\n",
      "Trained batch 7282 batch loss 5.86829853 epoch total loss 6.25036049\n",
      "Trained batch 7283 batch loss 6.18539476 epoch total loss 6.25035143\n",
      "Trained batch 7284 batch loss 4.90960073 epoch total loss 6.25016737\n",
      "Trained batch 7285 batch loss 6.21374416 epoch total loss 6.2501626\n",
      "Trained batch 7286 batch loss 5.72976208 epoch total loss 6.25009108\n",
      "Trained batch 7287 batch loss 4.99216747 epoch total loss 6.24991846\n",
      "Trained batch 7288 batch loss 5.78377533 epoch total loss 6.24985456\n",
      "Trained batch 7289 batch loss 6.59149933 epoch total loss 6.24990129\n",
      "Trained batch 7290 batch loss 6.25214386 epoch total loss 6.24990177\n",
      "Trained batch 7291 batch loss 5.61732388 epoch total loss 6.24981499\n",
      "Trained batch 7292 batch loss 6.1667614 epoch total loss 6.24980402\n",
      "Trained batch 7293 batch loss 6.65912914 epoch total loss 6.24986029\n",
      "Trained batch 7294 batch loss 6.18205357 epoch total loss 6.24985123\n",
      "Trained batch 7295 batch loss 5.59242582 epoch total loss 6.2497611\n",
      "Trained batch 7296 batch loss 6.72275257 epoch total loss 6.24982595\n",
      "Trained batch 7297 batch loss 6.00827789 epoch total loss 6.24979305\n",
      "Trained batch 7298 batch loss 6.40923405 epoch total loss 6.24981499\n",
      "Trained batch 7299 batch loss 4.19499969 epoch total loss 6.24953318\n",
      "Trained batch 7300 batch loss 4.21000481 epoch total loss 6.24925423\n",
      "Trained batch 7301 batch loss 3.50667238 epoch total loss 6.24887848\n",
      "Trained batch 7302 batch loss 5.15182734 epoch total loss 6.24872828\n",
      "Trained batch 7303 batch loss 4.89219856 epoch total loss 6.24854231\n",
      "Trained batch 7304 batch loss 5.95768309 epoch total loss 6.24850273\n",
      "Trained batch 7305 batch loss 5.92299557 epoch total loss 6.24845791\n",
      "Trained batch 7306 batch loss 6.15446854 epoch total loss 6.24844503\n",
      "Trained batch 7307 batch loss 5.97020912 epoch total loss 6.24840689\n",
      "Trained batch 7308 batch loss 6.46383381 epoch total loss 6.24843645\n",
      "Trained batch 7309 batch loss 6.36721849 epoch total loss 6.24845266\n",
      "Trained batch 7310 batch loss 6.54847383 epoch total loss 6.24849367\n",
      "Trained batch 7311 batch loss 6.77775288 epoch total loss 6.24856615\n",
      "Trained batch 7312 batch loss 6.28693199 epoch total loss 6.24857092\n",
      "Trained batch 7313 batch loss 6.3733654 epoch total loss 6.24858809\n",
      "Trained batch 7314 batch loss 6.88481522 epoch total loss 6.24867535\n",
      "Trained batch 7315 batch loss 6.00404072 epoch total loss 6.24864197\n",
      "Trained batch 7316 batch loss 6.3015461 epoch total loss 6.24864912\n",
      "Trained batch 7317 batch loss 6.23719883 epoch total loss 6.24864769\n",
      "Trained batch 7318 batch loss 6.11865711 epoch total loss 6.24862957\n",
      "Trained batch 7319 batch loss 5.93959808 epoch total loss 6.24858761\n",
      "Trained batch 7320 batch loss 5.31301785 epoch total loss 6.24846\n",
      "Trained batch 7321 batch loss 4.7983532 epoch total loss 6.24826145\n",
      "Trained batch 7322 batch loss 4.88460064 epoch total loss 6.24807501\n",
      "Trained batch 7323 batch loss 5.71670914 epoch total loss 6.24800253\n",
      "Trained batch 7324 batch loss 4.89878368 epoch total loss 6.24781799\n",
      "Trained batch 7325 batch loss 5.3866539 epoch total loss 6.24770069\n",
      "Trained batch 7326 batch loss 5.43310738 epoch total loss 6.24758959\n",
      "Trained batch 7327 batch loss 6.35734081 epoch total loss 6.24760389\n",
      "Trained batch 7328 batch loss 6.5369525 epoch total loss 6.24764347\n",
      "Trained batch 7329 batch loss 6.91815186 epoch total loss 6.24773502\n",
      "Trained batch 7330 batch loss 6.30030823 epoch total loss 6.24774218\n",
      "Trained batch 7331 batch loss 6.23575974 epoch total loss 6.24774027\n",
      "Trained batch 7332 batch loss 4.43988514 epoch total loss 6.24749374\n",
      "Trained batch 7333 batch loss 6.12915 epoch total loss 6.24747753\n",
      "Trained batch 7334 batch loss 6.00481224 epoch total loss 6.24744463\n",
      "Trained batch 7335 batch loss 6.07656479 epoch total loss 6.24742126\n",
      "Trained batch 7336 batch loss 6.09838772 epoch total loss 6.24740076\n",
      "Trained batch 7337 batch loss 6.91261053 epoch total loss 6.24749184\n",
      "Trained batch 7338 batch loss 5.05562878 epoch total loss 6.24732924\n",
      "Trained batch 7339 batch loss 6.69393826 epoch total loss 6.24739027\n",
      "Trained batch 7340 batch loss 6.14840794 epoch total loss 6.24737692\n",
      "Trained batch 7341 batch loss 6.42734718 epoch total loss 6.24740124\n",
      "Trained batch 7342 batch loss 6.35987473 epoch total loss 6.2474165\n",
      "Trained batch 7343 batch loss 6.48266935 epoch total loss 6.24744844\n",
      "Trained batch 7344 batch loss 5.27139425 epoch total loss 6.24731541\n",
      "Trained batch 7345 batch loss 6.38665867 epoch total loss 6.24733448\n",
      "Trained batch 7346 batch loss 6.26637363 epoch total loss 6.24733686\n",
      "Trained batch 7347 batch loss 5.78312063 epoch total loss 6.24727345\n",
      "Trained batch 7348 batch loss 5.9016819 epoch total loss 6.24722672\n",
      "Trained batch 7349 batch loss 6.34938 epoch total loss 6.24724\n",
      "Trained batch 7350 batch loss 5.51305485 epoch total loss 6.24714041\n",
      "Trained batch 7351 batch loss 6.02513075 epoch total loss 6.24711\n",
      "Trained batch 7352 batch loss 5.5916543 epoch total loss 6.24702024\n",
      "Trained batch 7353 batch loss 6.30952835 epoch total loss 6.24702883\n",
      "Trained batch 7354 batch loss 6.229352 epoch total loss 6.24702644\n",
      "Trained batch 7355 batch loss 6.33476925 epoch total loss 6.24703836\n",
      "Trained batch 7356 batch loss 6.15731335 epoch total loss 6.24702644\n",
      "Trained batch 7357 batch loss 6.46964073 epoch total loss 6.24705648\n",
      "Trained batch 7358 batch loss 6.47046 epoch total loss 6.24708652\n",
      "Trained batch 7359 batch loss 6.09177113 epoch total loss 6.24706507\n",
      "Trained batch 7360 batch loss 5.52889156 epoch total loss 6.24696732\n",
      "Trained batch 7361 batch loss 5.58622837 epoch total loss 6.24687767\n",
      "Trained batch 7362 batch loss 5.72434855 epoch total loss 6.24680614\n",
      "Trained batch 7363 batch loss 6.17617369 epoch total loss 6.24679661\n",
      "Trained batch 7364 batch loss 6.36134148 epoch total loss 6.24681234\n",
      "Trained batch 7365 batch loss 6.90885687 epoch total loss 6.24690247\n",
      "Trained batch 7366 batch loss 5.48355722 epoch total loss 6.24679899\n",
      "Trained batch 7367 batch loss 5.31165838 epoch total loss 6.24667215\n",
      "Trained batch 7368 batch loss 5.56515741 epoch total loss 6.24658\n",
      "Trained batch 7369 batch loss 5.6976862 epoch total loss 6.24650574\n",
      "Trained batch 7370 batch loss 6.07355547 epoch total loss 6.24648237\n",
      "Trained batch 7371 batch loss 6.64512587 epoch total loss 6.24653625\n",
      "Trained batch 7372 batch loss 6.6575079 epoch total loss 6.24659204\n",
      "Trained batch 7373 batch loss 6.44818974 epoch total loss 6.24661922\n",
      "Trained batch 7374 batch loss 7.28730965 epoch total loss 6.24676085\n",
      "Trained batch 7375 batch loss 6.94183064 epoch total loss 6.24685478\n",
      "Trained batch 7376 batch loss 6.63133335 epoch total loss 6.24690723\n",
      "Trained batch 7377 batch loss 6.35052681 epoch total loss 6.24692154\n",
      "Trained batch 7378 batch loss 6.63584423 epoch total loss 6.24697399\n",
      "Trained batch 7379 batch loss 6.68200111 epoch total loss 6.2470336\n",
      "Trained batch 7380 batch loss 6.39844322 epoch total loss 6.2470541\n",
      "Trained batch 7381 batch loss 6.66074467 epoch total loss 6.24711\n",
      "Trained batch 7382 batch loss 5.91253567 epoch total loss 6.24706459\n",
      "Trained batch 7383 batch loss 6.48846722 epoch total loss 6.24709749\n",
      "Trained batch 7384 batch loss 6.59910297 epoch total loss 6.2471447\n",
      "Trained batch 7385 batch loss 6.82591724 epoch total loss 6.2472229\n",
      "Trained batch 7386 batch loss 5.72581 epoch total loss 6.24715233\n",
      "Trained batch 7387 batch loss 7.41772842 epoch total loss 6.24731112\n",
      "Trained batch 7388 batch loss 6.35575104 epoch total loss 6.2473259\n",
      "Trained batch 7389 batch loss 6.99669313 epoch total loss 6.24742699\n",
      "Trained batch 7390 batch loss 7.37938738 epoch total loss 6.24758\n",
      "Trained batch 7391 batch loss 6.33356 epoch total loss 6.2475915\n",
      "Trained batch 7392 batch loss 6.44036293 epoch total loss 6.24761772\n",
      "Trained batch 7393 batch loss 6.28348923 epoch total loss 6.24762297\n",
      "Trained batch 7394 batch loss 6.13715363 epoch total loss 6.24760771\n",
      "Trained batch 7395 batch loss 6.2798171 epoch total loss 6.24761248\n",
      "Trained batch 7396 batch loss 5.76907444 epoch total loss 6.24754763\n",
      "Trained batch 7397 batch loss 5.84228516 epoch total loss 6.24749327\n",
      "Trained batch 7398 batch loss 5.74479532 epoch total loss 6.24742556\n",
      "Trained batch 7399 batch loss 6.00962448 epoch total loss 6.24739313\n",
      "Trained batch 7400 batch loss 6.33329773 epoch total loss 6.24740458\n",
      "Trained batch 7401 batch loss 6.7579875 epoch total loss 6.24747324\n",
      "Trained batch 7402 batch loss 5.79439116 epoch total loss 6.2474122\n",
      "Trained batch 7403 batch loss 4.84629 epoch total loss 6.2472229\n",
      "Trained batch 7404 batch loss 5.40877867 epoch total loss 6.24711\n",
      "Trained batch 7405 batch loss 6.6258378 epoch total loss 6.24716091\n",
      "Trained batch 7406 batch loss 6.69422436 epoch total loss 6.24722147\n",
      "Trained batch 7407 batch loss 6.74652481 epoch total loss 6.2472887\n",
      "Trained batch 7408 batch loss 5.9322834 epoch total loss 6.24724627\n",
      "Trained batch 7409 batch loss 5.98115635 epoch total loss 6.2472105\n",
      "Trained batch 7410 batch loss 5.94262695 epoch total loss 6.24716902\n",
      "Trained batch 7411 batch loss 6.71239424 epoch total loss 6.24723196\n",
      "Trained batch 7412 batch loss 5.44223642 epoch total loss 6.24712324\n",
      "Trained batch 7413 batch loss 5.66322422 epoch total loss 6.24704456\n",
      "Trained batch 7414 batch loss 6.21172857 epoch total loss 6.24703932\n",
      "Trained batch 7415 batch loss 5.91703892 epoch total loss 6.24699497\n",
      "Trained batch 7416 batch loss 5.51589918 epoch total loss 6.24689627\n",
      "Trained batch 7417 batch loss 4.28888893 epoch total loss 6.24663258\n",
      "Trained batch 7418 batch loss 6.05606365 epoch total loss 6.24660683\n",
      "Trained batch 7419 batch loss 6.70178795 epoch total loss 6.24666834\n",
      "Trained batch 7420 batch loss 5.62663412 epoch total loss 6.24658442\n",
      "Trained batch 7421 batch loss 5.95060539 epoch total loss 6.24654436\n",
      "Trained batch 7422 batch loss 7.00316906 epoch total loss 6.2466464\n",
      "Trained batch 7423 batch loss 5.42514849 epoch total loss 6.24653578\n",
      "Trained batch 7424 batch loss 5.65686321 epoch total loss 6.24645615\n",
      "Trained batch 7425 batch loss 6.68791294 epoch total loss 6.24651575\n",
      "Trained batch 7426 batch loss 6.73570824 epoch total loss 6.24658155\n",
      "Trained batch 7427 batch loss 6.22150278 epoch total loss 6.24657822\n",
      "Trained batch 7428 batch loss 5.63155 epoch total loss 6.24649572\n",
      "Trained batch 7429 batch loss 5.88704681 epoch total loss 6.24644709\n",
      "Trained batch 7430 batch loss 5.47627068 epoch total loss 6.24634361\n",
      "Trained batch 7431 batch loss 6.04384136 epoch total loss 6.24631596\n",
      "Trained batch 7432 batch loss 5.6244154 epoch total loss 6.24623251\n",
      "Trained batch 7433 batch loss 6.1776495 epoch total loss 6.24622297\n",
      "Trained batch 7434 batch loss 5.9403758 epoch total loss 6.24618196\n",
      "Trained batch 7435 batch loss 5.25721407 epoch total loss 6.24604893\n",
      "Trained batch 7436 batch loss 5.27811909 epoch total loss 6.24591875\n",
      "Trained batch 7437 batch loss 6.26762533 epoch total loss 6.24592209\n",
      "Trained batch 7438 batch loss 5.69924736 epoch total loss 6.24584866\n",
      "Trained batch 7439 batch loss 6.25517464 epoch total loss 6.24584961\n",
      "Trained batch 7440 batch loss 6.3034935 epoch total loss 6.24585772\n",
      "Trained batch 7441 batch loss 5.76405716 epoch total loss 6.24579287\n",
      "Trained batch 7442 batch loss 5.62169933 epoch total loss 6.24570894\n",
      "Trained batch 7443 batch loss 6.13555145 epoch total loss 6.24569416\n",
      "Trained batch 7444 batch loss 5.67198324 epoch total loss 6.24561739\n",
      "Trained batch 7445 batch loss 4.62239265 epoch total loss 6.245399\n",
      "Trained batch 7446 batch loss 6.21365261 epoch total loss 6.24539518\n",
      "Trained batch 7447 batch loss 5.75099564 epoch total loss 6.24532843\n",
      "Trained batch 7448 batch loss 5.76166344 epoch total loss 6.24526358\n",
      "Trained batch 7449 batch loss 6.10265541 epoch total loss 6.24524403\n",
      "Trained batch 7450 batch loss 6.1615262 epoch total loss 6.24523258\n",
      "Trained batch 7451 batch loss 5.74314737 epoch total loss 6.24516535\n",
      "Trained batch 7452 batch loss 6.31449127 epoch total loss 6.24517488\n",
      "Trained batch 7453 batch loss 5.5599432 epoch total loss 6.24508286\n",
      "Trained batch 7454 batch loss 6.12652159 epoch total loss 6.24506664\n",
      "Trained batch 7455 batch loss 6.26577663 epoch total loss 6.2450695\n",
      "Trained batch 7456 batch loss 5.98232937 epoch total loss 6.24503374\n",
      "Trained batch 7457 batch loss 6.49981403 epoch total loss 6.24506807\n",
      "Trained batch 7458 batch loss 6.47877455 epoch total loss 6.24509954\n",
      "Trained batch 7459 batch loss 6.16122913 epoch total loss 6.2450881\n",
      "Trained batch 7460 batch loss 5.74039602 epoch total loss 6.24502087\n",
      "Trained batch 7461 batch loss 5.76023293 epoch total loss 6.24495602\n",
      "Trained batch 7462 batch loss 5.42709446 epoch total loss 6.24484634\n",
      "Trained batch 7463 batch loss 5.59244442 epoch total loss 6.24475908\n",
      "Trained batch 7464 batch loss 6.14618349 epoch total loss 6.24474573\n",
      "Trained batch 7465 batch loss 6.21365833 epoch total loss 6.24474144\n",
      "Trained batch 7466 batch loss 5.874897 epoch total loss 6.24469185\n",
      "Trained batch 7467 batch loss 6.18879223 epoch total loss 6.24468422\n",
      "Trained batch 7468 batch loss 5.76840973 epoch total loss 6.2446208\n",
      "Trained batch 7469 batch loss 5.9011569 epoch total loss 6.24457502\n",
      "Trained batch 7470 batch loss 6.20551205 epoch total loss 6.24457\n",
      "Trained batch 7471 batch loss 5.83888578 epoch total loss 6.2445159\n",
      "Trained batch 7472 batch loss 5.91114235 epoch total loss 6.24447107\n",
      "Trained batch 7473 batch loss 5.6709013 epoch total loss 6.2443943\n",
      "Trained batch 7474 batch loss 5.97676849 epoch total loss 6.24435854\n",
      "Trained batch 7475 batch loss 4.94880342 epoch total loss 6.24418545\n",
      "Trained batch 7476 batch loss 5.32451 epoch total loss 6.24406242\n",
      "Trained batch 7477 batch loss 6.79437542 epoch total loss 6.24413586\n",
      "Trained batch 7478 batch loss 6.5783577 epoch total loss 6.24418\n",
      "Trained batch 7479 batch loss 6.83490944 epoch total loss 6.24425936\n",
      "Trained batch 7480 batch loss 6.79017353 epoch total loss 6.24433231\n",
      "Trained batch 7481 batch loss 5.88319063 epoch total loss 6.24428415\n",
      "Trained batch 7482 batch loss 6.27890968 epoch total loss 6.24428844\n",
      "Trained batch 7483 batch loss 6.05410576 epoch total loss 6.24426317\n",
      "Trained batch 7484 batch loss 5.45582867 epoch total loss 6.24415779\n",
      "Trained batch 7485 batch loss 5.73891497 epoch total loss 6.24409\n",
      "Trained batch 7486 batch loss 5.72990036 epoch total loss 6.24402142\n",
      "Trained batch 7487 batch loss 5.92487431 epoch total loss 6.24397898\n",
      "Trained batch 7488 batch loss 6.12790203 epoch total loss 6.24396372\n",
      "Trained batch 7489 batch loss 5.97091532 epoch total loss 6.24392748\n",
      "Trained batch 7490 batch loss 6.01682711 epoch total loss 6.24389696\n",
      "Trained batch 7491 batch loss 4.8976078 epoch total loss 6.24371767\n",
      "Trained batch 7492 batch loss 5.89658642 epoch total loss 6.24367142\n",
      "Trained batch 7493 batch loss 6.33433 epoch total loss 6.24368382\n",
      "Trained batch 7494 batch loss 5.25056219 epoch total loss 6.24355125\n",
      "Trained batch 7495 batch loss 5.16608238 epoch total loss 6.24340773\n",
      "Trained batch 7496 batch loss 6.06915283 epoch total loss 6.24338436\n",
      "Trained batch 7497 batch loss 5.75935507 epoch total loss 6.24331951\n",
      "Trained batch 7498 batch loss 5.90916634 epoch total loss 6.24327517\n",
      "Trained batch 7499 batch loss 7.14019203 epoch total loss 6.24339485\n",
      "Trained batch 7500 batch loss 7.11864471 epoch total loss 6.24351168\n",
      "Trained batch 7501 batch loss 5.79531527 epoch total loss 6.24345207\n",
      "Trained batch 7502 batch loss 6.20349407 epoch total loss 6.24344635\n",
      "Trained batch 7503 batch loss 7.35854959 epoch total loss 6.24359512\n",
      "Trained batch 7504 batch loss 6.88298798 epoch total loss 6.24368048\n",
      "Trained batch 7505 batch loss 5.59350157 epoch total loss 6.24359369\n",
      "Trained batch 7506 batch loss 5.6021142 epoch total loss 6.24350834\n",
      "Trained batch 7507 batch loss 5.11487198 epoch total loss 6.24335766\n",
      "Trained batch 7508 batch loss 5.76819706 epoch total loss 6.24329472\n",
      "Trained batch 7509 batch loss 5.71490097 epoch total loss 6.24322414\n",
      "Trained batch 7510 batch loss 5.6238184 epoch total loss 6.24314213\n",
      "Trained batch 7511 batch loss 6.30083799 epoch total loss 6.24314976\n",
      "Trained batch 7512 batch loss 5.91721869 epoch total loss 6.24310637\n",
      "Trained batch 7513 batch loss 5.98227596 epoch total loss 6.24307156\n",
      "Trained batch 7514 batch loss 6.860991 epoch total loss 6.24315357\n",
      "Trained batch 7515 batch loss 6.44217968 epoch total loss 6.24318\n",
      "Trained batch 7516 batch loss 6.88245106 epoch total loss 6.24326468\n",
      "Trained batch 7517 batch loss 6.29719734 epoch total loss 6.24327183\n",
      "Trained batch 7518 batch loss 7.20780373 epoch total loss 6.2434\n",
      "Trained batch 7519 batch loss 6.36597967 epoch total loss 6.24341679\n",
      "Trained batch 7520 batch loss 6.9742012 epoch total loss 6.24351358\n",
      "Trained batch 7521 batch loss 6.31140661 epoch total loss 6.24352264\n",
      "Trained batch 7522 batch loss 5.49522877 epoch total loss 6.24342346\n",
      "Trained batch 7523 batch loss 6.61470222 epoch total loss 6.24347258\n",
      "Trained batch 7524 batch loss 5.96081591 epoch total loss 6.24343491\n",
      "Trained batch 7525 batch loss 6.29152727 epoch total loss 6.24344158\n",
      "Trained batch 7526 batch loss 5.90363407 epoch total loss 6.24339628\n",
      "Trained batch 7527 batch loss 5.94408941 epoch total loss 6.2433567\n",
      "Trained batch 7528 batch loss 6.39687347 epoch total loss 6.24337721\n",
      "Trained batch 7529 batch loss 6.24948645 epoch total loss 6.24337816\n",
      "Trained batch 7530 batch loss 6.08981609 epoch total loss 6.24335766\n",
      "Trained batch 7531 batch loss 5.66161 epoch total loss 6.24328041\n",
      "Trained batch 7532 batch loss 6.64284039 epoch total loss 6.24333382\n",
      "Trained batch 7533 batch loss 6.13236475 epoch total loss 6.24331903\n",
      "Trained batch 7534 batch loss 6.3034544 epoch total loss 6.24332714\n",
      "Trained batch 7535 batch loss 6.52240324 epoch total loss 6.24336433\n",
      "Trained batch 7536 batch loss 6.09206152 epoch total loss 6.24334431\n",
      "Trained batch 7537 batch loss 5.91914082 epoch total loss 6.24330139\n",
      "Trained batch 7538 batch loss 6.16584396 epoch total loss 6.2432909\n",
      "Trained batch 7539 batch loss 5.89755869 epoch total loss 6.24324512\n",
      "Trained batch 7540 batch loss 5.9355793 epoch total loss 6.24320459\n",
      "Trained batch 7541 batch loss 6.23664665 epoch total loss 6.24320364\n",
      "Trained batch 7542 batch loss 5.94683456 epoch total loss 6.24316454\n",
      "Trained batch 7543 batch loss 6.17422438 epoch total loss 6.24315548\n",
      "Trained batch 7544 batch loss 6.39818382 epoch total loss 6.24317598\n",
      "Trained batch 7545 batch loss 5.12012863 epoch total loss 6.24302721\n",
      "Trained batch 7546 batch loss 5.06068611 epoch total loss 6.24287081\n",
      "Trained batch 7547 batch loss 6.52812815 epoch total loss 6.24290848\n",
      "Trained batch 7548 batch loss 8.26038551 epoch total loss 6.24317598\n",
      "Trained batch 7549 batch loss 7.04580975 epoch total loss 6.24328232\n",
      "Trained batch 7550 batch loss 7.17167044 epoch total loss 6.24340534\n",
      "Trained batch 7551 batch loss 6.58215094 epoch total loss 6.24345\n",
      "Trained batch 7552 batch loss 6.27557135 epoch total loss 6.24345493\n",
      "Trained batch 7553 batch loss 6.33227348 epoch total loss 6.24346638\n",
      "Trained batch 7554 batch loss 6.2309823 epoch total loss 6.24346495\n",
      "Trained batch 7555 batch loss 5.98511934 epoch total loss 6.24343061\n",
      "Trained batch 7556 batch loss 6.22638702 epoch total loss 6.24342823\n",
      "Trained batch 7557 batch loss 5.17823792 epoch total loss 6.24328756\n",
      "Trained batch 7558 batch loss 6.15844631 epoch total loss 6.2432766\n",
      "Trained batch 7559 batch loss 6.34230614 epoch total loss 6.24329\n",
      "Trained batch 7560 batch loss 4.79845524 epoch total loss 6.24309826\n",
      "Trained batch 7561 batch loss 5.7473135 epoch total loss 6.24303293\n",
      "Trained batch 7562 batch loss 5.74351501 epoch total loss 6.24296665\n",
      "Trained batch 7563 batch loss 6.529953 epoch total loss 6.2430048\n",
      "Trained batch 7564 batch loss 6.58278799 epoch total loss 6.24304962\n",
      "Trained batch 7565 batch loss 6.57352495 epoch total loss 6.24309301\n",
      "Trained batch 7566 batch loss 6.35627079 epoch total loss 6.24310827\n",
      "Trained batch 7567 batch loss 6.77913475 epoch total loss 6.24317884\n",
      "Trained batch 7568 batch loss 6.59487057 epoch total loss 6.2432251\n",
      "Trained batch 7569 batch loss 6.4478054 epoch total loss 6.24325228\n",
      "Trained batch 7570 batch loss 6.49348307 epoch total loss 6.24328518\n",
      "Trained batch 7571 batch loss 5.57802725 epoch total loss 6.24319744\n",
      "Trained batch 7572 batch loss 5.69671774 epoch total loss 6.24312496\n",
      "Trained batch 7573 batch loss 5.55723953 epoch total loss 6.24303436\n",
      "Trained batch 7574 batch loss 5.59004402 epoch total loss 6.24294806\n",
      "Trained batch 7575 batch loss 5.17243671 epoch total loss 6.24280691\n",
      "Trained batch 7576 batch loss 5.31441116 epoch total loss 6.24268389\n",
      "Trained batch 7577 batch loss 6.89232683 epoch total loss 6.24276972\n",
      "Trained batch 7578 batch loss 5.23716831 epoch total loss 6.24263716\n",
      "Trained batch 7579 batch loss 4.29836941 epoch total loss 6.24238\n",
      "Trained batch 7580 batch loss 5.68633032 epoch total loss 6.24230719\n",
      "Trained batch 7581 batch loss 5.5406127 epoch total loss 6.2422142\n",
      "Trained batch 7582 batch loss 5.5308 epoch total loss 6.24212074\n",
      "Trained batch 7583 batch loss 6.0050621 epoch total loss 6.24208927\n",
      "Trained batch 7584 batch loss 4.21054363 epoch total loss 6.24182129\n",
      "Trained batch 7585 batch loss 4.67507839 epoch total loss 6.24161482\n",
      "Trained batch 7586 batch loss 5.70627594 epoch total loss 6.24154425\n",
      "Trained batch 7587 batch loss 5.02076292 epoch total loss 6.24138308\n",
      "Trained batch 7588 batch loss 5.45515251 epoch total loss 6.24128\n",
      "Trained batch 7589 batch loss 4.89352512 epoch total loss 6.2411027\n",
      "Trained batch 7590 batch loss 5.75242662 epoch total loss 6.24103832\n",
      "Trained batch 7591 batch loss 5.19011402 epoch total loss 6.2409\n",
      "Trained batch 7592 batch loss 4.61150646 epoch total loss 6.24068546\n",
      "Trained batch 7593 batch loss 4.87348843 epoch total loss 6.2405057\n",
      "Trained batch 7594 batch loss 4.79482 epoch total loss 6.24031496\n",
      "Trained batch 7595 batch loss 7.02927113 epoch total loss 6.24041891\n",
      "Trained batch 7596 batch loss 6.04161358 epoch total loss 6.24039268\n",
      "Trained batch 7597 batch loss 7.0567708 epoch total loss 6.24050045\n",
      "Trained batch 7598 batch loss 6.78757811 epoch total loss 6.24057245\n",
      "Trained batch 7599 batch loss 6.9790554 epoch total loss 6.24067\n",
      "Trained batch 7600 batch loss 6.38315201 epoch total loss 6.2406888\n",
      "Trained batch 7601 batch loss 6.17999 epoch total loss 6.24068069\n",
      "Trained batch 7602 batch loss 6.78903294 epoch total loss 6.2407527\n",
      "Trained batch 7603 batch loss 6.14058781 epoch total loss 6.24074\n",
      "Trained batch 7604 batch loss 7.13785267 epoch total loss 6.2408576\n",
      "Trained batch 7605 batch loss 6.10447073 epoch total loss 6.24083948\n",
      "Trained batch 7606 batch loss 5.12345076 epoch total loss 6.24069309\n",
      "Trained batch 7607 batch loss 5.76475 epoch total loss 6.24063063\n",
      "Trained batch 7608 batch loss 4.81888199 epoch total loss 6.24044371\n",
      "Trained batch 7609 batch loss 5.76154661 epoch total loss 6.24038076\n",
      "Trained batch 7610 batch loss 5.36529446 epoch total loss 6.24026632\n",
      "Trained batch 7611 batch loss 5.95926332 epoch total loss 6.24022961\n",
      "Trained batch 7612 batch loss 6.60164881 epoch total loss 6.24027681\n",
      "Trained batch 7613 batch loss 6.71327114 epoch total loss 6.24033928\n",
      "Trained batch 7614 batch loss 6.29405165 epoch total loss 6.24034643\n",
      "Trained batch 7615 batch loss 7.22582817 epoch total loss 6.24047565\n",
      "Trained batch 7616 batch loss 6.9329958 epoch total loss 6.24056673\n",
      "Trained batch 7617 batch loss 6.56377411 epoch total loss 6.24060917\n",
      "Trained batch 7618 batch loss 6.85819674 epoch total loss 6.24069\n",
      "Trained batch 7619 batch loss 6.35579872 epoch total loss 6.24070549\n",
      "Trained batch 7620 batch loss 6.15926933 epoch total loss 6.24069452\n",
      "Trained batch 7621 batch loss 6.10497952 epoch total loss 6.24067688\n",
      "Trained batch 7622 batch loss 5.95986938 epoch total loss 6.24064\n",
      "Trained batch 7623 batch loss 6.40910625 epoch total loss 6.24066257\n",
      "Trained batch 7624 batch loss 6.13625765 epoch total loss 6.24064875\n",
      "Trained batch 7625 batch loss 6.60435 epoch total loss 6.24069691\n",
      "Trained batch 7626 batch loss 6.18406391 epoch total loss 6.24068928\n",
      "Trained batch 7627 batch loss 6.57349205 epoch total loss 6.24073315\n",
      "Trained batch 7628 batch loss 6.05426502 epoch total loss 6.24070835\n",
      "Trained batch 7629 batch loss 5.68222809 epoch total loss 6.2406354\n",
      "Trained batch 7630 batch loss 6.22932768 epoch total loss 6.24063444\n",
      "Trained batch 7631 batch loss 5.98410034 epoch total loss 6.24060059\n",
      "Trained batch 7632 batch loss 6.73674107 epoch total loss 6.24066591\n",
      "Trained batch 7633 batch loss 6.10481644 epoch total loss 6.24064827\n",
      "Trained batch 7634 batch loss 5.75691319 epoch total loss 6.24058485\n",
      "Trained batch 7635 batch loss 4.88982677 epoch total loss 6.24040794\n",
      "Trained batch 7636 batch loss 5.12863398 epoch total loss 6.24026251\n",
      "Trained batch 7637 batch loss 5.87829399 epoch total loss 6.2402153\n",
      "Trained batch 7638 batch loss 4.8029213 epoch total loss 6.24002743\n",
      "Trained batch 7639 batch loss 4.76352119 epoch total loss 6.23983383\n",
      "Trained batch 7640 batch loss 4.93135071 epoch total loss 6.23966217\n",
      "Trained batch 7641 batch loss 4.36353397 epoch total loss 6.2394166\n",
      "Trained batch 7642 batch loss 5.6077137 epoch total loss 6.23933411\n",
      "Trained batch 7643 batch loss 6.69676781 epoch total loss 6.23939371\n",
      "Trained batch 7644 batch loss 6.46468163 epoch total loss 6.23942327\n",
      "Trained batch 7645 batch loss 6.14412594 epoch total loss 6.23941088\n",
      "Trained batch 7646 batch loss 6.34949875 epoch total loss 6.23942518\n",
      "Trained batch 7647 batch loss 6.60593605 epoch total loss 6.23947287\n",
      "Trained batch 7648 batch loss 6.20208549 epoch total loss 6.2394681\n",
      "Trained batch 7649 batch loss 6.59448767 epoch total loss 6.23951435\n",
      "Trained batch 7650 batch loss 6.71910954 epoch total loss 6.23957729\n",
      "Trained batch 7651 batch loss 6.60913563 epoch total loss 6.23962545\n",
      "Trained batch 7652 batch loss 6.4002924 epoch total loss 6.23964643\n",
      "Trained batch 7653 batch loss 6.91887188 epoch total loss 6.23973513\n",
      "Trained batch 7654 batch loss 6.42061234 epoch total loss 6.23975849\n",
      "Trained batch 7655 batch loss 5.1272645 epoch total loss 6.23961353\n",
      "Trained batch 7656 batch loss 4.90211344 epoch total loss 6.23943901\n",
      "Trained batch 7657 batch loss 4.95908451 epoch total loss 6.23927212\n",
      "Trained batch 7658 batch loss 5.31071329 epoch total loss 6.239151\n",
      "Trained batch 7659 batch loss 5.72045135 epoch total loss 6.23908281\n",
      "Trained batch 7660 batch loss 5.09517097 epoch total loss 6.23893356\n",
      "Trained batch 7661 batch loss 4.73685932 epoch total loss 6.23873758\n",
      "Trained batch 7662 batch loss 4.76639795 epoch total loss 6.23854542\n",
      "Trained batch 7663 batch loss 5.3549633 epoch total loss 6.23843\n",
      "Trained batch 7664 batch loss 6.64313698 epoch total loss 6.23848295\n",
      "Trained batch 7665 batch loss 6.76410961 epoch total loss 6.23855209\n",
      "Trained batch 7666 batch loss 6.46178055 epoch total loss 6.2385807\n",
      "Trained batch 7667 batch loss 5.27844238 epoch total loss 6.2384553\n",
      "Trained batch 7668 batch loss 6.36257505 epoch total loss 6.23847198\n",
      "Trained batch 7669 batch loss 6.20240355 epoch total loss 6.23846722\n",
      "Trained batch 7670 batch loss 5.32912683 epoch total loss 6.23834848\n",
      "Trained batch 7671 batch loss 6.12893486 epoch total loss 6.23833418\n",
      "Trained batch 7672 batch loss 6.86761093 epoch total loss 6.23841619\n",
      "Trained batch 7673 batch loss 6.40544033 epoch total loss 6.23843813\n",
      "Trained batch 7674 batch loss 6.33981895 epoch total loss 6.23845148\n",
      "Trained batch 7675 batch loss 7.05452394 epoch total loss 6.23855782\n",
      "Trained batch 7676 batch loss 7.01308441 epoch total loss 6.23865843\n",
      "Trained batch 7677 batch loss 6.28278542 epoch total loss 6.23866367\n",
      "Trained batch 7678 batch loss 7.13555431 epoch total loss 6.23878098\n",
      "Trained batch 7679 batch loss 6.24931908 epoch total loss 6.23878241\n",
      "Trained batch 7680 batch loss 6.37582207 epoch total loss 6.2388\n",
      "Trained batch 7681 batch loss 6.14098 epoch total loss 6.23878717\n",
      "Trained batch 7682 batch loss 5.81884146 epoch total loss 6.23873281\n",
      "Trained batch 7683 batch loss 5.6938386 epoch total loss 6.23866224\n",
      "Trained batch 7684 batch loss 6.15151024 epoch total loss 6.2386508\n",
      "Trained batch 7685 batch loss 5.71412659 epoch total loss 6.23858261\n",
      "Trained batch 7686 batch loss 6.17716217 epoch total loss 6.2385745\n",
      "Trained batch 7687 batch loss 5.97439861 epoch total loss 6.2385397\n",
      "Trained batch 7688 batch loss 6.51248455 epoch total loss 6.23857546\n",
      "Trained batch 7689 batch loss 6.23726749 epoch total loss 6.23857546\n",
      "Trained batch 7690 batch loss 5.7689805 epoch total loss 6.23851442\n",
      "Trained batch 7691 batch loss 4.9877677 epoch total loss 6.23835182\n",
      "Trained batch 7692 batch loss 5.78263855 epoch total loss 6.23829222\n",
      "Trained batch 7693 batch loss 5.72208929 epoch total loss 6.23822546\n",
      "Trained batch 7694 batch loss 6.13777 epoch total loss 6.23821211\n",
      "Trained batch 7695 batch loss 5.89753151 epoch total loss 6.23816824\n",
      "Trained batch 7696 batch loss 5.71393108 epoch total loss 6.2381\n",
      "Trained batch 7697 batch loss 5.60608768 epoch total loss 6.23801804\n",
      "Trained batch 7698 batch loss 5.79928303 epoch total loss 6.23796129\n",
      "Trained batch 7699 batch loss 6.11652756 epoch total loss 6.23794556\n",
      "Trained batch 7700 batch loss 5.75004864 epoch total loss 6.23788214\n",
      "Trained batch 7701 batch loss 6.16454315 epoch total loss 6.2378726\n",
      "Trained batch 7702 batch loss 6.0506115 epoch total loss 6.23784828\n",
      "Trained batch 7703 batch loss 6.07877731 epoch total loss 6.2378273\n",
      "Trained batch 7704 batch loss 6.44439793 epoch total loss 6.23785448\n",
      "Trained batch 7705 batch loss 5.58628035 epoch total loss 6.2377696\n",
      "Trained batch 7706 batch loss 5.96803093 epoch total loss 6.23773479\n",
      "Trained batch 7707 batch loss 6.1511426 epoch total loss 6.23772383\n",
      "Trained batch 7708 batch loss 6.14691353 epoch total loss 6.23771191\n",
      "Trained batch 7709 batch loss 6.41311932 epoch total loss 6.23773479\n",
      "Trained batch 7710 batch loss 6.61316681 epoch total loss 6.23778391\n",
      "Trained batch 7711 batch loss 5.88719511 epoch total loss 6.23773813\n",
      "Trained batch 7712 batch loss 6.09050083 epoch total loss 6.23771906\n",
      "Trained batch 7713 batch loss 6.40570116 epoch total loss 6.23774099\n",
      "Trained batch 7714 batch loss 6.18235159 epoch total loss 6.23773384\n",
      "Trained batch 7715 batch loss 5.81832314 epoch total loss 6.237679\n",
      "Trained batch 7716 batch loss 6.10911369 epoch total loss 6.23766279\n",
      "Trained batch 7717 batch loss 5.89913082 epoch total loss 6.23761845\n",
      "Trained batch 7718 batch loss 6.07240534 epoch total loss 6.23759747\n",
      "Trained batch 7719 batch loss 5.98148918 epoch total loss 6.23756409\n",
      "Trained batch 7720 batch loss 6.23780155 epoch total loss 6.23756409\n",
      "Trained batch 7721 batch loss 7.77673626 epoch total loss 6.23776388\n",
      "Trained batch 7722 batch loss 7.18753862 epoch total loss 6.23788691\n",
      "Trained batch 7723 batch loss 7.27028513 epoch total loss 6.23802042\n",
      "Trained batch 7724 batch loss 7.40176773 epoch total loss 6.2381711\n",
      "Trained batch 7725 batch loss 6.65421963 epoch total loss 6.23822451\n",
      "Trained batch 7726 batch loss 6.76736736 epoch total loss 6.23829269\n",
      "Trained batch 7727 batch loss 6.15757895 epoch total loss 6.2382822\n",
      "Trained batch 7728 batch loss 6.13594151 epoch total loss 6.23826933\n",
      "Trained batch 7729 batch loss 5.85904741 epoch total loss 6.23822\n",
      "Trained batch 7730 batch loss 5.68592119 epoch total loss 6.23814869\n",
      "Trained batch 7731 batch loss 5.57500172 epoch total loss 6.23806286\n",
      "Trained batch 7732 batch loss 7.35084534 epoch total loss 6.23820686\n",
      "Trained batch 7733 batch loss 6.76015282 epoch total loss 6.23827457\n",
      "Trained batch 7734 batch loss 5.90788126 epoch total loss 6.23823166\n",
      "Trained batch 7735 batch loss 6.36176491 epoch total loss 6.23824787\n",
      "Trained batch 7736 batch loss 6.19431925 epoch total loss 6.23824215\n",
      "Trained batch 7737 batch loss 6.30577421 epoch total loss 6.23825073\n",
      "Trained batch 7738 batch loss 5.3745842 epoch total loss 6.23813963\n",
      "Trained batch 7739 batch loss 5.85650396 epoch total loss 6.23809\n",
      "Trained batch 7740 batch loss 6.20131302 epoch total loss 6.23808527\n",
      "Trained batch 7741 batch loss 6.43920708 epoch total loss 6.23811102\n",
      "Trained batch 7742 batch loss 6.21203327 epoch total loss 6.23810768\n",
      "Trained batch 7743 batch loss 6.15608692 epoch total loss 6.23809719\n",
      "Trained batch 7744 batch loss 5.9597435 epoch total loss 6.23806143\n",
      "Trained batch 7745 batch loss 6.49506664 epoch total loss 6.23809481\n",
      "Trained batch 7746 batch loss 6.30481195 epoch total loss 6.23810339\n",
      "Trained batch 7747 batch loss 5.75833797 epoch total loss 6.2380414\n",
      "Trained batch 7748 batch loss 5.83499098 epoch total loss 6.23798943\n",
      "Trained batch 7749 batch loss 6.27725315 epoch total loss 6.23799419\n",
      "Trained batch 7750 batch loss 6.35351467 epoch total loss 6.23800898\n",
      "Trained batch 7751 batch loss 6.27212858 epoch total loss 6.23801374\n",
      "Trained batch 7752 batch loss 5.5644455 epoch total loss 6.23792648\n",
      "Trained batch 7753 batch loss 6.10147953 epoch total loss 6.23790884\n",
      "Trained batch 7754 batch loss 6.55113745 epoch total loss 6.23794937\n",
      "Trained batch 7755 batch loss 6.4097662 epoch total loss 6.23797131\n",
      "Trained batch 7756 batch loss 6.45798779 epoch total loss 6.238\n",
      "Trained batch 7757 batch loss 6.33261681 epoch total loss 6.23801184\n",
      "Trained batch 7758 batch loss 6.10590363 epoch total loss 6.23799467\n",
      "Trained batch 7759 batch loss 6.02872276 epoch total loss 6.23796749\n",
      "Trained batch 7760 batch loss 6.06909227 epoch total loss 6.23794603\n",
      "Trained batch 7761 batch loss 4.52023506 epoch total loss 6.23772478\n",
      "Trained batch 7762 batch loss 4.50912094 epoch total loss 6.23750162\n",
      "Trained batch 7763 batch loss 5.65395355 epoch total loss 6.23742628\n",
      "Trained batch 7764 batch loss 5.62425232 epoch total loss 6.2373476\n",
      "Trained batch 7765 batch loss 6.66710854 epoch total loss 6.23740292\n",
      "Trained batch 7766 batch loss 5.09333181 epoch total loss 6.23725557\n",
      "Trained batch 7767 batch loss 6.67650318 epoch total loss 6.23731232\n",
      "Trained batch 7768 batch loss 7.08811474 epoch total loss 6.23742199\n",
      "Trained batch 7769 batch loss 6.30297375 epoch total loss 6.23743057\n",
      "Trained batch 7770 batch loss 6.00684452 epoch total loss 6.23740101\n",
      "Trained batch 7771 batch loss 6.12630749 epoch total loss 6.2373867\n",
      "Trained batch 7772 batch loss 6.79040098 epoch total loss 6.23745728\n",
      "Trained batch 7773 batch loss 5.15242481 epoch total loss 6.23731804\n",
      "Trained batch 7774 batch loss 6.20436859 epoch total loss 6.23731327\n",
      "Trained batch 7775 batch loss 7.28659 epoch total loss 6.23744822\n",
      "Trained batch 7776 batch loss 5.80289745 epoch total loss 6.23739243\n",
      "Trained batch 7777 batch loss 6.79221535 epoch total loss 6.23746395\n",
      "Trained batch 7778 batch loss 6.96777439 epoch total loss 6.23755789\n",
      "Trained batch 7779 batch loss 5.74945164 epoch total loss 6.23749542\n",
      "Trained batch 7780 batch loss 6.95924759 epoch total loss 6.23758841\n",
      "Trained batch 7781 batch loss 6.53003311 epoch total loss 6.23762608\n",
      "Trained batch 7782 batch loss 6.47013664 epoch total loss 6.23765564\n",
      "Trained batch 7783 batch loss 7.07321739 epoch total loss 6.2377634\n",
      "Trained batch 7784 batch loss 6.56759262 epoch total loss 6.23780537\n",
      "Trained batch 7785 batch loss 7.07844591 epoch total loss 6.23791361\n",
      "Trained batch 7786 batch loss 6.63492441 epoch total loss 6.23796463\n",
      "Trained batch 7787 batch loss 6.4361248 epoch total loss 6.23799038\n",
      "Trained batch 7788 batch loss 6.67166376 epoch total loss 6.23804617\n",
      "Trained batch 7789 batch loss 6.7092638 epoch total loss 6.23810673\n",
      "Trained batch 7790 batch loss 6.29011917 epoch total loss 6.2381134\n",
      "Trained batch 7791 batch loss 6.38450813 epoch total loss 6.238132\n",
      "Trained batch 7792 batch loss 6.1467104 epoch total loss 6.23812056\n",
      "Trained batch 7793 batch loss 6.46157837 epoch total loss 6.23814869\n",
      "Trained batch 7794 batch loss 6.34365129 epoch total loss 6.23816252\n",
      "Trained batch 7795 batch loss 6.06332111 epoch total loss 6.23814\n",
      "Trained batch 7796 batch loss 6.59549 epoch total loss 6.23818541\n",
      "Trained batch 7797 batch loss 5.54104185 epoch total loss 6.23809624\n",
      "Trained batch 7798 batch loss 5.9749136 epoch total loss 6.23806286\n",
      "Trained batch 7799 batch loss 4.87210655 epoch total loss 6.23788738\n",
      "Trained batch 7800 batch loss 4.44785881 epoch total loss 6.23765802\n",
      "Trained batch 7801 batch loss 5.13185692 epoch total loss 6.2375164\n",
      "Trained batch 7802 batch loss 5.94366503 epoch total loss 6.23747921\n",
      "Trained batch 7803 batch loss 5.65073204 epoch total loss 6.23740435\n",
      "Trained batch 7804 batch loss 4.78406858 epoch total loss 6.2372179\n",
      "Trained batch 7805 batch loss 5.33215427 epoch total loss 6.23710203\n",
      "Trained batch 7806 batch loss 6.70914268 epoch total loss 6.23716259\n",
      "Trained batch 7807 batch loss 6.1815896 epoch total loss 6.23715544\n",
      "Trained batch 7808 batch loss 5.78876734 epoch total loss 6.23709822\n",
      "Trained batch 7809 batch loss 7.24134922 epoch total loss 6.23722696\n",
      "Trained batch 7810 batch loss 4.06130457 epoch total loss 6.23694849\n",
      "Trained batch 7811 batch loss 5.72741413 epoch total loss 6.23688316\n",
      "Trained batch 7812 batch loss 5.90310669 epoch total loss 6.23684025\n",
      "Trained batch 7813 batch loss 6.05647755 epoch total loss 6.23681688\n",
      "Trained batch 7814 batch loss 6.05126953 epoch total loss 6.23679304\n",
      "Trained batch 7815 batch loss 5.25131559 epoch total loss 6.23666668\n",
      "Trained batch 7816 batch loss 5.06992912 epoch total loss 6.23651743\n",
      "Trained batch 7817 batch loss 5.41040468 epoch total loss 6.23641205\n",
      "Trained batch 7818 batch loss 5.5547905 epoch total loss 6.23632479\n",
      "Trained batch 7819 batch loss 5.8684864 epoch total loss 6.23627758\n",
      "Trained batch 7820 batch loss 5.74062729 epoch total loss 6.23621416\n",
      "Trained batch 7821 batch loss 5.35544777 epoch total loss 6.23610163\n",
      "Trained batch 7822 batch loss 6.635396 epoch total loss 6.23615265\n",
      "Trained batch 7823 batch loss 6.31329393 epoch total loss 6.23616266\n",
      "Trained batch 7824 batch loss 6.40259266 epoch total loss 6.23618364\n",
      "Trained batch 7825 batch loss 7.48032951 epoch total loss 6.23634291\n",
      "Trained batch 7826 batch loss 7.73823452 epoch total loss 6.2365346\n",
      "Trained batch 7827 batch loss 6.53807068 epoch total loss 6.23657322\n",
      "Trained batch 7828 batch loss 6.94963074 epoch total loss 6.2366643\n",
      "Trained batch 7829 batch loss 5.96904182 epoch total loss 6.23663044\n",
      "Trained batch 7830 batch loss 6.18553972 epoch total loss 6.23662329\n",
      "Trained batch 7831 batch loss 6.47480965 epoch total loss 6.23665428\n",
      "Trained batch 7832 batch loss 6.04657841 epoch total loss 6.23663\n",
      "Trained batch 7833 batch loss 6.42605591 epoch total loss 6.2366538\n",
      "Trained batch 7834 batch loss 5.2771945 epoch total loss 6.23653173\n",
      "Trained batch 7835 batch loss 4.26981974 epoch total loss 6.23628044\n",
      "Trained batch 7836 batch loss 4.56756496 epoch total loss 6.2360673\n",
      "Trained batch 7837 batch loss 4.36906528 epoch total loss 6.23582888\n",
      "Trained batch 7838 batch loss 4.22481298 epoch total loss 6.23557281\n",
      "Trained batch 7839 batch loss 5.22191143 epoch total loss 6.23544359\n",
      "Trained batch 7840 batch loss 5.3487606 epoch total loss 6.23533\n",
      "Trained batch 7841 batch loss 6.46843433 epoch total loss 6.23535967\n",
      "Trained batch 7842 batch loss 6.00953674 epoch total loss 6.23533106\n",
      "Trained batch 7843 batch loss 6.27135086 epoch total loss 6.23533535\n",
      "Trained batch 7844 batch loss 6.05219507 epoch total loss 6.23531151\n",
      "Trained batch 7845 batch loss 6.97160625 epoch total loss 6.23540592\n",
      "Trained batch 7846 batch loss 6.53856516 epoch total loss 6.23544455\n",
      "Trained batch 7847 batch loss 7.28861618 epoch total loss 6.23557854\n",
      "Trained batch 7848 batch loss 6.11057234 epoch total loss 6.2355628\n",
      "Trained batch 7849 batch loss 6.4154129 epoch total loss 6.23558521\n",
      "Trained batch 7850 batch loss 6.74796772 epoch total loss 6.23565054\n",
      "Trained batch 7851 batch loss 6.02039814 epoch total loss 6.23562288\n",
      "Trained batch 7852 batch loss 5.68187571 epoch total loss 6.23555231\n",
      "Trained batch 7853 batch loss 5.55679131 epoch total loss 6.23546648\n",
      "Trained batch 7854 batch loss 5.58902025 epoch total loss 6.23538399\n",
      "Trained batch 7855 batch loss 5.58423471 epoch total loss 6.23530149\n",
      "Trained batch 7856 batch loss 5.91385841 epoch total loss 6.23526049\n",
      "Trained batch 7857 batch loss 6.26262951 epoch total loss 6.23526382\n",
      "Trained batch 7858 batch loss 6.3783164 epoch total loss 6.23528242\n",
      "Trained batch 7859 batch loss 6.23752213 epoch total loss 6.23528242\n",
      "Trained batch 7860 batch loss 6.01449156 epoch total loss 6.23525476\n",
      "Trained batch 7861 batch loss 6.20097065 epoch total loss 6.23525\n",
      "Trained batch 7862 batch loss 6.37684774 epoch total loss 6.23526764\n",
      "Trained batch 7863 batch loss 5.95401907 epoch total loss 6.23523188\n",
      "Trained batch 7864 batch loss 7.00628567 epoch total loss 6.23533\n",
      "Trained batch 7865 batch loss 6.85531235 epoch total loss 6.23540926\n",
      "Trained batch 7866 batch loss 6.7600832 epoch total loss 6.23547602\n",
      "Trained batch 7867 batch loss 6.61824036 epoch total loss 6.23552465\n",
      "Trained batch 7868 batch loss 6.67296362 epoch total loss 6.23558\n",
      "Trained batch 7869 batch loss 6.10952091 epoch total loss 6.23556376\n",
      "Trained batch 7870 batch loss 6.61836052 epoch total loss 6.23561239\n",
      "Trained batch 7871 batch loss 6.44458103 epoch total loss 6.2356391\n",
      "Trained batch 7872 batch loss 6.98819447 epoch total loss 6.23573446\n",
      "Trained batch 7873 batch loss 6.62860966 epoch total loss 6.23578453\n",
      "Trained batch 7874 batch loss 6.6025 epoch total loss 6.23583126\n",
      "Trained batch 7875 batch loss 6.64435911 epoch total loss 6.23588276\n",
      "Trained batch 7876 batch loss 5.82711315 epoch total loss 6.23583126\n",
      "Trained batch 7877 batch loss 6.62090969 epoch total loss 6.23588\n",
      "Trained batch 7878 batch loss 6.42085123 epoch total loss 6.23590374\n",
      "Trained batch 7879 batch loss 6.21108103 epoch total loss 6.2359004\n",
      "Trained batch 7880 batch loss 6.44971704 epoch total loss 6.23592758\n",
      "Trained batch 7881 batch loss 6.69959545 epoch total loss 6.23598623\n",
      "Trained batch 7882 batch loss 5.18873596 epoch total loss 6.2358532\n",
      "Trained batch 7883 batch loss 5.34091091 epoch total loss 6.23573971\n",
      "Trained batch 7884 batch loss 5.3575 epoch total loss 6.2356286\n",
      "Trained batch 7885 batch loss 5.86761141 epoch total loss 6.23558187\n",
      "Trained batch 7886 batch loss 5.61351109 epoch total loss 6.23550272\n",
      "Trained batch 7887 batch loss 5.08834743 epoch total loss 6.23535776\n",
      "Trained batch 7888 batch loss 4.70198631 epoch total loss 6.23516321\n",
      "Trained batch 7889 batch loss 5.43835878 epoch total loss 6.23506212\n",
      "Trained batch 7890 batch loss 5.4594326 epoch total loss 6.23496437\n",
      "Trained batch 7891 batch loss 5.83305264 epoch total loss 6.23491287\n",
      "Trained batch 7892 batch loss 5.98687 epoch total loss 6.23488188\n",
      "Trained batch 7893 batch loss 6.56506538 epoch total loss 6.23492384\n",
      "Trained batch 7894 batch loss 6.138978 epoch total loss 6.23491192\n",
      "Trained batch 7895 batch loss 5.94192314 epoch total loss 6.23487473\n",
      "Trained batch 7896 batch loss 5.84396648 epoch total loss 6.23482513\n",
      "Trained batch 7897 batch loss 6.10018921 epoch total loss 6.23480844\n",
      "Trained batch 7898 batch loss 6.59758 epoch total loss 6.23485422\n",
      "Trained batch 7899 batch loss 6.51002693 epoch total loss 6.23488951\n",
      "Trained batch 7900 batch loss 5.87537241 epoch total loss 6.23484373\n",
      "Trained batch 7901 batch loss 5.61942625 epoch total loss 6.23476601\n",
      "Trained batch 7902 batch loss 5.4895 epoch total loss 6.23467159\n",
      "Trained batch 7903 batch loss 5.08705425 epoch total loss 6.23452616\n",
      "Trained batch 7904 batch loss 4.92256737 epoch total loss 6.23436\n",
      "Trained batch 7905 batch loss 5.22043848 epoch total loss 6.23423147\n",
      "Trained batch 7906 batch loss 6.2703867 epoch total loss 6.23423624\n",
      "Trained batch 7907 batch loss 5.22314835 epoch total loss 6.23410845\n",
      "Trained batch 7908 batch loss 6.57961082 epoch total loss 6.23415184\n",
      "Trained batch 7909 batch loss 6.36608601 epoch total loss 6.23416853\n",
      "Trained batch 7910 batch loss 6.41313934 epoch total loss 6.23419142\n",
      "Trained batch 7911 batch loss 6.30846071 epoch total loss 6.23420048\n",
      "Trained batch 7912 batch loss 5.67374516 epoch total loss 6.23412943\n",
      "Trained batch 7913 batch loss 5.89115906 epoch total loss 6.23408604\n",
      "Trained batch 7914 batch loss 6.24677181 epoch total loss 6.23408794\n",
      "Trained batch 7915 batch loss 6.18791914 epoch total loss 6.23408175\n",
      "Trained batch 7916 batch loss 6.41031837 epoch total loss 6.23410416\n",
      "Trained batch 7917 batch loss 6.08421421 epoch total loss 6.23408556\n",
      "Trained batch 7918 batch loss 5.81566477 epoch total loss 6.23403263\n",
      "Trained batch 7919 batch loss 6.02636719 epoch total loss 6.2340064\n",
      "Trained batch 7920 batch loss 5.95462608 epoch total loss 6.23397112\n",
      "Trained batch 7921 batch loss 6.19823265 epoch total loss 6.23396683\n",
      "Trained batch 7922 batch loss 6.19748831 epoch total loss 6.23396206\n",
      "Trained batch 7923 batch loss 6.45882034 epoch total loss 6.23399067\n",
      "Trained batch 7924 batch loss 6.24129629 epoch total loss 6.23399162\n",
      "Trained batch 7925 batch loss 6.50914955 epoch total loss 6.23402596\n",
      "Trained batch 7926 batch loss 6.30153942 epoch total loss 6.23403454\n",
      "Trained batch 7927 batch loss 6.57320404 epoch total loss 6.23407745\n",
      "Trained batch 7928 batch loss 5.12536764 epoch total loss 6.23393726\n",
      "Trained batch 7929 batch loss 6.31662607 epoch total loss 6.23394775\n",
      "Trained batch 7930 batch loss 6.72487545 epoch total loss 6.23401\n",
      "Trained batch 7931 batch loss 6.83243132 epoch total loss 6.23408556\n",
      "Trained batch 7932 batch loss 5.89861679 epoch total loss 6.23404312\n",
      "Trained batch 7933 batch loss 6.22787762 epoch total loss 6.23404217\n",
      "Trained batch 7934 batch loss 6.66072798 epoch total loss 6.23409605\n",
      "Trained batch 7935 batch loss 6.74925184 epoch total loss 6.2341609\n",
      "Trained batch 7936 batch loss 5.40639591 epoch total loss 6.23405647\n",
      "Trained batch 7937 batch loss 6.65740967 epoch total loss 6.23411\n",
      "Trained batch 7938 batch loss 6.41187906 epoch total loss 6.23413181\n",
      "Trained batch 7939 batch loss 6.90258408 epoch total loss 6.23421621\n",
      "Trained batch 7940 batch loss 6.14400816 epoch total loss 6.23420477\n",
      "Trained batch 7941 batch loss 6.63579512 epoch total loss 6.23425531\n",
      "Trained batch 7942 batch loss 6.79369783 epoch total loss 6.23432589\n",
      "Trained batch 7943 batch loss 6.98693466 epoch total loss 6.23442078\n",
      "Trained batch 7944 batch loss 6.29399776 epoch total loss 6.23442793\n",
      "Trained batch 7945 batch loss 6.23248577 epoch total loss 6.23442793\n",
      "Trained batch 7946 batch loss 6.7306509 epoch total loss 6.23449039\n",
      "Trained batch 7947 batch loss 6.58023453 epoch total loss 6.23453426\n",
      "Trained batch 7948 batch loss 6.28531456 epoch total loss 6.23454046\n",
      "Trained batch 7949 batch loss 6.5047617 epoch total loss 6.23457432\n",
      "Trained batch 7950 batch loss 6.54968643 epoch total loss 6.23461437\n",
      "Trained batch 7951 batch loss 6.31020832 epoch total loss 6.23462343\n",
      "Trained batch 7952 batch loss 7.09138489 epoch total loss 6.2347312\n",
      "Trained batch 7953 batch loss 6.06520176 epoch total loss 6.23470974\n",
      "Trained batch 7954 batch loss 6.60423 epoch total loss 6.23475647\n",
      "Trained batch 7955 batch loss 6.45599842 epoch total loss 6.2347846\n",
      "Trained batch 7956 batch loss 6.52401447 epoch total loss 6.23482084\n",
      "Trained batch 7957 batch loss 7.04378033 epoch total loss 6.23492241\n",
      "Trained batch 7958 batch loss 6.43856239 epoch total loss 6.23494768\n",
      "Trained batch 7959 batch loss 6.40580463 epoch total loss 6.23496914\n",
      "Trained batch 7960 batch loss 6.40525723 epoch total loss 6.23499107\n",
      "Trained batch 7961 batch loss 6.53738928 epoch total loss 6.23502922\n",
      "Trained batch 7962 batch loss 6.28452158 epoch total loss 6.23503542\n",
      "Trained batch 7963 batch loss 7.16410112 epoch total loss 6.23515224\n",
      "Trained batch 7964 batch loss 6.56123161 epoch total loss 6.23519325\n",
      "Trained batch 7965 batch loss 6.40414333 epoch total loss 6.23521423\n",
      "Trained batch 7966 batch loss 6.68892574 epoch total loss 6.23527098\n",
      "Trained batch 7967 batch loss 6.10903072 epoch total loss 6.23525524\n",
      "Trained batch 7968 batch loss 5.30757475 epoch total loss 6.23513889\n",
      "Trained batch 7969 batch loss 6.47114849 epoch total loss 6.23516846\n",
      "Trained batch 7970 batch loss 6.14230204 epoch total loss 6.23515654\n",
      "Trained batch 7971 batch loss 6.3817215 epoch total loss 6.23517513\n",
      "Trained batch 7972 batch loss 6.48674107 epoch total loss 6.23520708\n",
      "Trained batch 7973 batch loss 6.42900753 epoch total loss 6.2352314\n",
      "Trained batch 7974 batch loss 5.68537712 epoch total loss 6.23516226\n",
      "Trained batch 7975 batch loss 6.59038258 epoch total loss 6.2352066\n",
      "Trained batch 7976 batch loss 6.43262577 epoch total loss 6.2352314\n",
      "Trained batch 7977 batch loss 6.59077168 epoch total loss 6.23527622\n",
      "Trained batch 7978 batch loss 6.23157883 epoch total loss 6.23527527\n",
      "Trained batch 7979 batch loss 6.40758419 epoch total loss 6.23529673\n",
      "Trained batch 7980 batch loss 6.20524359 epoch total loss 6.23529339\n",
      "Trained batch 7981 batch loss 6.36672 epoch total loss 6.2353096\n",
      "Trained batch 7982 batch loss 5.93993187 epoch total loss 6.23527288\n",
      "Trained batch 7983 batch loss 5.55254316 epoch total loss 6.23518705\n",
      "Trained batch 7984 batch loss 5.98004436 epoch total loss 6.23515558\n",
      "Trained batch 7985 batch loss 5.75011826 epoch total loss 6.23509455\n",
      "Trained batch 7986 batch loss 6.45631266 epoch total loss 6.2351222\n",
      "Trained batch 7987 batch loss 6.07856083 epoch total loss 6.23510265\n",
      "Trained batch 7988 batch loss 6.53402185 epoch total loss 6.23514032\n",
      "Trained batch 7989 batch loss 6.42320347 epoch total loss 6.23516369\n",
      "Trained batch 7990 batch loss 6.25961256 epoch total loss 6.23516655\n",
      "Trained batch 7991 batch loss 6.54274893 epoch total loss 6.23520517\n",
      "Trained batch 7992 batch loss 6.22225761 epoch total loss 6.23520327\n",
      "Trained batch 7993 batch loss 5.46475029 epoch total loss 6.23510695\n",
      "Trained batch 7994 batch loss 5.75834084 epoch total loss 6.23504734\n",
      "Trained batch 7995 batch loss 5.95209217 epoch total loss 6.23501205\n",
      "Trained batch 7996 batch loss 6.21178246 epoch total loss 6.23500919\n",
      "Trained batch 7997 batch loss 6.47236252 epoch total loss 6.23503876\n",
      "Trained batch 7998 batch loss 6.62801361 epoch total loss 6.23508787\n",
      "Trained batch 7999 batch loss 6.60837126 epoch total loss 6.23513508\n",
      "Trained batch 8000 batch loss 6.17245674 epoch total loss 6.23512697\n",
      "Trained batch 8001 batch loss 6.16873884 epoch total loss 6.23511839\n",
      "Trained batch 8002 batch loss 6.29215527 epoch total loss 6.23512602\n",
      "Trained batch 8003 batch loss 6.77112293 epoch total loss 6.23519278\n",
      "Trained batch 8004 batch loss 6.24514389 epoch total loss 6.23519373\n",
      "Trained batch 8005 batch loss 7.29138327 epoch total loss 6.23532629\n",
      "Trained batch 8006 batch loss 6.50603437 epoch total loss 6.23536\n",
      "Trained batch 8007 batch loss 6.59214 epoch total loss 6.23540497\n",
      "Trained batch 8008 batch loss 6.65213299 epoch total loss 6.23545694\n",
      "Trained batch 8009 batch loss 6.2484684 epoch total loss 6.23545885\n",
      "Trained batch 8010 batch loss 4.77166367 epoch total loss 6.23527622\n",
      "Trained batch 8011 batch loss 5.75338745 epoch total loss 6.23521614\n",
      "Trained batch 8012 batch loss 6.32966375 epoch total loss 6.23522758\n",
      "Trained batch 8013 batch loss 5.40420771 epoch total loss 6.23512363\n",
      "Trained batch 8014 batch loss 6.12666607 epoch total loss 6.23511\n",
      "Trained batch 8015 batch loss 6.76223087 epoch total loss 6.23517561\n",
      "Trained batch 8016 batch loss 6.70134735 epoch total loss 6.23523426\n",
      "Trained batch 8017 batch loss 6.39959478 epoch total loss 6.23525429\n",
      "Trained batch 8018 batch loss 5.52313423 epoch total loss 6.2351656\n",
      "Trained batch 8019 batch loss 5.62182713 epoch total loss 6.2350893\n",
      "Trained batch 8020 batch loss 5.03138638 epoch total loss 6.2349391\n",
      "Trained batch 8021 batch loss 6.40615273 epoch total loss 6.23496056\n",
      "Trained batch 8022 batch loss 5.88971567 epoch total loss 6.23491764\n",
      "Trained batch 8023 batch loss 6.42567444 epoch total loss 6.23494101\n",
      "Trained batch 8024 batch loss 6.63568 epoch total loss 6.23499155\n",
      "Trained batch 8025 batch loss 5.93506384 epoch total loss 6.23495388\n",
      "Trained batch 8026 batch loss 6.30228138 epoch total loss 6.23496199\n",
      "Trained batch 8027 batch loss 6.58092785 epoch total loss 6.23500538\n",
      "Trained batch 8028 batch loss 6.19190645 epoch total loss 6.23499966\n",
      "Trained batch 8029 batch loss 6.1767025 epoch total loss 6.2349925\n",
      "Trained batch 8030 batch loss 6.24404526 epoch total loss 6.23499346\n",
      "Trained batch 8031 batch loss 6.40826607 epoch total loss 6.23501492\n",
      "Trained batch 8032 batch loss 6.49119949 epoch total loss 6.23504734\n",
      "Trained batch 8033 batch loss 5.51611376 epoch total loss 6.2349577\n",
      "Trained batch 8034 batch loss 6.20238972 epoch total loss 6.2349534\n",
      "Trained batch 8035 batch loss 5.77099323 epoch total loss 6.23489571\n",
      "Trained batch 8036 batch loss 4.31255245 epoch total loss 6.23465633\n",
      "Trained batch 8037 batch loss 5.14700794 epoch total loss 6.23452139\n",
      "Trained batch 8038 batch loss 4.60601473 epoch total loss 6.23431873\n",
      "Trained batch 8039 batch loss 5.14011288 epoch total loss 6.23418236\n",
      "Trained batch 8040 batch loss 4.90016413 epoch total loss 6.23401642\n",
      "Trained batch 8041 batch loss 5.28847456 epoch total loss 6.23389912\n",
      "Trained batch 8042 batch loss 6.04657316 epoch total loss 6.23387575\n",
      "Trained batch 8043 batch loss 5.960186 epoch total loss 6.2338419\n",
      "Trained batch 8044 batch loss 6.25063562 epoch total loss 6.2338438\n",
      "Trained batch 8045 batch loss 4.59701347 epoch total loss 6.23364\n",
      "Trained batch 8046 batch loss 6.91083145 epoch total loss 6.23372459\n",
      "Trained batch 8047 batch loss 6.44155312 epoch total loss 6.23375034\n",
      "Trained batch 8048 batch loss 5.98527336 epoch total loss 6.23371935\n",
      "Trained batch 8049 batch loss 6.86874819 epoch total loss 6.23379803\n",
      "Trained batch 8050 batch loss 6.27085781 epoch total loss 6.23380232\n",
      "Trained batch 8051 batch loss 6.55728245 epoch total loss 6.23384285\n",
      "Trained batch 8052 batch loss 5.69788 epoch total loss 6.23377657\n",
      "Trained batch 8053 batch loss 6.02688217 epoch total loss 6.23375082\n",
      "Trained batch 8054 batch loss 6.38668346 epoch total loss 6.23377\n",
      "Trained batch 8055 batch loss 6.24296188 epoch total loss 6.23377085\n",
      "Trained batch 8056 batch loss 5.99776363 epoch total loss 6.23374128\n",
      "Trained batch 8057 batch loss 6.29293346 epoch total loss 6.23374844\n",
      "Trained batch 8058 batch loss 6.72280645 epoch total loss 6.23380947\n",
      "Trained batch 8059 batch loss 5.79721069 epoch total loss 6.23375511\n",
      "Trained batch 8060 batch loss 5.14556 epoch total loss 6.23361969\n",
      "Trained batch 8061 batch loss 6.03007 epoch total loss 6.23359489\n",
      "Trained batch 8062 batch loss 6.36859274 epoch total loss 6.23361158\n",
      "Trained batch 8063 batch loss 6.38407326 epoch total loss 6.2336297\n",
      "Trained batch 8064 batch loss 6.00410032 epoch total loss 6.23360157\n",
      "Trained batch 8065 batch loss 6.57700157 epoch total loss 6.23364401\n",
      "Trained batch 8066 batch loss 6.38545179 epoch total loss 6.23366308\n",
      "Trained batch 8067 batch loss 5.9438014 epoch total loss 6.23362732\n",
      "Trained batch 8068 batch loss 6.1889081 epoch total loss 6.2336216\n",
      "Trained batch 8069 batch loss 6.29583359 epoch total loss 6.2336297\n",
      "Trained batch 8070 batch loss 5.69236183 epoch total loss 6.23356247\n",
      "Trained batch 8071 batch loss 5.91321659 epoch total loss 6.23352289\n",
      "Trained batch 8072 batch loss 5.69638681 epoch total loss 6.23345613\n",
      "Trained batch 8073 batch loss 5.86866426 epoch total loss 6.23341084\n",
      "Trained batch 8074 batch loss 6.01464272 epoch total loss 6.23338366\n",
      "Trained batch 8075 batch loss 5.5147438 epoch total loss 6.23329496\n",
      "Trained batch 8076 batch loss 5.70103073 epoch total loss 6.23322868\n",
      "Trained batch 8077 batch loss 5.85294485 epoch total loss 6.23318148\n",
      "Trained batch 8078 batch loss 5.50432205 epoch total loss 6.23309135\n",
      "Trained batch 8079 batch loss 5.68960142 epoch total loss 6.23302412\n",
      "Trained batch 8080 batch loss 5.87134933 epoch total loss 6.2329793\n",
      "Trained batch 8081 batch loss 5.55815029 epoch total loss 6.23289585\n",
      "Trained batch 8082 batch loss 6.07734966 epoch total loss 6.23287678\n",
      "Trained batch 8083 batch loss 5.69634914 epoch total loss 6.23281\n",
      "Trained batch 8084 batch loss 5.73169613 epoch total loss 6.23274803\n",
      "Trained batch 8085 batch loss 6.25476 epoch total loss 6.23275089\n",
      "Trained batch 8086 batch loss 6.2093339 epoch total loss 6.23274803\n",
      "Trained batch 8087 batch loss 5.78007698 epoch total loss 6.23269224\n",
      "Trained batch 8088 batch loss 7.1305685 epoch total loss 6.23280287\n",
      "Trained batch 8089 batch loss 6.52660084 epoch total loss 6.23283958\n",
      "Trained batch 8090 batch loss 5.79009438 epoch total loss 6.23278427\n",
      "Trained batch 8091 batch loss 4.70524454 epoch total loss 6.23259592\n",
      "Trained batch 8092 batch loss 5.32966566 epoch total loss 6.23248434\n",
      "Trained batch 8093 batch loss 5.58986187 epoch total loss 6.23240471\n",
      "Trained batch 8094 batch loss 5.91207886 epoch total loss 6.23236513\n",
      "Trained batch 8095 batch loss 6.4345603 epoch total loss 6.23239\n",
      "Trained batch 8096 batch loss 6.13445 epoch total loss 6.23237753\n",
      "Trained batch 8097 batch loss 6.82518864 epoch total loss 6.23245049\n",
      "Trained batch 8098 batch loss 6.17348909 epoch total loss 6.23244333\n",
      "Trained batch 8099 batch loss 6.22023201 epoch total loss 6.23244143\n",
      "Trained batch 8100 batch loss 6.96006203 epoch total loss 6.23253155\n",
      "Trained batch 8101 batch loss 4.85956478 epoch total loss 6.23236179\n",
      "Trained batch 8102 batch loss 6.15037918 epoch total loss 6.2323513\n",
      "Trained batch 8103 batch loss 5.57005787 epoch total loss 6.23227\n",
      "Trained batch 8104 batch loss 5.70199299 epoch total loss 6.23220444\n",
      "Trained batch 8105 batch loss 4.8881197 epoch total loss 6.2320385\n",
      "Trained batch 8106 batch loss 4.83432674 epoch total loss 6.23186636\n",
      "Trained batch 8107 batch loss 5.48531818 epoch total loss 6.23177385\n",
      "Trained batch 8108 batch loss 6.36759377 epoch total loss 6.23179054\n",
      "Trained batch 8109 batch loss 6.06427383 epoch total loss 6.23177\n",
      "Trained batch 8110 batch loss 5.45391941 epoch total loss 6.23167372\n",
      "Trained batch 8111 batch loss 5.88260794 epoch total loss 6.2316308\n",
      "Trained batch 8112 batch loss 5.47318 epoch total loss 6.23153734\n",
      "Trained batch 8113 batch loss 5.78640461 epoch total loss 6.23148203\n",
      "Trained batch 8114 batch loss 6.61352062 epoch total loss 6.23152924\n",
      "Trained batch 8115 batch loss 6.01025343 epoch total loss 6.23150206\n",
      "Trained batch 8116 batch loss 5.96216059 epoch total loss 6.23146868\n",
      "Trained batch 8117 batch loss 5.4873209 epoch total loss 6.23137712\n",
      "Trained batch 8118 batch loss 6.11873436 epoch total loss 6.2313633\n",
      "Trained batch 8119 batch loss 5.8864336 epoch total loss 6.23132086\n",
      "Trained batch 8120 batch loss 5.0884738 epoch total loss 6.23118\n",
      "Trained batch 8121 batch loss 6.3051734 epoch total loss 6.23118925\n",
      "Trained batch 8122 batch loss 6.19566345 epoch total loss 6.23118496\n",
      "Trained batch 8123 batch loss 6.09085274 epoch total loss 6.23116732\n",
      "Trained batch 8124 batch loss 6.30090141 epoch total loss 6.2311759\n",
      "Trained batch 8125 batch loss 6.55696106 epoch total loss 6.23121643\n",
      "Trained batch 8126 batch loss 6.22217464 epoch total loss 6.23121548\n",
      "Trained batch 8127 batch loss 6.02934551 epoch total loss 6.23119068\n",
      "Trained batch 8128 batch loss 6.04182386 epoch total loss 6.23116732\n",
      "Trained batch 8129 batch loss 6.13688803 epoch total loss 6.23115587\n",
      "Trained batch 8130 batch loss 6.4621954 epoch total loss 6.23118401\n",
      "Trained batch 8131 batch loss 5.42677879 epoch total loss 6.2310853\n",
      "Trained batch 8132 batch loss 5.90487099 epoch total loss 6.23104525\n",
      "Trained batch 8133 batch loss 5.93384075 epoch total loss 6.23100853\n",
      "Trained batch 8134 batch loss 6.66562796 epoch total loss 6.23106194\n",
      "Trained batch 8135 batch loss 6.34343529 epoch total loss 6.23107576\n",
      "Trained batch 8136 batch loss 7.5678463 epoch total loss 6.23124\n",
      "Trained batch 8137 batch loss 7.76339912 epoch total loss 6.23142815\n",
      "Trained batch 8138 batch loss 7.82853937 epoch total loss 6.23162413\n",
      "Trained batch 8139 batch loss 6.36881161 epoch total loss 6.23164082\n",
      "Trained batch 8140 batch loss 7.87374973 epoch total loss 6.23184252\n",
      "Trained batch 8141 batch loss 6.31505 epoch total loss 6.23185301\n",
      "Trained batch 8142 batch loss 6.85913849 epoch total loss 6.23193026\n",
      "Trained batch 8143 batch loss 7.12853909 epoch total loss 6.23204041\n",
      "Trained batch 8144 batch loss 5.87183094 epoch total loss 6.23199606\n",
      "Trained batch 8145 batch loss 6.83087444 epoch total loss 6.23206949\n",
      "Trained batch 8146 batch loss 6.87711859 epoch total loss 6.23214912\n",
      "Trained batch 8147 batch loss 6.36351395 epoch total loss 6.23216534\n",
      "Trained batch 8148 batch loss 5.87618256 epoch total loss 6.23212147\n",
      "Trained batch 8149 batch loss 6.83835077 epoch total loss 6.23219585\n",
      "Trained batch 8150 batch loss 6.39669609 epoch total loss 6.23221636\n",
      "Trained batch 8151 batch loss 7.17096615 epoch total loss 6.23233175\n",
      "Trained batch 8152 batch loss 6.57066059 epoch total loss 6.23237276\n",
      "Trained batch 8153 batch loss 6.8624692 epoch total loss 6.23245049\n",
      "Trained batch 8154 batch loss 6.89161634 epoch total loss 6.23253107\n",
      "Trained batch 8155 batch loss 6.80737162 epoch total loss 6.23260164\n",
      "Trained batch 8156 batch loss 7.24701214 epoch total loss 6.2327261\n",
      "Trained batch 8157 batch loss 6.15424347 epoch total loss 6.23271608\n",
      "Trained batch 8158 batch loss 6.29957294 epoch total loss 6.23272467\n",
      "Trained batch 8159 batch loss 5.71020317 epoch total loss 6.23266077\n",
      "Trained batch 8160 batch loss 6.26441956 epoch total loss 6.23266459\n",
      "Trained batch 8161 batch loss 5.75309277 epoch total loss 6.23260593\n",
      "Trained batch 8162 batch loss 5.90473413 epoch total loss 6.23256588\n",
      "Trained batch 8163 batch loss 6.51262188 epoch total loss 6.2326\n",
      "Trained batch 8164 batch loss 5.92292547 epoch total loss 6.23256207\n",
      "Trained batch 8165 batch loss 6.68833637 epoch total loss 6.23261786\n",
      "Trained batch 8166 batch loss 6.90261364 epoch total loss 6.2327\n",
      "Trained batch 8167 batch loss 6.40082836 epoch total loss 6.23272038\n",
      "Trained batch 8168 batch loss 6.85137272 epoch total loss 6.23279619\n",
      "Trained batch 8169 batch loss 6.02223253 epoch total loss 6.23277092\n",
      "Trained batch 8170 batch loss 6.81326914 epoch total loss 6.23284149\n",
      "Trained batch 8171 batch loss 4.97126102 epoch total loss 6.23268747\n",
      "Trained batch 8172 batch loss 5.41757154 epoch total loss 6.23258781\n",
      "Trained batch 8173 batch loss 4.72554207 epoch total loss 6.23240328\n",
      "Trained batch 8174 batch loss 5.72642708 epoch total loss 6.23234177\n",
      "Trained batch 8175 batch loss 6.48322487 epoch total loss 6.23237228\n",
      "Trained batch 8176 batch loss 6.07190323 epoch total loss 6.23235273\n",
      "Trained batch 8177 batch loss 6.50735283 epoch total loss 6.23238611\n",
      "Trained batch 8178 batch loss 5.81658554 epoch total loss 6.23233557\n",
      "Trained batch 8179 batch loss 6.31857967 epoch total loss 6.23234606\n",
      "Trained batch 8180 batch loss 6.17032051 epoch total loss 6.23233891\n",
      "Trained batch 8181 batch loss 7.05650425 epoch total loss 6.23243952\n",
      "Trained batch 8182 batch loss 7.51220322 epoch total loss 6.23259544\n",
      "Trained batch 8183 batch loss 6.7081666 epoch total loss 6.23265362\n",
      "Trained batch 8184 batch loss 5.59858465 epoch total loss 6.23257589\n",
      "Trained batch 8185 batch loss 5.5415554 epoch total loss 6.23249197\n",
      "Trained batch 8186 batch loss 5.86819363 epoch total loss 6.23244715\n",
      "Trained batch 8187 batch loss 5.94023085 epoch total loss 6.23241138\n",
      "Trained batch 8188 batch loss 5.59311199 epoch total loss 6.23233366\n",
      "Trained batch 8189 batch loss 6.41996479 epoch total loss 6.23235655\n",
      "Trained batch 8190 batch loss 5.95018053 epoch total loss 6.23232222\n",
      "Trained batch 8191 batch loss 5.40176725 epoch total loss 6.23222065\n",
      "Trained batch 8192 batch loss 6.25947666 epoch total loss 6.23222399\n",
      "Trained batch 8193 batch loss 6.79317665 epoch total loss 6.23229265\n",
      "Trained batch 8194 batch loss 6.6728487 epoch total loss 6.23234606\n",
      "Trained batch 8195 batch loss 6.70316792 epoch total loss 6.23240376\n",
      "Trained batch 8196 batch loss 5.41000843 epoch total loss 6.23230314\n",
      "Trained batch 8197 batch loss 6.07160568 epoch total loss 6.23228359\n",
      "Trained batch 8198 batch loss 6.34579563 epoch total loss 6.23229742\n",
      "Trained batch 8199 batch loss 6.28814173 epoch total loss 6.23230457\n",
      "Trained batch 8200 batch loss 6.48326 epoch total loss 6.23233509\n",
      "Trained batch 8201 batch loss 6.02013969 epoch total loss 6.23230934\n",
      "Trained batch 8202 batch loss 5.9032588 epoch total loss 6.23226881\n",
      "Trained batch 8203 batch loss 5.96403217 epoch total loss 6.23223639\n",
      "Trained batch 8204 batch loss 4.99401855 epoch total loss 6.23208523\n",
      "Trained batch 8205 batch loss 5.03972387 epoch total loss 6.23194\n",
      "Trained batch 8206 batch loss 5.9475913 epoch total loss 6.23190546\n",
      "Trained batch 8207 batch loss 5.48157883 epoch total loss 6.23181391\n",
      "Trained batch 8208 batch loss 6.34685135 epoch total loss 6.23182774\n",
      "Trained batch 8209 batch loss 6.06306 epoch total loss 6.23180723\n",
      "Trained batch 8210 batch loss 4.93478823 epoch total loss 6.2316494\n",
      "Trained batch 8211 batch loss 5.58077812 epoch total loss 6.23157024\n",
      "Trained batch 8212 batch loss 6.10430145 epoch total loss 6.23155451\n",
      "Trained batch 8213 batch loss 6.30160046 epoch total loss 6.23156309\n",
      "Trained batch 8214 batch loss 6.37712526 epoch total loss 6.23158121\n",
      "Trained batch 8215 batch loss 6.41434669 epoch total loss 6.23160315\n",
      "Trained batch 8216 batch loss 6.31591129 epoch total loss 6.23161364\n",
      "Trained batch 8217 batch loss 6.46725464 epoch total loss 6.23164225\n",
      "Trained batch 8218 batch loss 5.98101711 epoch total loss 6.23161173\n",
      "Trained batch 8219 batch loss 5.77270222 epoch total loss 6.23155594\n",
      "Trained batch 8220 batch loss 6.0378747 epoch total loss 6.23153257\n",
      "Trained batch 8221 batch loss 5.87292957 epoch total loss 6.2314887\n",
      "Trained batch 8222 batch loss 6.33534145 epoch total loss 6.23150158\n",
      "Trained batch 8223 batch loss 5.53796339 epoch total loss 6.23141718\n",
      "Trained batch 8224 batch loss 6.28505707 epoch total loss 6.23142385\n",
      "Trained batch 8225 batch loss 5.61249256 epoch total loss 6.23134899\n",
      "Trained batch 8226 batch loss 5.60067844 epoch total loss 6.23127222\n",
      "Trained batch 8227 batch loss 5.93045139 epoch total loss 6.2312355\n",
      "Trained batch 8228 batch loss 4.8016057 epoch total loss 6.23106194\n",
      "Trained batch 8229 batch loss 6.4226923 epoch total loss 6.23108482\n",
      "Trained batch 8230 batch loss 5.99604607 epoch total loss 6.23105621\n",
      "Trained batch 8231 batch loss 5.85900784 epoch total loss 6.23101139\n",
      "Trained batch 8232 batch loss 4.73800707 epoch total loss 6.23082972\n",
      "Trained batch 8233 batch loss 5.87752485 epoch total loss 6.23078728\n",
      "Trained batch 8234 batch loss 6.0269022 epoch total loss 6.23076248\n",
      "Trained batch 8235 batch loss 5.98464537 epoch total loss 6.23073244\n",
      "Trained batch 8236 batch loss 6.67403793 epoch total loss 6.23078632\n",
      "Trained batch 8237 batch loss 5.44462776 epoch total loss 6.23069096\n",
      "Trained batch 8238 batch loss 6.37712574 epoch total loss 6.23070908\n",
      "Trained batch 8239 batch loss 5.51807213 epoch total loss 6.23062277\n",
      "Trained batch 8240 batch loss 6.58677673 epoch total loss 6.23066616\n",
      "Trained batch 8241 batch loss 5.59227 epoch total loss 6.23058844\n",
      "Trained batch 8242 batch loss 5.63105 epoch total loss 6.23051596\n",
      "Trained batch 8243 batch loss 6.445261 epoch total loss 6.23054218\n",
      "Trained batch 8244 batch loss 5.74980259 epoch total loss 6.23048401\n",
      "Trained batch 8245 batch loss 6.46147156 epoch total loss 6.23051167\n",
      "Trained batch 8246 batch loss 6.22880888 epoch total loss 6.23051167\n",
      "Trained batch 8247 batch loss 5.79688072 epoch total loss 6.23045921\n",
      "Trained batch 8248 batch loss 6.97079897 epoch total loss 6.23054934\n",
      "Trained batch 8249 batch loss 6.93311882 epoch total loss 6.23063469\n",
      "Trained batch 8250 batch loss 6.41714 epoch total loss 6.2306571\n",
      "Trained batch 8251 batch loss 5.78765106 epoch total loss 6.23060369\n",
      "Trained batch 8252 batch loss 7.22538662 epoch total loss 6.23072433\n",
      "Trained batch 8253 batch loss 4.53953648 epoch total loss 6.23051929\n",
      "Trained batch 8254 batch loss 5.69111538 epoch total loss 6.23045397\n",
      "Trained batch 8255 batch loss 5.51697397 epoch total loss 6.23036766\n",
      "Trained batch 8256 batch loss 6.34723186 epoch total loss 6.23038149\n",
      "Trained batch 8257 batch loss 5.75488901 epoch total loss 6.23032379\n",
      "Trained batch 8258 batch loss 6.07433224 epoch total loss 6.23030519\n",
      "Trained batch 8259 batch loss 5.92588615 epoch total loss 6.230268\n",
      "Trained batch 8260 batch loss 6.6779232 epoch total loss 6.23032284\n",
      "Trained batch 8261 batch loss 6.30135059 epoch total loss 6.23033094\n",
      "Trained batch 8262 batch loss 6.8666625 epoch total loss 6.23040819\n",
      "Trained batch 8263 batch loss 6.12636757 epoch total loss 6.23039532\n",
      "Trained batch 8264 batch loss 6.56715727 epoch total loss 6.23043633\n",
      "Trained batch 8265 batch loss 7.20375347 epoch total loss 6.23055363\n",
      "Trained batch 8266 batch loss 4.9414978 epoch total loss 6.2303977\n",
      "Trained batch 8267 batch loss 6.29635525 epoch total loss 6.23040581\n",
      "Trained batch 8268 batch loss 5.71309566 epoch total loss 6.23034334\n",
      "Trained batch 8269 batch loss 6.97058153 epoch total loss 6.23043299\n",
      "Trained batch 8270 batch loss 6.18298912 epoch total loss 6.23042727\n",
      "Trained batch 8271 batch loss 6.32632065 epoch total loss 6.23043919\n",
      "Trained batch 8272 batch loss 6.42984104 epoch total loss 6.23046303\n",
      "Trained batch 8273 batch loss 6.69193316 epoch total loss 6.23051882\n",
      "Trained batch 8274 batch loss 6.06810284 epoch total loss 6.23049879\n",
      "Trained batch 8275 batch loss 6.63855028 epoch total loss 6.2305479\n",
      "Trained batch 8276 batch loss 6.35715771 epoch total loss 6.23056316\n",
      "Trained batch 8277 batch loss 6.08190632 epoch total loss 6.23054504\n",
      "Trained batch 8278 batch loss 6.53526306 epoch total loss 6.23058224\n",
      "Trained batch 8279 batch loss 6.38210297 epoch total loss 6.23060036\n",
      "Trained batch 8280 batch loss 6.66182566 epoch total loss 6.23065233\n",
      "Trained batch 8281 batch loss 6.29154301 epoch total loss 6.23066\n",
      "Trained batch 8282 batch loss 5.74094486 epoch total loss 6.23060083\n",
      "Trained batch 8283 batch loss 6.35845947 epoch total loss 6.23061657\n",
      "Trained batch 8284 batch loss 6.30846882 epoch total loss 6.23062563\n",
      "Trained batch 8285 batch loss 6.64724922 epoch total loss 6.23067617\n",
      "Trained batch 8286 batch loss 6.36961031 epoch total loss 6.23069334\n",
      "Trained batch 8287 batch loss 5.9854393 epoch total loss 6.2306633\n",
      "Trained batch 8288 batch loss 5.7482214 epoch total loss 6.2306056\n",
      "Trained batch 8289 batch loss 6.27673817 epoch total loss 6.23061085\n",
      "Trained batch 8290 batch loss 6.31649113 epoch total loss 6.23062134\n",
      "Trained batch 8291 batch loss 6.26003742 epoch total loss 6.23062515\n",
      "Trained batch 8292 batch loss 6.50828075 epoch total loss 6.23065853\n",
      "Trained batch 8293 batch loss 6.30222797 epoch total loss 6.23066711\n",
      "Trained batch 8294 batch loss 6.50319672 epoch total loss 6.2307\n",
      "Trained batch 8295 batch loss 6.14964581 epoch total loss 6.23069\n",
      "Trained batch 8296 batch loss 7.16597033 epoch total loss 6.23080254\n",
      "Trained batch 8297 batch loss 6.2003336 epoch total loss 6.23079872\n",
      "Trained batch 8298 batch loss 6.20501518 epoch total loss 6.23079538\n",
      "Trained batch 8299 batch loss 7.15992785 epoch total loss 6.23090744\n",
      "Trained batch 8300 batch loss 7.34404182 epoch total loss 6.23104143\n",
      "Trained batch 8301 batch loss 5.68044662 epoch total loss 6.23097515\n",
      "Trained batch 8302 batch loss 6.57002687 epoch total loss 6.23101616\n",
      "Trained batch 8303 batch loss 6.3156004 epoch total loss 6.23102617\n",
      "Trained batch 8304 batch loss 6.6941824 epoch total loss 6.23108196\n",
      "Trained batch 8305 batch loss 5.37196589 epoch total loss 6.23097849\n",
      "Trained batch 8306 batch loss 6.21366882 epoch total loss 6.23097658\n",
      "Trained batch 8307 batch loss 5.87770414 epoch total loss 6.23093414\n",
      "Trained batch 8308 batch loss 7.49946308 epoch total loss 6.23108721\n",
      "Trained batch 8309 batch loss 6.22438526 epoch total loss 6.23108625\n",
      "Trained batch 8310 batch loss 6.63111877 epoch total loss 6.23113441\n",
      "Trained batch 8311 batch loss 6.47129631 epoch total loss 6.2311635\n",
      "Trained batch 8312 batch loss 6.96909332 epoch total loss 6.23125219\n",
      "Trained batch 8313 batch loss 6.39130354 epoch total loss 6.23127127\n",
      "Trained batch 8314 batch loss 6.02482128 epoch total loss 6.23124647\n",
      "Trained batch 8315 batch loss 6.77063 epoch total loss 6.23131084\n",
      "Trained batch 8316 batch loss 6.06081963 epoch total loss 6.23129082\n",
      "Trained batch 8317 batch loss 6.62290668 epoch total loss 6.23133755\n",
      "Trained batch 8318 batch loss 5.09025812 epoch total loss 6.2312\n",
      "Trained batch 8319 batch loss 5.50917435 epoch total loss 6.23111343\n",
      "Trained batch 8320 batch loss 5.77366 epoch total loss 6.2310586\n",
      "Trained batch 8321 batch loss 5.53978348 epoch total loss 6.23097515\n",
      "Trained batch 8322 batch loss 6.17679882 epoch total loss 6.23096848\n",
      "Trained batch 8323 batch loss 5.78991604 epoch total loss 6.23091555\n",
      "Trained batch 8324 batch loss 6.06872034 epoch total loss 6.23089647\n",
      "Trained batch 8325 batch loss 5.89266396 epoch total loss 6.23085594\n",
      "Trained batch 8326 batch loss 6.325109 epoch total loss 6.23086691\n",
      "Trained batch 8327 batch loss 6.16295433 epoch total loss 6.23085928\n",
      "Trained batch 8328 batch loss 5.91069412 epoch total loss 6.23082066\n",
      "Trained batch 8329 batch loss 6.25159264 epoch total loss 6.23082304\n",
      "Trained batch 8330 batch loss 6.13371944 epoch total loss 6.23081112\n",
      "Trained batch 8331 batch loss 6.09877396 epoch total loss 6.23079491\n",
      "Trained batch 8332 batch loss 5.73543024 epoch total loss 6.2307353\n",
      "Trained batch 8333 batch loss 6.11597157 epoch total loss 6.23072195\n",
      "Trained batch 8334 batch loss 6.19279861 epoch total loss 6.23071718\n",
      "Trained batch 8335 batch loss 6.0041914 epoch total loss 6.23069\n",
      "Trained batch 8336 batch loss 4.48405313 epoch total loss 6.23048067\n",
      "Trained batch 8337 batch loss 5.32125664 epoch total loss 6.23037148\n",
      "Trained batch 8338 batch loss 5.14544725 epoch total loss 6.2302413\n",
      "Trained batch 8339 batch loss 4.93321705 epoch total loss 6.23008537\n",
      "Trained batch 8340 batch loss 4.12674809 epoch total loss 6.22983313\n",
      "Trained batch 8341 batch loss 5.57282543 epoch total loss 6.22975445\n",
      "Trained batch 8342 batch loss 6.20204258 epoch total loss 6.22975159\n",
      "Trained batch 8343 batch loss 5.24323273 epoch total loss 6.22963285\n",
      "Trained batch 8344 batch loss 6.78582096 epoch total loss 6.22969961\n",
      "Trained batch 8345 batch loss 6.15475655 epoch total loss 6.22969055\n",
      "Trained batch 8346 batch loss 6.47041464 epoch total loss 6.22971964\n",
      "Trained batch 8347 batch loss 5.98729706 epoch total loss 6.22969055\n",
      "Trained batch 8348 batch loss 5.9816618 epoch total loss 6.22966051\n",
      "Trained batch 8349 batch loss 6.36965561 epoch total loss 6.22967768\n",
      "Trained batch 8350 batch loss 6.11044884 epoch total loss 6.22966337\n",
      "Trained batch 8351 batch loss 5.38536024 epoch total loss 6.22956228\n",
      "Trained batch 8352 batch loss 5.89993286 epoch total loss 6.22952271\n",
      "Trained batch 8353 batch loss 6.3744173 epoch total loss 6.22954\n",
      "Trained batch 8354 batch loss 6.48278332 epoch total loss 6.22957039\n",
      "Trained batch 8355 batch loss 6.41698313 epoch total loss 6.22959328\n",
      "Trained batch 8356 batch loss 5.91364813 epoch total loss 6.22955513\n",
      "Trained batch 8357 batch loss 6.35603571 epoch total loss 6.22957039\n",
      "Trained batch 8358 batch loss 5.97563457 epoch total loss 6.22954\n",
      "Trained batch 8359 batch loss 6.09470844 epoch total loss 6.22952366\n",
      "Trained batch 8360 batch loss 6.63054466 epoch total loss 6.22957182\n",
      "Trained batch 8361 batch loss 6.22036695 epoch total loss 6.22957039\n",
      "Trained batch 8362 batch loss 6.61856651 epoch total loss 6.22961664\n",
      "Trained batch 8363 batch loss 6.29488564 epoch total loss 6.22962427\n",
      "Trained batch 8364 batch loss 6.30521679 epoch total loss 6.22963333\n",
      "Trained batch 8365 batch loss 6.14680862 epoch total loss 6.22962332\n",
      "Trained batch 8366 batch loss 5.58446312 epoch total loss 6.22954655\n",
      "Trained batch 8367 batch loss 4.53582859 epoch total loss 6.22934389\n",
      "Trained batch 8368 batch loss 5.60018539 epoch total loss 6.22926903\n",
      "Trained batch 8369 batch loss 6.24141598 epoch total loss 6.22927046\n",
      "Trained batch 8370 batch loss 5.31869936 epoch total loss 6.22916222\n",
      "Trained batch 8371 batch loss 5.95022154 epoch total loss 6.22912836\n",
      "Trained batch 8372 batch loss 6.52351665 epoch total loss 6.22916365\n",
      "Trained batch 8373 batch loss 6.64316082 epoch total loss 6.22921324\n",
      "Trained batch 8374 batch loss 4.78327465 epoch total loss 6.2290411\n",
      "Trained batch 8375 batch loss 6.03424 epoch total loss 6.22901773\n",
      "Trained batch 8376 batch loss 6.04799223 epoch total loss 6.2289958\n",
      "Trained batch 8377 batch loss 5.61430645 epoch total loss 6.22892237\n",
      "Trained batch 8378 batch loss 5.7902894 epoch total loss 6.22887\n",
      "Trained batch 8379 batch loss 6.58642673 epoch total loss 6.22891235\n",
      "Trained batch 8380 batch loss 6.28132725 epoch total loss 6.22891903\n",
      "Trained batch 8381 batch loss 6.24613667 epoch total loss 6.22892094\n",
      "Trained batch 8382 batch loss 6.5464468 epoch total loss 6.22895861\n",
      "Trained batch 8383 batch loss 6.18602371 epoch total loss 6.22895384\n",
      "Trained batch 8384 batch loss 6.09609413 epoch total loss 6.2289381\n",
      "Trained batch 8385 batch loss 6.74008799 epoch total loss 6.22899914\n",
      "Trained batch 8386 batch loss 6.73373318 epoch total loss 6.22905922\n",
      "Trained batch 8387 batch loss 6.96424 epoch total loss 6.22914696\n",
      "Trained batch 8388 batch loss 6.56886 epoch total loss 6.22918749\n",
      "Trained batch 8389 batch loss 6.65116739 epoch total loss 6.22923803\n",
      "Trained batch 8390 batch loss 6.14362049 epoch total loss 6.22922802\n",
      "Trained batch 8391 batch loss 5.79222965 epoch total loss 6.22917604\n",
      "Trained batch 8392 batch loss 5.70183182 epoch total loss 6.2291131\n",
      "Trained batch 8393 batch loss 6.43386555 epoch total loss 6.22913742\n",
      "Trained batch 8394 batch loss 6.17633724 epoch total loss 6.22913122\n",
      "Trained batch 8395 batch loss 5.35563564 epoch total loss 6.22902727\n",
      "Trained batch 8396 batch loss 6.62018156 epoch total loss 6.229074\n",
      "Trained batch 8397 batch loss 5.06746 epoch total loss 6.22893524\n",
      "Trained batch 8398 batch loss 6.48082161 epoch total loss 6.22896528\n",
      "Trained batch 8399 batch loss 6.03202105 epoch total loss 6.22894192\n",
      "Trained batch 8400 batch loss 5.65520811 epoch total loss 6.22887373\n",
      "Trained batch 8401 batch loss 6.22334957 epoch total loss 6.22887278\n",
      "Trained batch 8402 batch loss 6.92862177 epoch total loss 6.22895622\n",
      "Trained batch 8403 batch loss 4.57908535 epoch total loss 6.22876\n",
      "Trained batch 8404 batch loss 6.38636541 epoch total loss 6.22877884\n",
      "Trained batch 8405 batch loss 5.98566389 epoch total loss 6.22874975\n",
      "Trained batch 8406 batch loss 7.8433609 epoch total loss 6.22894192\n",
      "Trained batch 8407 batch loss 5.70730114 epoch total loss 6.22887945\n",
      "Trained batch 8408 batch loss 5.07524538 epoch total loss 6.22874212\n",
      "Trained batch 8409 batch loss 6.00523472 epoch total loss 6.22871542\n",
      "Trained batch 8410 batch loss 5.22902775 epoch total loss 6.22859669\n",
      "Trained batch 8411 batch loss 6.57757568 epoch total loss 6.22863865\n",
      "Trained batch 8412 batch loss 6.19991493 epoch total loss 6.22863483\n",
      "Trained batch 8413 batch loss 5.59235764 epoch total loss 6.22855949\n",
      "Trained batch 8414 batch loss 5.98803568 epoch total loss 6.22853088\n",
      "Trained batch 8415 batch loss 4.61183834 epoch total loss 6.2283392\n",
      "Trained batch 8416 batch loss 5.95870256 epoch total loss 6.22830677\n",
      "Trained batch 8417 batch loss 4.98863363 epoch total loss 6.22815943\n",
      "Trained batch 8418 batch loss 6.36216354 epoch total loss 6.22817564\n",
      "Trained batch 8419 batch loss 6.12780952 epoch total loss 6.22816372\n",
      "Trained batch 8420 batch loss 5.90326881 epoch total loss 6.2281251\n",
      "Trained batch 8421 batch loss 4.86053228 epoch total loss 6.22796249\n",
      "Trained batch 8422 batch loss 6.91183138 epoch total loss 6.22804356\n",
      "Trained batch 8423 batch loss 5.56574297 epoch total loss 6.22796488\n",
      "Trained batch 8424 batch loss 6.28835678 epoch total loss 6.22797203\n",
      "Trained batch 8425 batch loss 5.84808826 epoch total loss 6.22792721\n",
      "Trained batch 8426 batch loss 5.5053339 epoch total loss 6.2278409\n",
      "Trained batch 8427 batch loss 4.89778137 epoch total loss 6.22768354\n",
      "Trained batch 8428 batch loss 5.69193792 epoch total loss 6.22761965\n",
      "Trained batch 8429 batch loss 6.09263229 epoch total loss 6.22760391\n",
      "Trained batch 8430 batch loss 6.25791025 epoch total loss 6.22760725\n",
      "Trained batch 8431 batch loss 6.38216066 epoch total loss 6.22762585\n",
      "Trained batch 8432 batch loss 5.81733274 epoch total loss 6.22757721\n",
      "Trained batch 8433 batch loss 6.08625221 epoch total loss 6.22756\n",
      "Trained batch 8434 batch loss 5.8028326 epoch total loss 6.22751\n",
      "Trained batch 8435 batch loss 6.00725269 epoch total loss 6.22748423\n",
      "Trained batch 8436 batch loss 6.23587942 epoch total loss 6.2274847\n",
      "Trained batch 8437 batch loss 5.84102392 epoch total loss 6.22743893\n",
      "Trained batch 8438 batch loss 5.89754915 epoch total loss 6.2274\n",
      "Trained batch 8439 batch loss 6.15393162 epoch total loss 6.22739124\n",
      "Trained batch 8440 batch loss 6.34164953 epoch total loss 6.22740459\n",
      "Trained batch 8441 batch loss 6.09400654 epoch total loss 6.22738838\n",
      "Trained batch 8442 batch loss 6.00824642 epoch total loss 6.22736263\n",
      "Trained batch 8443 batch loss 6.13030624 epoch total loss 6.22735071\n",
      "Trained batch 8444 batch loss 6.07932615 epoch total loss 6.22733307\n",
      "Trained batch 8445 batch loss 5.69466686 epoch total loss 6.22727\n",
      "Trained batch 8446 batch loss 6.30148172 epoch total loss 6.22727871\n",
      "Trained batch 8447 batch loss 6.22696686 epoch total loss 6.22727871\n",
      "Trained batch 8448 batch loss 6.89275599 epoch total loss 6.22735786\n",
      "Trained batch 8449 batch loss 6.48471546 epoch total loss 6.22738838\n",
      "Trained batch 8450 batch loss 7.1438 epoch total loss 6.22749662\n",
      "Trained batch 8451 batch loss 6.43419552 epoch total loss 6.22752094\n",
      "Trained batch 8452 batch loss 6.29551458 epoch total loss 6.22752953\n",
      "Trained batch 8453 batch loss 7.2082181 epoch total loss 6.2276454\n",
      "Trained batch 8454 batch loss 6.47129345 epoch total loss 6.22767401\n",
      "Trained batch 8455 batch loss 5.70903778 epoch total loss 6.22761297\n",
      "Trained batch 8456 batch loss 7.3419528 epoch total loss 6.22774506\n",
      "Trained batch 8457 batch loss 6.62088108 epoch total loss 6.22779179\n",
      "Trained batch 8458 batch loss 6.75134945 epoch total loss 6.2278533\n",
      "Trained batch 8459 batch loss 6.93847799 epoch total loss 6.22793722\n",
      "Trained batch 8460 batch loss 7.74900532 epoch total loss 6.22811699\n",
      "Trained batch 8461 batch loss 5.99832344 epoch total loss 6.22809029\n",
      "Trained batch 8462 batch loss 6.51152039 epoch total loss 6.22812366\n",
      "Trained batch 8463 batch loss 7.67254162 epoch total loss 6.22829437\n",
      "Trained batch 8464 batch loss 7.30683517 epoch total loss 6.22842216\n",
      "Trained batch 8465 batch loss 6.04426336 epoch total loss 6.2284\n",
      "Trained batch 8466 batch loss 6.5959506 epoch total loss 6.22844362\n",
      "Trained batch 8467 batch loss 6.74687147 epoch total loss 6.22850466\n",
      "Trained batch 8468 batch loss 6.39323616 epoch total loss 6.22852421\n",
      "Trained batch 8469 batch loss 7.23172808 epoch total loss 6.22864246\n",
      "Trained batch 8470 batch loss 6.58217239 epoch total loss 6.22868443\n",
      "Trained batch 8471 batch loss 6.74748802 epoch total loss 6.22874546\n",
      "Trained batch 8472 batch loss 6.52846622 epoch total loss 6.22878075\n",
      "Trained batch 8473 batch loss 5.61775541 epoch total loss 6.22870874\n",
      "Trained batch 8474 batch loss 7.28553 epoch total loss 6.2288332\n",
      "Trained batch 8475 batch loss 6.01778507 epoch total loss 6.2288084\n",
      "Trained batch 8476 batch loss 6.7520957 epoch total loss 6.22887039\n",
      "Trained batch 8477 batch loss 5.89802742 epoch total loss 6.22883129\n",
      "Trained batch 8478 batch loss 6.56022739 epoch total loss 6.22887039\n",
      "Trained batch 8479 batch loss 6.25985432 epoch total loss 6.22887421\n",
      "Trained batch 8480 batch loss 7.05172348 epoch total loss 6.228971\n",
      "Trained batch 8481 batch loss 6.08842087 epoch total loss 6.22895479\n",
      "Trained batch 8482 batch loss 7.46178436 epoch total loss 6.2291\n",
      "Trained batch 8483 batch loss 7.12129259 epoch total loss 6.22920513\n",
      "Trained batch 8484 batch loss 5.95958424 epoch total loss 6.22917366\n",
      "Trained batch 8485 batch loss 5.50602722 epoch total loss 6.22908878\n",
      "Trained batch 8486 batch loss 5.62779522 epoch total loss 6.22901773\n",
      "Trained batch 8487 batch loss 5.15556335 epoch total loss 6.22889137\n",
      "Trained batch 8488 batch loss 5.4508543 epoch total loss 6.22879934\n",
      "Trained batch 8489 batch loss 6.07344151 epoch total loss 6.22878122\n",
      "Trained batch 8490 batch loss 5.15268517 epoch total loss 6.22865438\n",
      "Trained batch 8491 batch loss 5.65525246 epoch total loss 6.22858715\n",
      "Trained batch 8492 batch loss 5.76446724 epoch total loss 6.22853279\n",
      "Trained batch 8493 batch loss 5.24366522 epoch total loss 6.22841644\n",
      "Trained batch 8494 batch loss 4.42445755 epoch total loss 6.22820425\n",
      "Trained batch 8495 batch loss 4.77306843 epoch total loss 6.22803307\n",
      "Trained batch 8496 batch loss 5.20562935 epoch total loss 6.2279129\n",
      "Trained batch 8497 batch loss 4.80824709 epoch total loss 6.22774601\n",
      "Trained batch 8498 batch loss 5.31466293 epoch total loss 6.22763872\n",
      "Trained batch 8499 batch loss 5.13559771 epoch total loss 6.22751045\n",
      "Trained batch 8500 batch loss 5.89244461 epoch total loss 6.2274704\n",
      "Trained batch 8501 batch loss 4.87330532 epoch total loss 6.22731161\n",
      "Trained batch 8502 batch loss 4.91715527 epoch total loss 6.22715759\n",
      "Trained batch 8503 batch loss 4.73792887 epoch total loss 6.22698259\n",
      "Trained batch 8504 batch loss 4.58058357 epoch total loss 6.226789\n",
      "Trained batch 8505 batch loss 4.65852165 epoch total loss 6.22660494\n",
      "Trained batch 8506 batch loss 4.19728947 epoch total loss 6.22636652\n",
      "Trained batch 8507 batch loss 5.17627335 epoch total loss 6.22624302\n",
      "Trained batch 8508 batch loss 4.57988453 epoch total loss 6.22604942\n",
      "Trained batch 8509 batch loss 4.92065668 epoch total loss 6.22589588\n",
      "Trained batch 8510 batch loss 4.91680288 epoch total loss 6.22574234\n",
      "Trained batch 8511 batch loss 4.98782063 epoch total loss 6.2255969\n",
      "Trained batch 8512 batch loss 4.72469234 epoch total loss 6.22542095\n",
      "Trained batch 8513 batch loss 5.34369087 epoch total loss 6.225317\n",
      "Trained batch 8514 batch loss 5.61940527 epoch total loss 6.22524643\n",
      "Trained batch 8515 batch loss 4.36749935 epoch total loss 6.22502804\n",
      "Trained batch 8516 batch loss 5.21519947 epoch total loss 6.22490931\n",
      "Trained batch 8517 batch loss 4.71495914 epoch total loss 6.22473192\n",
      "Trained batch 8518 batch loss 4.76763 epoch total loss 6.22456121\n",
      "Trained batch 8519 batch loss 5.00509357 epoch total loss 6.22441816\n",
      "Trained batch 8520 batch loss 5.68657875 epoch total loss 6.22435474\n",
      "Trained batch 8521 batch loss 6.77923489 epoch total loss 6.22441959\n",
      "Trained batch 8522 batch loss 4.89299679 epoch total loss 6.22426367\n",
      "Trained batch 8523 batch loss 6.39548588 epoch total loss 6.2242837\n",
      "Trained batch 8524 batch loss 5.70632696 epoch total loss 6.22422314\n",
      "Trained batch 8525 batch loss 6.32078743 epoch total loss 6.2242341\n",
      "Trained batch 8526 batch loss 5.99847412 epoch total loss 6.22420788\n",
      "Trained batch 8527 batch loss 5.3319478 epoch total loss 6.22410345\n",
      "Trained batch 8528 batch loss 7.15505123 epoch total loss 6.22421265\n",
      "Trained batch 8529 batch loss 4.59550381 epoch total loss 6.22402143\n",
      "Trained batch 8530 batch loss 5.82307434 epoch total loss 6.2239747\n",
      "Trained batch 8531 batch loss 5.41812038 epoch total loss 6.22388029\n",
      "Trained batch 8532 batch loss 6.11488676 epoch total loss 6.22386742\n",
      "Trained batch 8533 batch loss 6.05482817 epoch total loss 6.22384739\n",
      "Trained batch 8534 batch loss 6.20622873 epoch total loss 6.22384548\n",
      "Trained batch 8535 batch loss 4.80125046 epoch total loss 6.22367859\n",
      "Trained batch 8536 batch loss 6.57945061 epoch total loss 6.22372\n",
      "Trained batch 8537 batch loss 4.38785934 epoch total loss 6.22350502\n",
      "Trained batch 8538 batch loss 6.63013363 epoch total loss 6.2235527\n",
      "Trained batch 8539 batch loss 6.09403229 epoch total loss 6.22353745\n",
      "Trained batch 8540 batch loss 6.05213785 epoch total loss 6.22351694\n",
      "Trained batch 8541 batch loss 6.28808689 epoch total loss 6.22352457\n",
      "Trained batch 8542 batch loss 6.55069447 epoch total loss 6.22356319\n",
      "Trained batch 8543 batch loss 6.51801872 epoch total loss 6.22359753\n",
      "Trained batch 8544 batch loss 6.22612333 epoch total loss 6.223598\n",
      "Trained batch 8545 batch loss 7.56166887 epoch total loss 6.22375488\n",
      "Trained batch 8546 batch loss 6.62831831 epoch total loss 6.22380209\n",
      "Trained batch 8547 batch loss 5.83851719 epoch total loss 6.22375727\n",
      "Trained batch 8548 batch loss 6.12098694 epoch total loss 6.22374535\n",
      "Trained batch 8549 batch loss 6.43801785 epoch total loss 6.22377\n",
      "Trained batch 8550 batch loss 6.06431103 epoch total loss 6.22375154\n",
      "Trained batch 8551 batch loss 5.15396 epoch total loss 6.22362614\n",
      "Trained batch 8552 batch loss 6.51312304 epoch total loss 6.22366\n",
      "Trained batch 8553 batch loss 5.59948158 epoch total loss 6.22358656\n",
      "Trained batch 8554 batch loss 5.86081553 epoch total loss 6.22354412\n",
      "Trained batch 8555 batch loss 6.37578249 epoch total loss 6.22356176\n",
      "Trained batch 8556 batch loss 6.15916634 epoch total loss 6.22355413\n",
      "Trained batch 8557 batch loss 6.27141857 epoch total loss 6.22356\n",
      "Trained batch 8558 batch loss 6.13712406 epoch total loss 6.22354937\n",
      "Trained batch 8559 batch loss 6.43816662 epoch total loss 6.22357464\n",
      "Trained batch 8560 batch loss 6.25508308 epoch total loss 6.22357798\n",
      "Trained batch 8561 batch loss 6.2724905 epoch total loss 6.2235837\n",
      "Trained batch 8562 batch loss 6.17098761 epoch total loss 6.22357798\n",
      "Trained batch 8563 batch loss 6.34176 epoch total loss 6.22359133\n",
      "Trained batch 8564 batch loss 5.33900547 epoch total loss 6.22348833\n",
      "Trained batch 8565 batch loss 6.61002302 epoch total loss 6.22353315\n",
      "Trained batch 8566 batch loss 6.42159653 epoch total loss 6.22355652\n",
      "Trained batch 8567 batch loss 5.63223839 epoch total loss 6.22348738\n",
      "Trained batch 8568 batch loss 5.58736515 epoch total loss 6.22341299\n",
      "Trained batch 8569 batch loss 6.18645382 epoch total loss 6.2234087\n",
      "Trained batch 8570 batch loss 4.7627821 epoch total loss 6.22323847\n",
      "Trained batch 8571 batch loss 6.39934206 epoch total loss 6.22325897\n",
      "Trained batch 8572 batch loss 5.08334351 epoch total loss 6.22312546\n",
      "Trained batch 8573 batch loss 5.39750099 epoch total loss 6.22302961\n",
      "Trained batch 8574 batch loss 5.53734779 epoch total loss 6.2229495\n",
      "Trained batch 8575 batch loss 5.33121252 epoch total loss 6.22284555\n",
      "Trained batch 8576 batch loss 5.08777475 epoch total loss 6.22271299\n",
      "Trained batch 8577 batch loss 5.77775145 epoch total loss 6.2226615\n",
      "Trained batch 8578 batch loss 5.09072924 epoch total loss 6.22252941\n",
      "Trained batch 8579 batch loss 6.83721733 epoch total loss 6.22260094\n",
      "Trained batch 8580 batch loss 6.45019054 epoch total loss 6.22262716\n",
      "Trained batch 8581 batch loss 6.03805065 epoch total loss 6.22260571\n",
      "Trained batch 8582 batch loss 5.06448841 epoch total loss 6.22247076\n",
      "Trained batch 8583 batch loss 5.8915987 epoch total loss 6.22243214\n",
      "Trained batch 8584 batch loss 6.54985762 epoch total loss 6.22247076\n",
      "Trained batch 8585 batch loss 6.06669044 epoch total loss 6.22245264\n",
      "Trained batch 8586 batch loss 6.38892651 epoch total loss 6.22247219\n",
      "Trained batch 8587 batch loss 5.85650921 epoch total loss 6.22242928\n",
      "Trained batch 8588 batch loss 6.22601461 epoch total loss 6.22242975\n",
      "Trained batch 8589 batch loss 4.886868 epoch total loss 6.2222743\n",
      "Trained batch 8590 batch loss 5.51757431 epoch total loss 6.22219181\n",
      "Trained batch 8591 batch loss 4.72708607 epoch total loss 6.22201777\n",
      "Trained batch 8592 batch loss 6.12155962 epoch total loss 6.22200632\n",
      "Trained batch 8593 batch loss 6.66435719 epoch total loss 6.22205734\n",
      "Trained batch 8594 batch loss 5.96734333 epoch total loss 6.22202826\n",
      "Trained batch 8595 batch loss 5.2910018 epoch total loss 6.22191954\n",
      "Trained batch 8596 batch loss 5.2176733 epoch total loss 6.22180271\n",
      "Trained batch 8597 batch loss 5.37978 epoch total loss 6.22170496\n",
      "Trained batch 8598 batch loss 5.55525398 epoch total loss 6.22162724\n",
      "Trained batch 8599 batch loss 5.2556715 epoch total loss 6.2215147\n",
      "Trained batch 8600 batch loss 5.07666826 epoch total loss 6.22138166\n",
      "Trained batch 8601 batch loss 5.85404587 epoch total loss 6.22133923\n",
      "Trained batch 8602 batch loss 5.65471411 epoch total loss 6.22127342\n",
      "Trained batch 8603 batch loss 5.71356773 epoch total loss 6.22121477\n",
      "Trained batch 8604 batch loss 5.9093914 epoch total loss 6.22117853\n",
      "Trained batch 8605 batch loss 5.96595764 epoch total loss 6.22114849\n",
      "Trained batch 8606 batch loss 5.26047 epoch total loss 6.22103739\n",
      "Trained batch 8607 batch loss 6.02897835 epoch total loss 6.2210145\n",
      "Trained batch 8608 batch loss 6.1038 epoch total loss 6.22100115\n",
      "Trained batch 8609 batch loss 6.28631783 epoch total loss 6.22100878\n",
      "Trained batch 8610 batch loss 6.82060623 epoch total loss 6.2210784\n",
      "Trained batch 8611 batch loss 5.76643372 epoch total loss 6.22102547\n",
      "Trained batch 8612 batch loss 6.28929329 epoch total loss 6.2210331\n",
      "Trained batch 8613 batch loss 5.73107624 epoch total loss 6.22097635\n",
      "Trained batch 8614 batch loss 5.68826485 epoch total loss 6.22091436\n",
      "Trained batch 8615 batch loss 5.21161175 epoch total loss 6.22079706\n",
      "Trained batch 8616 batch loss 5.04951525 epoch total loss 6.22066164\n",
      "Trained batch 8617 batch loss 5.23201084 epoch total loss 6.22054672\n",
      "Trained batch 8618 batch loss 5.3718071 epoch total loss 6.22044802\n",
      "Trained batch 8619 batch loss 5.60956955 epoch total loss 6.22037697\n",
      "Trained batch 8620 batch loss 5.26555729 epoch total loss 6.22026634\n",
      "Trained batch 8621 batch loss 4.91552353 epoch total loss 6.22011471\n",
      "Trained batch 8622 batch loss 5.73247242 epoch total loss 6.22005844\n",
      "Trained batch 8623 batch loss 6.06539726 epoch total loss 6.2200408\n",
      "Trained batch 8624 batch loss 6.46891403 epoch total loss 6.22006941\n",
      "Trained batch 8625 batch loss 5.45606709 epoch total loss 6.21998119\n",
      "Trained batch 8626 batch loss 6.65626097 epoch total loss 6.22003174\n",
      "Trained batch 8627 batch loss 5.56402 epoch total loss 6.21995544\n",
      "Trained batch 8628 batch loss 5.89237213 epoch total loss 6.2199173\n",
      "Trained batch 8629 batch loss 5.46531868 epoch total loss 6.21982956\n",
      "Trained batch 8630 batch loss 5.213274 epoch total loss 6.21971321\n",
      "Trained batch 8631 batch loss 4.9461832 epoch total loss 6.21956539\n",
      "Trained batch 8632 batch loss 5.4890542 epoch total loss 6.21948099\n",
      "Trained batch 8633 batch loss 5.20064592 epoch total loss 6.21936274\n",
      "Trained batch 8634 batch loss 3.57823133 epoch total loss 6.21905661\n",
      "Trained batch 8635 batch loss 5.31644344 epoch total loss 6.21895218\n",
      "Trained batch 8636 batch loss 4.93993521 epoch total loss 6.21880436\n",
      "Trained batch 8637 batch loss 5.80469608 epoch total loss 6.2187562\n",
      "Trained batch 8638 batch loss 6.80579948 epoch total loss 6.21882439\n",
      "Trained batch 8639 batch loss 6.1104269 epoch total loss 6.21881151\n",
      "Trained batch 8640 batch loss 5.97541237 epoch total loss 6.21878338\n",
      "Trained batch 8641 batch loss 6.17657804 epoch total loss 6.21877861\n",
      "Trained batch 8642 batch loss 5.27243614 epoch total loss 6.21866894\n",
      "Trained batch 8643 batch loss 6.38522959 epoch total loss 6.21868849\n",
      "Trained batch 8644 batch loss 7.04494953 epoch total loss 6.21878433\n",
      "Trained batch 8645 batch loss 6.191185 epoch total loss 6.21878099\n",
      "Trained batch 8646 batch loss 5.489604 epoch total loss 6.21869659\n",
      "Trained batch 8647 batch loss 5.66806 epoch total loss 6.21863317\n",
      "Trained batch 8648 batch loss 6.91250372 epoch total loss 6.21871328\n",
      "Trained batch 8649 batch loss 6.53758 epoch total loss 6.21875048\n",
      "Trained batch 8650 batch loss 5.83711338 epoch total loss 6.21870613\n",
      "Trained batch 8651 batch loss 6.10961056 epoch total loss 6.21869373\n",
      "Trained batch 8652 batch loss 6.14936352 epoch total loss 6.21868563\n",
      "Trained batch 8653 batch loss 4.8014431 epoch total loss 6.2185216\n",
      "Trained batch 8654 batch loss 5.64418888 epoch total loss 6.21845531\n",
      "Trained batch 8655 batch loss 5.99712944 epoch total loss 6.21842957\n",
      "Trained batch 8656 batch loss 5.68992043 epoch total loss 6.21836853\n",
      "Trained batch 8657 batch loss 6.81632137 epoch total loss 6.21843767\n",
      "Trained batch 8658 batch loss 5.19686747 epoch total loss 6.21831942\n",
      "Trained batch 8659 batch loss 5.73088455 epoch total loss 6.21826315\n",
      "Trained batch 8660 batch loss 5.93431854 epoch total loss 6.21823025\n",
      "Trained batch 8661 batch loss 4.75618076 epoch total loss 6.21806192\n",
      "Trained batch 8662 batch loss 5.83050156 epoch total loss 6.2180171\n",
      "Trained batch 8663 batch loss 6.77860689 epoch total loss 6.21808195\n",
      "Trained batch 8664 batch loss 6.59251833 epoch total loss 6.21812534\n",
      "Trained batch 8665 batch loss 6.85403252 epoch total loss 6.21819878\n",
      "Trained batch 8666 batch loss 6.60288906 epoch total loss 6.21824312\n",
      "Trained batch 8667 batch loss 6.51926327 epoch total loss 6.21827745\n",
      "Trained batch 8668 batch loss 6.7670908 epoch total loss 6.21834087\n",
      "Trained batch 8669 batch loss 6.82555103 epoch total loss 6.21841049\n",
      "Trained batch 8670 batch loss 6.01227808 epoch total loss 6.21838665\n",
      "Trained batch 8671 batch loss 6.40937519 epoch total loss 6.21840906\n",
      "Trained batch 8672 batch loss 6.24034929 epoch total loss 6.21841192\n",
      "Trained batch 8673 batch loss 6.07614231 epoch total loss 6.21839523\n",
      "Trained batch 8674 batch loss 6.05200481 epoch total loss 6.21837568\n",
      "Trained batch 8675 batch loss 6.06717157 epoch total loss 6.21835804\n",
      "Trained batch 8676 batch loss 6.65203 epoch total loss 6.21840811\n",
      "Trained batch 8677 batch loss 6.54152775 epoch total loss 6.21844578\n",
      "Trained batch 8678 batch loss 6.49597359 epoch total loss 6.21847773\n",
      "Trained batch 8679 batch loss 6.10874 epoch total loss 6.21846533\n",
      "Trained batch 8680 batch loss 6.36577559 epoch total loss 6.21848202\n",
      "Trained batch 8681 batch loss 6.03105736 epoch total loss 6.21846056\n",
      "Trained batch 8682 batch loss 5.96180534 epoch total loss 6.218431\n",
      "Trained batch 8683 batch loss 6.08227301 epoch total loss 6.21841526\n",
      "Trained batch 8684 batch loss 5.91665602 epoch total loss 6.21838093\n",
      "Trained batch 8685 batch loss 6.02037764 epoch total loss 6.21835756\n",
      "Trained batch 8686 batch loss 5.813416 epoch total loss 6.21831131\n",
      "Trained batch 8687 batch loss 5.6364336 epoch total loss 6.21824408\n",
      "Trained batch 8688 batch loss 6.11979914 epoch total loss 6.21823311\n",
      "Trained batch 8689 batch loss 5.96200609 epoch total loss 6.21820354\n",
      "Trained batch 8690 batch loss 6.11920738 epoch total loss 6.2181921\n",
      "Trained batch 8691 batch loss 6.3865633 epoch total loss 6.21821165\n",
      "Trained batch 8692 batch loss 5.90290928 epoch total loss 6.21817541\n",
      "Trained batch 8693 batch loss 6.57085 epoch total loss 6.21821594\n",
      "Trained batch 8694 batch loss 6.45519924 epoch total loss 6.21824312\n",
      "Trained batch 8695 batch loss 5.0229063 epoch total loss 6.21810579\n",
      "Trained batch 8696 batch loss 6.4072814 epoch total loss 6.21812725\n",
      "Trained batch 8697 batch loss 6.19403934 epoch total loss 6.21812487\n",
      "Trained batch 8698 batch loss 5.90501595 epoch total loss 6.2180891\n",
      "Trained batch 8699 batch loss 6.29836082 epoch total loss 6.21809816\n",
      "Trained batch 8700 batch loss 6.61319447 epoch total loss 6.21814346\n",
      "Trained batch 8701 batch loss 6.7814436 epoch total loss 6.21820831\n",
      "Trained batch 8702 batch loss 6.16349506 epoch total loss 6.21820211\n",
      "Trained batch 8703 batch loss 6.1497097 epoch total loss 6.21819401\n",
      "Trained batch 8704 batch loss 6.42244911 epoch total loss 6.21821737\n",
      "Trained batch 8705 batch loss 6.13576603 epoch total loss 6.21820784\n",
      "Trained batch 8706 batch loss 5.63981247 epoch total loss 6.21814156\n",
      "Trained batch 8707 batch loss 6.76720524 epoch total loss 6.2182045\n",
      "Trained batch 8708 batch loss 5.95893526 epoch total loss 6.21817446\n",
      "Trained batch 8709 batch loss 4.1766119 epoch total loss 6.21794\n",
      "Trained batch 8710 batch loss 6.2472105 epoch total loss 6.21794319\n",
      "Trained batch 8711 batch loss 5.78173256 epoch total loss 6.21789312\n",
      "Trained batch 8712 batch loss 5.6868639 epoch total loss 6.21783209\n",
      "Trained batch 8713 batch loss 6.24853802 epoch total loss 6.2178359\n",
      "Trained batch 8714 batch loss 5.0084095 epoch total loss 6.21769714\n",
      "Trained batch 8715 batch loss 6.41755867 epoch total loss 6.21772\n",
      "Trained batch 8716 batch loss 7.01759148 epoch total loss 6.21781206\n",
      "Trained batch 8717 batch loss 6.66475 epoch total loss 6.21786308\n",
      "Trained batch 8718 batch loss 6.62429905 epoch total loss 6.21791\n",
      "Trained batch 8719 batch loss 7.08308697 epoch total loss 6.218009\n",
      "Trained batch 8720 batch loss 6.12733507 epoch total loss 6.21799898\n",
      "Trained batch 8721 batch loss 5.26254034 epoch total loss 6.21788931\n",
      "Trained batch 8722 batch loss 5.04776192 epoch total loss 6.21775484\n",
      "Trained batch 8723 batch loss 5.36136675 epoch total loss 6.21765709\n",
      "Trained batch 8724 batch loss 4.8755188 epoch total loss 6.21750307\n",
      "Trained batch 8725 batch loss 5.63822556 epoch total loss 6.21743631\n",
      "Trained batch 8726 batch loss 4.28931522 epoch total loss 6.21721554\n",
      "Trained batch 8727 batch loss 4.40877914 epoch total loss 6.21700859\n",
      "Trained batch 8728 batch loss 5.6740489 epoch total loss 6.21694613\n",
      "Trained batch 8729 batch loss 5.86190319 epoch total loss 6.21690607\n",
      "Trained batch 8730 batch loss 4.347435 epoch total loss 6.21669149\n",
      "Trained batch 8731 batch loss 5.31056881 epoch total loss 6.21658802\n",
      "Trained batch 8732 batch loss 6.68755913 epoch total loss 6.2166419\n",
      "Trained batch 8733 batch loss 6.12418699 epoch total loss 6.21663141\n",
      "Trained batch 8734 batch loss 5.57788324 epoch total loss 6.21655846\n",
      "Trained batch 8735 batch loss 5.59745502 epoch total loss 6.21648741\n",
      "Trained batch 8736 batch loss 5.61747313 epoch total loss 6.21641922\n",
      "Trained batch 8737 batch loss 5.69736671 epoch total loss 6.21635962\n",
      "Trained batch 8738 batch loss 6.11290359 epoch total loss 6.21634817\n",
      "Trained batch 8739 batch loss 6.29357433 epoch total loss 6.21635675\n",
      "Trained batch 8740 batch loss 6.39438152 epoch total loss 6.21637726\n",
      "Trained batch 8741 batch loss 6.42374611 epoch total loss 6.21640062\n",
      "Trained batch 8742 batch loss 6.15156651 epoch total loss 6.21639347\n",
      "Trained batch 8743 batch loss 5.69034863 epoch total loss 6.21633339\n",
      "Trained batch 8744 batch loss 5.79998064 epoch total loss 6.21628571\n",
      "Trained batch 8745 batch loss 6.45039272 epoch total loss 6.21631241\n",
      "Trained batch 8746 batch loss 5.79411554 epoch total loss 6.21626425\n",
      "Trained batch 8747 batch loss 6.22342 epoch total loss 6.21626472\n",
      "Trained batch 8748 batch loss 6.36332703 epoch total loss 6.21628141\n",
      "Trained batch 8749 batch loss 5.45518589 epoch total loss 6.21619463\n",
      "Trained batch 8750 batch loss 5.61445713 epoch total loss 6.21612597\n",
      "Trained batch 8751 batch loss 4.58805752 epoch total loss 6.21594\n",
      "Trained batch 8752 batch loss 4.68597221 epoch total loss 6.21576548\n",
      "Trained batch 8753 batch loss 4.373631 epoch total loss 6.21555519\n",
      "Trained batch 8754 batch loss 4.83088398 epoch total loss 6.21539688\n",
      "Trained batch 8755 batch loss 4.88753319 epoch total loss 6.21524525\n",
      "Trained batch 8756 batch loss 5.10514975 epoch total loss 6.21511841\n",
      "Trained batch 8757 batch loss 5.26350498 epoch total loss 6.21500969\n",
      "Trained batch 8758 batch loss 5.80089092 epoch total loss 6.21496248\n",
      "Trained batch 8759 batch loss 5.01052284 epoch total loss 6.21482515\n",
      "Trained batch 8760 batch loss 4.7063055 epoch total loss 6.21465302\n",
      "Trained batch 8761 batch loss 4.82631969 epoch total loss 6.21449471\n",
      "Trained batch 8762 batch loss 4.74831533 epoch total loss 6.21432734\n",
      "Trained batch 8763 batch loss 5.1901269 epoch total loss 6.21421099\n",
      "Trained batch 8764 batch loss 4.94345379 epoch total loss 6.21406603\n",
      "Trained batch 8765 batch loss 4.18810844 epoch total loss 6.21383476\n",
      "Trained batch 8766 batch loss 4.78613377 epoch total loss 6.21367168\n",
      "Trained batch 8767 batch loss 5.39528751 epoch total loss 6.21357822\n",
      "Trained batch 8768 batch loss 4.95626926 epoch total loss 6.21343517\n",
      "Trained batch 8769 batch loss 4.48275518 epoch total loss 6.21323776\n",
      "Trained batch 8770 batch loss 4.31857872 epoch total loss 6.21302223\n",
      "Trained batch 8771 batch loss 4.68963814 epoch total loss 6.21284866\n",
      "Trained batch 8772 batch loss 5.02938652 epoch total loss 6.21271372\n",
      "Trained batch 8773 batch loss 5.29595661 epoch total loss 6.21260929\n",
      "Trained batch 8774 batch loss 6.37490654 epoch total loss 6.21262789\n",
      "Trained batch 8775 batch loss 6.88936424 epoch total loss 6.21270514\n",
      "Trained batch 8776 batch loss 5.73505497 epoch total loss 6.21265078\n",
      "Trained batch 8777 batch loss 5.89417315 epoch total loss 6.21261454\n",
      "Trained batch 8778 batch loss 5.19660187 epoch total loss 6.21249866\n",
      "Trained batch 8779 batch loss 5.97819042 epoch total loss 6.21247149\n",
      "Trained batch 8780 batch loss 6.31781101 epoch total loss 6.21248341\n",
      "Trained batch 8781 batch loss 5.05097723 epoch total loss 6.21235132\n",
      "Trained batch 8782 batch loss 5.62808228 epoch total loss 6.21228504\n",
      "Trained batch 8783 batch loss 6.35083 epoch total loss 6.21230078\n",
      "Trained batch 8784 batch loss 6.25064564 epoch total loss 6.21230507\n",
      "Trained batch 8785 batch loss 6.63214588 epoch total loss 6.21235275\n",
      "Trained batch 8786 batch loss 4.75915718 epoch total loss 6.21218729\n",
      "Trained batch 8787 batch loss 6.07743597 epoch total loss 6.21217203\n",
      "Trained batch 8788 batch loss 4.6570611 epoch total loss 6.21199512\n",
      "Trained batch 8789 batch loss 5.01331329 epoch total loss 6.21185827\n",
      "Trained batch 8790 batch loss 4.88959742 epoch total loss 6.21170807\n",
      "Trained batch 8791 batch loss 4.77097225 epoch total loss 6.21154404\n",
      "Trained batch 8792 batch loss 4.74021816 epoch total loss 6.21137667\n",
      "Trained batch 8793 batch loss 6.0885973 epoch total loss 6.21136284\n",
      "Trained batch 8794 batch loss 5.08239126 epoch total loss 6.21123409\n",
      "Trained batch 8795 batch loss 5.16633034 epoch total loss 6.21111584\n",
      "Trained batch 8796 batch loss 5.15009 epoch total loss 6.21099472\n",
      "Trained batch 8797 batch loss 5.40551853 epoch total loss 6.21090317\n",
      "Trained batch 8798 batch loss 6.0272522 epoch total loss 6.21088266\n",
      "Trained batch 8799 batch loss 6.0064764 epoch total loss 6.2108593\n",
      "Trained batch 8800 batch loss 5.89839268 epoch total loss 6.21082401\n",
      "Trained batch 8801 batch loss 6.04909325 epoch total loss 6.21080589\n",
      "Trained batch 8802 batch loss 6.29012108 epoch total loss 6.21081448\n",
      "Trained batch 8803 batch loss 6.26286 epoch total loss 6.21082\n",
      "Trained batch 8804 batch loss 6.19853783 epoch total loss 6.21081924\n",
      "Trained batch 8805 batch loss 6.26652241 epoch total loss 6.21082544\n",
      "Trained batch 8806 batch loss 6.31268024 epoch total loss 6.21083689\n",
      "Trained batch 8807 batch loss 5.5764308 epoch total loss 6.21076488\n",
      "Trained batch 8808 batch loss 5.76308727 epoch total loss 6.21071386\n",
      "Trained batch 8809 batch loss 5.67395353 epoch total loss 6.21065331\n",
      "Trained batch 8810 batch loss 5.12895298 epoch total loss 6.21053028\n",
      "Trained batch 8811 batch loss 5.34469271 epoch total loss 6.21043205\n",
      "Trained batch 8812 batch loss 5.66111851 epoch total loss 6.21036959\n",
      "Trained batch 8813 batch loss 5.77430344 epoch total loss 6.21032\n",
      "Trained batch 8814 batch loss 5.9681077 epoch total loss 6.21029282\n",
      "Trained batch 8815 batch loss 5.72691965 epoch total loss 6.21023798\n",
      "Trained batch 8816 batch loss 5.83244038 epoch total loss 6.21019506\n",
      "Trained batch 8817 batch loss 5.29631519 epoch total loss 6.21009111\n",
      "Trained batch 8818 batch loss 5.81143141 epoch total loss 6.21004629\n",
      "Trained batch 8819 batch loss 5.79741764 epoch total loss 6.20999956\n",
      "Trained batch 8820 batch loss 5.62294388 epoch total loss 6.2099328\n",
      "Trained batch 8821 batch loss 5.93071365 epoch total loss 6.20990086\n",
      "Trained batch 8822 batch loss 6.42597103 epoch total loss 6.20992517\n",
      "Trained batch 8823 batch loss 6.15900326 epoch total loss 6.20991945\n",
      "Trained batch 8824 batch loss 4.74527836 epoch total loss 6.20975399\n",
      "Trained batch 8825 batch loss 5.83210754 epoch total loss 6.20971107\n",
      "Trained batch 8826 batch loss 4.57111645 epoch total loss 6.20952511\n",
      "Trained batch 8827 batch loss 5.02057457 epoch total loss 6.20939\n",
      "Trained batch 8828 batch loss 5.39811754 epoch total loss 6.20929861\n",
      "Trained batch 8829 batch loss 5.54422092 epoch total loss 6.20922327\n",
      "Trained batch 8830 batch loss 5.52911663 epoch total loss 6.20914602\n",
      "Trained batch 8831 batch loss 5.383605 epoch total loss 6.20905209\n",
      "Trained batch 8832 batch loss 6.57675457 epoch total loss 6.20909405\n",
      "Trained batch 8833 batch loss 4.91178608 epoch total loss 6.20894718\n",
      "Trained batch 8834 batch loss 5.44117928 epoch total loss 6.20886\n",
      "Trained batch 8835 batch loss 4.91877556 epoch total loss 6.20871401\n",
      "Trained batch 8836 batch loss 4.91934252 epoch total loss 6.2085681\n",
      "Trained batch 8837 batch loss 5.25052309 epoch total loss 6.20845938\n",
      "Trained batch 8838 batch loss 5.49816418 epoch total loss 6.20837927\n",
      "Trained batch 8839 batch loss 5.64388609 epoch total loss 6.20831537\n",
      "Trained batch 8840 batch loss 6.85646248 epoch total loss 6.20838881\n",
      "Trained batch 8841 batch loss 6.93925047 epoch total loss 6.2084713\n",
      "Trained batch 8842 batch loss 6.38675404 epoch total loss 6.20849133\n",
      "Trained batch 8843 batch loss 6.09925175 epoch total loss 6.20847893\n",
      "Trained batch 8844 batch loss 5.86135244 epoch total loss 6.20844\n",
      "Trained batch 8845 batch loss 6.33810902 epoch total loss 6.20845461\n",
      "Trained batch 8846 batch loss 6.34533 epoch total loss 6.20847\n",
      "Trained batch 8847 batch loss 6.2152977 epoch total loss 6.20847082\n",
      "Trained batch 8848 batch loss 6.84265137 epoch total loss 6.20854235\n",
      "Trained batch 8849 batch loss 6.63927746 epoch total loss 6.20859146\n",
      "Trained batch 8850 batch loss 6.4793334 epoch total loss 6.20862198\n",
      "Trained batch 8851 batch loss 6.12026167 epoch total loss 6.20861197\n",
      "Trained batch 8852 batch loss 6.34666777 epoch total loss 6.2086277\n",
      "Trained batch 8853 batch loss 5.73261356 epoch total loss 6.2085743\n",
      "Trained batch 8854 batch loss 6.02772617 epoch total loss 6.20855379\n",
      "Trained batch 8855 batch loss 6.61659431 epoch total loss 6.2086\n",
      "Trained batch 8856 batch loss 6.57420063 epoch total loss 6.20864105\n",
      "Trained batch 8857 batch loss 6.2540803 epoch total loss 6.2086463\n",
      "Trained batch 8858 batch loss 6.21980667 epoch total loss 6.20864725\n",
      "Trained batch 8859 batch loss 6.22140408 epoch total loss 6.20864916\n",
      "Trained batch 8860 batch loss 6.54861641 epoch total loss 6.20868731\n",
      "Trained batch 8861 batch loss 6.67183 epoch total loss 6.20873928\n",
      "Trained batch 8862 batch loss 6.3339777 epoch total loss 6.20875359\n",
      "Trained batch 8863 batch loss 6.64687347 epoch total loss 6.20880318\n",
      "Trained batch 8864 batch loss 6.62411404 epoch total loss 6.20885\n",
      "Trained batch 8865 batch loss 6.56725264 epoch total loss 6.20889044\n",
      "Trained batch 8866 batch loss 6.05584192 epoch total loss 6.2088728\n",
      "Trained batch 8867 batch loss 5.865345 epoch total loss 6.20883417\n",
      "Trained batch 8868 batch loss 4.95842 epoch total loss 6.20869303\n",
      "Trained batch 8869 batch loss 6.14616871 epoch total loss 6.20868587\n",
      "Trained batch 8870 batch loss 5.8929534 epoch total loss 6.20865059\n",
      "Trained batch 8871 batch loss 6.30714798 epoch total loss 6.20866203\n",
      "Trained batch 8872 batch loss 6.03588295 epoch total loss 6.20864248\n",
      "Trained batch 8873 batch loss 6.0870719 epoch total loss 6.20862865\n",
      "Trained batch 8874 batch loss 5.74305439 epoch total loss 6.20857573\n",
      "Trained batch 8875 batch loss 6.12686062 epoch total loss 6.20856667\n",
      "Trained batch 8876 batch loss 5.84983635 epoch total loss 6.20852613\n",
      "Trained batch 8877 batch loss 5.96235847 epoch total loss 6.20849848\n",
      "Trained batch 8878 batch loss 5.51376963 epoch total loss 6.20842028\n",
      "Trained batch 8879 batch loss 5.6242795 epoch total loss 6.20835447\n",
      "Trained batch 8880 batch loss 6.18350458 epoch total loss 6.20835161\n",
      "Trained batch 8881 batch loss 4.87013388 epoch total loss 6.20820141\n",
      "Trained batch 8882 batch loss 6.43040848 epoch total loss 6.2082262\n",
      "Trained batch 8883 batch loss 5.20800304 epoch total loss 6.20811367\n",
      "Trained batch 8884 batch loss 5.63267899 epoch total loss 6.20804882\n",
      "Trained batch 8885 batch loss 6.04335 epoch total loss 6.20803\n",
      "Trained batch 8886 batch loss 5.9290905 epoch total loss 6.20799875\n",
      "Trained batch 8887 batch loss 6.4723959 epoch total loss 6.20802879\n",
      "Trained batch 8888 batch loss 6.22432899 epoch total loss 6.20803\n",
      "Trained batch 8889 batch loss 6.73166656 epoch total loss 6.20808887\n",
      "Trained batch 8890 batch loss 5.36794853 epoch total loss 6.20799446\n",
      "Trained batch 8891 batch loss 6.16340208 epoch total loss 6.20798969\n",
      "Trained batch 8892 batch loss 6.18775272 epoch total loss 6.20798731\n",
      "Trained batch 8893 batch loss 6.59861 epoch total loss 6.20803118\n",
      "Trained batch 8894 batch loss 6.19527912 epoch total loss 6.20802975\n",
      "Trained batch 8895 batch loss 6.00951195 epoch total loss 6.20800686\n",
      "Trained batch 8896 batch loss 6.19203854 epoch total loss 6.20800495\n",
      "Trained batch 8897 batch loss 5.79986811 epoch total loss 6.20795918\n",
      "Trained batch 8898 batch loss 6.01586914 epoch total loss 6.20793772\n",
      "Trained batch 8899 batch loss 5.80512524 epoch total loss 6.20789242\n",
      "Trained batch 8900 batch loss 5.6313324 epoch total loss 6.20782804\n",
      "Trained batch 8901 batch loss 5.656744 epoch total loss 6.20776606\n",
      "Trained batch 8902 batch loss 5.78484631 epoch total loss 6.20771837\n",
      "Trained batch 8903 batch loss 5.86322403 epoch total loss 6.20767975\n",
      "Trained batch 8904 batch loss 5.42845917 epoch total loss 6.20759249\n",
      "Trained batch 8905 batch loss 5.69234467 epoch total loss 6.20753431\n",
      "Trained batch 8906 batch loss 5.61577892 epoch total loss 6.20746803\n",
      "Trained batch 8907 batch loss 5.67393398 epoch total loss 6.20740843\n",
      "Trained batch 8908 batch loss 6.21504116 epoch total loss 6.20740938\n",
      "Trained batch 8909 batch loss 5.97938 epoch total loss 6.20738363\n",
      "Trained batch 8910 batch loss 5.70837641 epoch total loss 6.20732784\n",
      "Trained batch 8911 batch loss 5.6222496 epoch total loss 6.20726204\n",
      "Trained batch 8912 batch loss 5.10068607 epoch total loss 6.20713758\n",
      "Trained batch 8913 batch loss 4.12011623 epoch total loss 6.20690393\n",
      "Trained batch 8914 batch loss 6.30543089 epoch total loss 6.2069149\n",
      "Trained batch 8915 batch loss 5.82618237 epoch total loss 6.20687199\n",
      "Trained batch 8916 batch loss 6.72146893 epoch total loss 6.20693\n",
      "Trained batch 8917 batch loss 6.35150433 epoch total loss 6.20694637\n",
      "Trained batch 8918 batch loss 5.95126295 epoch total loss 6.20691776\n",
      "Trained batch 8919 batch loss 6.60778332 epoch total loss 6.20696306\n",
      "Trained batch 8920 batch loss 6.12997341 epoch total loss 6.206954\n",
      "Trained batch 8921 batch loss 6.62055349 epoch total loss 6.20700073\n",
      "Trained batch 8922 batch loss 5.60128689 epoch total loss 6.20693254\n",
      "Trained batch 8923 batch loss 5.83471966 epoch total loss 6.20689106\n",
      "Trained batch 8924 batch loss 5.52071953 epoch total loss 6.20681429\n",
      "Trained batch 8925 batch loss 4.88725758 epoch total loss 6.20666599\n",
      "Trained batch 8926 batch loss 6.51812077 epoch total loss 6.20670128\n",
      "Trained batch 8927 batch loss 5.93352509 epoch total loss 6.20667076\n",
      "Trained batch 8928 batch loss 5.75456238 epoch total loss 6.20661974\n",
      "Trained batch 8929 batch loss 5.99548197 epoch total loss 6.20659637\n",
      "Trained batch 8930 batch loss 5.79628468 epoch total loss 6.2065506\n",
      "Trained batch 8931 batch loss 5.98435402 epoch total loss 6.2065258\n",
      "Trained batch 8932 batch loss 5.44781399 epoch total loss 6.20644093\n",
      "Trained batch 8933 batch loss 5.27364206 epoch total loss 6.2063365\n",
      "Trained batch 8934 batch loss 6.08392429 epoch total loss 6.20632267\n",
      "Trained batch 8935 batch loss 5.35700893 epoch total loss 6.2062273\n",
      "Trained batch 8936 batch loss 5.31088448 epoch total loss 6.20612717\n",
      "Trained batch 8937 batch loss 5.95570564 epoch total loss 6.20609951\n",
      "Trained batch 8938 batch loss 6.11728382 epoch total loss 6.2060895\n",
      "Trained batch 8939 batch loss 5.84360123 epoch total loss 6.20604897\n",
      "Trained batch 8940 batch loss 6.43234682 epoch total loss 6.20607424\n",
      "Trained batch 8941 batch loss 5.98399448 epoch total loss 6.20604944\n",
      "Trained batch 8942 batch loss 5.82428694 epoch total loss 6.206007\n",
      "Trained batch 8943 batch loss 5.87644863 epoch total loss 6.20597\n",
      "Trained batch 8944 batch loss 5.57297802 epoch total loss 6.20589924\n",
      "Trained batch 8945 batch loss 6.09509706 epoch total loss 6.20588684\n",
      "Trained batch 8946 batch loss 4.74582958 epoch total loss 6.20572329\n",
      "Trained batch 8947 batch loss 6.2307272 epoch total loss 6.20572615\n",
      "Trained batch 8948 batch loss 4.5723 epoch total loss 6.20554399\n",
      "Trained batch 8949 batch loss 6.03906584 epoch total loss 6.2055254\n",
      "Trained batch 8950 batch loss 5.14858294 epoch total loss 6.20540714\n",
      "Trained batch 8951 batch loss 5.4667244 epoch total loss 6.20532465\n",
      "Trained batch 8952 batch loss 4.12023354 epoch total loss 6.20509148\n",
      "Trained batch 8953 batch loss 5.59623241 epoch total loss 6.20502377\n",
      "Trained batch 8954 batch loss 6.65459442 epoch total loss 6.20507431\n",
      "Trained batch 8955 batch loss 6.28819895 epoch total loss 6.20508337\n",
      "Trained batch 8956 batch loss 5.7515893 epoch total loss 6.20503283\n",
      "Trained batch 8957 batch loss 6.67194796 epoch total loss 6.2050848\n",
      "Trained batch 8958 batch loss 6.23241 epoch total loss 6.20508766\n",
      "Trained batch 8959 batch loss 5.85957 epoch total loss 6.20504904\n",
      "Trained batch 8960 batch loss 6.1858573 epoch total loss 6.20504713\n",
      "Trained batch 8961 batch loss 6.147995 epoch total loss 6.20504093\n",
      "Trained batch 8962 batch loss 6.38975954 epoch total loss 6.20506144\n",
      "Trained batch 8963 batch loss 7.29773426 epoch total loss 6.20518351\n",
      "Trained batch 8964 batch loss 5.05846405 epoch total loss 6.20505524\n",
      "Trained batch 8965 batch loss 6.88132715 epoch total loss 6.20513105\n",
      "Trained batch 8966 batch loss 7.28234529 epoch total loss 6.20525122\n",
      "Trained batch 8967 batch loss 7.20596504 epoch total loss 6.2053628\n",
      "Trained batch 8968 batch loss 7.39333296 epoch total loss 6.20549536\n",
      "Trained batch 8969 batch loss 6.71419048 epoch total loss 6.2055521\n",
      "Trained batch 8970 batch loss 5.77350855 epoch total loss 6.20550394\n",
      "Trained batch 8971 batch loss 6.10244 epoch total loss 6.2054925\n",
      "Trained batch 8972 batch loss 4.88288307 epoch total loss 6.20534515\n",
      "Trained batch 8973 batch loss 6.23604393 epoch total loss 6.20534801\n",
      "Trained batch 8974 batch loss 5.91961527 epoch total loss 6.20531607\n",
      "Trained batch 8975 batch loss 6.21121216 epoch total loss 6.20531702\n",
      "Trained batch 8976 batch loss 6.26629162 epoch total loss 6.2053237\n",
      "Trained batch 8977 batch loss 6.46000671 epoch total loss 6.20535183\n",
      "Trained batch 8978 batch loss 5.72926855 epoch total loss 6.20529938\n",
      "Trained batch 8979 batch loss 5.64600372 epoch total loss 6.20523691\n",
      "Trained batch 8980 batch loss 5.8716464 epoch total loss 6.20519972\n",
      "Trained batch 8981 batch loss 5.77299452 epoch total loss 6.20515156\n",
      "Trained batch 8982 batch loss 5.80248308 epoch total loss 6.20510626\n",
      "Trained batch 8983 batch loss 6.4602232 epoch total loss 6.20513487\n",
      "Trained batch 8984 batch loss 6.33426952 epoch total loss 6.20514965\n",
      "Trained batch 8985 batch loss 6.26968718 epoch total loss 6.2051568\n",
      "Trained batch 8986 batch loss 6.28133297 epoch total loss 6.20516491\n",
      "Trained batch 8987 batch loss 6.22518253 epoch total loss 6.20516729\n",
      "Trained batch 8988 batch loss 6.24155188 epoch total loss 6.20517159\n",
      "Trained batch 8989 batch loss 6.27536201 epoch total loss 6.20517921\n",
      "Trained batch 8990 batch loss 6.06855679 epoch total loss 6.20516396\n",
      "Trained batch 8991 batch loss 5.2268796 epoch total loss 6.20505524\n",
      "Trained batch 8992 batch loss 5.29366255 epoch total loss 6.20495367\n",
      "Trained batch 8993 batch loss 5.99520874 epoch total loss 6.20493078\n",
      "Trained batch 8994 batch loss 5.05677366 epoch total loss 6.20480299\n",
      "Trained batch 8995 batch loss 5.05326462 epoch total loss 6.2046752\n",
      "Trained batch 8996 batch loss 4.96335697 epoch total loss 6.20453739\n",
      "Trained batch 8997 batch loss 4.89887142 epoch total loss 6.20439243\n",
      "Trained batch 8998 batch loss 4.94149113 epoch total loss 6.20425177\n",
      "Trained batch 8999 batch loss 6.23906565 epoch total loss 6.20425558\n",
      "Trained batch 9000 batch loss 5.87221622 epoch total loss 6.20421886\n",
      "Trained batch 9001 batch loss 5.77244234 epoch total loss 6.2041707\n",
      "Trained batch 9002 batch loss 5.81702 epoch total loss 6.20412779\n",
      "Trained batch 9003 batch loss 6.13803768 epoch total loss 6.20412\n",
      "Trained batch 9004 batch loss 5.89831257 epoch total loss 6.2040863\n",
      "Trained batch 9005 batch loss 5.59877634 epoch total loss 6.20401907\n",
      "Trained batch 9006 batch loss 6.06614971 epoch total loss 6.20400381\n",
      "Trained batch 9007 batch loss 5.71853113 epoch total loss 6.20395\n",
      "Trained batch 9008 batch loss 5.91699457 epoch total loss 6.20391798\n",
      "Trained batch 9009 batch loss 6.10572195 epoch total loss 6.20390701\n",
      "Trained batch 9010 batch loss 5.65663052 epoch total loss 6.20384645\n",
      "Trained batch 9011 batch loss 5.6721468 epoch total loss 6.20378733\n",
      "Trained batch 9012 batch loss 5.82070065 epoch total loss 6.20374489\n",
      "Trained batch 9013 batch loss 5.4791894 epoch total loss 6.20366478\n",
      "Trained batch 9014 batch loss 5.00499725 epoch total loss 6.20353127\n",
      "Trained batch 9015 batch loss 5.8289814 epoch total loss 6.20349\n",
      "Trained batch 9016 batch loss 6.78929043 epoch total loss 6.20355463\n",
      "Trained batch 9017 batch loss 6.41736889 epoch total loss 6.20357847\n",
      "Trained batch 9018 batch loss 6.1274 epoch total loss 6.20357037\n",
      "Trained batch 9019 batch loss 6.52501869 epoch total loss 6.20360565\n",
      "Trained batch 9020 batch loss 5.22838783 epoch total loss 6.20349741\n",
      "Trained batch 9021 batch loss 4.71928 epoch total loss 6.2033329\n",
      "Trained batch 9022 batch loss 5.42022 epoch total loss 6.20324612\n",
      "Trained batch 9023 batch loss 5.41727066 epoch total loss 6.20315933\n",
      "Trained batch 9024 batch loss 5.89780617 epoch total loss 6.20312548\n",
      "Trained batch 9025 batch loss 5.70896578 epoch total loss 6.20307064\n",
      "Trained batch 9026 batch loss 6.79311037 epoch total loss 6.20313597\n",
      "Trained batch 9027 batch loss 7.1892643 epoch total loss 6.20324469\n",
      "Trained batch 9028 batch loss 7.10888672 epoch total loss 6.2033453\n",
      "Trained batch 9029 batch loss 5.15488815 epoch total loss 6.20322943\n",
      "Trained batch 9030 batch loss 6.62741566 epoch total loss 6.20327663\n",
      "Trained batch 9031 batch loss 6.61861515 epoch total loss 6.20332241\n",
      "Trained batch 9032 batch loss 6.75340366 epoch total loss 6.20338297\n",
      "Trained batch 9033 batch loss 6.69542503 epoch total loss 6.20343781\n",
      "Trained batch 9034 batch loss 4.49473333 epoch total loss 6.2032485\n",
      "Trained batch 9035 batch loss 5.53981924 epoch total loss 6.20317507\n",
      "Trained batch 9036 batch loss 6.05392647 epoch total loss 6.20315886\n",
      "Trained batch 9037 batch loss 6.0069375 epoch total loss 6.20313692\n",
      "Trained batch 9038 batch loss 5.66027164 epoch total loss 6.20307684\n",
      "Trained batch 9039 batch loss 6.36319065 epoch total loss 6.20309496\n",
      "Trained batch 9040 batch loss 6.66402626 epoch total loss 6.2031455\n",
      "Trained batch 9041 batch loss 6.09956551 epoch total loss 6.20313406\n",
      "Trained batch 9042 batch loss 6.30357933 epoch total loss 6.2031455\n",
      "Trained batch 9043 batch loss 6.55752754 epoch total loss 6.2031846\n",
      "Trained batch 9044 batch loss 5.77889 epoch total loss 6.2031374\n",
      "Trained batch 9045 batch loss 5.38070822 epoch total loss 6.20304632\n",
      "Trained batch 9046 batch loss 5.57709455 epoch total loss 6.20297718\n",
      "Trained batch 9047 batch loss 4.97656345 epoch total loss 6.20284176\n",
      "Trained batch 9048 batch loss 5.85269451 epoch total loss 6.20280313\n",
      "Trained batch 9049 batch loss 6.09189701 epoch total loss 6.20279074\n",
      "Trained batch 9050 batch loss 5.86741304 epoch total loss 6.20275402\n",
      "Trained batch 9051 batch loss 5.99751425 epoch total loss 6.20273113\n",
      "Trained batch 9052 batch loss 6.54821968 epoch total loss 6.2027688\n",
      "Trained batch 9053 batch loss 5.61178589 epoch total loss 6.20270395\n",
      "Trained batch 9054 batch loss 4.95338535 epoch total loss 6.20256567\n",
      "Trained batch 9055 batch loss 5.96921396 epoch total loss 6.20254\n",
      "Trained batch 9056 batch loss 5.95604324 epoch total loss 6.20251274\n",
      "Trained batch 9057 batch loss 6.04807758 epoch total loss 6.20249557\n",
      "Trained batch 9058 batch loss 6.26237488 epoch total loss 6.20250225\n",
      "Trained batch 9059 batch loss 5.68882704 epoch total loss 6.20244551\n",
      "Trained batch 9060 batch loss 5.61506176 epoch total loss 6.20238\n",
      "Trained batch 9061 batch loss 5.54979324 epoch total loss 6.20230865\n",
      "Trained batch 9062 batch loss 4.97106934 epoch total loss 6.20217276\n",
      "Trained batch 9063 batch loss 5.9163723 epoch total loss 6.20214128\n",
      "Trained batch 9064 batch loss 4.84189081 epoch total loss 6.20199156\n",
      "Trained batch 9065 batch loss 5.92576933 epoch total loss 6.20196104\n",
      "Trained batch 9066 batch loss 5.82059765 epoch total loss 6.20191908\n",
      "Trained batch 9067 batch loss 6.50529432 epoch total loss 6.20195246\n",
      "Trained batch 9068 batch loss 6.25674534 epoch total loss 6.20195866\n",
      "Trained batch 9069 batch loss 6.46532822 epoch total loss 6.20198727\n",
      "Trained batch 9070 batch loss 5.9205637 epoch total loss 6.20195675\n",
      "Trained batch 9071 batch loss 6.00532532 epoch total loss 6.20193481\n",
      "Trained batch 9072 batch loss 6.72298717 epoch total loss 6.20199203\n",
      "Trained batch 9073 batch loss 6.58720112 epoch total loss 6.20203447\n",
      "Trained batch 9074 batch loss 5.86000824 epoch total loss 6.2019968\n",
      "Trained batch 9075 batch loss 6.19529724 epoch total loss 6.20199585\n",
      "Trained batch 9076 batch loss 5.64859962 epoch total loss 6.20193481\n",
      "Trained batch 9077 batch loss 4.6781311 epoch total loss 6.20176744\n",
      "Trained batch 9078 batch loss 5.9418726 epoch total loss 6.20173836\n",
      "Trained batch 9079 batch loss 5.89868641 epoch total loss 6.20170498\n",
      "Trained batch 9080 batch loss 5.51832676 epoch total loss 6.20163\n",
      "Trained batch 9081 batch loss 6.1249361 epoch total loss 6.20162153\n",
      "Trained batch 9082 batch loss 5.2543807 epoch total loss 6.20151711\n",
      "Trained batch 9083 batch loss 6.2446909 epoch total loss 6.20152235\n",
      "Trained batch 9084 batch loss 4.83700275 epoch total loss 6.20137167\n",
      "Trained batch 9085 batch loss 5.66416454 epoch total loss 6.20131254\n",
      "Trained batch 9086 batch loss 5.67931271 epoch total loss 6.20125532\n",
      "Trained batch 9087 batch loss 6.22880173 epoch total loss 6.20125866\n",
      "Trained batch 9088 batch loss 5.57487154 epoch total loss 6.20118952\n",
      "Trained batch 9089 batch loss 6.25976276 epoch total loss 6.20119572\n",
      "Trained batch 9090 batch loss 6.09114456 epoch total loss 6.20118332\n",
      "Trained batch 9091 batch loss 4.87555647 epoch total loss 6.20103741\n",
      "Trained batch 9092 batch loss 5.66262341 epoch total loss 6.20097876\n",
      "Trained batch 9093 batch loss 5.69440937 epoch total loss 6.20092297\n",
      "Trained batch 9094 batch loss 5.50829124 epoch total loss 6.20084667\n",
      "Trained batch 9095 batch loss 6.10939312 epoch total loss 6.20083666\n",
      "Trained batch 9096 batch loss 6.26273918 epoch total loss 6.20084333\n",
      "Trained batch 9097 batch loss 4.88152313 epoch total loss 6.20069838\n",
      "Trained batch 9098 batch loss 5.86712933 epoch total loss 6.20066166\n",
      "Trained batch 9099 batch loss 6.27053642 epoch total loss 6.20066929\n",
      "Trained batch 9100 batch loss 6.0344677 epoch total loss 6.20065117\n",
      "Trained batch 9101 batch loss 5.50603104 epoch total loss 6.20057487\n",
      "Trained batch 9102 batch loss 5.59704304 epoch total loss 6.20050859\n",
      "Trained batch 9103 batch loss 5.61947346 epoch total loss 6.20044518\n",
      "Trained batch 9104 batch loss 6.73643351 epoch total loss 6.2005043\n",
      "Trained batch 9105 batch loss 6.29723549 epoch total loss 6.20051479\n",
      "Trained batch 9106 batch loss 5.81350327 epoch total loss 6.20047235\n",
      "Trained batch 9107 batch loss 6.14484882 epoch total loss 6.20046616\n",
      "Trained batch 9108 batch loss 5.79025555 epoch total loss 6.20042086\n",
      "Trained batch 9109 batch loss 5.92017269 epoch total loss 6.20039034\n",
      "Trained batch 9110 batch loss 6.74941921 epoch total loss 6.20045042\n",
      "Trained batch 9111 batch loss 6.21343327 epoch total loss 6.20045233\n",
      "Trained batch 9112 batch loss 5.85095692 epoch total loss 6.20041418\n",
      "Trained batch 9113 batch loss 4.83942938 epoch total loss 6.20026445\n",
      "Trained batch 9114 batch loss 5.94036627 epoch total loss 6.20023632\n",
      "Trained batch 9115 batch loss 5.66373348 epoch total loss 6.20017719\n",
      "Trained batch 9116 batch loss 5.17458439 epoch total loss 6.20006514\n",
      "Trained batch 9117 batch loss 4.99997711 epoch total loss 6.19993353\n",
      "Trained batch 9118 batch loss 6.86449766 epoch total loss 6.20000601\n",
      "Trained batch 9119 batch loss 6.60357761 epoch total loss 6.20005083\n",
      "Trained batch 9120 batch loss 5.91303635 epoch total loss 6.20001936\n",
      "Trained batch 9121 batch loss 6.62566948 epoch total loss 6.20006609\n",
      "Trained batch 9122 batch loss 6.85311031 epoch total loss 6.20013714\n",
      "Trained batch 9123 batch loss 6.44117832 epoch total loss 6.20016384\n",
      "Trained batch 9124 batch loss 6.32290173 epoch total loss 6.20017719\n",
      "Trained batch 9125 batch loss 6.66944313 epoch total loss 6.20022869\n",
      "Trained batch 9126 batch loss 6.27877903 epoch total loss 6.20023727\n",
      "Trained batch 9127 batch loss 6.80951071 epoch total loss 6.20030355\n",
      "Trained batch 9128 batch loss 5.45807266 epoch total loss 6.20022249\n",
      "Trained batch 9129 batch loss 6.76270676 epoch total loss 6.200284\n",
      "Trained batch 9130 batch loss 6.34466457 epoch total loss 6.20029926\n",
      "Trained batch 9131 batch loss 6.58734 epoch total loss 6.2003417\n",
      "Trained batch 9132 batch loss 6.48068619 epoch total loss 6.20037222\n",
      "Trained batch 9133 batch loss 6.50277233 epoch total loss 6.2004056\n",
      "Trained batch 9134 batch loss 6.13684654 epoch total loss 6.20039845\n",
      "Trained batch 9135 batch loss 5.754076 epoch total loss 6.20035\n",
      "Trained batch 9136 batch loss 4.91376066 epoch total loss 6.20020914\n",
      "Trained batch 9137 batch loss 4.99086332 epoch total loss 6.20007658\n",
      "Trained batch 9138 batch loss 5.2840538 epoch total loss 6.19997644\n",
      "Trained batch 9139 batch loss 5.15997505 epoch total loss 6.19986296\n",
      "Trained batch 9140 batch loss 5.3088727 epoch total loss 6.19976521\n",
      "Trained batch 9141 batch loss 4.78586674 epoch total loss 6.19961071\n",
      "Trained batch 9142 batch loss 5.04074478 epoch total loss 6.19948387\n",
      "Trained batch 9143 batch loss 5.96871662 epoch total loss 6.1994586\n",
      "Trained batch 9144 batch loss 6.05752 epoch total loss 6.19944286\n",
      "Trained batch 9145 batch loss 6.54163647 epoch total loss 6.19948053\n",
      "Trained batch 9146 batch loss 6.09008026 epoch total loss 6.19946861\n",
      "Trained batch 9147 batch loss 6.23009396 epoch total loss 6.19947195\n",
      "Trained batch 9148 batch loss 6.13110161 epoch total loss 6.1994648\n",
      "Trained batch 9149 batch loss 5.90399 epoch total loss 6.19943237\n",
      "Trained batch 9150 batch loss 6.06295872 epoch total loss 6.19941711\n",
      "Trained batch 9151 batch loss 6.92514467 epoch total loss 6.19949675\n",
      "Trained batch 9152 batch loss 6.16979694 epoch total loss 6.19949341\n",
      "Trained batch 9153 batch loss 6.22280788 epoch total loss 6.19949579\n",
      "Trained batch 9154 batch loss 6.30375719 epoch total loss 6.19950724\n",
      "Trained batch 9155 batch loss 6.17797947 epoch total loss 6.19950485\n",
      "Trained batch 9156 batch loss 6.02421665 epoch total loss 6.19948578\n",
      "Trained batch 9157 batch loss 6.18840218 epoch total loss 6.19948435\n",
      "Trained batch 9158 batch loss 6.21043205 epoch total loss 6.19948578\n",
      "Trained batch 9159 batch loss 6.38614225 epoch total loss 6.19950628\n",
      "Trained batch 9160 batch loss 6.39672136 epoch total loss 6.19952774\n",
      "Trained batch 9161 batch loss 6.2909708 epoch total loss 6.19953775\n",
      "Trained batch 9162 batch loss 6.23479033 epoch total loss 6.19954157\n",
      "Trained batch 9163 batch loss 6.30749416 epoch total loss 6.19955349\n",
      "Trained batch 9164 batch loss 6.04659 epoch total loss 6.1995368\n",
      "Trained batch 9165 batch loss 5.85239887 epoch total loss 6.19949865\n",
      "Trained batch 9166 batch loss 5.69359827 epoch total loss 6.19944382\n",
      "Trained batch 9167 batch loss 5.73557663 epoch total loss 6.1993928\n",
      "Trained batch 9168 batch loss 6.14343786 epoch total loss 6.19938707\n",
      "Trained batch 9169 batch loss 6.37314701 epoch total loss 6.19940615\n",
      "Trained batch 9170 batch loss 6.00283957 epoch total loss 6.19938469\n",
      "Trained batch 9171 batch loss 6.42957354 epoch total loss 6.19941\n",
      "Trained batch 9172 batch loss 6.60196352 epoch total loss 6.19945383\n",
      "Trained batch 9173 batch loss 6.23481369 epoch total loss 6.19945765\n",
      "Trained batch 9174 batch loss 6.17175198 epoch total loss 6.19945478\n",
      "Trained batch 9175 batch loss 6.06303501 epoch total loss 6.19943953\n",
      "Trained batch 9176 batch loss 6.05303097 epoch total loss 6.19942379\n",
      "Trained batch 9177 batch loss 6.32666779 epoch total loss 6.1994381\n",
      "Trained batch 9178 batch loss 6.11460304 epoch total loss 6.19942856\n",
      "Trained batch 9179 batch loss 6.35707 epoch total loss 6.19944572\n",
      "Trained batch 9180 batch loss 6.09704542 epoch total loss 6.19943428\n",
      "Trained batch 9181 batch loss 6.23852253 epoch total loss 6.19943857\n",
      "Trained batch 9182 batch loss 6.22753811 epoch total loss 6.19944191\n",
      "Trained batch 9183 batch loss 5.77948236 epoch total loss 6.19939613\n",
      "Trained batch 9184 batch loss 5.56322861 epoch total loss 6.19932699\n",
      "Trained batch 9185 batch loss 4.8342905 epoch total loss 6.19917822\n",
      "Trained batch 9186 batch loss 5.05142832 epoch total loss 6.19905329\n",
      "Trained batch 9187 batch loss 4.56831 epoch total loss 6.19887543\n",
      "Trained batch 9188 batch loss 5.41165638 epoch total loss 6.1987896\n",
      "Trained batch 9189 batch loss 6.12555599 epoch total loss 6.19878197\n",
      "Trained batch 9190 batch loss 6.57863617 epoch total loss 6.19882298\n",
      "Trained batch 9191 batch loss 6.12543535 epoch total loss 6.19881487\n",
      "Trained batch 9192 batch loss 6.14511967 epoch total loss 6.19880915\n",
      "Trained batch 9193 batch loss 5.73588896 epoch total loss 6.1987586\n",
      "Trained batch 9194 batch loss 6.06234 epoch total loss 6.19874382\n",
      "Trained batch 9195 batch loss 6.12083292 epoch total loss 6.19873524\n",
      "Trained batch 9196 batch loss 5.97366714 epoch total loss 6.19871092\n",
      "Trained batch 9197 batch loss 5.23789358 epoch total loss 6.19860649\n",
      "Trained batch 9198 batch loss 5.9348073 epoch total loss 6.1985774\n",
      "Trained batch 9199 batch loss 5.08220196 epoch total loss 6.19845629\n",
      "Trained batch 9200 batch loss 5.61896896 epoch total loss 6.19839287\n",
      "Trained batch 9201 batch loss 6.19448185 epoch total loss 6.19839239\n",
      "Trained batch 9202 batch loss 6.35835075 epoch total loss 6.19841\n",
      "Trained batch 9203 batch loss 6.96959209 epoch total loss 6.19849396\n",
      "Trained batch 9204 batch loss 6.01253748 epoch total loss 6.19847345\n",
      "Trained batch 9205 batch loss 6.06619024 epoch total loss 6.19845915\n",
      "Trained batch 9206 batch loss 5.89160252 epoch total loss 6.19842577\n",
      "Trained batch 9207 batch loss 6.83326149 epoch total loss 6.19849443\n",
      "Trained batch 9208 batch loss 5.101408 epoch total loss 6.19837523\n",
      "Trained batch 9209 batch loss 6.81079674 epoch total loss 6.19844198\n",
      "Trained batch 9210 batch loss 5.91963959 epoch total loss 6.19841146\n",
      "Trained batch 9211 batch loss 4.07152462 epoch total loss 6.19818068\n",
      "Trained batch 9212 batch loss 4.41705322 epoch total loss 6.19798756\n",
      "Trained batch 9213 batch loss 4.20416737 epoch total loss 6.1977706\n",
      "Trained batch 9214 batch loss 4.72794104 epoch total loss 6.19761133\n",
      "Trained batch 9215 batch loss 5.59456301 epoch total loss 6.19754553\n",
      "Trained batch 9216 batch loss 6.81691504 epoch total loss 6.19761276\n",
      "Trained batch 9217 batch loss 5.66330814 epoch total loss 6.19755507\n",
      "Trained batch 9218 batch loss 6.97781754 epoch total loss 6.19763947\n",
      "Trained batch 9219 batch loss 5.89590311 epoch total loss 6.19760656\n",
      "Trained batch 9220 batch loss 5.02745104 epoch total loss 6.19747972\n",
      "Trained batch 9221 batch loss 5.42631435 epoch total loss 6.1973958\n",
      "Trained batch 9222 batch loss 5.7010994 epoch total loss 6.19734192\n",
      "Trained batch 9223 batch loss 5.55498505 epoch total loss 6.1972723\n",
      "Trained batch 9224 batch loss 6.39391804 epoch total loss 6.19729376\n",
      "Trained batch 9225 batch loss 7.11612415 epoch total loss 6.19739342\n",
      "Trained batch 9226 batch loss 7.67337322 epoch total loss 6.19755316\n",
      "Trained batch 9227 batch loss 4.83615208 epoch total loss 6.19740534\n",
      "Trained batch 9228 batch loss 6.24515343 epoch total loss 6.19741058\n",
      "Trained batch 9229 batch loss 5.12198305 epoch total loss 6.19729424\n",
      "Trained batch 9230 batch loss 5.08231783 epoch total loss 6.1971736\n",
      "Trained batch 9231 batch loss 6.17796326 epoch total loss 6.19717169\n",
      "Trained batch 9232 batch loss 5.29299688 epoch total loss 6.19707346\n",
      "Trained batch 9233 batch loss 6.6079793 epoch total loss 6.19711828\n",
      "Trained batch 9234 batch loss 5.91683626 epoch total loss 6.19708776\n",
      "Trained batch 9235 batch loss 5.29405594 epoch total loss 6.19699\n",
      "Trained batch 9236 batch loss 6.61874104 epoch total loss 6.19703531\n",
      "Trained batch 9237 batch loss 6.00767136 epoch total loss 6.19701529\n",
      "Trained batch 9238 batch loss 6.2821784 epoch total loss 6.19702435\n",
      "Trained batch 9239 batch loss 6.63873672 epoch total loss 6.19707203\n",
      "Trained batch 9240 batch loss 4.82018805 epoch total loss 6.19692326\n",
      "Trained batch 9241 batch loss 6.36920929 epoch total loss 6.19694185\n",
      "Trained batch 9242 batch loss 5.5096221 epoch total loss 6.19686747\n",
      "Trained batch 9243 batch loss 4.88764477 epoch total loss 6.19672585\n",
      "Trained batch 9244 batch loss 5.03233242 epoch total loss 6.19659948\n",
      "Trained batch 9245 batch loss 4.82314491 epoch total loss 6.19645119\n",
      "Trained batch 9246 batch loss 5.19879293 epoch total loss 6.19634342\n",
      "Trained batch 9247 batch loss 5.01116085 epoch total loss 6.19621515\n",
      "Trained batch 9248 batch loss 5.52315044 epoch total loss 6.19614267\n",
      "Trained batch 9249 batch loss 5.51131439 epoch total loss 6.19606829\n",
      "Trained batch 9250 batch loss 5.14060497 epoch total loss 6.19595432\n",
      "Trained batch 9251 batch loss 4.94549799 epoch total loss 6.19581938\n",
      "Trained batch 9252 batch loss 4.93913078 epoch total loss 6.195683\n",
      "Trained batch 9253 batch loss 5.30345297 epoch total loss 6.19558668\n",
      "Trained batch 9254 batch loss 4.57537365 epoch total loss 6.19541168\n",
      "Trained batch 9255 batch loss 5.95233202 epoch total loss 6.19538546\n",
      "Trained batch 9256 batch loss 5.58866692 epoch total loss 6.19532\n",
      "Trained batch 9257 batch loss 5.83613 epoch total loss 6.19528151\n",
      "Trained batch 9258 batch loss 5.34680939 epoch total loss 6.19519\n",
      "Trained batch 9259 batch loss 5.95784807 epoch total loss 6.1951642\n",
      "Trained batch 9260 batch loss 5.22789383 epoch total loss 6.1950593\n",
      "Trained batch 9261 batch loss 5.0483737 epoch total loss 6.19493532\n",
      "Trained batch 9262 batch loss 5.21758604 epoch total loss 6.19483\n",
      "Trained batch 9263 batch loss 6.38352299 epoch total loss 6.19485044\n",
      "Trained batch 9264 batch loss 5.94976711 epoch total loss 6.19482374\n",
      "Trained batch 9265 batch loss 5.34040308 epoch total loss 6.19473171\n",
      "Trained batch 9266 batch loss 7.17162418 epoch total loss 6.19483709\n",
      "Trained batch 9267 batch loss 6.22681761 epoch total loss 6.19484043\n",
      "Trained batch 9268 batch loss 5.8793149 epoch total loss 6.1948061\n",
      "Trained batch 9269 batch loss 4.96839905 epoch total loss 6.19467402\n",
      "Trained batch 9270 batch loss 3.76698303 epoch total loss 6.19441223\n",
      "Trained batch 9271 batch loss 5.68472147 epoch total loss 6.19435692\n",
      "Trained batch 9272 batch loss 6.44633675 epoch total loss 6.1943841\n",
      "Trained batch 9273 batch loss 4.76751518 epoch total loss 6.19423\n",
      "Trained batch 9274 batch loss 6.07781601 epoch total loss 6.19421721\n",
      "Trained batch 9275 batch loss 5.98081 epoch total loss 6.19419432\n",
      "Trained batch 9276 batch loss 6.47650051 epoch total loss 6.19422483\n",
      "Trained batch 9277 batch loss 6.26911259 epoch total loss 6.19423294\n",
      "Trained batch 9278 batch loss 6.00873947 epoch total loss 6.19421291\n",
      "Trained batch 9279 batch loss 6.0474844 epoch total loss 6.1941967\n",
      "Trained batch 9280 batch loss 5.63313293 epoch total loss 6.19413662\n",
      "Trained batch 9281 batch loss 6.22845364 epoch total loss 6.19414\n",
      "Trained batch 9282 batch loss 6.39528656 epoch total loss 6.19416142\n",
      "Trained batch 9283 batch loss 6.42191029 epoch total loss 6.19418621\n",
      "Trained batch 9284 batch loss 5.9263382 epoch total loss 6.19415712\n",
      "Trained batch 9285 batch loss 6.19703102 epoch total loss 6.19415712\n",
      "Trained batch 9286 batch loss 5.82364273 epoch total loss 6.19411755\n",
      "Trained batch 9287 batch loss 5.99670124 epoch total loss 6.19409609\n",
      "Trained batch 9288 batch loss 5.99914 epoch total loss 6.19407511\n",
      "Trained batch 9289 batch loss 5.93293905 epoch total loss 6.19404697\n",
      "Trained batch 9290 batch loss 5.95673275 epoch total loss 6.1940217\n",
      "Trained batch 9291 batch loss 5.93455029 epoch total loss 6.19399357\n",
      "Trained batch 9292 batch loss 5.99771 epoch total loss 6.19397211\n",
      "Trained batch 9293 batch loss 6.17145109 epoch total loss 6.19396973\n",
      "Trained batch 9294 batch loss 6.35094547 epoch total loss 6.19398689\n",
      "Trained batch 9295 batch loss 6.96223 epoch total loss 6.19406939\n",
      "Trained batch 9296 batch loss 6.19834328 epoch total loss 6.19407\n",
      "Trained batch 9297 batch loss 6.26639748 epoch total loss 6.19407749\n",
      "Trained batch 9298 batch loss 6.37808132 epoch total loss 6.19409752\n",
      "Trained batch 9299 batch loss 6.2273407 epoch total loss 6.19410086\n",
      "Trained batch 9300 batch loss 5.94667578 epoch total loss 6.19407415\n",
      "Trained batch 9301 batch loss 5.9859972 epoch total loss 6.19405174\n",
      "Trained batch 9302 batch loss 5.62331057 epoch total loss 6.19399071\n",
      "Trained batch 9303 batch loss 6.38765669 epoch total loss 6.19401121\n",
      "Trained batch 9304 batch loss 5.87891102 epoch total loss 6.19397736\n",
      "Trained batch 9305 batch loss 5.62962437 epoch total loss 6.1939168\n",
      "Trained batch 9306 batch loss 5.50653791 epoch total loss 6.19384289\n",
      "Trained batch 9307 batch loss 5.75075769 epoch total loss 6.1937952\n",
      "Trained batch 9308 batch loss 5.44744682 epoch total loss 6.1937151\n",
      "Trained batch 9309 batch loss 5.86892271 epoch total loss 6.19368029\n",
      "Trained batch 9310 batch loss 6.04481411 epoch total loss 6.19366407\n",
      "Trained batch 9311 batch loss 5.73350477 epoch total loss 6.19361448\n",
      "Trained batch 9312 batch loss 5.97300053 epoch total loss 6.19359112\n",
      "Trained batch 9313 batch loss 5.58119869 epoch total loss 6.19352531\n",
      "Trained batch 9314 batch loss 5.60789776 epoch total loss 6.19346237\n",
      "Trained batch 9315 batch loss 5.67558765 epoch total loss 6.19340706\n",
      "Trained batch 9316 batch loss 5.5746994 epoch total loss 6.1933403\n",
      "Trained batch 9317 batch loss 5.98768234 epoch total loss 6.19331837\n",
      "Trained batch 9318 batch loss 6.22701 epoch total loss 6.19332218\n",
      "Trained batch 9319 batch loss 5.77730942 epoch total loss 6.19327736\n",
      "Trained batch 9320 batch loss 5.93650627 epoch total loss 6.19325\n",
      "Trained batch 9321 batch loss 5.70228291 epoch total loss 6.19319725\n",
      "Trained batch 9322 batch loss 5.98682404 epoch total loss 6.19317532\n",
      "Trained batch 9323 batch loss 5.73768616 epoch total loss 6.19312668\n",
      "Trained batch 9324 batch loss 5.80859375 epoch total loss 6.19308519\n",
      "Trained batch 9325 batch loss 6.38187027 epoch total loss 6.1931057\n",
      "Trained batch 9326 batch loss 6.1837635 epoch total loss 6.19310474\n",
      "Trained batch 9327 batch loss 5.86449814 epoch total loss 6.19306946\n",
      "Trained batch 9328 batch loss 5.61866856 epoch total loss 6.19300747\n",
      "Trained batch 9329 batch loss 6.62458706 epoch total loss 6.19305372\n",
      "Trained batch 9330 batch loss 5.53238297 epoch total loss 6.19298315\n",
      "Trained batch 9331 batch loss 5.69922352 epoch total loss 6.19293\n",
      "Trained batch 9332 batch loss 5.99275 epoch total loss 6.19290876\n",
      "Trained batch 9333 batch loss 5.15173197 epoch total loss 6.19279718\n",
      "Trained batch 9334 batch loss 5.26589584 epoch total loss 6.19269753\n",
      "Trained batch 9335 batch loss 5.21115923 epoch total loss 6.19259262\n",
      "Trained batch 9336 batch loss 4.96756077 epoch total loss 6.19246149\n",
      "Trained batch 9337 batch loss 5.00741196 epoch total loss 6.19233465\n",
      "Trained batch 9338 batch loss 5.05756378 epoch total loss 6.19221306\n",
      "Trained batch 9339 batch loss 4.89644861 epoch total loss 6.1920743\n",
      "Trained batch 9340 batch loss 4.71120167 epoch total loss 6.19191551\n",
      "Trained batch 9341 batch loss 4.8779583 epoch total loss 6.19177532\n",
      "Trained batch 9342 batch loss 6.08594322 epoch total loss 6.19176388\n",
      "Trained batch 9343 batch loss 6.41658306 epoch total loss 6.1917882\n",
      "Trained batch 9344 batch loss 6.28548622 epoch total loss 6.19179773\n",
      "Trained batch 9345 batch loss 5.58339405 epoch total loss 6.19173288\n",
      "Trained batch 9346 batch loss 6.5128355 epoch total loss 6.19176674\n",
      "Trained batch 9347 batch loss 6.63392258 epoch total loss 6.19181395\n",
      "Trained batch 9348 batch loss 6.84088 epoch total loss 6.19188356\n",
      "Trained batch 9349 batch loss 6.09710169 epoch total loss 6.19187355\n",
      "Trained batch 9350 batch loss 6.21801567 epoch total loss 6.19187641\n",
      "Trained batch 9351 batch loss 6.16400719 epoch total loss 6.19187307\n",
      "Trained batch 9352 batch loss 6.28828335 epoch total loss 6.19188356\n",
      "Trained batch 9353 batch loss 6.41050959 epoch total loss 6.19190693\n",
      "Trained batch 9354 batch loss 6.3473525 epoch total loss 6.19192362\n",
      "Trained batch 9355 batch loss 6.04639626 epoch total loss 6.19190836\n",
      "Trained batch 9356 batch loss 5.90108776 epoch total loss 6.19187737\n",
      "Trained batch 9357 batch loss 6.29197502 epoch total loss 6.19188786\n",
      "Trained batch 9358 batch loss 6.22099304 epoch total loss 6.19189119\n",
      "Trained batch 9359 batch loss 6.88023281 epoch total loss 6.19196463\n",
      "Trained batch 9360 batch loss 6.36261 epoch total loss 6.19198322\n",
      "Trained batch 9361 batch loss 6.02284336 epoch total loss 6.1919651\n",
      "Trained batch 9362 batch loss 5.77716732 epoch total loss 6.19192076\n",
      "Trained batch 9363 batch loss 5.45247746 epoch total loss 6.1918416\n",
      "Trained batch 9364 batch loss 6.34135532 epoch total loss 6.19185781\n",
      "Trained batch 9365 batch loss 6.3156929 epoch total loss 6.19187069\n",
      "Trained batch 9366 batch loss 5.80201817 epoch total loss 6.1918292\n",
      "Trained batch 9367 batch loss 6.18812752 epoch total loss 6.19182873\n",
      "Trained batch 9368 batch loss 6.12033844 epoch total loss 6.1918211\n",
      "Trained batch 9369 batch loss 6.22371531 epoch total loss 6.19182444\n",
      "Trained batch 9370 batch loss 6.35472584 epoch total loss 6.19184208\n",
      "Trained batch 9371 batch loss 5.90788126 epoch total loss 6.19181156\n",
      "Trained batch 9372 batch loss 5.89427185 epoch total loss 6.19177961\n",
      "Trained batch 9373 batch loss 5.63418865 epoch total loss 6.19172\n",
      "Trained batch 9374 batch loss 5.45834923 epoch total loss 6.19164181\n",
      "Trained batch 9375 batch loss 5.60888481 epoch total loss 6.19158\n",
      "Trained batch 9376 batch loss 5.46171284 epoch total loss 6.19150162\n",
      "Trained batch 9377 batch loss 5.75098372 epoch total loss 6.19145441\n",
      "Trained batch 9378 batch loss 6.27665281 epoch total loss 6.19146395\n",
      "Trained batch 9379 batch loss 6.27520704 epoch total loss 6.19147253\n",
      "Trained batch 9380 batch loss 5.82321119 epoch total loss 6.19143343\n",
      "Trained batch 9381 batch loss 5.78600788 epoch total loss 6.19139\n",
      "Trained batch 9382 batch loss 6.08221054 epoch total loss 6.19137812\n",
      "Trained batch 9383 batch loss 5.59149456 epoch total loss 6.19131422\n",
      "Trained batch 9384 batch loss 5.31383753 epoch total loss 6.19122076\n",
      "Trained batch 9385 batch loss 5.92760038 epoch total loss 6.19119215\n",
      "Trained batch 9386 batch loss 5.98059177 epoch total loss 6.19116974\n",
      "Trained batch 9387 batch loss 5.12457 epoch total loss 6.19105625\n",
      "Trained batch 9388 batch loss 5.72937393 epoch total loss 6.19100714\n",
      "Trained batch 9389 batch loss 6.43051434 epoch total loss 6.19103241\n",
      "Trained batch 9390 batch loss 5.97315788 epoch total loss 6.19100952\n",
      "Trained batch 9391 batch loss 6.12071419 epoch total loss 6.19100189\n",
      "Trained batch 9392 batch loss 6.55047512 epoch total loss 6.19104\n",
      "Trained batch 9393 batch loss 6.14949512 epoch total loss 6.19103575\n",
      "Trained batch 9394 batch loss 6.16592121 epoch total loss 6.19103289\n",
      "Trained batch 9395 batch loss 6.50427628 epoch total loss 6.19106627\n",
      "Trained batch 9396 batch loss 6.45896149 epoch total loss 6.1910944\n",
      "Trained batch 9397 batch loss 6.36278582 epoch total loss 6.191113\n",
      "Trained batch 9398 batch loss 6.20408487 epoch total loss 6.19111395\n",
      "Trained batch 9399 batch loss 6.24804 epoch total loss 6.19111967\n",
      "Trained batch 9400 batch loss 6.59233046 epoch total loss 6.19116259\n",
      "Trained batch 9401 batch loss 6.49527 epoch total loss 6.19119501\n",
      "Trained batch 9402 batch loss 6.23200846 epoch total loss 6.1911993\n",
      "Trained batch 9403 batch loss 6.50230646 epoch total loss 6.19123268\n",
      "Trained batch 9404 batch loss 6.13310242 epoch total loss 6.19122648\n",
      "Trained batch 9405 batch loss 6.44538975 epoch total loss 6.19125319\n",
      "Trained batch 9406 batch loss 6.30980396 epoch total loss 6.19126606\n",
      "Trained batch 9407 batch loss 6.23025608 epoch total loss 6.19127\n",
      "Trained batch 9408 batch loss 6.09083843 epoch total loss 6.19125938\n",
      "Trained batch 9409 batch loss 6.28776073 epoch total loss 6.19127\n",
      "Trained batch 9410 batch loss 6.27676392 epoch total loss 6.19127893\n",
      "Trained batch 9411 batch loss 6.19744 epoch total loss 6.19128\n",
      "Trained batch 9412 batch loss 5.99926805 epoch total loss 6.19125938\n",
      "Trained batch 9413 batch loss 6.35411263 epoch total loss 6.19127655\n",
      "Trained batch 9414 batch loss 6.00992107 epoch total loss 6.19125748\n",
      "Trained batch 9415 batch loss 5.65946102 epoch total loss 6.19120121\n",
      "Trained batch 9416 batch loss 6.17528248 epoch total loss 6.1912\n",
      "Trained batch 9417 batch loss 5.96519947 epoch total loss 6.19117546\n",
      "Trained batch 9418 batch loss 6.05111885 epoch total loss 6.19116068\n",
      "Trained batch 9419 batch loss 5.9828577 epoch total loss 6.19113874\n",
      "Trained batch 9420 batch loss 5.80476713 epoch total loss 6.19109774\n",
      "Trained batch 9421 batch loss 6.31321287 epoch total loss 6.19111061\n",
      "Trained batch 9422 batch loss 5.84324312 epoch total loss 6.19107389\n",
      "Trained batch 9423 batch loss 5.80181837 epoch total loss 6.19103241\n",
      "Trained batch 9424 batch loss 5.66026211 epoch total loss 6.19097614\n",
      "Trained batch 9425 batch loss 5.37997913 epoch total loss 6.19089\n",
      "Trained batch 9426 batch loss 5.66116333 epoch total loss 6.19083357\n",
      "Trained batch 9427 batch loss 5.77499342 epoch total loss 6.19078922\n",
      "Trained batch 9428 batch loss 5.89310169 epoch total loss 6.19075775\n",
      "Trained batch 9429 batch loss 5.32469463 epoch total loss 6.19066572\n",
      "Trained batch 9430 batch loss 6.12387943 epoch total loss 6.19065905\n",
      "Trained batch 9431 batch loss 5.6280756 epoch total loss 6.19059944\n",
      "Trained batch 9432 batch loss 5.91242027 epoch total loss 6.19057\n",
      "Trained batch 9433 batch loss 5.96665382 epoch total loss 6.19054604\n",
      "Trained batch 9434 batch loss 6.12600422 epoch total loss 6.19053936\n",
      "Trained batch 9435 batch loss 5.55571175 epoch total loss 6.19047165\n",
      "Trained batch 9436 batch loss 5.99589729 epoch total loss 6.19045115\n",
      "Trained batch 9437 batch loss 5.10106087 epoch total loss 6.19033575\n",
      "Trained batch 9438 batch loss 5.6985631 epoch total loss 6.19028378\n",
      "Trained batch 9439 batch loss 6.39627838 epoch total loss 6.19030523\n",
      "Trained batch 9440 batch loss 6.08973217 epoch total loss 6.19029474\n",
      "Trained batch 9441 batch loss 7.2758112 epoch total loss 6.19041\n",
      "Trained batch 9442 batch loss 6.65968132 epoch total loss 6.19045973\n",
      "Trained batch 9443 batch loss 6.1205287 epoch total loss 6.1904521\n",
      "Trained batch 9444 batch loss 6.39571381 epoch total loss 6.19047403\n",
      "Trained batch 9445 batch loss 6.36178112 epoch total loss 6.19049215\n",
      "Trained batch 9446 batch loss 6.24936438 epoch total loss 6.19049835\n",
      "Trained batch 9447 batch loss 6.1108613 epoch total loss 6.19049\n",
      "Trained batch 9448 batch loss 6.29356432 epoch total loss 6.19050074\n",
      "Trained batch 9449 batch loss 6.42458773 epoch total loss 6.19052553\n",
      "Trained batch 9450 batch loss 6.2056694 epoch total loss 6.19052744\n",
      "Trained batch 9451 batch loss 6.39657116 epoch total loss 6.19054937\n",
      "Trained batch 9452 batch loss 6.0973711 epoch total loss 6.19054\n",
      "Trained batch 9453 batch loss 6.04636049 epoch total loss 6.19052458\n",
      "Trained batch 9454 batch loss 6.37756443 epoch total loss 6.19054413\n",
      "Trained batch 9455 batch loss 6.59443092 epoch total loss 6.19058704\n",
      "Trained batch 9456 batch loss 6.59540844 epoch total loss 6.19062948\n",
      "Trained batch 9457 batch loss 7.30673122 epoch total loss 6.19074774\n",
      "Trained batch 9458 batch loss 6.50001287 epoch total loss 6.19078064\n",
      "Trained batch 9459 batch loss 7.23306656 epoch total loss 6.19089079\n",
      "Trained batch 9460 batch loss 7.15607214 epoch total loss 6.19099283\n",
      "Trained batch 9461 batch loss 6.58184195 epoch total loss 6.19103432\n",
      "Trained batch 9462 batch loss 6.73093224 epoch total loss 6.19109106\n",
      "Trained batch 9463 batch loss 7.49456453 epoch total loss 6.19122934\n",
      "Trained batch 9464 batch loss 5.83839 epoch total loss 6.19119215\n",
      "Trained batch 9465 batch loss 7.56972408 epoch total loss 6.19133759\n",
      "Trained batch 9466 batch loss 6.75624847 epoch total loss 6.19139767\n",
      "Trained batch 9467 batch loss 6.65690041 epoch total loss 6.19144678\n",
      "Trained batch 9468 batch loss 6.10247803 epoch total loss 6.19143724\n",
      "Trained batch 9469 batch loss 6.64500237 epoch total loss 6.19148493\n",
      "Trained batch 9470 batch loss 6.63803959 epoch total loss 6.19153214\n",
      "Trained batch 9471 batch loss 6.43441105 epoch total loss 6.19155741\n",
      "Trained batch 9472 batch loss 7.12579632 epoch total loss 6.19165611\n",
      "Trained batch 9473 batch loss 6.17662811 epoch total loss 6.19165468\n",
      "Trained batch 9474 batch loss 6.80707026 epoch total loss 6.19171953\n",
      "Trained batch 9475 batch loss 6.01428509 epoch total loss 6.19170094\n",
      "Trained batch 9476 batch loss 6.01783419 epoch total loss 6.19168282\n",
      "Trained batch 9477 batch loss 6.55105 epoch total loss 6.19172096\n",
      "Trained batch 9478 batch loss 6.63850355 epoch total loss 6.19176769\n",
      "Trained batch 9479 batch loss 6.48056889 epoch total loss 6.19179821\n",
      "Trained batch 9480 batch loss 6.46369457 epoch total loss 6.19182682\n",
      "Trained batch 9481 batch loss 6.16571045 epoch total loss 6.19182396\n",
      "Trained batch 9482 batch loss 6.06380653 epoch total loss 6.19181061\n",
      "Trained batch 9483 batch loss 6.11743355 epoch total loss 6.1918025\n",
      "Trained batch 9484 batch loss 6.30405235 epoch total loss 6.19181442\n",
      "Trained batch 9485 batch loss 6.29936028 epoch total loss 6.19182587\n",
      "Trained batch 9486 batch loss 6.00103664 epoch total loss 6.19180584\n",
      "Trained batch 9487 batch loss 6.0711174 epoch total loss 6.19179296\n",
      "Trained batch 9488 batch loss 6.20910215 epoch total loss 6.19179487\n",
      "Trained batch 9489 batch loss 6.60346889 epoch total loss 6.19183826\n",
      "Trained batch 9490 batch loss 6.83692741 epoch total loss 6.19190598\n",
      "Trained batch 9491 batch loss 6.76850462 epoch total loss 6.19196701\n",
      "Trained batch 9492 batch loss 6.24357939 epoch total loss 6.19197226\n",
      "Trained batch 9493 batch loss 6.20999813 epoch total loss 6.19197416\n",
      "Trained batch 9494 batch loss 5.91574907 epoch total loss 6.1919446\n",
      "Trained batch 9495 batch loss 6.06396818 epoch total loss 6.19193125\n",
      "Trained batch 9496 batch loss 5.85082054 epoch total loss 6.19189548\n",
      "Trained batch 9497 batch loss 6.3562727 epoch total loss 6.19191265\n",
      "Trained batch 9498 batch loss 5.98771524 epoch total loss 6.19189119\n",
      "Trained batch 9499 batch loss 5.78652287 epoch total loss 6.19184828\n",
      "Trained batch 9500 batch loss 5.55557489 epoch total loss 6.19178104\n",
      "Trained batch 9501 batch loss 5.80045891 epoch total loss 6.19174\n",
      "Trained batch 9502 batch loss 6.03434753 epoch total loss 6.19172382\n",
      "Trained batch 9503 batch loss 5.94357491 epoch total loss 6.1916976\n",
      "Trained batch 9504 batch loss 5.65544891 epoch total loss 6.19164133\n",
      "Trained batch 9505 batch loss 5.86258793 epoch total loss 6.191607\n",
      "Trained batch 9506 batch loss 5.37679434 epoch total loss 6.19152069\n",
      "Trained batch 9507 batch loss 5.54130745 epoch total loss 6.1914525\n",
      "Trained batch 9508 batch loss 6.67079 epoch total loss 6.19150305\n",
      "Trained batch 9509 batch loss 5.55670452 epoch total loss 6.19143677\n",
      "Trained batch 9510 batch loss 6.2144742 epoch total loss 6.19143915\n",
      "Trained batch 9511 batch loss 5.93828344 epoch total loss 6.19141245\n",
      "Trained batch 9512 batch loss 6.24090385 epoch total loss 6.19141769\n",
      "Trained batch 9513 batch loss 5.09346294 epoch total loss 6.1913023\n",
      "Trained batch 9514 batch loss 5.6102972 epoch total loss 6.19124126\n",
      "Trained batch 9515 batch loss 6.5824337 epoch total loss 6.19128227\n",
      "Trained batch 9516 batch loss 6.57036114 epoch total loss 6.19132233\n",
      "Trained batch 9517 batch loss 6.78627205 epoch total loss 6.19138432\n",
      "Trained batch 9518 batch loss 6.85264254 epoch total loss 6.19145393\n",
      "Trained batch 9519 batch loss 6.80932045 epoch total loss 6.19151878\n",
      "Trained batch 9520 batch loss 5.82159424 epoch total loss 6.19147968\n",
      "Trained batch 9521 batch loss 6.65622616 epoch total loss 6.19152832\n",
      "Trained batch 9522 batch loss 6.36872196 epoch total loss 6.19154692\n",
      "Trained batch 9523 batch loss 6.86460543 epoch total loss 6.19161749\n",
      "Trained batch 9524 batch loss 6.1979537 epoch total loss 6.19161844\n",
      "Trained batch 9525 batch loss 6.12763214 epoch total loss 6.19161177\n",
      "Trained batch 9526 batch loss 6.5007143 epoch total loss 6.19164419\n",
      "Trained batch 9527 batch loss 6.14655161 epoch total loss 6.19163942\n",
      "Trained batch 9528 batch loss 6.30360699 epoch total loss 6.19165134\n",
      "Trained batch 9529 batch loss 6.53386 epoch total loss 6.19168758\n",
      "Trained batch 9530 batch loss 7.01468849 epoch total loss 6.19177389\n",
      "Trained batch 9531 batch loss 5.97393894 epoch total loss 6.191751\n",
      "Trained batch 9532 batch loss 6.14875412 epoch total loss 6.19174623\n",
      "Trained batch 9533 batch loss 6.16821861 epoch total loss 6.19174385\n",
      "Trained batch 9534 batch loss 6.00563 epoch total loss 6.1917243\n",
      "Trained batch 9535 batch loss 5.46320772 epoch total loss 6.19164801\n",
      "Trained batch 9536 batch loss 6.19629 epoch total loss 6.19164848\n",
      "Trained batch 9537 batch loss 6.25707626 epoch total loss 6.19165516\n",
      "Trained batch 9538 batch loss 6.00615692 epoch total loss 6.19163609\n",
      "Trained batch 9539 batch loss 6.17805052 epoch total loss 6.19163465\n",
      "Trained batch 9540 batch loss 6.16670895 epoch total loss 6.19163227\n",
      "Trained batch 9541 batch loss 6.46252537 epoch total loss 6.1916604\n",
      "Trained batch 9542 batch loss 5.71029854 epoch total loss 6.19161034\n",
      "Trained batch 9543 batch loss 6.237257 epoch total loss 6.1916151\n",
      "Trained batch 9544 batch loss 6.18397236 epoch total loss 6.19161415\n",
      "Trained batch 9545 batch loss 5.9882822 epoch total loss 6.19159269\n",
      "Trained batch 9546 batch loss 5.74787378 epoch total loss 6.19154596\n",
      "Trained batch 9547 batch loss 6.1670742 epoch total loss 6.19154358\n",
      "Trained batch 9548 batch loss 6.17853165 epoch total loss 6.19154263\n",
      "Trained batch 9549 batch loss 5.93165302 epoch total loss 6.19151545\n",
      "Trained batch 9550 batch loss 5.02020645 epoch total loss 6.1913929\n",
      "Trained batch 9551 batch loss 6.25834703 epoch total loss 6.19139957\n",
      "Trained batch 9552 batch loss 5.9952507 epoch total loss 6.19137907\n",
      "Trained batch 9553 batch loss 6.16963196 epoch total loss 6.19137669\n",
      "Trained batch 9554 batch loss 6.10400677 epoch total loss 6.19136763\n",
      "Trained batch 9555 batch loss 5.94472075 epoch total loss 6.19134188\n",
      "Trained batch 9556 batch loss 6.21808147 epoch total loss 6.19134474\n",
      "Trained batch 9557 batch loss 5.66404 epoch total loss 6.19129\n",
      "Trained batch 9558 batch loss 6.64562035 epoch total loss 6.19133711\n",
      "Trained batch 9559 batch loss 5.72189426 epoch total loss 6.19128799\n",
      "Trained batch 9560 batch loss 6.06424236 epoch total loss 6.19127464\n",
      "Trained batch 9561 batch loss 6.58162832 epoch total loss 6.19131565\n",
      "Trained batch 9562 batch loss 5.27050734 epoch total loss 6.19121933\n",
      "Trained batch 9563 batch loss 5.65326071 epoch total loss 6.19116259\n",
      "Trained batch 9564 batch loss 5.80539608 epoch total loss 6.19112253\n",
      "Trained batch 9565 batch loss 4.8409977 epoch total loss 6.19098091\n",
      "Trained batch 9566 batch loss 4.40099144 epoch total loss 6.19079399\n",
      "Trained batch 9567 batch loss 5.07060766 epoch total loss 6.19067717\n",
      "Trained batch 9568 batch loss 5.59314299 epoch total loss 6.1906147\n",
      "Trained batch 9569 batch loss 4.94874763 epoch total loss 6.190485\n",
      "Trained batch 9570 batch loss 5.72603559 epoch total loss 6.19043636\n",
      "Trained batch 9571 batch loss 6.78596973 epoch total loss 6.19049835\n",
      "Trained batch 9572 batch loss 4.59329796 epoch total loss 6.19033194\n",
      "Trained batch 9573 batch loss 7.28665876 epoch total loss 6.1904459\n",
      "Trained batch 9574 batch loss 5.92269421 epoch total loss 6.19041824\n",
      "Trained batch 9575 batch loss 6.66085 epoch total loss 6.19046688\n",
      "Trained batch 9576 batch loss 6.24532318 epoch total loss 6.19047308\n",
      "Trained batch 9577 batch loss 6.69987488 epoch total loss 6.19052601\n",
      "Trained batch 9578 batch loss 6.85890388 epoch total loss 6.1905961\n",
      "Trained batch 9579 batch loss 6.59904337 epoch total loss 6.19063854\n",
      "Trained batch 9580 batch loss 6.2447958 epoch total loss 6.19064426\n",
      "Trained batch 9581 batch loss 6.09269047 epoch total loss 6.19063425\n",
      "Trained batch 9582 batch loss 4.40383053 epoch total loss 6.19044733\n",
      "Trained batch 9583 batch loss 6.27987051 epoch total loss 6.19045687\n",
      "Trained batch 9584 batch loss 5.07528925 epoch total loss 6.19034052\n",
      "Trained batch 9585 batch loss 6.22644043 epoch total loss 6.19034433\n",
      "Trained batch 9586 batch loss 7.18835115 epoch total loss 6.19044828\n",
      "Trained batch 9587 batch loss 5.88197899 epoch total loss 6.19041634\n",
      "Trained batch 9588 batch loss 6.02194 epoch total loss 6.19039869\n",
      "Trained batch 9589 batch loss 5.95735693 epoch total loss 6.19037437\n",
      "Trained batch 9590 batch loss 6.48707056 epoch total loss 6.19040537\n",
      "Trained batch 9591 batch loss 6.14229822 epoch total loss 6.1904\n",
      "Trained batch 9592 batch loss 5.63090611 epoch total loss 6.19034195\n",
      "Trained batch 9593 batch loss 6.06113482 epoch total loss 6.1903286\n",
      "Trained batch 9594 batch loss 6.1409893 epoch total loss 6.19032383\n",
      "Trained batch 9595 batch loss 6.50653172 epoch total loss 6.19035673\n",
      "Trained batch 9596 batch loss 6.60928917 epoch total loss 6.1904006\n",
      "Trained batch 9597 batch loss 6.33282948 epoch total loss 6.19041491\n",
      "Trained batch 9598 batch loss 6.29154778 epoch total loss 6.19042587\n",
      "Trained batch 9599 batch loss 6.85018349 epoch total loss 6.19049454\n",
      "Trained batch 9600 batch loss 5.48021078 epoch total loss 6.19042063\n",
      "Trained batch 9601 batch loss 6.07756901 epoch total loss 6.19040918\n",
      "Trained batch 9602 batch loss 6.04860258 epoch total loss 6.19039392\n",
      "Trained batch 9603 batch loss 6.33519554 epoch total loss 6.19040918\n",
      "Trained batch 9604 batch loss 6.7227 epoch total loss 6.1904645\n",
      "Trained batch 9605 batch loss 6.68472242 epoch total loss 6.190516\n",
      "Trained batch 9606 batch loss 6.15716553 epoch total loss 6.19051266\n",
      "Trained batch 9607 batch loss 5.81263733 epoch total loss 6.19047308\n",
      "Trained batch 9608 batch loss 5.98899603 epoch total loss 6.1904521\n",
      "Trained batch 9609 batch loss 5.5668087 epoch total loss 6.19038725\n",
      "Trained batch 9610 batch loss 5.53875113 epoch total loss 6.19031954\n",
      "Trained batch 9611 batch loss 6.09141541 epoch total loss 6.19030905\n",
      "Trained batch 9612 batch loss 6.39180756 epoch total loss 6.19032955\n",
      "Trained batch 9613 batch loss 5.9793272 epoch total loss 6.19030809\n",
      "Trained batch 9614 batch loss 5.81393957 epoch total loss 6.19026852\n",
      "Trained batch 9615 batch loss 6.07473803 epoch total loss 6.1902566\n",
      "Trained batch 9616 batch loss 5.56358051 epoch total loss 6.19019127\n",
      "Trained batch 9617 batch loss 5.73151588 epoch total loss 6.19014359\n",
      "Trained batch 9618 batch loss 5.78221512 epoch total loss 6.19010115\n",
      "Trained batch 9619 batch loss 5.99108219 epoch total loss 6.19008\n",
      "Trained batch 9620 batch loss 7.02936554 epoch total loss 6.1901679\n",
      "Trained batch 9621 batch loss 6.09837818 epoch total loss 6.19015837\n",
      "Trained batch 9622 batch loss 6.22799587 epoch total loss 6.19016218\n",
      "Trained batch 9623 batch loss 5.37686157 epoch total loss 6.1900773\n",
      "Trained batch 9624 batch loss 4.77764606 epoch total loss 6.18993044\n",
      "Trained batch 9625 batch loss 4.89446878 epoch total loss 6.18979597\n",
      "Trained batch 9626 batch loss 4.89494896 epoch total loss 6.1896615\n",
      "Trained batch 9627 batch loss 6.67986679 epoch total loss 6.18971205\n",
      "Trained batch 9628 batch loss 6.60405827 epoch total loss 6.18975544\n",
      "Trained batch 9629 batch loss 6.43202209 epoch total loss 6.18978071\n",
      "Trained batch 9630 batch loss 6.4908123 epoch total loss 6.18981218\n",
      "Trained batch 9631 batch loss 6.12041 epoch total loss 6.18980503\n",
      "Trained batch 9632 batch loss 6.10532665 epoch total loss 6.18979645\n",
      "Trained batch 9633 batch loss 6.15674877 epoch total loss 6.18979263\n",
      "Trained batch 9634 batch loss 5.88501358 epoch total loss 6.18976116\n",
      "Trained batch 9635 batch loss 6.81838799 epoch total loss 6.18982649\n",
      "Trained batch 9636 batch loss 7.17168903 epoch total loss 6.18992853\n",
      "Trained batch 9637 batch loss 6.69994783 epoch total loss 6.18998146\n",
      "Trained batch 9638 batch loss 5.49065351 epoch total loss 6.18990898\n",
      "Trained batch 9639 batch loss 6.30412292 epoch total loss 6.1899209\n",
      "Trained batch 9640 batch loss 6.35862637 epoch total loss 6.18993855\n",
      "Trained batch 9641 batch loss 6.1073637 epoch total loss 6.18993\n",
      "Trained batch 9642 batch loss 6.53255796 epoch total loss 6.18996525\n",
      "Trained batch 9643 batch loss 6.11983585 epoch total loss 6.1899581\n",
      "Trained batch 9644 batch loss 4.72702122 epoch total loss 6.18980646\n",
      "Trained batch 9645 batch loss 6.13147736 epoch total loss 6.18980026\n",
      "Trained batch 9646 batch loss 6.10872746 epoch total loss 6.18979216\n",
      "Trained batch 9647 batch loss 6.00254631 epoch total loss 6.18977261\n",
      "Trained batch 9648 batch loss 5.9105978 epoch total loss 6.189744\n",
      "Trained batch 9649 batch loss 6.01348352 epoch total loss 6.1897254\n",
      "Trained batch 9650 batch loss 5.15689468 epoch total loss 6.18961811\n",
      "Trained batch 9651 batch loss 6.20178604 epoch total loss 6.18961954\n",
      "Trained batch 9652 batch loss 7.23626328 epoch total loss 6.18972778\n",
      "Trained batch 9653 batch loss 6.04601383 epoch total loss 6.189713\n",
      "Trained batch 9654 batch loss 6.2129221 epoch total loss 6.18971586\n",
      "Trained batch 9655 batch loss 6.06280136 epoch total loss 6.18970251\n",
      "Trained batch 9656 batch loss 6.35363626 epoch total loss 6.18971968\n",
      "Trained batch 9657 batch loss 6.06263447 epoch total loss 6.18970633\n",
      "Trained batch 9658 batch loss 6.18104362 epoch total loss 6.18970537\n",
      "Trained batch 9659 batch loss 5.95159483 epoch total loss 6.18968105\n",
      "Trained batch 9660 batch loss 6.15379953 epoch total loss 6.18967724\n",
      "Trained batch 9661 batch loss 5.82121849 epoch total loss 6.18963909\n",
      "Trained batch 9662 batch loss 5.70151138 epoch total loss 6.18958855\n",
      "Trained batch 9663 batch loss 5.64783287 epoch total loss 6.18953276\n",
      "Trained batch 9664 batch loss 6.5428133 epoch total loss 6.189569\n",
      "Trained batch 9665 batch loss 6.05832577 epoch total loss 6.18955564\n",
      "Trained batch 9666 batch loss 6.4565053 epoch total loss 6.1895833\n",
      "Trained batch 9667 batch loss 5.52763414 epoch total loss 6.18951464\n",
      "Trained batch 9668 batch loss 5.07129431 epoch total loss 6.18939877\n",
      "Trained batch 9669 batch loss 5.58979654 epoch total loss 6.18933678\n",
      "Trained batch 9670 batch loss 5.06353092 epoch total loss 6.18922043\n",
      "Trained batch 9671 batch loss 4.55861378 epoch total loss 6.18905163\n",
      "Trained batch 9672 batch loss 4.68671417 epoch total loss 6.18889666\n",
      "Trained batch 9673 batch loss 5.72352791 epoch total loss 6.1888485\n",
      "Trained batch 9674 batch loss 4.89396572 epoch total loss 6.1887145\n",
      "Trained batch 9675 batch loss 5.42712212 epoch total loss 6.18863583\n",
      "Trained batch 9676 batch loss 5.7886076 epoch total loss 6.18859434\n",
      "Trained batch 9677 batch loss 6.50964355 epoch total loss 6.18862724\n",
      "Trained batch 9678 batch loss 6.28444099 epoch total loss 6.18863726\n",
      "Trained batch 9679 batch loss 4.96012783 epoch total loss 6.18851042\n",
      "Trained batch 9680 batch loss 6.14663458 epoch total loss 6.1885066\n",
      "Trained batch 9681 batch loss 5.21555519 epoch total loss 6.18840599\n",
      "Trained batch 9682 batch loss 5.47168159 epoch total loss 6.18833208\n",
      "Trained batch 9683 batch loss 5.51423883 epoch total loss 6.18826246\n",
      "Trained batch 9684 batch loss 4.91710186 epoch total loss 6.18813133\n",
      "Trained batch 9685 batch loss 6.06625748 epoch total loss 6.18811893\n",
      "Trained batch 9686 batch loss 5.1271162 epoch total loss 6.18800926\n",
      "Trained batch 9687 batch loss 4.86454582 epoch total loss 6.18787241\n",
      "Trained batch 9688 batch loss 5.25681114 epoch total loss 6.18777657\n",
      "Trained batch 9689 batch loss 5.41624594 epoch total loss 6.18769693\n",
      "Trained batch 9690 batch loss 4.78211641 epoch total loss 6.18755198\n",
      "Trained batch 9691 batch loss 5.16778564 epoch total loss 6.18744659\n",
      "Trained batch 9692 batch loss 4.53539371 epoch total loss 6.18727636\n",
      "Trained batch 9693 batch loss 4.47657251 epoch total loss 6.1871\n",
      "Trained batch 9694 batch loss 4.94153166 epoch total loss 6.18697119\n",
      "Trained batch 9695 batch loss 5.12821579 epoch total loss 6.18686199\n",
      "Trained batch 9696 batch loss 4.79037189 epoch total loss 6.18671799\n",
      "Trained batch 9697 batch loss 5.12408638 epoch total loss 6.18660831\n",
      "Trained batch 9698 batch loss 5.52370739 epoch total loss 6.18654\n",
      "Trained batch 9699 batch loss 5.0097208 epoch total loss 6.18641853\n",
      "Trained batch 9700 batch loss 5.21950626 epoch total loss 6.18631887\n",
      "Trained batch 9701 batch loss 5.19658566 epoch total loss 6.18621683\n",
      "Trained batch 9702 batch loss 5.96909332 epoch total loss 6.18619442\n",
      "Trained batch 9703 batch loss 6.01204109 epoch total loss 6.1861763\n",
      "Trained batch 9704 batch loss 6.3751893 epoch total loss 6.18619585\n",
      "Trained batch 9705 batch loss 6.60940266 epoch total loss 6.18623924\n",
      "Trained batch 9706 batch loss 6.62533474 epoch total loss 6.18628454\n",
      "Trained batch 9707 batch loss 6.80233955 epoch total loss 6.18634796\n",
      "Trained batch 9708 batch loss 5.29481792 epoch total loss 6.18625593\n",
      "Trained batch 9709 batch loss 7.10910606 epoch total loss 6.18635082\n",
      "Trained batch 9710 batch loss 7.82611656 epoch total loss 6.18651962\n",
      "Trained batch 9711 batch loss 6.36867237 epoch total loss 6.18653822\n",
      "Trained batch 9712 batch loss 7.30177212 epoch total loss 6.18665314\n",
      "Trained batch 9713 batch loss 6.25349236 epoch total loss 6.18666\n",
      "Trained batch 9714 batch loss 6.18865919 epoch total loss 6.18666\n",
      "Trained batch 9715 batch loss 5.68682 epoch total loss 6.18660879\n",
      "Trained batch 9716 batch loss 5.64159203 epoch total loss 6.18655252\n",
      "Trained batch 9717 batch loss 6.43535709 epoch total loss 6.1865778\n",
      "Trained batch 9718 batch loss 6.21509838 epoch total loss 6.18658066\n",
      "Trained batch 9719 batch loss 6.20082188 epoch total loss 6.18658209\n",
      "Trained batch 9720 batch loss 6.08187962 epoch total loss 6.18657112\n",
      "Trained batch 9721 batch loss 6.22459555 epoch total loss 6.18657494\n",
      "Trained batch 9722 batch loss 6.70619345 epoch total loss 6.18662834\n",
      "Trained batch 9723 batch loss 7.0437479 epoch total loss 6.18671656\n",
      "Trained batch 9724 batch loss 6.99260092 epoch total loss 6.18679953\n",
      "Trained batch 9725 batch loss 6.9522624 epoch total loss 6.1868782\n",
      "Trained batch 9726 batch loss 6.86506414 epoch total loss 6.18694782\n",
      "Trained batch 9727 batch loss 7.32738876 epoch total loss 6.18706512\n",
      "Trained batch 9728 batch loss 7.07848358 epoch total loss 6.18715668\n",
      "Trained batch 9729 batch loss 6.73756695 epoch total loss 6.18721342\n",
      "Trained batch 9730 batch loss 5.80392551 epoch total loss 6.18717384\n",
      "Trained batch 9731 batch loss 5.52369 epoch total loss 6.18710566\n",
      "Trained batch 9732 batch loss 6.69318867 epoch total loss 6.18715763\n",
      "Trained batch 9733 batch loss 6.86012554 epoch total loss 6.18722677\n",
      "Trained batch 9734 batch loss 6.65928459 epoch total loss 6.18727541\n",
      "Trained batch 9735 batch loss 6.56308699 epoch total loss 6.18731403\n",
      "Trained batch 9736 batch loss 6.68269444 epoch total loss 6.18736458\n",
      "Trained batch 9737 batch loss 6.61430836 epoch total loss 6.18740845\n",
      "Trained batch 9738 batch loss 6.25204229 epoch total loss 6.18741512\n",
      "Trained batch 9739 batch loss 6.67006 epoch total loss 6.18746519\n",
      "Trained batch 9740 batch loss 6.55700207 epoch total loss 6.18750334\n",
      "Trained batch 9741 batch loss 6.59218693 epoch total loss 6.18754482\n",
      "Trained batch 9742 batch loss 6.44072723 epoch total loss 6.18757105\n",
      "Trained batch 9743 batch loss 6.20125389 epoch total loss 6.18757248\n",
      "Trained batch 9744 batch loss 5.77917385 epoch total loss 6.18753052\n",
      "Trained batch 9745 batch loss 5.72311401 epoch total loss 6.18748283\n",
      "Trained batch 9746 batch loss 5.7581358 epoch total loss 6.18743849\n",
      "Trained batch 9747 batch loss 6.0408349 epoch total loss 6.18742323\n",
      "Trained batch 9748 batch loss 7.11408138 epoch total loss 6.1875186\n",
      "Trained batch 9749 batch loss 6.25420713 epoch total loss 6.18752527\n",
      "Trained batch 9750 batch loss 6.97117138 epoch total loss 6.18760586\n",
      "Trained batch 9751 batch loss 6.88345671 epoch total loss 6.18767691\n",
      "Trained batch 9752 batch loss 5.87981129 epoch total loss 6.18764544\n",
      "Trained batch 9753 batch loss 6.54013205 epoch total loss 6.1876812\n",
      "Trained batch 9754 batch loss 7.02915764 epoch total loss 6.18776751\n",
      "Trained batch 9755 batch loss 6.4404335 epoch total loss 6.18779373\n",
      "Trained batch 9756 batch loss 7.09024715 epoch total loss 6.18788576\n",
      "Trained batch 9757 batch loss 6.39648104 epoch total loss 6.18790722\n",
      "Trained batch 9758 batch loss 6.95587444 epoch total loss 6.1879859\n",
      "Trained batch 9759 batch loss 7.21733475 epoch total loss 6.18809175\n",
      "Trained batch 9760 batch loss 6.30811119 epoch total loss 6.18810415\n",
      "Trained batch 9761 batch loss 7.36016655 epoch total loss 6.18822384\n",
      "Trained batch 9762 batch loss 7.48009968 epoch total loss 6.1883564\n",
      "Trained batch 9763 batch loss 6.35770607 epoch total loss 6.18837404\n",
      "Trained batch 9764 batch loss 6.46633053 epoch total loss 6.18840218\n",
      "Trained batch 9765 batch loss 6.44239235 epoch total loss 6.18842793\n",
      "Trained batch 9766 batch loss 6.85168171 epoch total loss 6.18849611\n",
      "Trained batch 9767 batch loss 6.65576601 epoch total loss 6.1885438\n",
      "Trained batch 9768 batch loss 6.65385056 epoch total loss 6.18859148\n",
      "Trained batch 9769 batch loss 6.18355 epoch total loss 6.188591\n",
      "Trained batch 9770 batch loss 6.33717871 epoch total loss 6.18860579\n",
      "Trained batch 9771 batch loss 6.9831543 epoch total loss 6.18868732\n",
      "Trained batch 9772 batch loss 6.60632658 epoch total loss 6.18873\n",
      "Trained batch 9773 batch loss 6.93205547 epoch total loss 6.18880606\n",
      "Trained batch 9774 batch loss 6.87917471 epoch total loss 6.18887663\n",
      "Trained batch 9775 batch loss 6.56840658 epoch total loss 6.18891573\n",
      "Trained batch 9776 batch loss 6.94837332 epoch total loss 6.18899345\n",
      "Trained batch 9777 batch loss 6.64884377 epoch total loss 6.18904066\n",
      "Trained batch 9778 batch loss 5.86179066 epoch total loss 6.18900728\n",
      "Trained batch 9779 batch loss 6.13156366 epoch total loss 6.18900156\n",
      "Trained batch 9780 batch loss 6.60713768 epoch total loss 6.189044\n",
      "Trained batch 9781 batch loss 6.7515955 epoch total loss 6.1891017\n",
      "Trained batch 9782 batch loss 6.97799397 epoch total loss 6.1891818\n",
      "Trained batch 9783 batch loss 6.44761944 epoch total loss 6.18920851\n",
      "Trained batch 9784 batch loss 6.14257717 epoch total loss 6.18920374\n",
      "Trained batch 9785 batch loss 5.95747185 epoch total loss 6.18918\n",
      "Trained batch 9786 batch loss 5.18132401 epoch total loss 6.1890769\n",
      "Trained batch 9787 batch loss 5.71868134 epoch total loss 6.18902874\n",
      "Trained batch 9788 batch loss 5.39946222 epoch total loss 6.18894768\n",
      "Trained batch 9789 batch loss 6.1916213 epoch total loss 6.18894815\n",
      "Trained batch 9790 batch loss 6.52831268 epoch total loss 6.18898249\n",
      "Trained batch 9791 batch loss 5.91038275 epoch total loss 6.18895435\n",
      "Trained batch 9792 batch loss 6.3989377 epoch total loss 6.18897581\n",
      "Trained batch 9793 batch loss 5.99286 epoch total loss 6.18895531\n",
      "Trained batch 9794 batch loss 5.93426847 epoch total loss 6.18892956\n",
      "Trained batch 9795 batch loss 6.29348183 epoch total loss 6.18894\n",
      "Trained batch 9796 batch loss 6.01921415 epoch total loss 6.18892288\n",
      "Trained batch 9797 batch loss 6.09554577 epoch total loss 6.18891287\n",
      "Trained batch 9798 batch loss 5.56853771 epoch total loss 6.18885\n",
      "Trained batch 9799 batch loss 4.80202484 epoch total loss 6.18870831\n",
      "Trained batch 9800 batch loss 5.93723488 epoch total loss 6.18868256\n",
      "Trained batch 9801 batch loss 5.66858578 epoch total loss 6.18862963\n",
      "Trained batch 9802 batch loss 5.87986422 epoch total loss 6.18859768\n",
      "Trained batch 9803 batch loss 5.95981359 epoch total loss 6.18857479\n",
      "Trained batch 9804 batch loss 6.09789276 epoch total loss 6.18856525\n",
      "Trained batch 9805 batch loss 5.77138519 epoch total loss 6.18852282\n",
      "Trained batch 9806 batch loss 7.13142967 epoch total loss 6.18861914\n",
      "Trained batch 9807 batch loss 6.6395669 epoch total loss 6.18866491\n",
      "Trained batch 9808 batch loss 6.45168972 epoch total loss 6.18869209\n",
      "Trained batch 9809 batch loss 6.30836296 epoch total loss 6.18870401\n",
      "Trained batch 9810 batch loss 6.99622488 epoch total loss 6.18878651\n",
      "Trained batch 9811 batch loss 6.8604579 epoch total loss 6.18885469\n",
      "Trained batch 9812 batch loss 6.39489651 epoch total loss 6.18887568\n",
      "Trained batch 9813 batch loss 6.86789036 epoch total loss 6.18894482\n",
      "Trained batch 9814 batch loss 6.46630192 epoch total loss 6.18897295\n",
      "Trained batch 9815 batch loss 6.76661253 epoch total loss 6.18903208\n",
      "Trained batch 9816 batch loss 5.45563126 epoch total loss 6.18895721\n",
      "Trained batch 9817 batch loss 5.83834 epoch total loss 6.18892193\n",
      "Trained batch 9818 batch loss 5.99975872 epoch total loss 6.18890238\n",
      "Trained batch 9819 batch loss 7.04569149 epoch total loss 6.18898964\n",
      "Trained batch 9820 batch loss 6.41602182 epoch total loss 6.189013\n",
      "Trained batch 9821 batch loss 7.47117472 epoch total loss 6.18914366\n",
      "Trained batch 9822 batch loss 7.31241322 epoch total loss 6.1892581\n",
      "Trained batch 9823 batch loss 5.95257092 epoch total loss 6.18923426\n",
      "Trained batch 9824 batch loss 6.13032293 epoch total loss 6.18922806\n",
      "Trained batch 9825 batch loss 6.82249641 epoch total loss 6.18929291\n",
      "Trained batch 9826 batch loss 6.30655479 epoch total loss 6.18930435\n",
      "Trained batch 9827 batch loss 5.92973423 epoch total loss 6.18927813\n",
      "Trained batch 9828 batch loss 5.37071037 epoch total loss 6.18919468\n",
      "Trained batch 9829 batch loss 6.02748489 epoch total loss 6.18917847\n",
      "Trained batch 9830 batch loss 5.38526869 epoch total loss 6.18909645\n",
      "Trained batch 9831 batch loss 6.17475128 epoch total loss 6.1890955\n",
      "Trained batch 9832 batch loss 5.9342308 epoch total loss 6.18906927\n",
      "Trained batch 9833 batch loss 6.02906609 epoch total loss 6.18905306\n",
      "Trained batch 9834 batch loss 5.95493889 epoch total loss 6.18902874\n",
      "Trained batch 9835 batch loss 6.06293106 epoch total loss 6.18901587\n",
      "Trained batch 9836 batch loss 5.18192768 epoch total loss 6.18891382\n",
      "Trained batch 9837 batch loss 5.74594879 epoch total loss 6.188869\n",
      "Trained batch 9838 batch loss 6.10780811 epoch total loss 6.18886089\n",
      "Trained batch 9839 batch loss 5.81701279 epoch total loss 6.18882275\n",
      "Trained batch 9840 batch loss 5.71554661 epoch total loss 6.18877459\n",
      "Trained batch 9841 batch loss 5.90745 epoch total loss 6.18874598\n",
      "Trained batch 9842 batch loss 5.82394218 epoch total loss 6.18870878\n",
      "Trained batch 9843 batch loss 6.43871975 epoch total loss 6.18873405\n",
      "Trained batch 9844 batch loss 6.36332655 epoch total loss 6.18875217\n",
      "Trained batch 9845 batch loss 6.69246674 epoch total loss 6.1888032\n",
      "Trained batch 9846 batch loss 6.0809679 epoch total loss 6.18879223\n",
      "Trained batch 9847 batch loss 5.95964336 epoch total loss 6.18876886\n",
      "Trained batch 9848 batch loss 5.85707474 epoch total loss 6.18873501\n",
      "Trained batch 9849 batch loss 5.67088366 epoch total loss 6.18868256\n",
      "Trained batch 9850 batch loss 5.78128242 epoch total loss 6.18864155\n",
      "Trained batch 9851 batch loss 5.68426418 epoch total loss 6.18859\n",
      "Trained batch 9852 batch loss 5.73980427 epoch total loss 6.18854427\n",
      "Trained batch 9853 batch loss 6.12390518 epoch total loss 6.18853807\n",
      "Trained batch 9854 batch loss 5.66460848 epoch total loss 6.18848467\n",
      "Trained batch 9855 batch loss 5.68304348 epoch total loss 6.18843365\n",
      "Trained batch 9856 batch loss 5.58097458 epoch total loss 6.18837214\n",
      "Trained batch 9857 batch loss 6.07261229 epoch total loss 6.18836\n",
      "Trained batch 9858 batch loss 6.30082893 epoch total loss 6.18837166\n",
      "Trained batch 9859 batch loss 6.42834139 epoch total loss 6.18839645\n",
      "Trained batch 9860 batch loss 7.05195522 epoch total loss 6.18848372\n",
      "Trained batch 9861 batch loss 6.85916042 epoch total loss 6.1885519\n",
      "Trained batch 9862 batch loss 6.98553085 epoch total loss 6.18863249\n",
      "Trained batch 9863 batch loss 7.0228796 epoch total loss 6.18871689\n",
      "Trained batch 9864 batch loss 6.61486 epoch total loss 6.18876028\n",
      "Trained batch 9865 batch loss 7.16374969 epoch total loss 6.18885899\n",
      "Trained batch 9866 batch loss 6.49213886 epoch total loss 6.1888895\n",
      "Trained batch 9867 batch loss 6.99972725 epoch total loss 6.188972\n",
      "Trained batch 9868 batch loss 7.25255299 epoch total loss 6.18907976\n",
      "Trained batch 9869 batch loss 6.5149231 epoch total loss 6.18911314\n",
      "Trained batch 9870 batch loss 7.13775253 epoch total loss 6.18920898\n",
      "Trained batch 9871 batch loss 6.70564365 epoch total loss 6.18926144\n",
      "Trained batch 9872 batch loss 6.75018406 epoch total loss 6.18931818\n",
      "Trained batch 9873 batch loss 6.85038567 epoch total loss 6.18938541\n",
      "Trained batch 9874 batch loss 5.99159384 epoch total loss 6.18936539\n",
      "Trained batch 9875 batch loss 6.81012535 epoch total loss 6.18942785\n",
      "Trained batch 9876 batch loss 6.60888863 epoch total loss 6.18947029\n",
      "Trained batch 9877 batch loss 6.73982763 epoch total loss 6.18952608\n",
      "Trained batch 9878 batch loss 6.29819107 epoch total loss 6.18953705\n",
      "Trained batch 9879 batch loss 7.31939507 epoch total loss 6.18965149\n",
      "Trained batch 9880 batch loss 6.29787588 epoch total loss 6.18966246\n",
      "Trained batch 9881 batch loss 6.60264111 epoch total loss 6.18970394\n",
      "Trained batch 9882 batch loss 7.21779251 epoch total loss 6.18980789\n",
      "Trained batch 9883 batch loss 6.36527252 epoch total loss 6.18982601\n",
      "Trained batch 9884 batch loss 6.37537575 epoch total loss 6.18984461\n",
      "Trained batch 9885 batch loss 6.13411522 epoch total loss 6.18983889\n",
      "Trained batch 9886 batch loss 6.2186327 epoch total loss 6.18984175\n",
      "Trained batch 9887 batch loss 6.31231976 epoch total loss 6.18985415\n",
      "Trained batch 9888 batch loss 6.14583 epoch total loss 6.18985\n",
      "Trained batch 9889 batch loss 5.55309963 epoch total loss 6.18978548\n",
      "Trained batch 9890 batch loss 5.70910454 epoch total loss 6.18973732\n",
      "Trained batch 9891 batch loss 6.10650253 epoch total loss 6.18972874\n",
      "Trained batch 9892 batch loss 5.26922083 epoch total loss 6.18963575\n",
      "Trained batch 9893 batch loss 6.06971741 epoch total loss 6.18962336\n",
      "Trained batch 9894 batch loss 7.03974771 epoch total loss 6.18970919\n",
      "Trained batch 9895 batch loss 6.53847122 epoch total loss 6.18974447\n",
      "Trained batch 9896 batch loss 6.54120636 epoch total loss 6.18978\n",
      "Trained batch 9897 batch loss 6.81288147 epoch total loss 6.18984318\n",
      "Trained batch 9898 batch loss 7.04980087 epoch total loss 6.18993044\n",
      "Trained batch 9899 batch loss 6.3531065 epoch total loss 6.18994665\n",
      "Trained batch 9900 batch loss 6.83607483 epoch total loss 6.19001198\n",
      "Trained batch 9901 batch loss 7.03756332 epoch total loss 6.19009781\n",
      "Trained batch 9902 batch loss 6.80688334 epoch total loss 6.19016027\n",
      "Trained batch 9903 batch loss 6.78043556 epoch total loss 6.19022\n",
      "Trained batch 9904 batch loss 6.766922 epoch total loss 6.19027805\n",
      "Trained batch 9905 batch loss 6.78449917 epoch total loss 6.19033813\n",
      "Trained batch 9906 batch loss 6.75759268 epoch total loss 6.19039536\n",
      "Trained batch 9907 batch loss 6.97091198 epoch total loss 6.19047403\n",
      "Trained batch 9908 batch loss 6.76532 epoch total loss 6.19053221\n",
      "Trained batch 9909 batch loss 6.28077412 epoch total loss 6.19054127\n",
      "Trained batch 9910 batch loss 6.75636959 epoch total loss 6.19059849\n",
      "Trained batch 9911 batch loss 6.40263462 epoch total loss 6.19062\n",
      "Trained batch 9912 batch loss 6.86607647 epoch total loss 6.19068813\n",
      "Trained batch 9913 batch loss 6.35154152 epoch total loss 6.19070435\n",
      "Trained batch 9914 batch loss 6.55046797 epoch total loss 6.19074059\n",
      "Trained batch 9915 batch loss 5.84712172 epoch total loss 6.19070625\n",
      "Trained batch 9916 batch loss 6.19012547 epoch total loss 6.19070625\n",
      "Trained batch 9917 batch loss 6.73654318 epoch total loss 6.19076157\n",
      "Trained batch 9918 batch loss 5.25193644 epoch total loss 6.19066668\n",
      "Trained batch 9919 batch loss 5.77906 epoch total loss 6.19062471\n",
      "Trained batch 9920 batch loss 6.04992056 epoch total loss 6.19061089\n",
      "Trained batch 9921 batch loss 7.11667109 epoch total loss 6.19070435\n",
      "Trained batch 9922 batch loss 5.28104877 epoch total loss 6.19061279\n",
      "Trained batch 9923 batch loss 6.59049511 epoch total loss 6.19065285\n",
      "Trained batch 9924 batch loss 6.5805769 epoch total loss 6.19069242\n",
      "Trained batch 9925 batch loss 5.75159359 epoch total loss 6.1906476\n",
      "Trained batch 9926 batch loss 5.91597462 epoch total loss 6.19062\n",
      "Trained batch 9927 batch loss 6.48196793 epoch total loss 6.19064903\n",
      "Trained batch 9928 batch loss 6.151824 epoch total loss 6.19064522\n",
      "Trained batch 9929 batch loss 6.72689 epoch total loss 6.1906991\n",
      "Trained batch 9930 batch loss 5.4300375 epoch total loss 6.19062281\n",
      "Trained batch 9931 batch loss 5.17024469 epoch total loss 6.19052029\n",
      "Trained batch 9932 batch loss 5.71135235 epoch total loss 6.19047165\n",
      "Trained batch 9933 batch loss 6.34525871 epoch total loss 6.19048738\n",
      "Trained batch 9934 batch loss 5.02504349 epoch total loss 6.19036961\n",
      "Trained batch 9935 batch loss 5.97316742 epoch total loss 6.19034767\n",
      "Trained batch 9936 batch loss 5.45696354 epoch total loss 6.19027424\n",
      "Trained batch 9937 batch loss 5.64958906 epoch total loss 6.1902194\n",
      "Trained batch 9938 batch loss 6.51113462 epoch total loss 6.19025183\n",
      "Trained batch 9939 batch loss 6.87672091 epoch total loss 6.19032049\n",
      "Trained batch 9940 batch loss 5.4299221 epoch total loss 6.1902442\n",
      "Trained batch 9941 batch loss 5.3752265 epoch total loss 6.19016218\n",
      "Trained batch 9942 batch loss 5.60936642 epoch total loss 6.19010401\n",
      "Trained batch 9943 batch loss 6.25803 epoch total loss 6.19011068\n",
      "Trained batch 9944 batch loss 6.66534185 epoch total loss 6.19015837\n",
      "Trained batch 9945 batch loss 6.18545628 epoch total loss 6.19015741\n",
      "Trained batch 9946 batch loss 6.31457281 epoch total loss 6.19017029\n",
      "Trained batch 9947 batch loss 6.13718128 epoch total loss 6.19016504\n",
      "Trained batch 9948 batch loss 5.78926945 epoch total loss 6.19012451\n",
      "Trained batch 9949 batch loss 5.79732 epoch total loss 6.19008493\n",
      "Trained batch 9950 batch loss 5.37212467 epoch total loss 6.19000292\n",
      "Trained batch 9951 batch loss 5.17639923 epoch total loss 6.18990088\n",
      "Trained batch 9952 batch loss 5.24325132 epoch total loss 6.18980551\n",
      "Trained batch 9953 batch loss 5.00846195 epoch total loss 6.18968678\n",
      "Trained batch 9954 batch loss 4.55531788 epoch total loss 6.18952274\n",
      "Trained batch 9955 batch loss 5.3497858 epoch total loss 6.18943834\n",
      "Trained batch 9956 batch loss 5.20109463 epoch total loss 6.18933916\n",
      "Trained batch 9957 batch loss 5.11255884 epoch total loss 6.18923092\n",
      "Trained batch 9958 batch loss 5.30347919 epoch total loss 6.18914223\n",
      "Trained batch 9959 batch loss 5.2067132 epoch total loss 6.18904352\n",
      "Trained batch 9960 batch loss 5.36975288 epoch total loss 6.18896151\n",
      "Trained batch 9961 batch loss 6.09008408 epoch total loss 6.18895149\n",
      "Trained batch 9962 batch loss 5.84774733 epoch total loss 6.18891716\n",
      "Trained batch 9963 batch loss 5.62486362 epoch total loss 6.18886042\n",
      "Trained batch 9964 batch loss 5.78218842 epoch total loss 6.18881941\n",
      "Trained batch 9965 batch loss 6.54137802 epoch total loss 6.18885517\n",
      "Trained batch 9966 batch loss 5.67664766 epoch total loss 6.18880367\n",
      "Trained batch 9967 batch loss 6.05622864 epoch total loss 6.18879032\n",
      "Trained batch 9968 batch loss 5.63638067 epoch total loss 6.18873501\n",
      "Trained batch 9969 batch loss 5.87923717 epoch total loss 6.18870354\n",
      "Trained batch 9970 batch loss 5.64871883 epoch total loss 6.18864965\n",
      "Trained batch 9971 batch loss 5.12035036 epoch total loss 6.18854237\n",
      "Trained batch 9972 batch loss 5.80244446 epoch total loss 6.18850374\n",
      "Trained batch 9973 batch loss 5.60049248 epoch total loss 6.18844461\n",
      "Trained batch 9974 batch loss 5.82581806 epoch total loss 6.18840837\n",
      "Trained batch 9975 batch loss 6.10116291 epoch total loss 6.18839931\n",
      "Trained batch 9976 batch loss 5.51522207 epoch total loss 6.18833208\n",
      "Trained batch 9977 batch loss 5.36013699 epoch total loss 6.18824911\n",
      "Trained batch 9978 batch loss 5.40419 epoch total loss 6.18817043\n",
      "Trained batch 9979 batch loss 5.30397177 epoch total loss 6.18808174\n",
      "Trained batch 9980 batch loss 6.00552 epoch total loss 6.18806314\n",
      "Trained batch 9981 batch loss 6.50835562 epoch total loss 6.18809509\n",
      "Trained batch 9982 batch loss 6.15250397 epoch total loss 6.18809175\n",
      "Trained batch 9983 batch loss 6.02724457 epoch total loss 6.18807554\n",
      "Trained batch 9984 batch loss 6.86541557 epoch total loss 6.18814373\n",
      "Trained batch 9985 batch loss 6.15426445 epoch total loss 6.18814\n",
      "Trained batch 9986 batch loss 6.20663357 epoch total loss 6.18814182\n",
      "Trained batch 9987 batch loss 5.31038 epoch total loss 6.18805361\n",
      "Trained batch 9988 batch loss 5.78515816 epoch total loss 6.18801355\n",
      "Trained batch 9989 batch loss 5.74267387 epoch total loss 6.18796873\n",
      "Trained batch 9990 batch loss 5.87920094 epoch total loss 6.18793774\n",
      "Trained batch 9991 batch loss 5.85142326 epoch total loss 6.18790436\n",
      "Trained batch 9992 batch loss 6.12760735 epoch total loss 6.18789816\n",
      "Trained batch 9993 batch loss 4.95393324 epoch total loss 6.18777466\n",
      "Trained batch 9994 batch loss 5.50051546 epoch total loss 6.18770599\n",
      "Trained batch 9995 batch loss 6.10761738 epoch total loss 6.18769836\n",
      "Trained batch 9996 batch loss 6.38369656 epoch total loss 6.18771744\n",
      "Trained batch 9997 batch loss 6.10275507 epoch total loss 6.18770885\n",
      "Trained batch 9998 batch loss 6.91082 epoch total loss 6.18778133\n",
      "Trained batch 9999 batch loss 7.16178131 epoch total loss 6.18787861\n",
      "Trained batch 10000 batch loss 6.28561878 epoch total loss 6.18788815\n",
      "Trained batch 10001 batch loss 6.65812778 epoch total loss 6.18793488\n",
      "Trained batch 10002 batch loss 5.42261887 epoch total loss 6.18785858\n",
      "Trained batch 10003 batch loss 6.03894 epoch total loss 6.1878438\n",
      "Trained batch 10004 batch loss 5.04560041 epoch total loss 6.18772936\n",
      "Trained batch 10005 batch loss 5.39199448 epoch total loss 6.18764973\n",
      "Trained batch 10006 batch loss 5.45304394 epoch total loss 6.18757629\n",
      "Trained batch 10007 batch loss 5.79607677 epoch total loss 6.18753767\n",
      "Trained batch 10008 batch loss 5.6885848 epoch total loss 6.1874876\n",
      "Trained batch 10009 batch loss 6.1332612 epoch total loss 6.18748188\n",
      "Trained batch 10010 batch loss 5.80449867 epoch total loss 6.18744373\n",
      "Trained batch 10011 batch loss 6.12216282 epoch total loss 6.18743706\n",
      "Trained batch 10012 batch loss 6.55391884 epoch total loss 6.18747377\n",
      "Trained batch 10013 batch loss 6.41069603 epoch total loss 6.18749619\n",
      "Trained batch 10014 batch loss 7.16717148 epoch total loss 6.18759394\n",
      "Trained batch 10015 batch loss 5.67417574 epoch total loss 6.18754292\n",
      "Trained batch 10016 batch loss 6.61847687 epoch total loss 6.18758583\n",
      "Trained batch 10017 batch loss 5.43470383 epoch total loss 6.18751049\n",
      "Trained batch 10018 batch loss 6.35980034 epoch total loss 6.18752766\n",
      "Trained batch 10019 batch loss 5.63096094 epoch total loss 6.18747234\n",
      "Trained batch 10020 batch loss 6.3345623 epoch total loss 6.18748713\n",
      "Trained batch 10021 batch loss 7.90275717 epoch total loss 6.18765831\n",
      "Trained batch 10022 batch loss 7.40133286 epoch total loss 6.18777943\n",
      "Trained batch 10023 batch loss 6.05646706 epoch total loss 6.18776608\n",
      "Trained batch 10024 batch loss 6.50382233 epoch total loss 6.18779755\n",
      "Trained batch 10025 batch loss 5.86288118 epoch total loss 6.18776512\n",
      "Trained batch 10026 batch loss 6.69668388 epoch total loss 6.18781614\n",
      "Trained batch 10027 batch loss 6.27545357 epoch total loss 6.18782473\n",
      "Trained batch 10028 batch loss 6.46472454 epoch total loss 6.18785238\n",
      "Trained batch 10029 batch loss 4.87778091 epoch total loss 6.18772221\n",
      "Trained batch 10030 batch loss 5.08985806 epoch total loss 6.18761253\n",
      "Trained batch 10031 batch loss 6.20887327 epoch total loss 6.18761444\n",
      "Trained batch 10032 batch loss 6.34789324 epoch total loss 6.18763065\n",
      "Trained batch 10033 batch loss 4.96602 epoch total loss 6.18750858\n",
      "Trained batch 10034 batch loss 4.21757936 epoch total loss 6.18731213\n",
      "Trained batch 10035 batch loss 6.79134369 epoch total loss 6.18737268\n",
      "Trained batch 10036 batch loss 5.80980968 epoch total loss 6.18733501\n",
      "Trained batch 10037 batch loss 6.24646 epoch total loss 6.18734074\n",
      "Trained batch 10038 batch loss 4.2453804 epoch total loss 6.18714762\n",
      "Trained batch 10039 batch loss 5.75846291 epoch total loss 6.1871047\n",
      "Trained batch 10040 batch loss 6.00718784 epoch total loss 6.18708658\n",
      "Trained batch 10041 batch loss 6.92658901 epoch total loss 6.18716049\n",
      "Trained batch 10042 batch loss 6.03756809 epoch total loss 6.18714571\n",
      "Trained batch 10043 batch loss 6.44758701 epoch total loss 6.18717194\n",
      "Trained batch 10044 batch loss 6.75766182 epoch total loss 6.18722868\n",
      "Trained batch 10045 batch loss 6.12152386 epoch total loss 6.187222\n",
      "Trained batch 10046 batch loss 6.250453 epoch total loss 6.1872282\n",
      "Trained batch 10047 batch loss 5.48265171 epoch total loss 6.18715811\n",
      "Trained batch 10048 batch loss 6.55661583 epoch total loss 6.18719482\n",
      "Trained batch 10049 batch loss 5.5868597 epoch total loss 6.18713522\n",
      "Trained batch 10050 batch loss 6.35337543 epoch total loss 6.18715143\n",
      "Trained batch 10051 batch loss 6.15902138 epoch total loss 6.18714857\n",
      "Trained batch 10052 batch loss 6.17909431 epoch total loss 6.18714809\n",
      "Trained batch 10053 batch loss 6.68767166 epoch total loss 6.18719769\n",
      "Trained batch 10054 batch loss 6.66928196 epoch total loss 6.18724537\n",
      "Trained batch 10055 batch loss 5.39675331 epoch total loss 6.18716717\n",
      "Trained batch 10056 batch loss 6.470788 epoch total loss 6.1871953\n",
      "Trained batch 10057 batch loss 6.24746513 epoch total loss 6.1872015\n",
      "Trained batch 10058 batch loss 5.67949724 epoch total loss 6.18715096\n",
      "Trained batch 10059 batch loss 5.8788743 epoch total loss 6.18712044\n",
      "Trained batch 10060 batch loss 6.10115337 epoch total loss 6.18711185\n",
      "Trained batch 10061 batch loss 6.43683624 epoch total loss 6.18713665\n",
      "Trained batch 10062 batch loss 5.45466089 epoch total loss 6.18706369\n",
      "Trained batch 10063 batch loss 5.92228699 epoch total loss 6.18703747\n",
      "Trained batch 10064 batch loss 5.53053951 epoch total loss 6.18697214\n",
      "Trained batch 10065 batch loss 5.7356987 epoch total loss 6.18692732\n",
      "Trained batch 10066 batch loss 5.76381636 epoch total loss 6.18688536\n",
      "Trained batch 10067 batch loss 6.43787861 epoch total loss 6.18691\n",
      "Trained batch 10068 batch loss 6.84853888 epoch total loss 6.18697596\n",
      "Trained batch 10069 batch loss 5.60103512 epoch total loss 6.18691778\n",
      "Trained batch 10070 batch loss 6.57786083 epoch total loss 6.18695641\n",
      "Trained batch 10071 batch loss 7.14482164 epoch total loss 6.18705177\n",
      "Trained batch 10072 batch loss 7.378088 epoch total loss 6.18717\n",
      "Trained batch 10073 batch loss 7.12277317 epoch total loss 6.18726254\n",
      "Trained batch 10074 batch loss 5.70284939 epoch total loss 6.18721437\n",
      "Trained batch 10075 batch loss 6.50046253 epoch total loss 6.18724585\n",
      "Trained batch 10076 batch loss 6.8951807 epoch total loss 6.18731594\n",
      "Trained batch 10077 batch loss 6.39873838 epoch total loss 6.18733692\n",
      "Trained batch 10078 batch loss 6.31480026 epoch total loss 6.18735\n",
      "Trained batch 10079 batch loss 6.00034761 epoch total loss 6.1873312\n",
      "Trained batch 10080 batch loss 5.9064312 epoch total loss 6.18730307\n",
      "Trained batch 10081 batch loss 6.26862669 epoch total loss 6.18731117\n",
      "Trained batch 10082 batch loss 6.75131702 epoch total loss 6.18736696\n",
      "Trained batch 10083 batch loss 6.23040628 epoch total loss 6.18737125\n",
      "Trained batch 10084 batch loss 6.16460896 epoch total loss 6.18736887\n",
      "Trained batch 10085 batch loss 6.34259892 epoch total loss 6.18738461\n",
      "Trained batch 10086 batch loss 6.26025772 epoch total loss 6.18739176\n",
      "Trained batch 10087 batch loss 5.82024288 epoch total loss 6.18735552\n",
      "Trained batch 10088 batch loss 5.90323067 epoch total loss 6.18732738\n",
      "Trained batch 10089 batch loss 5.92778587 epoch total loss 6.18730164\n",
      "Trained batch 10090 batch loss 5.49658203 epoch total loss 6.18723345\n",
      "Trained batch 10091 batch loss 5.99227047 epoch total loss 6.1872139\n",
      "Trained batch 10092 batch loss 5.81472969 epoch total loss 6.18717718\n",
      "Trained batch 10093 batch loss 5.25941133 epoch total loss 6.18708515\n",
      "Trained batch 10094 batch loss 4.96038818 epoch total loss 6.18696356\n",
      "Trained batch 10095 batch loss 4.90598822 epoch total loss 6.18683672\n",
      "Trained batch 10096 batch loss 5.78023767 epoch total loss 6.18679667\n",
      "Trained batch 10097 batch loss 5.81512165 epoch total loss 6.18676\n",
      "Trained batch 10098 batch loss 4.76629925 epoch total loss 6.18661928\n",
      "Trained batch 10099 batch loss 6.35788918 epoch total loss 6.18663645\n",
      "Trained batch 10100 batch loss 6.26358366 epoch total loss 6.1866436\n",
      "Trained batch 10101 batch loss 5.06795406 epoch total loss 6.18653297\n",
      "Trained batch 10102 batch loss 6.28342438 epoch total loss 6.18654251\n",
      "Trained batch 10103 batch loss 6.11433315 epoch total loss 6.18653536\n",
      "Trained batch 10104 batch loss 4.9565506 epoch total loss 6.18641376\n",
      "Trained batch 10105 batch loss 4.5124321 epoch total loss 6.18624783\n",
      "Trained batch 10106 batch loss 6.53951311 epoch total loss 6.18628263\n",
      "Trained batch 10107 batch loss 5.9736743 epoch total loss 6.18626165\n",
      "Trained batch 10108 batch loss 6.71705341 epoch total loss 6.18631458\n",
      "Trained batch 10109 batch loss 7.86981344 epoch total loss 6.186481\n",
      "Trained batch 10110 batch loss 6.51707172 epoch total loss 6.18651342\n",
      "Trained batch 10111 batch loss 5.74280167 epoch total loss 6.18646955\n",
      "Trained batch 10112 batch loss 6.09478378 epoch total loss 6.18646049\n",
      "Trained batch 10113 batch loss 6.11668253 epoch total loss 6.18645382\n",
      "Trained batch 10114 batch loss 6.14108181 epoch total loss 6.18644905\n",
      "Trained batch 10115 batch loss 5.58608389 epoch total loss 6.18639\n",
      "Trained batch 10116 batch loss 6.19048929 epoch total loss 6.1863904\n",
      "Trained batch 10117 batch loss 6.11346102 epoch total loss 6.18638277\n",
      "Trained batch 10118 batch loss 6.0383997 epoch total loss 6.18636847\n",
      "Trained batch 10119 batch loss 5.44160223 epoch total loss 6.18629503\n",
      "Trained batch 10120 batch loss 4.93355083 epoch total loss 6.18617105\n",
      "Trained batch 10121 batch loss 5.94928312 epoch total loss 6.18614769\n",
      "Trained batch 10122 batch loss 5.97418976 epoch total loss 6.18612671\n",
      "Trained batch 10123 batch loss 6.02361155 epoch total loss 6.1861105\n",
      "Trained batch 10124 batch loss 5.0704565 epoch total loss 6.18600035\n",
      "Trained batch 10125 batch loss 4.53164959 epoch total loss 6.18583679\n",
      "Trained batch 10126 batch loss 4.8270607 epoch total loss 6.1857028\n",
      "Trained batch 10127 batch loss 5.14529324 epoch total loss 6.1856\n",
      "Trained batch 10128 batch loss 5.20482683 epoch total loss 6.18550301\n",
      "Trained batch 10129 batch loss 5.2446785 epoch total loss 6.18541\n",
      "Trained batch 10130 batch loss 5.92373371 epoch total loss 6.18538427\n",
      "Trained batch 10131 batch loss 5.92039776 epoch total loss 6.18535805\n",
      "Trained batch 10132 batch loss 5.58607 epoch total loss 6.18529892\n",
      "Trained batch 10133 batch loss 5.64732838 epoch total loss 6.18524599\n",
      "Trained batch 10134 batch loss 5.4965415 epoch total loss 6.1851778\n",
      "Trained batch 10135 batch loss 5.3252306 epoch total loss 6.18509293\n",
      "Trained batch 10136 batch loss 5.2191782 epoch total loss 6.18499756\n",
      "Trained batch 10137 batch loss 5.59734392 epoch total loss 6.18494\n",
      "Trained batch 10138 batch loss 5.89226818 epoch total loss 6.18491077\n",
      "Trained batch 10139 batch loss 4.93222427 epoch total loss 6.18478727\n",
      "Trained batch 10140 batch loss 5.26296234 epoch total loss 6.1846962\n",
      "Trained batch 10141 batch loss 4.79595518 epoch total loss 6.18455935\n",
      "Trained batch 10142 batch loss 6.05724812 epoch total loss 6.18454695\n",
      "Trained batch 10143 batch loss 5.84221 epoch total loss 6.18451357\n",
      "Trained batch 10144 batch loss 6.15644026 epoch total loss 6.18451071\n",
      "Trained batch 10145 batch loss 4.27369595 epoch total loss 6.18432236\n",
      "Trained batch 10146 batch loss 5.23744106 epoch total loss 6.1842289\n",
      "Trained batch 10147 batch loss 3.72954535 epoch total loss 6.18398714\n",
      "Trained batch 10148 batch loss 6.3517828 epoch total loss 6.18400383\n",
      "Trained batch 10149 batch loss 5.50335264 epoch total loss 6.1839366\n",
      "Trained batch 10150 batch loss 6.21950245 epoch total loss 6.18394\n",
      "Trained batch 10151 batch loss 4.86190891 epoch total loss 6.18381\n",
      "Trained batch 10152 batch loss 5.55116749 epoch total loss 6.18374777\n",
      "Trained batch 10153 batch loss 5.81081963 epoch total loss 6.18371105\n",
      "Trained batch 10154 batch loss 6.00212574 epoch total loss 6.18369341\n",
      "Trained batch 10155 batch loss 6.32761097 epoch total loss 6.18370771\n",
      "Trained batch 10156 batch loss 5.87583828 epoch total loss 6.1836772\n",
      "Trained batch 10157 batch loss 6.12450647 epoch total loss 6.18367147\n",
      "Trained batch 10158 batch loss 6.21120882 epoch total loss 6.18367434\n",
      "Trained batch 10159 batch loss 6.29223347 epoch total loss 6.18368483\n",
      "Trained batch 10160 batch loss 5.80759144 epoch total loss 6.18364811\n",
      "Trained batch 10161 batch loss 6.43114471 epoch total loss 6.18367195\n",
      "Trained batch 10162 batch loss 6.96718454 epoch total loss 6.1837492\n",
      "Trained batch 10163 batch loss 6.25258303 epoch total loss 6.18375635\n",
      "Trained batch 10164 batch loss 6.34251595 epoch total loss 6.18377209\n",
      "Trained batch 10165 batch loss 6.00089455 epoch total loss 6.18375397\n",
      "Trained batch 10166 batch loss 6.56259584 epoch total loss 6.18379116\n",
      "Trained batch 10167 batch loss 5.93973255 epoch total loss 6.18376732\n",
      "Trained batch 10168 batch loss 6.49521208 epoch total loss 6.18379831\n",
      "Trained batch 10169 batch loss 6.39796066 epoch total loss 6.18381929\n",
      "Trained batch 10170 batch loss 6.1525178 epoch total loss 6.18381596\n",
      "Trained batch 10171 batch loss 6.17185497 epoch total loss 6.183815\n",
      "Trained batch 10172 batch loss 6.49595976 epoch total loss 6.18384552\n",
      "Trained batch 10173 batch loss 6.17911482 epoch total loss 6.18384504\n",
      "Trained batch 10174 batch loss 5.96558619 epoch total loss 6.18382359\n",
      "Trained batch 10175 batch loss 6.14307213 epoch total loss 6.18382\n",
      "Trained batch 10176 batch loss 6.15558624 epoch total loss 6.18381739\n",
      "Trained batch 10177 batch loss 5.79374075 epoch total loss 6.18377876\n",
      "Trained batch 10178 batch loss 6.28467369 epoch total loss 6.18378878\n",
      "Trained batch 10179 batch loss 6.48137 epoch total loss 6.18381786\n",
      "Trained batch 10180 batch loss 5.46797228 epoch total loss 6.18374777\n",
      "Trained batch 10181 batch loss 4.54808521 epoch total loss 6.18358707\n",
      "Trained batch 10182 batch loss 5.9055357 epoch total loss 6.18355942\n",
      "Trained batch 10183 batch loss 5.60112953 epoch total loss 6.18350267\n",
      "Trained batch 10184 batch loss 5.7211957 epoch total loss 6.18345737\n",
      "Trained batch 10185 batch loss 5.57771111 epoch total loss 6.18339777\n",
      "Trained batch 10186 batch loss 5.65613461 epoch total loss 6.18334579\n",
      "Trained batch 10187 batch loss 5.77022839 epoch total loss 6.18330526\n",
      "Trained batch 10188 batch loss 5.19410849 epoch total loss 6.18320847\n",
      "Trained batch 10189 batch loss 5.38285112 epoch total loss 6.18313\n",
      "Trained batch 10190 batch loss 4.84498596 epoch total loss 6.18299866\n",
      "Trained batch 10191 batch loss 6.27412319 epoch total loss 6.18300724\n",
      "Trained batch 10192 batch loss 5.78584099 epoch total loss 6.18296814\n",
      "Trained batch 10193 batch loss 5.26507092 epoch total loss 6.18287849\n",
      "Trained batch 10194 batch loss 5.76884079 epoch total loss 6.18283749\n",
      "Trained batch 10195 batch loss 6.03215504 epoch total loss 6.1828227\n",
      "Trained batch 10196 batch loss 5.83804893 epoch total loss 6.18278933\n",
      "Trained batch 10197 batch loss 6.33224678 epoch total loss 6.18280363\n",
      "Trained batch 10198 batch loss 6.08413601 epoch total loss 6.18279457\n",
      "Trained batch 10199 batch loss 5.84233761 epoch total loss 6.18276119\n",
      "Trained batch 10200 batch loss 5.92825413 epoch total loss 6.1827364\n",
      "Trained batch 10201 batch loss 5.9318471 epoch total loss 6.18271208\n",
      "Trained batch 10202 batch loss 6.36670399 epoch total loss 6.18272972\n",
      "Trained batch 10203 batch loss 5.78840351 epoch total loss 6.18269157\n",
      "Trained batch 10204 batch loss 6.55821323 epoch total loss 6.18272829\n",
      "Trained batch 10205 batch loss 6.18570852 epoch total loss 6.18272877\n",
      "Trained batch 10206 batch loss 6.12650394 epoch total loss 6.18272305\n",
      "Trained batch 10207 batch loss 6.74093437 epoch total loss 6.18277788\n",
      "Trained batch 10208 batch loss 6.66434479 epoch total loss 6.18282509\n",
      "Trained batch 10209 batch loss 6.16121912 epoch total loss 6.1828227\n",
      "Trained batch 10210 batch loss 6.65222025 epoch total loss 6.18286896\n",
      "Trained batch 10211 batch loss 6.25337696 epoch total loss 6.18287563\n",
      "Trained batch 10212 batch loss 6.23569107 epoch total loss 6.18288088\n",
      "Trained batch 10213 batch loss 5.60785866 epoch total loss 6.18282461\n",
      "Trained batch 10214 batch loss 6.26564693 epoch total loss 6.18283272\n",
      "Trained batch 10215 batch loss 5.76932526 epoch total loss 6.18279219\n",
      "Trained batch 10216 batch loss 5.93469429 epoch total loss 6.18276787\n",
      "Trained batch 10217 batch loss 5.71443701 epoch total loss 6.18272209\n",
      "Trained batch 10218 batch loss 6.27571392 epoch total loss 6.18273115\n",
      "Trained batch 10219 batch loss 5.98262835 epoch total loss 6.18271208\n",
      "Trained batch 10220 batch loss 6.24804974 epoch total loss 6.18271828\n",
      "Trained batch 10221 batch loss 6.15105152 epoch total loss 6.18271542\n",
      "Trained batch 10222 batch loss 5.87168741 epoch total loss 6.1826849\n",
      "Trained batch 10223 batch loss 5.81560135 epoch total loss 6.18264914\n",
      "Trained batch 10224 batch loss 6.36329 epoch total loss 6.18266678\n",
      "Trained batch 10225 batch loss 6.16900587 epoch total loss 6.18266535\n",
      "Trained batch 10226 batch loss 5.50030327 epoch total loss 6.18259859\n",
      "Trained batch 10227 batch loss 5.20965385 epoch total loss 6.1825037\n",
      "Trained batch 10228 batch loss 5.79397106 epoch total loss 6.18246555\n",
      "Trained batch 10229 batch loss 5.60057545 epoch total loss 6.18240881\n",
      "Trained batch 10230 batch loss 5.9646349 epoch total loss 6.18238735\n",
      "Trained batch 10231 batch loss 6.0138464 epoch total loss 6.18237114\n",
      "Trained batch 10232 batch loss 5.56146908 epoch total loss 6.18231058\n",
      "Trained batch 10233 batch loss 5.65878963 epoch total loss 6.18225956\n",
      "Trained batch 10234 batch loss 5.80142927 epoch total loss 6.18222237\n",
      "Trained batch 10235 batch loss 5.79842091 epoch total loss 6.1821847\n",
      "Trained batch 10236 batch loss 5.43434143 epoch total loss 6.18211174\n",
      "Trained batch 10237 batch loss 5.38202286 epoch total loss 6.18203354\n",
      "Trained batch 10238 batch loss 5.6857996 epoch total loss 6.18198538\n",
      "Trained batch 10239 batch loss 5.11669493 epoch total loss 6.18188095\n",
      "Trained batch 10240 batch loss 5.15881729 epoch total loss 6.18178129\n",
      "Trained batch 10241 batch loss 5.37987852 epoch total loss 6.18170309\n",
      "Trained batch 10242 batch loss 5.11078453 epoch total loss 6.18159819\n",
      "Trained batch 10243 batch loss 5.41538 epoch total loss 6.18152332\n",
      "Trained batch 10244 batch loss 5.47930813 epoch total loss 6.18145514\n",
      "Trained batch 10245 batch loss 5.67228699 epoch total loss 6.18140507\n",
      "Trained batch 10246 batch loss 5.89499187 epoch total loss 6.18137741\n",
      "Trained batch 10247 batch loss 5.19193602 epoch total loss 6.18128061\n",
      "Trained batch 10248 batch loss 5.99062634 epoch total loss 6.18126202\n",
      "Trained batch 10249 batch loss 5.08327436 epoch total loss 6.18115473\n",
      "Trained batch 10250 batch loss 5.97085953 epoch total loss 6.1811347\n",
      "Trained batch 10251 batch loss 5.63047314 epoch total loss 6.18108082\n",
      "Trained batch 10252 batch loss 5.46653557 epoch total loss 6.18101072\n",
      "Trained batch 10253 batch loss 5.86951065 epoch total loss 6.18098068\n",
      "Trained batch 10254 batch loss 5.33060503 epoch total loss 6.18089771\n",
      "Trained batch 10255 batch loss 5.64961576 epoch total loss 6.18084574\n",
      "Trained batch 10256 batch loss 6.40442848 epoch total loss 6.18086767\n",
      "Trained batch 10257 batch loss 6.87753391 epoch total loss 6.18093586\n",
      "Trained batch 10258 batch loss 6.90804386 epoch total loss 6.18100643\n",
      "Trained batch 10259 batch loss 6.96093082 epoch total loss 6.18108273\n",
      "Trained batch 10260 batch loss 6.30674267 epoch total loss 6.18109512\n",
      "Trained batch 10261 batch loss 7.37311077 epoch total loss 6.18121147\n",
      "Trained batch 10262 batch loss 6.44431305 epoch total loss 6.18123722\n",
      "Trained batch 10263 batch loss 6.12123108 epoch total loss 6.1812315\n",
      "Trained batch 10264 batch loss 6.20949697 epoch total loss 6.18123436\n",
      "Trained batch 10265 batch loss 6.17974234 epoch total loss 6.18123388\n",
      "Trained batch 10266 batch loss 6.7436657 epoch total loss 6.18128872\n",
      "Trained batch 10267 batch loss 5.93730164 epoch total loss 6.18126488\n",
      "Trained batch 10268 batch loss 6.84878254 epoch total loss 6.18132973\n",
      "Trained batch 10269 batch loss 6.65832043 epoch total loss 6.18137646\n",
      "Trained batch 10270 batch loss 6.12433386 epoch total loss 6.18137074\n",
      "Trained batch 10271 batch loss 6.21871567 epoch total loss 6.18137455\n",
      "Trained batch 10272 batch loss 6.54656696 epoch total loss 6.18141031\n",
      "Trained batch 10273 batch loss 6.92951965 epoch total loss 6.18148279\n",
      "Trained batch 10274 batch loss 6.2718544 epoch total loss 6.18149185\n",
      "Trained batch 10275 batch loss 5.94692 epoch total loss 6.18146896\n",
      "Trained batch 10276 batch loss 5.30274725 epoch total loss 6.18138361\n",
      "Trained batch 10277 batch loss 6.20463657 epoch total loss 6.18138599\n",
      "Trained batch 10278 batch loss 6.48756647 epoch total loss 6.18141556\n",
      "Trained batch 10279 batch loss 6.81721592 epoch total loss 6.18147755\n",
      "Trained batch 10280 batch loss 6.35469723 epoch total loss 6.18149424\n",
      "Trained batch 10281 batch loss 6.70067883 epoch total loss 6.18154478\n",
      "Trained batch 10282 batch loss 6.76661205 epoch total loss 6.18160152\n",
      "Trained batch 10283 batch loss 6.46934605 epoch total loss 6.18162918\n",
      "Trained batch 10284 batch loss 6.35931873 epoch total loss 6.18164682\n",
      "Trained batch 10285 batch loss 6.07121372 epoch total loss 6.18163586\n",
      "Trained batch 10286 batch loss 6.40005064 epoch total loss 6.18165684\n",
      "Trained batch 10287 batch loss 6.3939271 epoch total loss 6.18167782\n",
      "Trained batch 10288 batch loss 6.07823753 epoch total loss 6.1816678\n",
      "Trained batch 10289 batch loss 5.98757935 epoch total loss 6.18164873\n",
      "Trained batch 10290 batch loss 5.13905239 epoch total loss 6.18154764\n",
      "Trained batch 10291 batch loss 5.74578762 epoch total loss 6.1815052\n",
      "Trained batch 10292 batch loss 5.38889313 epoch total loss 6.18142843\n",
      "Trained batch 10293 batch loss 6.28444529 epoch total loss 6.18143845\n",
      "Trained batch 10294 batch loss 6.46334267 epoch total loss 6.1814661\n",
      "Trained batch 10295 batch loss 6.48539686 epoch total loss 6.18149567\n",
      "Trained batch 10296 batch loss 6.57354546 epoch total loss 6.18153381\n",
      "Trained batch 10297 batch loss 6.90785885 epoch total loss 6.18160391\n",
      "Trained batch 10298 batch loss 5.54639626 epoch total loss 6.1815424\n",
      "Trained batch 10299 batch loss 5.25684738 epoch total loss 6.18145275\n",
      "Trained batch 10300 batch loss 5.3016386 epoch total loss 6.1813674\n",
      "Trained batch 10301 batch loss 5.98739147 epoch total loss 6.18134832\n",
      "Trained batch 10302 batch loss 5.58390331 epoch total loss 6.18129\n",
      "Trained batch 10303 batch loss 5.80502129 epoch total loss 6.18125391\n",
      "Trained batch 10304 batch loss 6.14158821 epoch total loss 6.18124962\n",
      "Trained batch 10305 batch loss 6.39416885 epoch total loss 6.1812706\n",
      "Trained batch 10306 batch loss 6.21590805 epoch total loss 6.18127394\n",
      "Trained batch 10307 batch loss 5.59260321 epoch total loss 6.18121672\n",
      "Trained batch 10308 batch loss 6.47735786 epoch total loss 6.18124533\n",
      "Trained batch 10309 batch loss 6.05732107 epoch total loss 6.18123341\n",
      "Trained batch 10310 batch loss 5.90240097 epoch total loss 6.18120623\n",
      "Trained batch 10311 batch loss 5.90224266 epoch total loss 6.18117952\n",
      "Trained batch 10312 batch loss 5.84333706 epoch total loss 6.18114662\n",
      "Trained batch 10313 batch loss 6.52222443 epoch total loss 6.18118\n",
      "Trained batch 10314 batch loss 5.86724043 epoch total loss 6.18114948\n",
      "Trained batch 10315 batch loss 6.03916168 epoch total loss 6.18113565\n",
      "Trained batch 10316 batch loss 6.53389072 epoch total loss 6.18117\n",
      "Trained batch 10317 batch loss 6.50788784 epoch total loss 6.18120146\n",
      "Trained batch 10318 batch loss 6.80549622 epoch total loss 6.18126202\n",
      "Trained batch 10319 batch loss 6.71520233 epoch total loss 6.18131351\n",
      "Trained batch 10320 batch loss 6.55322599 epoch total loss 6.18134975\n",
      "Trained batch 10321 batch loss 6.70933914 epoch total loss 6.18140125\n",
      "Trained batch 10322 batch loss 6.15824699 epoch total loss 6.18139935\n",
      "Trained batch 10323 batch loss 5.71180391 epoch total loss 6.18135357\n",
      "Trained batch 10324 batch loss 6.39864159 epoch total loss 6.18137455\n",
      "Trained batch 10325 batch loss 5.91117096 epoch total loss 6.18134832\n",
      "Trained batch 10326 batch loss 5.67998934 epoch total loss 6.18129969\n",
      "Trained batch 10327 batch loss 5.68161678 epoch total loss 6.18125105\n",
      "Trained batch 10328 batch loss 5.91791439 epoch total loss 6.18122578\n",
      "Trained batch 10329 batch loss 6.19099188 epoch total loss 6.18122673\n",
      "Trained batch 10330 batch loss 6.69352388 epoch total loss 6.18127632\n",
      "Trained batch 10331 batch loss 5.86501217 epoch total loss 6.1812458\n",
      "Trained batch 10332 batch loss 6.19700336 epoch total loss 6.18124723\n",
      "Trained batch 10333 batch loss 6.21166134 epoch total loss 6.18125\n",
      "Trained batch 10334 batch loss 6.24332762 epoch total loss 6.18125582\n",
      "Trained batch 10335 batch loss 6.1856966 epoch total loss 6.18125629\n",
      "Trained batch 10336 batch loss 6.17878342 epoch total loss 6.18125629\n",
      "Trained batch 10337 batch loss 6.10721493 epoch total loss 6.18124914\n",
      "Trained batch 10338 batch loss 5.99671602 epoch total loss 6.18123102\n",
      "Trained batch 10339 batch loss 6.07946491 epoch total loss 6.18122101\n",
      "Trained batch 10340 batch loss 5.96148539 epoch total loss 6.18119955\n",
      "Trained batch 10341 batch loss 5.86932135 epoch total loss 6.18117\n",
      "Trained batch 10342 batch loss 5.92296219 epoch total loss 6.18114471\n",
      "Trained batch 10343 batch loss 6.19242287 epoch total loss 6.18114567\n",
      "Trained batch 10344 batch loss 6.14808369 epoch total loss 6.18114233\n",
      "Trained batch 10345 batch loss 5.84558916 epoch total loss 6.18111\n",
      "Trained batch 10346 batch loss 5.86395121 epoch total loss 6.18107939\n",
      "Trained batch 10347 batch loss 5.99715328 epoch total loss 6.18106127\n",
      "Trained batch 10348 batch loss 5.5597353 epoch total loss 6.18100119\n",
      "Trained batch 10349 batch loss 5.03688812 epoch total loss 6.18089056\n",
      "Trained batch 10350 batch loss 5.8819313 epoch total loss 6.18086147\n",
      "Trained batch 10351 batch loss 6.2045126 epoch total loss 6.18086386\n",
      "Trained batch 10352 batch loss 5.56151247 epoch total loss 6.18080425\n",
      "Trained batch 10353 batch loss 6.70914268 epoch total loss 6.18085527\n",
      "Trained batch 10354 batch loss 6.9182539 epoch total loss 6.18092632\n",
      "Trained batch 10355 batch loss 7.08508492 epoch total loss 6.18101406\n",
      "Trained batch 10356 batch loss 5.65182972 epoch total loss 6.18096304\n",
      "Trained batch 10357 batch loss 5.6168561 epoch total loss 6.1809082\n",
      "Trained batch 10358 batch loss 6.05388975 epoch total loss 6.18089628\n",
      "Trained batch 10359 batch loss 6.48868322 epoch total loss 6.18092585\n",
      "Trained batch 10360 batch loss 5.44934559 epoch total loss 6.18085527\n",
      "Trained batch 10361 batch loss 6.15641594 epoch total loss 6.18085289\n",
      "Trained batch 10362 batch loss 6.14696836 epoch total loss 6.18084955\n",
      "Trained batch 10363 batch loss 6.81038952 epoch total loss 6.18091\n",
      "Trained batch 10364 batch loss 7.14457178 epoch total loss 6.18100309\n",
      "Trained batch 10365 batch loss 6.54843616 epoch total loss 6.18103838\n",
      "Trained batch 10366 batch loss 6.21212864 epoch total loss 6.18104124\n",
      "Trained batch 10367 batch loss 5.7319212 epoch total loss 6.18099785\n",
      "Trained batch 10368 batch loss 6.42906952 epoch total loss 6.18102217\n",
      "Trained batch 10369 batch loss 6.04407501 epoch total loss 6.18100882\n",
      "Trained batch 10370 batch loss 6.26025295 epoch total loss 6.18101645\n",
      "Trained batch 10371 batch loss 6.15301323 epoch total loss 6.18101358\n",
      "Trained batch 10372 batch loss 6.00931835 epoch total loss 6.18099689\n",
      "Trained batch 10373 batch loss 5.99499512 epoch total loss 6.18097925\n",
      "Trained batch 10374 batch loss 6.05846262 epoch total loss 6.18096733\n",
      "Trained batch 10375 batch loss 6.27652597 epoch total loss 6.18097687\n",
      "Trained batch 10376 batch loss 5.91372776 epoch total loss 6.18095112\n",
      "Trained batch 10377 batch loss 5.8127265 epoch total loss 6.18091536\n",
      "Trained batch 10378 batch loss 6.20257473 epoch total loss 6.18091774\n",
      "Trained batch 10379 batch loss 5.55559158 epoch total loss 6.18085718\n",
      "Trained batch 10380 batch loss 5.96941 epoch total loss 6.18083668\n",
      "Trained batch 10381 batch loss 5.46891689 epoch total loss 6.18076801\n",
      "Trained batch 10382 batch loss 6.15570354 epoch total loss 6.18076563\n",
      "Trained batch 10383 batch loss 6.3029 epoch total loss 6.18077755\n",
      "Trained batch 10384 batch loss 6.64566612 epoch total loss 6.18082237\n",
      "Trained batch 10385 batch loss 6.3981514 epoch total loss 6.18084335\n",
      "Trained batch 10386 batch loss 6.27143383 epoch total loss 6.18085194\n",
      "Trained batch 10387 batch loss 5.75860214 epoch total loss 6.18081141\n",
      "Trained batch 10388 batch loss 5.50787258 epoch total loss 6.18074656\n",
      "Trained batch 10389 batch loss 5.43223143 epoch total loss 6.18067455\n",
      "Trained batch 10390 batch loss 5.70693827 epoch total loss 6.18062878\n",
      "Trained batch 10391 batch loss 6.51058626 epoch total loss 6.18066072\n",
      "Trained batch 10392 batch loss 7.04827595 epoch total loss 6.18074417\n",
      "Trained batch 10393 batch loss 6.35594559 epoch total loss 6.18076086\n",
      "Trained batch 10394 batch loss 6.56663704 epoch total loss 6.18079805\n",
      "Trained batch 10395 batch loss 4.98622131 epoch total loss 6.18068314\n",
      "Trained batch 10396 batch loss 5.84908962 epoch total loss 6.18065071\n",
      "Trained batch 10397 batch loss 6.17087793 epoch total loss 6.18065\n",
      "Trained batch 10398 batch loss 7.08757687 epoch total loss 6.18073702\n",
      "Trained batch 10399 batch loss 6.0721364 epoch total loss 6.18072653\n",
      "Trained batch 10400 batch loss 5.8841629 epoch total loss 6.18069792\n",
      "Trained batch 10401 batch loss 5.3786478 epoch total loss 6.18062067\n",
      "Trained batch 10402 batch loss 6.29300976 epoch total loss 6.18063164\n",
      "Trained batch 10403 batch loss 6.33050156 epoch total loss 6.18064594\n",
      "Trained batch 10404 batch loss 5.79388142 epoch total loss 6.18060875\n",
      "Trained batch 10405 batch loss 6.16112137 epoch total loss 6.18060684\n",
      "Trained batch 10406 batch loss 5.35466 epoch total loss 6.18052769\n",
      "Trained batch 10407 batch loss 6.57502842 epoch total loss 6.18056536\n",
      "Trained batch 10408 batch loss 4.44431543 epoch total loss 6.18039894\n",
      "Trained batch 10409 batch loss 4.61503124 epoch total loss 6.18024826\n",
      "Trained batch 10410 batch loss 5.9843297 epoch total loss 6.18022919\n",
      "Trained batch 10411 batch loss 6.57152 epoch total loss 6.18026686\n",
      "Trained batch 10412 batch loss 5.65024471 epoch total loss 6.18021584\n",
      "Trained batch 10413 batch loss 6.40003824 epoch total loss 6.18023682\n",
      "Trained batch 10414 batch loss 6.20673943 epoch total loss 6.1802392\n",
      "Trained batch 10415 batch loss 5.55738783 epoch total loss 6.1801796\n",
      "Trained batch 10416 batch loss 4.47455597 epoch total loss 6.18001556\n",
      "Trained batch 10417 batch loss 5.71054935 epoch total loss 6.17997074\n",
      "Trained batch 10418 batch loss 5.38102627 epoch total loss 6.17989397\n",
      "Trained batch 10419 batch loss 5.16287565 epoch total loss 6.1797967\n",
      "Trained batch 10420 batch loss 5.80501938 epoch total loss 6.17976046\n",
      "Trained batch 10421 batch loss 6.45285702 epoch total loss 6.17978668\n",
      "Trained batch 10422 batch loss 4.94202471 epoch total loss 6.17966795\n",
      "Trained batch 10423 batch loss 4.77292061 epoch total loss 6.179533\n",
      "Trained batch 10424 batch loss 4.63142109 epoch total loss 6.17938471\n",
      "Trained batch 10425 batch loss 4.84769535 epoch total loss 6.17925692\n",
      "Trained batch 10426 batch loss 4.93982792 epoch total loss 6.17913818\n",
      "Trained batch 10427 batch loss 5.468997 epoch total loss 6.17907\n",
      "Trained batch 10428 batch loss 5.05563736 epoch total loss 6.17896223\n",
      "Trained batch 10429 batch loss 4.67476559 epoch total loss 6.17881823\n",
      "Trained batch 10430 batch loss 5.84915686 epoch total loss 6.17878628\n",
      "Trained batch 10431 batch loss 6.40388 epoch total loss 6.17880774\n",
      "Trained batch 10432 batch loss 5.70014143 epoch total loss 6.17876196\n",
      "Trained batch 10433 batch loss 7.08174038 epoch total loss 6.17884827\n",
      "Trained batch 10434 batch loss 5.04030943 epoch total loss 6.17873907\n",
      "Trained batch 10435 batch loss 5.65981483 epoch total loss 6.17868948\n",
      "Trained batch 10436 batch loss 6.48942518 epoch total loss 6.17871904\n",
      "Trained batch 10437 batch loss 5.79783726 epoch total loss 6.1786828\n",
      "Trained batch 10438 batch loss 4.93977165 epoch total loss 6.17856407\n",
      "Trained batch 10439 batch loss 6.79053593 epoch total loss 6.17862272\n",
      "Trained batch 10440 batch loss 6.36553955 epoch total loss 6.17864037\n",
      "Trained batch 10441 batch loss 5.83491421 epoch total loss 6.17860794\n",
      "Trained batch 10442 batch loss 5.8680439 epoch total loss 6.1785779\n",
      "Trained batch 10443 batch loss 6.28124714 epoch total loss 6.17858791\n",
      "Trained batch 10444 batch loss 5.4826684 epoch total loss 6.17852116\n",
      "Trained batch 10445 batch loss 5.83454323 epoch total loss 6.17848873\n",
      "Trained batch 10446 batch loss 6.32609272 epoch total loss 6.17850256\n",
      "Trained batch 10447 batch loss 6.04254055 epoch total loss 6.17848969\n",
      "Trained batch 10448 batch loss 6.0346756 epoch total loss 6.17847586\n",
      "Trained batch 10449 batch loss 5.83912706 epoch total loss 6.17844343\n",
      "Trained batch 10450 batch loss 5.81530094 epoch total loss 6.17840862\n",
      "Trained batch 10451 batch loss 5.98911 epoch total loss 6.1783905\n",
      "Trained batch 10452 batch loss 5.06827736 epoch total loss 6.17828417\n",
      "Trained batch 10453 batch loss 5.58367252 epoch total loss 6.17822695\n",
      "Trained batch 10454 batch loss 5.65293217 epoch total loss 6.17817688\n",
      "Trained batch 10455 batch loss 5.733634 epoch total loss 6.17813444\n",
      "Trained batch 10456 batch loss 6.08860302 epoch total loss 6.17812586\n",
      "Trained batch 10457 batch loss 4.88033247 epoch total loss 6.17800188\n",
      "Trained batch 10458 batch loss 5.72491932 epoch total loss 6.17795849\n",
      "Trained batch 10459 batch loss 5.93553829 epoch total loss 6.17793512\n",
      "Trained batch 10460 batch loss 5.98213673 epoch total loss 6.17791605\n",
      "Trained batch 10461 batch loss 6.92265 epoch total loss 6.17798758\n",
      "Trained batch 10462 batch loss 5.29501915 epoch total loss 6.17790318\n",
      "Trained batch 10463 batch loss 4.49692 epoch total loss 6.17774248\n",
      "Trained batch 10464 batch loss 6.22297049 epoch total loss 6.17774677\n",
      "Trained batch 10465 batch loss 5.73046064 epoch total loss 6.17770386\n",
      "Trained batch 10466 batch loss 6.19175386 epoch total loss 6.17770529\n",
      "Trained batch 10467 batch loss 6.36590099 epoch total loss 6.17772341\n",
      "Trained batch 10468 batch loss 5.65401077 epoch total loss 6.17767334\n",
      "Trained batch 10469 batch loss 6.08419418 epoch total loss 6.17766428\n",
      "Trained batch 10470 batch loss 6.3228 epoch total loss 6.17767859\n",
      "Trained batch 10471 batch loss 6.09799 epoch total loss 6.17767096\n",
      "Trained batch 10472 batch loss 6.16024733 epoch total loss 6.17766905\n",
      "Trained batch 10473 batch loss 5.61235 epoch total loss 6.17761517\n",
      "Trained batch 10474 batch loss 5.86578655 epoch total loss 6.1775856\n",
      "Trained batch 10475 batch loss 6.50517845 epoch total loss 6.1776166\n",
      "Trained batch 10476 batch loss 6.2049408 epoch total loss 6.17761898\n",
      "Trained batch 10477 batch loss 6.76954842 epoch total loss 6.17767572\n",
      "Trained batch 10478 batch loss 7.14933109 epoch total loss 6.17776823\n",
      "Trained batch 10479 batch loss 5.1820879 epoch total loss 6.17767334\n",
      "Trained batch 10480 batch loss 6.98266506 epoch total loss 6.17775059\n",
      "Trained batch 10481 batch loss 6.13240719 epoch total loss 6.1777463\n",
      "Trained batch 10482 batch loss 4.35764837 epoch total loss 6.17757273\n",
      "Trained batch 10483 batch loss 6.41771698 epoch total loss 6.17759562\n",
      "Trained batch 10484 batch loss 6.18634129 epoch total loss 6.17759657\n",
      "Trained batch 10485 batch loss 6.13145494 epoch total loss 6.17759228\n",
      "Trained batch 10486 batch loss 6.27851 epoch total loss 6.17760181\n",
      "Trained batch 10487 batch loss 5.77563334 epoch total loss 6.17756367\n",
      "Trained batch 10488 batch loss 6.43634892 epoch total loss 6.17758846\n",
      "Trained batch 10489 batch loss 6.25134468 epoch total loss 6.17759514\n",
      "Trained batch 10490 batch loss 6.51561356 epoch total loss 6.17762756\n",
      "Trained batch 10491 batch loss 7.20697498 epoch total loss 6.17772579\n",
      "Trained batch 10492 batch loss 5.92895222 epoch total loss 6.17770195\n",
      "Trained batch 10493 batch loss 5.39623356 epoch total loss 6.17762756\n",
      "Trained batch 10494 batch loss 4.60567522 epoch total loss 6.17747736\n",
      "Trained batch 10495 batch loss 5.82229233 epoch total loss 6.17744398\n",
      "Trained batch 10496 batch loss 5.65653515 epoch total loss 6.17739439\n",
      "Trained batch 10497 batch loss 5.2468915 epoch total loss 6.1773057\n",
      "Trained batch 10498 batch loss 6.20321274 epoch total loss 6.17730808\n",
      "Trained batch 10499 batch loss 5.8762846 epoch total loss 6.177279\n",
      "Trained batch 10500 batch loss 5.14313173 epoch total loss 6.17718077\n",
      "Trained batch 10501 batch loss 6.10341597 epoch total loss 6.17717361\n",
      "Trained batch 10502 batch loss 6.33348274 epoch total loss 6.1771884\n",
      "Trained batch 10503 batch loss 6.62611103 epoch total loss 6.17723083\n",
      "Trained batch 10504 batch loss 5.83882618 epoch total loss 6.17719889\n",
      "Trained batch 10505 batch loss 5.95214081 epoch total loss 6.17717743\n",
      "Trained batch 10506 batch loss 5.93420792 epoch total loss 6.17715454\n",
      "Trained batch 10507 batch loss 6.38551331 epoch total loss 6.17717409\n",
      "Trained batch 10508 batch loss 5.92398548 epoch total loss 6.17715025\n",
      "Trained batch 10509 batch loss 5.17244387 epoch total loss 6.17705488\n",
      "Trained batch 10510 batch loss 5.23419952 epoch total loss 6.17696524\n",
      "Trained batch 10511 batch loss 5.44011974 epoch total loss 6.17689514\n",
      "Trained batch 10512 batch loss 4.86565971 epoch total loss 6.17677\n",
      "Trained batch 10513 batch loss 6.65098095 epoch total loss 6.17681551\n",
      "Trained batch 10514 batch loss 6.66130304 epoch total loss 6.17686176\n",
      "Trained batch 10515 batch loss 6.73437929 epoch total loss 6.17691469\n",
      "Trained batch 10516 batch loss 5.23776817 epoch total loss 6.17682552\n",
      "Trained batch 10517 batch loss 6.504498 epoch total loss 6.17685652\n",
      "Trained batch 10518 batch loss 5.89131641 epoch total loss 6.17682934\n",
      "Trained batch 10519 batch loss 6.20802402 epoch total loss 6.1768322\n",
      "Trained batch 10520 batch loss 5.38292694 epoch total loss 6.17675686\n",
      "Trained batch 10521 batch loss 6.08907 epoch total loss 6.17674828\n",
      "Trained batch 10522 batch loss 5.62929249 epoch total loss 6.1766963\n",
      "Trained batch 10523 batch loss 5.97440529 epoch total loss 6.17667675\n",
      "Trained batch 10524 batch loss 5.46546078 epoch total loss 6.17660952\n",
      "Trained batch 10525 batch loss 6.11469 epoch total loss 6.17660332\n",
      "Trained batch 10526 batch loss 6.42808628 epoch total loss 6.17662716\n",
      "Trained batch 10527 batch loss 6.3819809 epoch total loss 6.17664719\n",
      "Trained batch 10528 batch loss 6.36263 epoch total loss 6.17666483\n",
      "Trained batch 10529 batch loss 5.38195801 epoch total loss 6.17658949\n",
      "Trained batch 10530 batch loss 6.44707966 epoch total loss 6.17661476\n",
      "Trained batch 10531 batch loss 5.0747695 epoch total loss 6.17651033\n",
      "Trained batch 10532 batch loss 5.64552593 epoch total loss 6.17646\n",
      "Trained batch 10533 batch loss 5.61009216 epoch total loss 6.17640591\n",
      "Trained batch 10534 batch loss 5.71373558 epoch total loss 6.17636204\n",
      "Trained batch 10535 batch loss 6.38461924 epoch total loss 6.17638159\n",
      "Trained batch 10536 batch loss 5.35266495 epoch total loss 6.17630339\n",
      "Trained batch 10537 batch loss 6.28675 epoch total loss 6.1763134\n",
      "Trained batch 10538 batch loss 5.23947191 epoch total loss 6.17622471\n",
      "Trained batch 10539 batch loss 4.88008118 epoch total loss 6.17610168\n",
      "Trained batch 10540 batch loss 5.11551285 epoch total loss 6.17600107\n",
      "Trained batch 10541 batch loss 5.74785471 epoch total loss 6.17596\n",
      "Trained batch 10542 batch loss 5.62228203 epoch total loss 6.17590761\n",
      "Trained batch 10543 batch loss 6.08521366 epoch total loss 6.17589903\n",
      "Trained batch 10544 batch loss 6.12558842 epoch total loss 6.17589426\n",
      "Trained batch 10545 batch loss 5.51239681 epoch total loss 6.17583132\n",
      "Trained batch 10546 batch loss 6.10470486 epoch total loss 6.17582464\n",
      "Trained batch 10547 batch loss 6.17962646 epoch total loss 6.17582512\n",
      "Trained batch 10548 batch loss 5.94827318 epoch total loss 6.17580366\n",
      "Trained batch 10549 batch loss 6.19700718 epoch total loss 6.17580509\n",
      "Trained batch 10550 batch loss 5.83269501 epoch total loss 6.17577267\n",
      "Trained batch 10551 batch loss 6.20011711 epoch total loss 6.17577505\n",
      "Trained batch 10552 batch loss 5.93616581 epoch total loss 6.17575216\n",
      "Trained batch 10553 batch loss 7.16036415 epoch total loss 6.17584562\n",
      "Trained batch 10554 batch loss 6.31314325 epoch total loss 6.1758585\n",
      "Trained batch 10555 batch loss 6.38599777 epoch total loss 6.17587852\n",
      "Trained batch 10556 batch loss 5.10085583 epoch total loss 6.17577696\n",
      "Trained batch 10557 batch loss 5.05421448 epoch total loss 6.17567062\n",
      "Trained batch 10558 batch loss 5.44985867 epoch total loss 6.17560196\n",
      "Trained batch 10559 batch loss 5.52791405 epoch total loss 6.17554045\n",
      "Trained batch 10560 batch loss 6.41514492 epoch total loss 6.17556286\n",
      "Trained batch 10561 batch loss 5.11515331 epoch total loss 6.17546225\n",
      "Trained batch 10562 batch loss 7.02597952 epoch total loss 6.17554331\n",
      "Trained batch 10563 batch loss 6.9077034 epoch total loss 6.17561245\n",
      "Trained batch 10564 batch loss 7.23780918 epoch total loss 6.17571306\n",
      "Trained batch 10565 batch loss 6.1768117 epoch total loss 6.17571306\n",
      "Trained batch 10566 batch loss 5.82522488 epoch total loss 6.17567968\n",
      "Trained batch 10567 batch loss 6.17106438 epoch total loss 6.17567921\n",
      "Trained batch 10568 batch loss 5.49181 epoch total loss 6.17561436\n",
      "Trained batch 10569 batch loss 5.48012686 epoch total loss 6.17554855\n",
      "Trained batch 10570 batch loss 5.55947447 epoch total loss 6.17549038\n",
      "Trained batch 10571 batch loss 5.9194088 epoch total loss 6.17546606\n",
      "Trained batch 10572 batch loss 6.23906136 epoch total loss 6.17547178\n",
      "Trained batch 10573 batch loss 5.90582895 epoch total loss 6.17544651\n",
      "Trained batch 10574 batch loss 6.05728436 epoch total loss 6.17543554\n",
      "Trained batch 10575 batch loss 5.98454618 epoch total loss 6.17541742\n",
      "Trained batch 10576 batch loss 5.82951832 epoch total loss 6.17538452\n",
      "Trained batch 10577 batch loss 4.81209755 epoch total loss 6.17525578\n",
      "Trained batch 10578 batch loss 5.9505682 epoch total loss 6.17523432\n",
      "Trained batch 10579 batch loss 7.04205036 epoch total loss 6.17531633\n",
      "Trained batch 10580 batch loss 6.91101933 epoch total loss 6.17538595\n",
      "Trained batch 10581 batch loss 6.23847675 epoch total loss 6.17539167\n",
      "Trained batch 10582 batch loss 6.51933575 epoch total loss 6.1754241\n",
      "Trained batch 10583 batch loss 5.34167385 epoch total loss 6.17534542\n",
      "Trained batch 10584 batch loss 6.11517048 epoch total loss 6.1753397\n",
      "Trained batch 10585 batch loss 6.02553654 epoch total loss 6.17532539\n",
      "Trained batch 10586 batch loss 5.62329483 epoch total loss 6.17527342\n",
      "Trained batch 10587 batch loss 5.44906187 epoch total loss 6.17520475\n",
      "Trained batch 10588 batch loss 6.07194471 epoch total loss 6.17519522\n",
      "Trained batch 10589 batch loss 6.56505823 epoch total loss 6.17523193\n",
      "Trained batch 10590 batch loss 5.30409622 epoch total loss 6.17515\n",
      "Trained batch 10591 batch loss 7.28293228 epoch total loss 6.17525434\n",
      "Trained batch 10592 batch loss 5.61969137 epoch total loss 6.17520189\n",
      "Trained batch 10593 batch loss 6.64493847 epoch total loss 6.17524624\n",
      "Trained batch 10594 batch loss 6.77403402 epoch total loss 6.17530251\n",
      "Trained batch 10595 batch loss 6.65338945 epoch total loss 6.17534781\n",
      "Trained batch 10596 batch loss 6.19480133 epoch total loss 6.17534971\n",
      "Trained batch 10597 batch loss 4.98893356 epoch total loss 6.17523766\n",
      "Trained batch 10598 batch loss 6.01581335 epoch total loss 6.1752224\n",
      "Trained batch 10599 batch loss 5.23263454 epoch total loss 6.17513371\n",
      "Trained batch 10600 batch loss 5.35259771 epoch total loss 6.17505598\n",
      "Trained batch 10601 batch loss 5.35923815 epoch total loss 6.17497921\n",
      "Trained batch 10602 batch loss 4.62967443 epoch total loss 6.1748333\n",
      "Trained batch 10603 batch loss 6.52845097 epoch total loss 6.17486668\n",
      "Trained batch 10604 batch loss 6.67657804 epoch total loss 6.17491388\n",
      "Trained batch 10605 batch loss 6.58898067 epoch total loss 6.17495298\n",
      "Trained batch 10606 batch loss 7.0955534 epoch total loss 6.17503929\n",
      "Trained batch 10607 batch loss 7.04045105 epoch total loss 6.17512083\n",
      "Trained batch 10608 batch loss 6.76331806 epoch total loss 6.17517614\n",
      "Trained batch 10609 batch loss 6.26811123 epoch total loss 6.1751852\n",
      "Trained batch 10610 batch loss 6.23518944 epoch total loss 6.17519093\n",
      "Trained batch 10611 batch loss 6.38907146 epoch total loss 6.17521095\n",
      "Trained batch 10612 batch loss 5.62495756 epoch total loss 6.17515898\n",
      "Trained batch 10613 batch loss 6.61276817 epoch total loss 6.1752\n",
      "Trained batch 10614 batch loss 7.0193367 epoch total loss 6.17527914\n",
      "Trained batch 10615 batch loss 7.12336349 epoch total loss 6.17536879\n",
      "Trained batch 10616 batch loss 5.81384182 epoch total loss 6.17533445\n",
      "Trained batch 10617 batch loss 5.85691833 epoch total loss 6.17530489\n",
      "Trained batch 10618 batch loss 6.02556467 epoch total loss 6.17529058\n",
      "Trained batch 10619 batch loss 6.07469845 epoch total loss 6.17528152\n",
      "Trained batch 10620 batch loss 6.11645794 epoch total loss 6.1752758\n",
      "Trained batch 10621 batch loss 6.26637268 epoch total loss 6.17528439\n",
      "Trained batch 10622 batch loss 6.25228834 epoch total loss 6.17529154\n",
      "Trained batch 10623 batch loss 6.16203165 epoch total loss 6.17529058\n",
      "Trained batch 10624 batch loss 5.78324795 epoch total loss 6.17525339\n",
      "Trained batch 10625 batch loss 6.94352245 epoch total loss 6.17532587\n",
      "Trained batch 10626 batch loss 5.94022179 epoch total loss 6.17530346\n",
      "Trained batch 10627 batch loss 6.24882793 epoch total loss 6.17531061\n",
      "Trained batch 10628 batch loss 6.31419706 epoch total loss 6.17532349\n",
      "Trained batch 10629 batch loss 6.27755642 epoch total loss 6.17533302\n",
      "Trained batch 10630 batch loss 6.02202511 epoch total loss 6.17531919\n",
      "Trained batch 10631 batch loss 6.12647438 epoch total loss 6.17531443\n",
      "Trained batch 10632 batch loss 5.79710388 epoch total loss 6.17527866\n",
      "Trained batch 10633 batch loss 6.46162796 epoch total loss 6.17530537\n",
      "Trained batch 10634 batch loss 6.98593235 epoch total loss 6.17538166\n",
      "Trained batch 10635 batch loss 6.934412 epoch total loss 6.17545319\n",
      "Trained batch 10636 batch loss 7.07104301 epoch total loss 6.17553759\n",
      "Trained batch 10637 batch loss 6.96100855 epoch total loss 6.17561102\n",
      "Trained batch 10638 batch loss 6.28531551 epoch total loss 6.17562199\n",
      "Trained batch 10639 batch loss 6.60618782 epoch total loss 6.17566252\n",
      "Trained batch 10640 batch loss 5.48411751 epoch total loss 6.17559767\n",
      "Trained batch 10641 batch loss 5.45599556 epoch total loss 6.17553\n",
      "Trained batch 10642 batch loss 6.21602821 epoch total loss 6.17553377\n",
      "Trained batch 10643 batch loss 5.94952917 epoch total loss 6.17551279\n",
      "Trained batch 10644 batch loss 6.34264946 epoch total loss 6.17552853\n",
      "Trained batch 10645 batch loss 6.14680243 epoch total loss 6.17552614\n",
      "Trained batch 10646 batch loss 5.93676901 epoch total loss 6.17550373\n",
      "Trained batch 10647 batch loss 5.42755938 epoch total loss 6.17543364\n",
      "Trained batch 10648 batch loss 5.21552324 epoch total loss 6.17534399\n",
      "Trained batch 10649 batch loss 6.26310921 epoch total loss 6.17535257\n",
      "Trained batch 10650 batch loss 6.97393942 epoch total loss 6.17542744\n",
      "Trained batch 10651 batch loss 6.55088615 epoch total loss 6.1754632\n",
      "Trained batch 10652 batch loss 6.81309891 epoch total loss 6.17552328\n",
      "Trained batch 10653 batch loss 6.27930975 epoch total loss 6.17553282\n",
      "Trained batch 10654 batch loss 6.059062 epoch total loss 6.17552233\n",
      "Trained batch 10655 batch loss 6.64025784 epoch total loss 6.1755662\n",
      "Trained batch 10656 batch loss 6.52276564 epoch total loss 6.17559862\n",
      "Trained batch 10657 batch loss 4.86771774 epoch total loss 6.17547607\n",
      "Trained batch 10658 batch loss 5.32023239 epoch total loss 6.17539549\n",
      "Trained batch 10659 batch loss 5.41962576 epoch total loss 6.17532492\n",
      "Trained batch 10660 batch loss 4.96795034 epoch total loss 6.17521191\n",
      "Trained batch 10661 batch loss 5.82158184 epoch total loss 6.17517853\n",
      "Trained batch 10662 batch loss 5.88621044 epoch total loss 6.17515087\n",
      "Trained batch 10663 batch loss 6.52153683 epoch total loss 6.17518377\n",
      "Trained batch 10664 batch loss 6.10055494 epoch total loss 6.17517662\n",
      "Trained batch 10665 batch loss 6.70875072 epoch total loss 6.17522717\n",
      "Trained batch 10666 batch loss 6.73015308 epoch total loss 6.17527866\n",
      "Trained batch 10667 batch loss 6.86805725 epoch total loss 6.17534351\n",
      "Trained batch 10668 batch loss 5.68333244 epoch total loss 6.17529726\n",
      "Trained batch 10669 batch loss 6.30357552 epoch total loss 6.17530918\n",
      "Trained batch 10670 batch loss 6.29319239 epoch total loss 6.17532063\n",
      "Trained batch 10671 batch loss 6.78335571 epoch total loss 6.17537737\n",
      "Trained batch 10672 batch loss 5.94473028 epoch total loss 6.17535591\n",
      "Trained batch 10673 batch loss 5.97992516 epoch total loss 6.17533731\n",
      "Trained batch 10674 batch loss 5.86573696 epoch total loss 6.17530823\n",
      "Trained batch 10675 batch loss 5.99474812 epoch total loss 6.17529106\n",
      "Trained batch 10676 batch loss 4.90857029 epoch total loss 6.17517233\n",
      "Trained batch 10677 batch loss 5.72159863 epoch total loss 6.17513\n",
      "Trained batch 10678 batch loss 5.61650705 epoch total loss 6.17507744\n",
      "Trained batch 10679 batch loss 5.63888168 epoch total loss 6.17502737\n",
      "Trained batch 10680 batch loss 5.94961596 epoch total loss 6.17500639\n",
      "Trained batch 10681 batch loss 4.7651515 epoch total loss 6.17487478\n",
      "Trained batch 10682 batch loss 5.53846931 epoch total loss 6.17481518\n",
      "Trained batch 10683 batch loss 6.07178831 epoch total loss 6.17480516\n",
      "Trained batch 10684 batch loss 5.87776661 epoch total loss 6.17477703\n",
      "Trained batch 10685 batch loss 6.68915653 epoch total loss 6.17482519\n",
      "Trained batch 10686 batch loss 6.41120815 epoch total loss 6.1748476\n",
      "Trained batch 10687 batch loss 5.9248457 epoch total loss 6.17482376\n",
      "Trained batch 10688 batch loss 5.93212795 epoch total loss 6.17480087\n",
      "Trained batch 10689 batch loss 6.24030304 epoch total loss 6.17480755\n",
      "Trained batch 10690 batch loss 5.83070946 epoch total loss 6.17477512\n",
      "Trained batch 10691 batch loss 6.33835411 epoch total loss 6.17479\n",
      "Trained batch 10692 batch loss 6.03848267 epoch total loss 6.17477751\n",
      "Trained batch 10693 batch loss 6.2444768 epoch total loss 6.17478371\n",
      "Trained batch 10694 batch loss 5.56008196 epoch total loss 6.17472649\n",
      "Trained batch 10695 batch loss 6.17929077 epoch total loss 6.17472696\n",
      "Trained batch 10696 batch loss 5.61616135 epoch total loss 6.17467451\n",
      "Trained batch 10697 batch loss 5.57807636 epoch total loss 6.17461872\n",
      "Trained batch 10698 batch loss 6.30189753 epoch total loss 6.17463112\n",
      "Trained batch 10699 batch loss 6.22210264 epoch total loss 6.17463541\n",
      "Trained batch 10700 batch loss 5.15307283 epoch total loss 6.17454\n",
      "Trained batch 10701 batch loss 5.67817307 epoch total loss 6.17449379\n",
      "Trained batch 10702 batch loss 6.45812035 epoch total loss 6.17452049\n",
      "Trained batch 10703 batch loss 6.28503466 epoch total loss 6.17453051\n",
      "Trained batch 10704 batch loss 5.81417418 epoch total loss 6.17449665\n",
      "Trained batch 10705 batch loss 6.27390289 epoch total loss 6.17450571\n",
      "Trained batch 10706 batch loss 6.46781683 epoch total loss 6.17453337\n",
      "Trained batch 10707 batch loss 6.45278072 epoch total loss 6.17455959\n",
      "Trained batch 10708 batch loss 4.66060352 epoch total loss 6.17441845\n",
      "Trained batch 10709 batch loss 6.16637373 epoch total loss 6.1744175\n",
      "Trained batch 10710 batch loss 5.52189541 epoch total loss 6.17435646\n",
      "Trained batch 10711 batch loss 5.71974516 epoch total loss 6.17431402\n",
      "Trained batch 10712 batch loss 5.73703289 epoch total loss 6.17427301\n",
      "Trained batch 10713 batch loss 5.4693327 epoch total loss 6.17420721\n",
      "Trained batch 10714 batch loss 5.13649178 epoch total loss 6.17411\n",
      "Trained batch 10715 batch loss 5.49735546 epoch total loss 6.17404699\n",
      "Trained batch 10716 batch loss 5.8933568 epoch total loss 6.17402077\n",
      "Trained batch 10717 batch loss 5.35154057 epoch total loss 6.173944\n",
      "Trained batch 10718 batch loss 5.67044353 epoch total loss 6.17389679\n",
      "Trained batch 10719 batch loss 6.09293461 epoch total loss 6.17388964\n",
      "Trained batch 10720 batch loss 6.11830616 epoch total loss 6.17388439\n",
      "Trained batch 10721 batch loss 6.07271814 epoch total loss 6.17387438\n",
      "Trained batch 10722 batch loss 5.50668621 epoch total loss 6.17381239\n",
      "Trained batch 10723 batch loss 6.05815792 epoch total loss 6.17380142\n",
      "Trained batch 10724 batch loss 5.69711494 epoch total loss 6.1737566\n",
      "Trained batch 10725 batch loss 6.00420094 epoch total loss 6.17374134\n",
      "Trained batch 10726 batch loss 6.01834822 epoch total loss 6.17372656\n",
      "Trained batch 10727 batch loss 5.88279152 epoch total loss 6.17369938\n",
      "Trained batch 10728 batch loss 5.73134041 epoch total loss 6.17365837\n",
      "Trained batch 10729 batch loss 5.54878807 epoch total loss 6.1736\n",
      "Trained batch 10730 batch loss 5.44716406 epoch total loss 6.17353201\n",
      "Trained batch 10731 batch loss 5.42751026 epoch total loss 6.17346287\n",
      "Trained batch 10732 batch loss 5.35354853 epoch total loss 6.1733861\n",
      "Trained batch 10733 batch loss 5.47480679 epoch total loss 6.17332125\n",
      "Trained batch 10734 batch loss 5.73922634 epoch total loss 6.17328119\n",
      "Trained batch 10735 batch loss 5.34314871 epoch total loss 6.17320395\n",
      "Trained batch 10736 batch loss 5.77391958 epoch total loss 6.17316675\n",
      "Trained batch 10737 batch loss 6.02866268 epoch total loss 6.1731534\n",
      "Trained batch 10738 batch loss 5.91782379 epoch total loss 6.17312908\n",
      "Trained batch 10739 batch loss 5.23686218 epoch total loss 6.17304182\n",
      "Trained batch 10740 batch loss 5.38059759 epoch total loss 6.17296839\n",
      "Trained batch 10741 batch loss 6.3954277 epoch total loss 6.17298937\n",
      "Trained batch 10742 batch loss 5.5512805 epoch total loss 6.17293167\n",
      "Trained batch 10743 batch loss 5.82593 epoch total loss 6.17289972\n",
      "Trained batch 10744 batch loss 5.98023891 epoch total loss 6.1728816\n",
      "Trained batch 10745 batch loss 5.52021933 epoch total loss 6.17282104\n",
      "Trained batch 10746 batch loss 6.20586205 epoch total loss 6.17282391\n",
      "Trained batch 10747 batch loss 5.89273262 epoch total loss 6.17279768\n",
      "Trained batch 10748 batch loss 6.23912907 epoch total loss 6.17280388\n",
      "Trained batch 10749 batch loss 4.9315567 epoch total loss 6.17268848\n",
      "Trained batch 10750 batch loss 4.62639332 epoch total loss 6.17254448\n",
      "Trained batch 10751 batch loss 5.38670731 epoch total loss 6.17247105\n",
      "Trained batch 10752 batch loss 5.6740818 epoch total loss 6.17242432\n",
      "Trained batch 10753 batch loss 6.01679897 epoch total loss 6.17240953\n",
      "Trained batch 10754 batch loss 6.46818304 epoch total loss 6.17243719\n",
      "Trained batch 10755 batch loss 6.03797 epoch total loss 6.17242479\n",
      "Trained batch 10756 batch loss 5.61702347 epoch total loss 6.17237329\n",
      "Trained batch 10757 batch loss 5.77232265 epoch total loss 6.1723361\n",
      "Trained batch 10758 batch loss 5.84705257 epoch total loss 6.17230558\n",
      "Trained batch 10759 batch loss 5.41397381 epoch total loss 6.17223501\n",
      "Trained batch 10760 batch loss 5.34796286 epoch total loss 6.17215872\n",
      "Trained batch 10761 batch loss 4.6023016 epoch total loss 6.17201281\n",
      "Trained batch 10762 batch loss 5.98197699 epoch total loss 6.17199564\n",
      "Trained batch 10763 batch loss 5.35598707 epoch total loss 6.17192\n",
      "Trained batch 10764 batch loss 5.42657375 epoch total loss 6.17185116\n",
      "Trained batch 10765 batch loss 4.51613951 epoch total loss 6.17169714\n",
      "Trained batch 10766 batch loss 5.07566071 epoch total loss 6.17159557\n",
      "Trained batch 10767 batch loss 5.48185635 epoch total loss 6.17153168\n",
      "Trained batch 10768 batch loss 4.60131645 epoch total loss 6.17138577\n",
      "Trained batch 10769 batch loss 4.9902668 epoch total loss 6.17127657\n",
      "Trained batch 10770 batch loss 5.83256 epoch total loss 6.17124557\n",
      "Trained batch 10771 batch loss 6.47091103 epoch total loss 6.17127275\n",
      "Trained batch 10772 batch loss 5.60399866 epoch total loss 6.1712203\n",
      "Trained batch 10773 batch loss 5.86806679 epoch total loss 6.17119169\n",
      "Trained batch 10774 batch loss 6.07671 epoch total loss 6.17118311\n",
      "Trained batch 10775 batch loss 5.18502808 epoch total loss 6.17109203\n",
      "Trained batch 10776 batch loss 5.08202505 epoch total loss 6.17099047\n",
      "Trained batch 10777 batch loss 4.92485428 epoch total loss 6.1708746\n",
      "Trained batch 10778 batch loss 5.7876091 epoch total loss 6.17083931\n",
      "Trained batch 10779 batch loss 5.59464216 epoch total loss 6.17078543\n",
      "Trained batch 10780 batch loss 3.45169592 epoch total loss 6.17053366\n",
      "Trained batch 10781 batch loss 5.87404871 epoch total loss 6.170506\n",
      "Trained batch 10782 batch loss 4.48935032 epoch total loss 6.17035055\n",
      "Trained batch 10783 batch loss 5.71632862 epoch total loss 6.17030859\n",
      "Trained batch 10784 batch loss 5.34371662 epoch total loss 6.17023182\n",
      "Trained batch 10785 batch loss 6.02400351 epoch total loss 6.17021847\n",
      "Trained batch 10786 batch loss 5.76481438 epoch total loss 6.1701808\n",
      "Trained batch 10787 batch loss 5.53309584 epoch total loss 6.17012167\n",
      "Trained batch 10788 batch loss 5.49922562 epoch total loss 6.17005968\n",
      "Trained batch 10789 batch loss 5.80304432 epoch total loss 6.17002583\n",
      "Trained batch 10790 batch loss 5.78343201 epoch total loss 6.16998959\n",
      "Trained batch 10791 batch loss 5.7268877 epoch total loss 6.16994858\n",
      "Trained batch 10792 batch loss 7.21683693 epoch total loss 6.17004585\n",
      "Trained batch 10793 batch loss 7.73393488 epoch total loss 6.17019081\n",
      "Trained batch 10794 batch loss 7.53681 epoch total loss 6.17031765\n",
      "Trained batch 10795 batch loss 6.45671463 epoch total loss 6.1703434\n",
      "Trained batch 10796 batch loss 7.44064426 epoch total loss 6.17046118\n",
      "Trained batch 10797 batch loss 7.14834356 epoch total loss 6.17055178\n",
      "Trained batch 10798 batch loss 7.30812073 epoch total loss 6.17065668\n",
      "Trained batch 10799 batch loss 6.33826447 epoch total loss 6.17067194\n",
      "Trained batch 10800 batch loss 6.32394314 epoch total loss 6.17068577\n",
      "Trained batch 10801 batch loss 6.44334555 epoch total loss 6.17071104\n",
      "Trained batch 10802 batch loss 6.68142366 epoch total loss 6.17075825\n",
      "Trained batch 10803 batch loss 6.69257212 epoch total loss 6.17080688\n",
      "Trained batch 10804 batch loss 6.40680695 epoch total loss 6.17082882\n",
      "Trained batch 10805 batch loss 6.16840219 epoch total loss 6.17082882\n",
      "Trained batch 10806 batch loss 6.28127766 epoch total loss 6.17083883\n",
      "Trained batch 10807 batch loss 6.44167566 epoch total loss 6.17086458\n",
      "Trained batch 10808 batch loss 6.75381422 epoch total loss 6.17091799\n",
      "Trained batch 10809 batch loss 6.68047762 epoch total loss 6.17096519\n",
      "Trained batch 10810 batch loss 6.67162514 epoch total loss 6.17101145\n",
      "Trained batch 10811 batch loss 6.35089159 epoch total loss 6.17102814\n",
      "Trained batch 10812 batch loss 6.43810129 epoch total loss 6.17105293\n",
      "Trained batch 10813 batch loss 6.2075696 epoch total loss 6.17105627\n",
      "Trained batch 10814 batch loss 6.37964725 epoch total loss 6.17107582\n",
      "Trained batch 10815 batch loss 6.35197687 epoch total loss 6.17109251\n",
      "Trained batch 10816 batch loss 6.51510334 epoch total loss 6.17112446\n",
      "Trained batch 10817 batch loss 5.77620602 epoch total loss 6.17108774\n",
      "Trained batch 10818 batch loss 6.68175745 epoch total loss 6.17113495\n",
      "Trained batch 10819 batch loss 6.94474411 epoch total loss 6.17120647\n",
      "Trained batch 10820 batch loss 6.86340237 epoch total loss 6.17127085\n",
      "Trained batch 10821 batch loss 6.5873189 epoch total loss 6.17130899\n",
      "Trained batch 10822 batch loss 6.40634155 epoch total loss 6.17133045\n",
      "Trained batch 10823 batch loss 6.48842335 epoch total loss 6.17136049\n",
      "Trained batch 10824 batch loss 6.5194788 epoch total loss 6.17139196\n",
      "Trained batch 10825 batch loss 6.7413497 epoch total loss 6.17144489\n",
      "Trained batch 10826 batch loss 6.90618944 epoch total loss 6.1715126\n",
      "Trained batch 10827 batch loss 6.54256392 epoch total loss 6.17154646\n",
      "Trained batch 10828 batch loss 6.74436092 epoch total loss 6.17159939\n",
      "Trained batch 10829 batch loss 6.57767105 epoch total loss 6.17163706\n",
      "Trained batch 10830 batch loss 6.45133305 epoch total loss 6.17166281\n",
      "Trained batch 10831 batch loss 6.14926672 epoch total loss 6.1716609\n",
      "Trained batch 10832 batch loss 6.22134 epoch total loss 6.17166519\n",
      "Trained batch 10833 batch loss 5.74194622 epoch total loss 6.17162561\n",
      "Trained batch 10834 batch loss 5.8819 epoch total loss 6.17159891\n",
      "Trained batch 10835 batch loss 5.80021954 epoch total loss 6.1715641\n",
      "Trained batch 10836 batch loss 6.21112251 epoch total loss 6.17156792\n",
      "Trained batch 10837 batch loss 5.09442043 epoch total loss 6.17146826\n",
      "Trained batch 10838 batch loss 6.01963234 epoch total loss 6.17145491\n",
      "Trained batch 10839 batch loss 5.48936033 epoch total loss 6.17139196\n",
      "Trained batch 10840 batch loss 5.78466845 epoch total loss 6.1713562\n",
      "Trained batch 10841 batch loss 5.5287056 epoch total loss 6.17129707\n",
      "Trained batch 10842 batch loss 5.75621939 epoch total loss 6.17125893\n",
      "Trained batch 10843 batch loss 5.76577234 epoch total loss 6.17122173\n",
      "Trained batch 10844 batch loss 5.02934 epoch total loss 6.17111635\n",
      "Trained batch 10845 batch loss 5.80695486 epoch total loss 6.1710825\n",
      "Trained batch 10846 batch loss 5.19926262 epoch total loss 6.17099333\n",
      "Trained batch 10847 batch loss 4.784482 epoch total loss 6.17086506\n",
      "Trained batch 10848 batch loss 5.85963917 epoch total loss 6.17083645\n",
      "Trained batch 10849 batch loss 5.92619753 epoch total loss 6.17081404\n",
      "Trained batch 10850 batch loss 6.19017696 epoch total loss 6.17081594\n",
      "Trained batch 10851 batch loss 6.48710918 epoch total loss 6.17084455\n",
      "Trained batch 10852 batch loss 6.52253628 epoch total loss 6.17087698\n",
      "Trained batch 10853 batch loss 6.4798913 epoch total loss 6.17090559\n",
      "Trained batch 10854 batch loss 6.39067459 epoch total loss 6.17092562\n",
      "Trained batch 10855 batch loss 6.34658241 epoch total loss 6.17094135\n",
      "Trained batch 10856 batch loss 6.45689392 epoch total loss 6.17096758\n",
      "Trained batch 10857 batch loss 6.2436409 epoch total loss 6.17097425\n",
      "Trained batch 10858 batch loss 5.94715118 epoch total loss 6.17095327\n",
      "Trained batch 10859 batch loss 6.00597286 epoch total loss 6.17093849\n",
      "Trained batch 10860 batch loss 6.67875195 epoch total loss 6.17098522\n",
      "Trained batch 10861 batch loss 6.32993889 epoch total loss 6.17099953\n",
      "Trained batch 10862 batch loss 6.56533909 epoch total loss 6.17103577\n",
      "Trained batch 10863 batch loss 6.53539753 epoch total loss 6.17106962\n",
      "Trained batch 10864 batch loss 6.33534908 epoch total loss 6.17108488\n",
      "Trained batch 10865 batch loss 5.88115072 epoch total loss 6.17105818\n",
      "Trained batch 10866 batch loss 5.59067726 epoch total loss 6.17100525\n",
      "Trained batch 10867 batch loss 5.59810162 epoch total loss 6.1709528\n",
      "Trained batch 10868 batch loss 4.91567612 epoch total loss 6.17083693\n",
      "Trained batch 10869 batch loss 4.75156307 epoch total loss 6.17070627\n",
      "Trained batch 10870 batch loss 5.98691893 epoch total loss 6.17068911\n",
      "Trained batch 10871 batch loss 5.67429066 epoch total loss 6.17064333\n",
      "Trained batch 10872 batch loss 5.73150349 epoch total loss 6.17060328\n",
      "Trained batch 10873 batch loss 6.25481558 epoch total loss 6.1706109\n",
      "Trained batch 10874 batch loss 6.33565092 epoch total loss 6.17062616\n",
      "Trained batch 10875 batch loss 6.12478781 epoch total loss 6.17062235\n",
      "Trained batch 10876 batch loss 4.94033432 epoch total loss 6.17050886\n",
      "Trained batch 10877 batch loss 6.04766464 epoch total loss 6.17049742\n",
      "Trained batch 10878 batch loss 6.68025541 epoch total loss 6.17054415\n",
      "Trained batch 10879 batch loss 6.72452831 epoch total loss 6.17059517\n",
      "Trained batch 10880 batch loss 6.66361809 epoch total loss 6.17064047\n",
      "Trained batch 10881 batch loss 6.38822317 epoch total loss 6.17066097\n",
      "Trained batch 10882 batch loss 5.81988335 epoch total loss 6.17062855\n",
      "Trained batch 10883 batch loss 6.59958601 epoch total loss 6.17066813\n",
      "Trained batch 10884 batch loss 6.20861292 epoch total loss 6.17067194\n",
      "Trained batch 10885 batch loss 7.44990921 epoch total loss 6.17078972\n",
      "Trained batch 10886 batch loss 6.25235462 epoch total loss 6.17079687\n",
      "Trained batch 10887 batch loss 6.28556347 epoch total loss 6.17080784\n",
      "Trained batch 10888 batch loss 6.62579155 epoch total loss 6.17085\n",
      "Trained batch 10889 batch loss 5.85791492 epoch total loss 6.17082119\n",
      "Trained batch 10890 batch loss 5.24756145 epoch total loss 6.17073631\n",
      "Trained batch 10891 batch loss 5.08716774 epoch total loss 6.17063665\n",
      "Trained batch 10892 batch loss 5.51147842 epoch total loss 6.1705761\n",
      "Trained batch 10893 batch loss 6.23989916 epoch total loss 6.17058277\n",
      "Trained batch 10894 batch loss 5.42910194 epoch total loss 6.17051458\n",
      "Trained batch 10895 batch loss 4.93802738 epoch total loss 6.17040157\n",
      "Trained batch 10896 batch loss 5.68981028 epoch total loss 6.17035723\n",
      "Trained batch 10897 batch loss 5.96756935 epoch total loss 6.17033863\n",
      "Trained batch 10898 batch loss 5.89534521 epoch total loss 6.17031384\n",
      "Trained batch 10899 batch loss 6.22960758 epoch total loss 6.1703186\n",
      "Trained batch 10900 batch loss 5.43117428 epoch total loss 6.17025089\n",
      "Trained batch 10901 batch loss 6.77449417 epoch total loss 6.17030621\n",
      "Trained batch 10902 batch loss 6.67712975 epoch total loss 6.17035294\n",
      "Trained batch 10903 batch loss 5.64351654 epoch total loss 6.1703043\n",
      "Trained batch 10904 batch loss 6.01735163 epoch total loss 6.17029\n",
      "Trained batch 10905 batch loss 5.23940468 epoch total loss 6.17020512\n",
      "Trained batch 10906 batch loss 5.83131027 epoch total loss 6.17017365\n",
      "Trained batch 10907 batch loss 6.06806278 epoch total loss 6.17016459\n",
      "Trained batch 10908 batch loss 5.61645174 epoch total loss 6.17011404\n",
      "Trained batch 10909 batch loss 5.49891853 epoch total loss 6.17005253\n",
      "Trained batch 10910 batch loss 6.57157803 epoch total loss 6.17008924\n",
      "Trained batch 10911 batch loss 6.30427933 epoch total loss 6.17010164\n",
      "Trained batch 10912 batch loss 6.73797607 epoch total loss 6.17015314\n",
      "Trained batch 10913 batch loss 6.66162586 epoch total loss 6.17019844\n",
      "Trained batch 10914 batch loss 6.47853184 epoch total loss 6.17022657\n",
      "Trained batch 10915 batch loss 6.2587719 epoch total loss 6.17023468\n",
      "Trained batch 10916 batch loss 6.53132248 epoch total loss 6.17026758\n",
      "Trained batch 10917 batch loss 6.89606524 epoch total loss 6.17033434\n",
      "Trained batch 10918 batch loss 6.54939842 epoch total loss 6.17036867\n",
      "Trained batch 10919 batch loss 6.3379612 epoch total loss 6.17038393\n",
      "Trained batch 10920 batch loss 7.05668163 epoch total loss 6.17046499\n",
      "Trained batch 10921 batch loss 6.27739716 epoch total loss 6.17047501\n",
      "Trained batch 10922 batch loss 6.3796339 epoch total loss 6.17049456\n",
      "Trained batch 10923 batch loss 6.41526461 epoch total loss 6.17051697\n",
      "Trained batch 10924 batch loss 6.02681065 epoch total loss 6.17050314\n",
      "Trained batch 10925 batch loss 5.87344933 epoch total loss 6.17047644\n",
      "Trained batch 10926 batch loss 5.59917927 epoch total loss 6.17042398\n",
      "Trained batch 10927 batch loss 5.34241 epoch total loss 6.17034864\n",
      "Trained batch 10928 batch loss 6.24611664 epoch total loss 6.1703558\n",
      "Trained batch 10929 batch loss 6.09549427 epoch total loss 6.17034864\n",
      "Trained batch 10930 batch loss 6.11690521 epoch total loss 6.17034388\n",
      "Trained batch 10931 batch loss 6.27483463 epoch total loss 6.17035341\n",
      "Trained batch 10932 batch loss 6.73002052 epoch total loss 6.17040443\n",
      "Trained batch 10933 batch loss 6.5658288 epoch total loss 6.17044\n",
      "Trained batch 10934 batch loss 6.39061928 epoch total loss 6.17046\n",
      "Trained batch 10935 batch loss 6.42196083 epoch total loss 6.17048311\n",
      "Trained batch 10936 batch loss 6.65551519 epoch total loss 6.17052746\n",
      "Trained batch 10937 batch loss 6.84156322 epoch total loss 6.17058945\n",
      "Trained batch 10938 batch loss 6.36658525 epoch total loss 6.17060709\n",
      "Trained batch 10939 batch loss 6.10736275 epoch total loss 6.17060137\n",
      "Trained batch 10940 batch loss 6.35901451 epoch total loss 6.17061901\n",
      "Trained batch 10941 batch loss 6.50346851 epoch total loss 6.17064905\n",
      "Trained batch 10942 batch loss 6.19819832 epoch total loss 6.17065144\n",
      "Trained batch 10943 batch loss 6.09924889 epoch total loss 6.17064476\n",
      "Trained batch 10944 batch loss 6.92348146 epoch total loss 6.17071342\n",
      "Trained batch 10945 batch loss 6.12999201 epoch total loss 6.17071\n",
      "Trained batch 10946 batch loss 6.14537048 epoch total loss 6.17070818\n",
      "Trained batch 10947 batch loss 5.75340939 epoch total loss 6.17066956\n",
      "Trained batch 10948 batch loss 6.18117571 epoch total loss 6.17067051\n",
      "Trained batch 10949 batch loss 6.36058331 epoch total loss 6.17068768\n",
      "Trained batch 10950 batch loss 5.78709221 epoch total loss 6.17065287\n",
      "Trained batch 10951 batch loss 5.08330536 epoch total loss 6.17055368\n",
      "Trained batch 10952 batch loss 5.46653414 epoch total loss 6.17049\n",
      "Trained batch 10953 batch loss 4.6718235 epoch total loss 6.17035294\n",
      "Trained batch 10954 batch loss 5.69954205 epoch total loss 6.17031\n",
      "Trained batch 10955 batch loss 4.95968914 epoch total loss 6.1702\n",
      "Trained batch 10956 batch loss 5.27576 epoch total loss 6.17011786\n",
      "Trained batch 10957 batch loss 5.15929842 epoch total loss 6.17002535\n",
      "Trained batch 10958 batch loss 6.05420446 epoch total loss 6.17001486\n",
      "Trained batch 10959 batch loss 6.13648605 epoch total loss 6.17001152\n",
      "Trained batch 10960 batch loss 5.64527702 epoch total loss 6.16996384\n",
      "Trained batch 10961 batch loss 5.07005024 epoch total loss 6.1698637\n",
      "Trained batch 10962 batch loss 4.73907948 epoch total loss 6.16973352\n",
      "Trained batch 10963 batch loss 4.91062737 epoch total loss 6.16961861\n",
      "Trained batch 10964 batch loss 4.94946289 epoch total loss 6.16950798\n",
      "Trained batch 10965 batch loss 6.23323345 epoch total loss 6.1695137\n",
      "Trained batch 10966 batch loss 5.02201366 epoch total loss 6.16940928\n",
      "Trained batch 10967 batch loss 5.6800375 epoch total loss 6.16936445\n",
      "Trained batch 10968 batch loss 4.98306847 epoch total loss 6.16925669\n",
      "Trained batch 10969 batch loss 5.96056271 epoch total loss 6.16923761\n",
      "Trained batch 10970 batch loss 6.65422487 epoch total loss 6.16928196\n",
      "Trained batch 10971 batch loss 6.4310751 epoch total loss 6.1693058\n",
      "Trained batch 10972 batch loss 6.08030605 epoch total loss 6.16929722\n",
      "Trained batch 10973 batch loss 6.28161669 epoch total loss 6.16930771\n",
      "Trained batch 10974 batch loss 6.72692633 epoch total loss 6.16935825\n",
      "Trained batch 10975 batch loss 6.51930237 epoch total loss 6.16938972\n",
      "Trained batch 10976 batch loss 6.20492554 epoch total loss 6.16939306\n",
      "Trained batch 10977 batch loss 6.6762538 epoch total loss 6.16943932\n",
      "Trained batch 10978 batch loss 6.6785078 epoch total loss 6.16948605\n",
      "Trained batch 10979 batch loss 6.39388323 epoch total loss 6.16950607\n",
      "Trained batch 10980 batch loss 6.22852516 epoch total loss 6.16951132\n",
      "Trained batch 10981 batch loss 6.01290703 epoch total loss 6.16949749\n",
      "Trained batch 10982 batch loss 5.68002 epoch total loss 6.16945267\n",
      "Trained batch 10983 batch loss 6.14908695 epoch total loss 6.16945076\n",
      "Trained batch 10984 batch loss 6.04753113 epoch total loss 6.16944\n",
      "Trained batch 10985 batch loss 6.3651247 epoch total loss 6.16945744\n",
      "Trained batch 10986 batch loss 6.12218189 epoch total loss 6.16945362\n",
      "Trained batch 10987 batch loss 6.14641476 epoch total loss 6.16945171\n",
      "Trained batch 10988 batch loss 6.14023399 epoch total loss 6.16944885\n",
      "Trained batch 10989 batch loss 6.12381458 epoch total loss 6.16944504\n",
      "Trained batch 10990 batch loss 5.69643497 epoch total loss 6.16940165\n",
      "Trained batch 10991 batch loss 6.01370239 epoch total loss 6.16938782\n",
      "Trained batch 10992 batch loss 5.91395 epoch total loss 6.16936445\n",
      "Trained batch 10993 batch loss 6.7559042 epoch total loss 6.16941833\n",
      "Trained batch 10994 batch loss 6.73330545 epoch total loss 6.16946936\n",
      "Trained batch 10995 batch loss 6.13997316 epoch total loss 6.16946697\n",
      "Trained batch 10996 batch loss 6.36299038 epoch total loss 6.16948414\n",
      "Trained batch 10997 batch loss 5.4996376 epoch total loss 6.1694231\n",
      "Trained batch 10998 batch loss 5.78039742 epoch total loss 6.16938782\n",
      "Trained batch 10999 batch loss 4.69228268 epoch total loss 6.16925383\n",
      "Trained batch 11000 batch loss 5.48302364 epoch total loss 6.16919184\n",
      "Trained batch 11001 batch loss 5.92707062 epoch total loss 6.16917\n",
      "Trained batch 11002 batch loss 6.18659973 epoch total loss 6.16917181\n",
      "Trained batch 11003 batch loss 6.93572712 epoch total loss 6.16924143\n",
      "Trained batch 11004 batch loss 5.57320404 epoch total loss 6.16918707\n",
      "Trained batch 11005 batch loss 5.89877176 epoch total loss 6.16916227\n",
      "Trained batch 11006 batch loss 5.60515404 epoch total loss 6.16911077\n",
      "Trained batch 11007 batch loss 5.55279732 epoch total loss 6.16905499\n",
      "Trained batch 11008 batch loss 4.94826126 epoch total loss 6.16894388\n",
      "Trained batch 11009 batch loss 4.68232346 epoch total loss 6.16880846\n",
      "Trained batch 11010 batch loss 5.99132252 epoch total loss 6.16879272\n",
      "Trained batch 11011 batch loss 4.90203094 epoch total loss 6.16867733\n",
      "Trained batch 11012 batch loss 4.58253288 epoch total loss 6.16853333\n",
      "Trained batch 11013 batch loss 5.54704475 epoch total loss 6.16847706\n",
      "Trained batch 11014 batch loss 5.19604778 epoch total loss 6.16838884\n",
      "Trained batch 11015 batch loss 5.64900923 epoch total loss 6.16834164\n",
      "Trained batch 11016 batch loss 5.78924608 epoch total loss 6.16830683\n",
      "Trained batch 11017 batch loss 6.06205273 epoch total loss 6.16829729\n",
      "Trained batch 11018 batch loss 6.1012373 epoch total loss 6.16829157\n",
      "Trained batch 11019 batch loss 6.20732164 epoch total loss 6.16829538\n",
      "Trained batch 11020 batch loss 6.02423048 epoch total loss 6.16828203\n",
      "Trained batch 11021 batch loss 5.63056612 epoch total loss 6.16823339\n",
      "Trained batch 11022 batch loss 5.9666853 epoch total loss 6.16821527\n",
      "Trained batch 11023 batch loss 6.1496644 epoch total loss 6.16821384\n",
      "Trained batch 11024 batch loss 6.71315622 epoch total loss 6.16826296\n",
      "Trained batch 11025 batch loss 5.80119801 epoch total loss 6.16823\n",
      "Trained batch 11026 batch loss 6.51344728 epoch total loss 6.16826153\n",
      "Trained batch 11027 batch loss 6.58771086 epoch total loss 6.1682992\n",
      "Trained batch 11028 batch loss 6.34437275 epoch total loss 6.16831493\n",
      "Trained batch 11029 batch loss 6.74031401 epoch total loss 6.16836739\n",
      "Trained batch 11030 batch loss 6.32315302 epoch total loss 6.16838121\n",
      "Trained batch 11031 batch loss 6.57139778 epoch total loss 6.16841745\n",
      "Trained batch 11032 batch loss 5.88785076 epoch total loss 6.16839218\n",
      "Trained batch 11033 batch loss 6.65594912 epoch total loss 6.16843653\n",
      "Trained batch 11034 batch loss 5.83562 epoch total loss 6.16840649\n",
      "Trained batch 11035 batch loss 6.02441502 epoch total loss 6.16839314\n",
      "Trained batch 11036 batch loss 5.92884254 epoch total loss 6.16837168\n",
      "Trained batch 11037 batch loss 6.29462147 epoch total loss 6.16838312\n",
      "Trained batch 11038 batch loss 5.93783569 epoch total loss 6.16836214\n",
      "Trained batch 11039 batch loss 5.80093956 epoch total loss 6.16832924\n",
      "Trained batch 11040 batch loss 5.37426519 epoch total loss 6.16825724\n",
      "Trained batch 11041 batch loss 5.83576 epoch total loss 6.1682272\n",
      "Trained batch 11042 batch loss 5.41388273 epoch total loss 6.16815901\n",
      "Trained batch 11043 batch loss 5.8531208 epoch total loss 6.1681304\n",
      "Trained batch 11044 batch loss 5.6903162 epoch total loss 6.16808701\n",
      "Trained batch 11045 batch loss 5.13980865 epoch total loss 6.16799402\n",
      "Trained batch 11046 batch loss 5.73193359 epoch total loss 6.16795444\n",
      "Trained batch 11047 batch loss 5.70302963 epoch total loss 6.16791248\n",
      "Trained batch 11048 batch loss 5.2776804 epoch total loss 6.16783237\n",
      "Trained batch 11049 batch loss 5.22297955 epoch total loss 6.16774702\n",
      "Trained batch 11050 batch loss 6.07035398 epoch total loss 6.16773844\n",
      "Trained batch 11051 batch loss 4.46089649 epoch total loss 6.16758394\n",
      "Trained batch 11052 batch loss 5.55916739 epoch total loss 6.16752911\n",
      "Trained batch 11053 batch loss 6.06625319 epoch total loss 6.16751957\n",
      "Trained batch 11054 batch loss 6.08344746 epoch total loss 6.16751242\n",
      "Trained batch 11055 batch loss 6.66358805 epoch total loss 6.16755724\n",
      "Trained batch 11056 batch loss 6.21849394 epoch total loss 6.16756153\n",
      "Trained batch 11057 batch loss 6.29071856 epoch total loss 6.1675725\n",
      "Trained batch 11058 batch loss 6.20542097 epoch total loss 6.16757584\n",
      "Trained batch 11059 batch loss 5.07729149 epoch total loss 6.16747761\n",
      "Trained batch 11060 batch loss 4.50925875 epoch total loss 6.1673274\n",
      "Trained batch 11061 batch loss 6.60208654 epoch total loss 6.1673665\n",
      "Trained batch 11062 batch loss 5.32436466 epoch total loss 6.16729069\n",
      "Trained batch 11063 batch loss 6.33637714 epoch total loss 6.16730595\n",
      "Trained batch 11064 batch loss 6.90646362 epoch total loss 6.1673727\n",
      "Trained batch 11065 batch loss 5.33400583 epoch total loss 6.16729784\n",
      "Trained batch 11066 batch loss 5.13022566 epoch total loss 6.16720438\n",
      "Trained batch 11067 batch loss 5.87156677 epoch total loss 6.16717768\n",
      "Trained batch 11068 batch loss 6.62924099 epoch total loss 6.16721964\n",
      "Trained batch 11069 batch loss 6.5314045 epoch total loss 6.16725254\n",
      "Trained batch 11070 batch loss 6.63559437 epoch total loss 6.16729498\n",
      "Trained batch 11071 batch loss 6.04060173 epoch total loss 6.16728306\n",
      "Trained batch 11072 batch loss 6.51151943 epoch total loss 6.16731405\n",
      "Trained batch 11073 batch loss 6.63517809 epoch total loss 6.16735601\n",
      "Trained batch 11074 batch loss 5.59804296 epoch total loss 6.16730499\n",
      "Trained batch 11075 batch loss 7.08834743 epoch total loss 6.16738796\n",
      "Trained batch 11076 batch loss 6.94865799 epoch total loss 6.16745806\n",
      "Trained batch 11077 batch loss 6.22674084 epoch total loss 6.1674633\n",
      "Trained batch 11078 batch loss 5.78423405 epoch total loss 6.16742849\n",
      "Trained batch 11079 batch loss 6.40503597 epoch total loss 6.16745\n",
      "Trained batch 11080 batch loss 6.23971939 epoch total loss 6.16745663\n",
      "Trained batch 11081 batch loss 6.37678242 epoch total loss 6.1674757\n",
      "Trained batch 11082 batch loss 6.17229319 epoch total loss 6.16747618\n",
      "Trained batch 11083 batch loss 5.57215643 epoch total loss 6.16742229\n",
      "Trained batch 11084 batch loss 5.96759605 epoch total loss 6.16740417\n",
      "Trained batch 11085 batch loss 6.39735126 epoch total loss 6.16742516\n",
      "Trained batch 11086 batch loss 6.41470623 epoch total loss 6.16744709\n",
      "Trained batch 11087 batch loss 6.47131729 epoch total loss 6.16747427\n",
      "Trained batch 11088 batch loss 6.20351219 epoch total loss 6.16747761\n",
      "Trained batch 11089 batch loss 5.55794048 epoch total loss 6.16742229\n",
      "Trained batch 11090 batch loss 5.62690735 epoch total loss 6.16737366\n",
      "Trained batch 11091 batch loss 5.48044968 epoch total loss 6.16731119\n",
      "Trained batch 11092 batch loss 5.55799627 epoch total loss 6.16725588\n",
      "Trained batch 11093 batch loss 5.57246876 epoch total loss 6.167202\n",
      "Trained batch 11094 batch loss 5.85682 epoch total loss 6.16717434\n",
      "Trained batch 11095 batch loss 5.88866043 epoch total loss 6.16714954\n",
      "Trained batch 11096 batch loss 6.08690166 epoch total loss 6.16714239\n",
      "Trained batch 11097 batch loss 5.54774284 epoch total loss 6.16708612\n",
      "Trained batch 11098 batch loss 5.89956665 epoch total loss 6.16706228\n",
      "Trained batch 11099 batch loss 5.78984404 epoch total loss 6.16702795\n",
      "Trained batch 11100 batch loss 5.16505814 epoch total loss 6.16693783\n",
      "Trained batch 11101 batch loss 5.41239548 epoch total loss 6.16686964\n",
      "Trained batch 11102 batch loss 5.68455124 epoch total loss 6.16682673\n",
      "Trained batch 11103 batch loss 5.81759644 epoch total loss 6.16679525\n",
      "Trained batch 11104 batch loss 5.94719219 epoch total loss 6.1667757\n",
      "Trained batch 11105 batch loss 5.75170803 epoch total loss 6.16673803\n",
      "Trained batch 11106 batch loss 5.29968929 epoch total loss 6.16666\n",
      "Trained batch 11107 batch loss 6.15473557 epoch total loss 6.16665888\n",
      "Trained batch 11108 batch loss 5.71612501 epoch total loss 6.16661835\n",
      "Trained batch 11109 batch loss 5.75004482 epoch total loss 6.16658068\n",
      "Trained batch 11110 batch loss 6.01911974 epoch total loss 6.16656733\n",
      "Trained batch 11111 batch loss 5.7849946 epoch total loss 6.16653252\n",
      "Trained batch 11112 batch loss 5.7168479 epoch total loss 6.16649246\n",
      "Trained batch 11113 batch loss 5.7277627 epoch total loss 6.16645288\n",
      "Trained batch 11114 batch loss 5.68912315 epoch total loss 6.16640949\n",
      "Trained batch 11115 batch loss 5.14452457 epoch total loss 6.16631746\n",
      "Trained batch 11116 batch loss 5.83173847 epoch total loss 6.16628695\n",
      "Trained batch 11117 batch loss 5.32028389 epoch total loss 6.16621065\n",
      "Trained batch 11118 batch loss 6.16077518 epoch total loss 6.16621065\n",
      "Trained batch 11119 batch loss 6.19429874 epoch total loss 6.16621304\n",
      "Trained batch 11120 batch loss 5.72487354 epoch total loss 6.16617393\n",
      "Trained batch 11121 batch loss 5.53472471 epoch total loss 6.16611671\n",
      "Trained batch 11122 batch loss 5.95560551 epoch total loss 6.16609764\n",
      "Trained batch 11123 batch loss 5.50738049 epoch total loss 6.16603851\n",
      "Epoch 1 train loss 6.166038513183594\n",
      "Validated batch 1 batch loss 5.72542858\n",
      "Validated batch 2 batch loss 5.34816551\n",
      "Validated batch 3 batch loss 6.14691162\n",
      "Validated batch 4 batch loss 6.50330305\n",
      "Validated batch 5 batch loss 6.48309946\n",
      "Validated batch 6 batch loss 6.06329918\n",
      "Validated batch 7 batch loss 6.25760746\n",
      "Validated batch 8 batch loss 6.23731709\n",
      "Validated batch 9 batch loss 6.65365887\n",
      "Validated batch 10 batch loss 6.07884884\n",
      "Validated batch 11 batch loss 6.47691059\n",
      "Validated batch 12 batch loss 7.25958586\n",
      "Validated batch 13 batch loss 5.68821526\n",
      "Validated batch 14 batch loss 6.270998\n",
      "Validated batch 15 batch loss 6.24636173\n",
      "Validated batch 16 batch loss 5.70149803\n",
      "Validated batch 17 batch loss 6.95754147\n",
      "Validated batch 18 batch loss 6.18966389\n",
      "Validated batch 19 batch loss 6.43798447\n",
      "Validated batch 20 batch loss 5.10321236\n",
      "Validated batch 21 batch loss 6.59073639\n",
      "Validated batch 22 batch loss 5.96907043\n",
      "Validated batch 23 batch loss 4.76126671\n",
      "Validated batch 24 batch loss 4.54257822\n",
      "Validated batch 25 batch loss 5.22968292\n",
      "Validated batch 26 batch loss 5.51792908\n",
      "Validated batch 27 batch loss 5.60138321\n",
      "Validated batch 28 batch loss 5.94272614\n",
      "Validated batch 29 batch loss 6.30320501\n",
      "Validated batch 30 batch loss 6.0524168\n",
      "Validated batch 31 batch loss 6.62887955\n",
      "Validated batch 32 batch loss 5.93709278\n",
      "Validated batch 33 batch loss 5.96796513\n",
      "Validated batch 34 batch loss 6.832829\n",
      "Validated batch 35 batch loss 6.3176918\n",
      "Validated batch 36 batch loss 5.59839869\n",
      "Validated batch 37 batch loss 5.51557827\n",
      "Validated batch 38 batch loss 4.87999535\n",
      "Validated batch 39 batch loss 7.33023167\n",
      "Validated batch 40 batch loss 6.76446486\n",
      "Validated batch 41 batch loss 6.95959473\n",
      "Validated batch 42 batch loss 6.58152294\n",
      "Validated batch 43 batch loss 7.92483521\n",
      "Validated batch 44 batch loss 6.57394695\n",
      "Validated batch 45 batch loss 6.12161732\n",
      "Validated batch 46 batch loss 5.56052113\n",
      "Validated batch 47 batch loss 4.96515656\n",
      "Validated batch 48 batch loss 5.54269886\n",
      "Validated batch 49 batch loss 6.4959\n",
      "Validated batch 50 batch loss 6.79742813\n",
      "Validated batch 51 batch loss 6.36166477\n",
      "Validated batch 52 batch loss 5.46832752\n",
      "Validated batch 53 batch loss 5.74690247\n",
      "Validated batch 54 batch loss 6.3413763\n",
      "Validated batch 55 batch loss 6.82793951\n",
      "Validated batch 56 batch loss 6.15496445\n",
      "Validated batch 57 batch loss 5.93304825\n",
      "Validated batch 58 batch loss 6.30432272\n",
      "Validated batch 59 batch loss 5.49818802\n",
      "Validated batch 60 batch loss 5.7788372\n",
      "Validated batch 61 batch loss 5.97936153\n",
      "Validated batch 62 batch loss 6.63614893\n",
      "Validated batch 63 batch loss 6.1723671\n",
      "Validated batch 64 batch loss 6.26182699\n",
      "Validated batch 65 batch loss 6.01429176\n",
      "Validated batch 66 batch loss 5.67502785\n",
      "Validated batch 67 batch loss 6.44882727\n",
      "Validated batch 68 batch loss 6.35523701\n",
      "Validated batch 69 batch loss 6.19101381\n",
      "Validated batch 70 batch loss 6.27057171\n",
      "Validated batch 71 batch loss 6.12209129\n",
      "Validated batch 72 batch loss 5.97892523\n",
      "Validated batch 73 batch loss 6.81450272\n",
      "Validated batch 74 batch loss 5.56205463\n",
      "Validated batch 75 batch loss 6.72751856\n",
      "Validated batch 76 batch loss 6.44489\n",
      "Validated batch 77 batch loss 6.80804443\n",
      "Validated batch 78 batch loss 6.01006031\n",
      "Validated batch 79 batch loss 6.76741457\n",
      "Validated batch 80 batch loss 6.48874283\n",
      "Validated batch 81 batch loss 6.68722391\n",
      "Validated batch 82 batch loss 7.11557674\n",
      "Validated batch 83 batch loss 6.24660397\n",
      "Validated batch 84 batch loss 5.68368435\n",
      "Validated batch 85 batch loss 5.65480471\n",
      "Validated batch 86 batch loss 5.93119049\n",
      "Validated batch 87 batch loss 5.76980639\n",
      "Validated batch 88 batch loss 5.58391333\n",
      "Validated batch 89 batch loss 7.17109299\n",
      "Validated batch 90 batch loss 6.73421907\n",
      "Validated batch 91 batch loss 7.83671761\n",
      "Validated batch 92 batch loss 5.63648176\n",
      "Validated batch 93 batch loss 6.25850677\n",
      "Validated batch 94 batch loss 4.96846199\n",
      "Validated batch 95 batch loss 5.27206802\n",
      "Validated batch 96 batch loss 6.19629574\n",
      "Validated batch 97 batch loss 6.80902672\n",
      "Validated batch 98 batch loss 6.40350914\n",
      "Validated batch 99 batch loss 5.90932178\n",
      "Validated batch 100 batch loss 5.66167974\n",
      "Validated batch 101 batch loss 6.18111849\n",
      "Validated batch 102 batch loss 7.5907445\n",
      "Validated batch 103 batch loss 6.42892075\n",
      "Validated batch 104 batch loss 6.57603645\n",
      "Validated batch 105 batch loss 6.53781033\n",
      "Validated batch 106 batch loss 5.59075785\n",
      "Validated batch 107 batch loss 5.9075222\n",
      "Validated batch 108 batch loss 6.51417065\n",
      "Validated batch 109 batch loss 6.52797413\n",
      "Validated batch 110 batch loss 6.37152147\n",
      "Validated batch 111 batch loss 6.43786907\n",
      "Validated batch 112 batch loss 6.02507877\n",
      "Validated batch 113 batch loss 7.29901123\n",
      "Validated batch 114 batch loss 6.2278204\n",
      "Validated batch 115 batch loss 6.2817049\n",
      "Validated batch 116 batch loss 6.76902294\n",
      "Validated batch 117 batch loss 6.70434141\n",
      "Validated batch 118 batch loss 5.74045563\n",
      "Validated batch 119 batch loss 7.31197453\n",
      "Validated batch 120 batch loss 6.50050735\n",
      "Validated batch 121 batch loss 7.30818796\n",
      "Validated batch 122 batch loss 6.41984272\n",
      "Validated batch 123 batch loss 6.38022137\n",
      "Validated batch 124 batch loss 7.03129292\n",
      "Validated batch 125 batch loss 6.48282528\n",
      "Validated batch 126 batch loss 6.74392033\n",
      "Validated batch 127 batch loss 5.91842556\n",
      "Validated batch 128 batch loss 5.94159317\n",
      "Validated batch 129 batch loss 6.51416445\n",
      "Validated batch 130 batch loss 6.72732544\n",
      "Validated batch 131 batch loss 6.38238144\n",
      "Validated batch 132 batch loss 5.3738718\n",
      "Validated batch 133 batch loss 5.24467659\n",
      "Validated batch 134 batch loss 5.33723\n",
      "Validated batch 135 batch loss 6.82029963\n",
      "Validated batch 136 batch loss 7.26204491\n",
      "Validated batch 137 batch loss 7.18534756\n",
      "Validated batch 138 batch loss 6.75895548\n",
      "Validated batch 139 batch loss 6.74261665\n",
      "Validated batch 140 batch loss 6.97534084\n",
      "Validated batch 141 batch loss 5.92734241\n",
      "Validated batch 142 batch loss 5.65547657\n",
      "Validated batch 143 batch loss 7.50069141\n",
      "Validated batch 144 batch loss 6.23896313\n",
      "Validated batch 145 batch loss 6.0195\n",
      "Validated batch 146 batch loss 6.04366\n",
      "Validated batch 147 batch loss 5.87517166\n",
      "Validated batch 148 batch loss 5.69023514\n",
      "Validated batch 149 batch loss 5.72350597\n",
      "Validated batch 150 batch loss 5.47685862\n",
      "Validated batch 151 batch loss 5.69461775\n",
      "Validated batch 152 batch loss 6.67180157\n",
      "Validated batch 153 batch loss 4.98565483\n",
      "Validated batch 154 batch loss 5.85590267\n",
      "Validated batch 155 batch loss 6.07486057\n",
      "Validated batch 156 batch loss 5.920928\n",
      "Validated batch 157 batch loss 6.0530262\n",
      "Validated batch 158 batch loss 6.4409914\n",
      "Validated batch 159 batch loss 5.91945028\n",
      "Validated batch 160 batch loss 6.5104332\n",
      "Validated batch 161 batch loss 6.17444324\n",
      "Validated batch 162 batch loss 5.76023912\n",
      "Validated batch 163 batch loss 5.87539101\n",
      "Validated batch 164 batch loss 6.15850353\n",
      "Validated batch 165 batch loss 5.7110281\n",
      "Validated batch 166 batch loss 6.31796598\n",
      "Validated batch 167 batch loss 5.84854412\n",
      "Validated batch 168 batch loss 5.85235691\n",
      "Validated batch 169 batch loss 5.93146276\n",
      "Validated batch 170 batch loss 6.22671413\n",
      "Validated batch 171 batch loss 6.33485222\n",
      "Validated batch 172 batch loss 6.33432961\n",
      "Validated batch 173 batch loss 6.37120581\n",
      "Validated batch 174 batch loss 6.72686195\n",
      "Validated batch 175 batch loss 6.27695751\n",
      "Validated batch 176 batch loss 6.68682146\n",
      "Validated batch 177 batch loss 6.7406435\n",
      "Validated batch 178 batch loss 7.32543612\n",
      "Validated batch 179 batch loss 7.11542749\n",
      "Validated batch 180 batch loss 6.37477589\n",
      "Validated batch 181 batch loss 6.04767656\n",
      "Validated batch 182 batch loss 6.57809925\n",
      "Validated batch 183 batch loss 6.99011278\n",
      "Validated batch 184 batch loss 6.05160475\n",
      "Validated batch 185 batch loss 6.9980092\n",
      "Validated batch 186 batch loss 6.4017415\n",
      "Validated batch 187 batch loss 6.00082588\n",
      "Validated batch 188 batch loss 6.38313198\n",
      "Validated batch 189 batch loss 6.20610332\n",
      "Validated batch 190 batch loss 5.72089577\n",
      "Validated batch 191 batch loss 6.67681074\n",
      "Validated batch 192 batch loss 5.98278427\n",
      "Validated batch 193 batch loss 7.30444813\n",
      "Validated batch 194 batch loss 7.24003315\n",
      "Validated batch 195 batch loss 5.83998537\n",
      "Validated batch 196 batch loss 5.7485466\n",
      "Validated batch 197 batch loss 5.98643064\n",
      "Validated batch 198 batch loss 6.46859169\n",
      "Validated batch 199 batch loss 5.79691124\n",
      "Validated batch 200 batch loss 5.73749447\n",
      "Validated batch 201 batch loss 7.03893089\n",
      "Validated batch 202 batch loss 6.06572866\n",
      "Validated batch 203 batch loss 7.28245831\n",
      "Validated batch 204 batch loss 5.76693726\n",
      "Validated batch 205 batch loss 6.13241148\n",
      "Validated batch 206 batch loss 5.7408762\n",
      "Validated batch 207 batch loss 5.21456337\n",
      "Validated batch 208 batch loss 6.35851097\n",
      "Validated batch 209 batch loss 5.73354149\n",
      "Validated batch 210 batch loss 6.42460632\n",
      "Validated batch 211 batch loss 5.95311451\n",
      "Validated batch 212 batch loss 6.23839855\n",
      "Validated batch 213 batch loss 4.84961176\n",
      "Validated batch 214 batch loss 6.05149841\n",
      "Validated batch 215 batch loss 5.3182435\n",
      "Validated batch 216 batch loss 6.74654913\n",
      "Validated batch 217 batch loss 6.06556749\n",
      "Validated batch 218 batch loss 5.85335875\n",
      "Validated batch 219 batch loss 6.33862448\n",
      "Validated batch 220 batch loss 5.20039797\n",
      "Validated batch 221 batch loss 6.41165257\n",
      "Validated batch 222 batch loss 6.28446293\n",
      "Validated batch 223 batch loss 5.65606642\n",
      "Validated batch 224 batch loss 6.60469866\n",
      "Validated batch 225 batch loss 5.99045086\n",
      "Validated batch 226 batch loss 5.42314339\n",
      "Validated batch 227 batch loss 7.04527521\n",
      "Validated batch 228 batch loss 7.07958889\n",
      "Validated batch 229 batch loss 7.2399168\n",
      "Validated batch 230 batch loss 6.6902709\n",
      "Validated batch 231 batch loss 7.15477657\n",
      "Validated batch 232 batch loss 5.44502115\n",
      "Validated batch 233 batch loss 5.66961575\n",
      "Validated batch 234 batch loss 5.72145128\n",
      "Validated batch 235 batch loss 6.13253641\n",
      "Validated batch 236 batch loss 5.56635571\n",
      "Validated batch 237 batch loss 5.55822277\n",
      "Validated batch 238 batch loss 6.49275684\n",
      "Validated batch 239 batch loss 6.63837433\n",
      "Validated batch 240 batch loss 6.52135849\n",
      "Validated batch 241 batch loss 5.54662704\n",
      "Validated batch 242 batch loss 6.12559319\n",
      "Validated batch 243 batch loss 6.55345297\n",
      "Validated batch 244 batch loss 5.2901597\n",
      "Validated batch 245 batch loss 5.30128241\n",
      "Validated batch 246 batch loss 6.59800768\n",
      "Validated batch 247 batch loss 6.70314264\n",
      "Validated batch 248 batch loss 6.28151178\n",
      "Validated batch 249 batch loss 6.66662741\n",
      "Validated batch 250 batch loss 6.31941557\n",
      "Validated batch 251 batch loss 6.45658207\n",
      "Validated batch 252 batch loss 6.15468454\n",
      "Validated batch 253 batch loss 5.75014305\n",
      "Validated batch 254 batch loss 5.8836422\n",
      "Validated batch 255 batch loss 6.26943874\n",
      "Validated batch 256 batch loss 5.78629303\n",
      "Validated batch 257 batch loss 6.93097591\n",
      "Validated batch 258 batch loss 6.89881134\n",
      "Validated batch 259 batch loss 6.69622469\n",
      "Validated batch 260 batch loss 6.33700562\n",
      "Validated batch 261 batch loss 7.46819925\n",
      "Validated batch 262 batch loss 4.91066\n",
      "Validated batch 263 batch loss 5.82045841\n",
      "Validated batch 264 batch loss 6.18742752\n",
      "Validated batch 265 batch loss 6.22578049\n",
      "Validated batch 266 batch loss 5.96412516\n",
      "Validated batch 267 batch loss 6.20900488\n",
      "Validated batch 268 batch loss 5.55520535\n",
      "Validated batch 269 batch loss 5.76134968\n",
      "Validated batch 270 batch loss 6.38826084\n",
      "Validated batch 271 batch loss 5.99007225\n",
      "Validated batch 272 batch loss 5.84973431\n",
      "Validated batch 273 batch loss 6.40751457\n",
      "Validated batch 274 batch loss 6.28934813\n",
      "Validated batch 275 batch loss 6.28969049\n",
      "Validated batch 276 batch loss 5.93686056\n",
      "Validated batch 277 batch loss 6.08789062\n",
      "Validated batch 278 batch loss 6.39313316\n",
      "Validated batch 279 batch loss 5.21842766\n",
      "Validated batch 280 batch loss 5.65507\n",
      "Validated batch 281 batch loss 4.86938858\n",
      "Validated batch 282 batch loss 5.25790119\n",
      "Validated batch 283 batch loss 6.17455769\n",
      "Validated batch 284 batch loss 6.12947607\n",
      "Validated batch 285 batch loss 5.86607838\n",
      "Validated batch 286 batch loss 5.55144119\n",
      "Validated batch 287 batch loss 6.6756978\n",
      "Validated batch 288 batch loss 7.91460896\n",
      "Validated batch 289 batch loss 6.53881168\n",
      "Validated batch 290 batch loss 8.04702091\n",
      "Validated batch 291 batch loss 7.35528755\n",
      "Validated batch 292 batch loss 7.17628384\n",
      "Validated batch 293 batch loss 6.74849033\n",
      "Validated batch 294 batch loss 6.60553074\n",
      "Validated batch 295 batch loss 5.6428175\n",
      "Validated batch 296 batch loss 4.78584385\n",
      "Validated batch 297 batch loss 6.58480024\n",
      "Validated batch 298 batch loss 5.8048563\n",
      "Validated batch 299 batch loss 4.9998579\n",
      "Validated batch 300 batch loss 6.12599373\n",
      "Validated batch 301 batch loss 6.36892223\n",
      "Validated batch 302 batch loss 6.52482605\n",
      "Validated batch 303 batch loss 5.93386555\n",
      "Validated batch 304 batch loss 6.78267\n",
      "Validated batch 305 batch loss 6.71078491\n",
      "Validated batch 306 batch loss 5.93003273\n",
      "Validated batch 307 batch loss 5.53762913\n",
      "Validated batch 308 batch loss 5.72404766\n",
      "Validated batch 309 batch loss 6.44515896\n",
      "Validated batch 310 batch loss 5.86422348\n",
      "Validated batch 311 batch loss 6.08208847\n",
      "Validated batch 312 batch loss 6.78985\n",
      "Validated batch 313 batch loss 6.47157192\n",
      "Validated batch 314 batch loss 6.18062496\n",
      "Validated batch 315 batch loss 5.51172924\n",
      "Validated batch 316 batch loss 7.17507601\n",
      "Validated batch 317 batch loss 6.15953159\n",
      "Validated batch 318 batch loss 6.56984091\n",
      "Validated batch 319 batch loss 5.78912735\n",
      "Validated batch 320 batch loss 7.0536\n",
      "Validated batch 321 batch loss 5.40101719\n",
      "Validated batch 322 batch loss 6.24988031\n",
      "Validated batch 323 batch loss 6.72659492\n",
      "Validated batch 324 batch loss 6.55859327\n",
      "Validated batch 325 batch loss 6.84313917\n",
      "Validated batch 326 batch loss 5.50975609\n",
      "Validated batch 327 batch loss 4.91413498\n",
      "Validated batch 328 batch loss 4.6378\n",
      "Validated batch 329 batch loss 5.59670067\n",
      "Validated batch 330 batch loss 5.27341509\n",
      "Validated batch 331 batch loss 5.21949816\n",
      "Validated batch 332 batch loss 6.4428978\n",
      "Validated batch 333 batch loss 6.00952578\n",
      "Validated batch 334 batch loss 6.28844261\n",
      "Validated batch 335 batch loss 5.74718952\n",
      "Validated batch 336 batch loss 4.83271313\n",
      "Validated batch 337 batch loss 5.5157423\n",
      "Validated batch 338 batch loss 6.77435493\n",
      "Validated batch 339 batch loss 5.36558723\n",
      "Validated batch 340 batch loss 6.47998\n",
      "Validated batch 341 batch loss 6.08993149\n",
      "Validated batch 342 batch loss 6.36768723\n",
      "Validated batch 343 batch loss 6.29487801\n",
      "Validated batch 344 batch loss 6.20488501\n",
      "Validated batch 345 batch loss 6.09951067\n",
      "Validated batch 346 batch loss 6.1096139\n",
      "Validated batch 347 batch loss 5.48759651\n",
      "Validated batch 348 batch loss 6.27529812\n",
      "Validated batch 349 batch loss 5.66690063\n",
      "Validated batch 350 batch loss 6.50738144\n",
      "Validated batch 351 batch loss 6.45016909\n",
      "Validated batch 352 batch loss 5.51514721\n",
      "Validated batch 353 batch loss 4.66309452\n",
      "Validated batch 354 batch loss 5.9104681\n",
      "Validated batch 355 batch loss 6.44394\n",
      "Validated batch 356 batch loss 6.2127862\n",
      "Validated batch 357 batch loss 6.021945\n",
      "Validated batch 358 batch loss 5.87665653\n",
      "Validated batch 359 batch loss 5.74846935\n",
      "Validated batch 360 batch loss 5.11709642\n",
      "Validated batch 361 batch loss 7.74900532\n",
      "Validated batch 362 batch loss 7.020473\n",
      "Validated batch 363 batch loss 6.75231266\n",
      "Validated batch 364 batch loss 6.81187153\n",
      "Validated batch 365 batch loss 6.33507204\n",
      "Validated batch 366 batch loss 5.36305237\n",
      "Validated batch 367 batch loss 7.13475323\n",
      "Validated batch 368 batch loss 5.65728807\n",
      "Validated batch 369 batch loss 6.06557846\n",
      "Validated batch 370 batch loss 6.66147709\n",
      "Validated batch 371 batch loss 6.82405376\n",
      "Validated batch 372 batch loss 7.08265305\n",
      "Validated batch 373 batch loss 5.88932133\n",
      "Validated batch 374 batch loss 7.25758934\n",
      "Validated batch 375 batch loss 6.16725445\n",
      "Validated batch 376 batch loss 5.54344559\n",
      "Validated batch 377 batch loss 4.63538599\n",
      "Validated batch 378 batch loss 5.24352455\n",
      "Validated batch 379 batch loss 6.28559875\n",
      "Validated batch 380 batch loss 7.01413679\n",
      "Validated batch 381 batch loss 6.96073246\n",
      "Validated batch 382 batch loss 5.97740459\n",
      "Validated batch 383 batch loss 7.304636\n",
      "Validated batch 384 batch loss 7.59445763\n",
      "Validated batch 385 batch loss 6.53265905\n",
      "Validated batch 386 batch loss 5.84841776\n",
      "Validated batch 387 batch loss 6.42065144\n",
      "Validated batch 388 batch loss 6.39818668\n",
      "Validated batch 389 batch loss 6.62360191\n",
      "Validated batch 390 batch loss 6.88474083\n",
      "Validated batch 391 batch loss 5.9153347\n",
      "Validated batch 392 batch loss 6.12411833\n",
      "Validated batch 393 batch loss 5.77156878\n",
      "Validated batch 394 batch loss 5.75482655\n",
      "Validated batch 395 batch loss 6.22256231\n",
      "Validated batch 396 batch loss 6.34651852\n",
      "Validated batch 397 batch loss 7.1670804\n",
      "Validated batch 398 batch loss 6.89044142\n",
      "Validated batch 399 batch loss 6.96078873\n",
      "Validated batch 400 batch loss 6.37624645\n",
      "Validated batch 401 batch loss 4.87169\n",
      "Validated batch 402 batch loss 6.25306511\n",
      "Validated batch 403 batch loss 5.97982\n",
      "Validated batch 404 batch loss 6.3564086\n",
      "Validated batch 405 batch loss 6.575212\n",
      "Validated batch 406 batch loss 5.92056847\n",
      "Validated batch 407 batch loss 6.50240183\n",
      "Validated batch 408 batch loss 6.4907217\n",
      "Validated batch 409 batch loss 6.54500866\n",
      "Validated batch 410 batch loss 6.2281971\n",
      "Validated batch 411 batch loss 6.05092669\n",
      "Validated batch 412 batch loss 6.88583231\n",
      "Validated batch 413 batch loss 7.71089\n",
      "Validated batch 414 batch loss 6.85184908\n",
      "Validated batch 415 batch loss 6.54961205\n",
      "Validated batch 416 batch loss 6.36722279\n",
      "Validated batch 417 batch loss 6.4697237\n",
      "Validated batch 418 batch loss 6.26365042\n",
      "Validated batch 419 batch loss 6.10665083\n",
      "Validated batch 420 batch loss 6.07214546\n",
      "Validated batch 421 batch loss 6.18875885\n",
      "Validated batch 422 batch loss 6.90932417\n",
      "Validated batch 423 batch loss 4.50546646\n",
      "Validated batch 424 batch loss 5.52965832\n",
      "Validated batch 425 batch loss 5.6507473\n",
      "Validated batch 426 batch loss 6.62718773\n",
      "Validated batch 427 batch loss 6.3078351\n",
      "Validated batch 428 batch loss 7.22292614\n",
      "Validated batch 429 batch loss 6.98164749\n",
      "Validated batch 430 batch loss 6.3514514\n",
      "Validated batch 431 batch loss 5.90467167\n",
      "Validated batch 432 batch loss 6.33943033\n",
      "Validated batch 433 batch loss 7.20063496\n",
      "Validated batch 434 batch loss 6.51947117\n",
      "Validated batch 435 batch loss 6.1692276\n",
      "Validated batch 436 batch loss 6.4041996\n",
      "Validated batch 437 batch loss 6.44111872\n",
      "Validated batch 438 batch loss 6.38368416\n",
      "Validated batch 439 batch loss 6.51254\n",
      "Validated batch 440 batch loss 4.85069084\n",
      "Validated batch 441 batch loss 5.32552624\n",
      "Validated batch 442 batch loss 5.89359283\n",
      "Validated batch 443 batch loss 6.62705421\n",
      "Validated batch 444 batch loss 6.72014809\n",
      "Validated batch 445 batch loss 6.57309151\n",
      "Validated batch 446 batch loss 6.22848034\n",
      "Validated batch 447 batch loss 7.30034351\n",
      "Validated batch 448 batch loss 6.61628199\n",
      "Validated batch 449 batch loss 5.44556618\n",
      "Validated batch 450 batch loss 6.70336056\n",
      "Validated batch 451 batch loss 6.38961124\n",
      "Validated batch 452 batch loss 6.69593\n",
      "Validated batch 453 batch loss 5.81303263\n",
      "Validated batch 454 batch loss 6.91376543\n",
      "Validated batch 455 batch loss 5.64040041\n",
      "Validated batch 456 batch loss 6.62523413\n",
      "Validated batch 457 batch loss 7.19516039\n",
      "Validated batch 458 batch loss 6.84117365\n",
      "Validated batch 459 batch loss 6.09863043\n",
      "Validated batch 460 batch loss 5.43431902\n",
      "Validated batch 461 batch loss 5.70524\n",
      "Validated batch 462 batch loss 6.40635\n",
      "Validated batch 463 batch loss 5.54304028\n",
      "Validated batch 464 batch loss 5.97565746\n",
      "Validated batch 465 batch loss 5.44516087\n",
      "Validated batch 466 batch loss 5.61856\n",
      "Validated batch 467 batch loss 5.24644947\n",
      "Validated batch 468 batch loss 4.79140615\n",
      "Validated batch 469 batch loss 4.84295177\n",
      "Validated batch 470 batch loss 4.74621582\n",
      "Validated batch 471 batch loss 4.61574\n",
      "Validated batch 472 batch loss 5.76806068\n",
      "Validated batch 473 batch loss 6.63302422\n",
      "Validated batch 474 batch loss 7.48595858\n",
      "Validated batch 475 batch loss 6.20408154\n",
      "Validated batch 476 batch loss 6.16337\n",
      "Validated batch 477 batch loss 6.73123741\n",
      "Validated batch 478 batch loss 6.42474461\n",
      "Validated batch 479 batch loss 6.16068077\n",
      "Validated batch 480 batch loss 5.87635374\n",
      "Validated batch 481 batch loss 6.42463\n",
      "Validated batch 482 batch loss 6.65078\n",
      "Validated batch 483 batch loss 6.66562748\n",
      "Validated batch 484 batch loss 6.46328259\n",
      "Validated batch 485 batch loss 5.9524765\n",
      "Validated batch 486 batch loss 6.10898209\n",
      "Validated batch 487 batch loss 6.40000057\n",
      "Validated batch 488 batch loss 5.88915062\n",
      "Validated batch 489 batch loss 6.07817\n",
      "Validated batch 490 batch loss 6.77990818\n",
      "Validated batch 491 batch loss 6.68731689\n",
      "Validated batch 492 batch loss 6.20298195\n",
      "Validated batch 493 batch loss 5.93527699\n",
      "Validated batch 494 batch loss 6.74773598\n",
      "Validated batch 495 batch loss 5.24721241\n",
      "Validated batch 496 batch loss 6.19315529\n",
      "Validated batch 497 batch loss 6.22329283\n",
      "Validated batch 498 batch loss 6.2764926\n",
      "Validated batch 499 batch loss 4.944561\n",
      "Validated batch 500 batch loss 7.36988449\n",
      "Validated batch 501 batch loss 6.71178436\n",
      "Validated batch 502 batch loss 6.54342508\n",
      "Validated batch 503 batch loss 6.1668005\n",
      "Validated batch 504 batch loss 6.5878377\n",
      "Validated batch 505 batch loss 6.37624359\n",
      "Validated batch 506 batch loss 6.17190647\n",
      "Validated batch 507 batch loss 5.81259394\n",
      "Validated batch 508 batch loss 6.68540096\n",
      "Validated batch 509 batch loss 7.12264347\n",
      "Validated batch 510 batch loss 6.13110542\n",
      "Validated batch 511 batch loss 6.45367813\n",
      "Validated batch 512 batch loss 6.09164429\n",
      "Validated batch 513 batch loss 6.32021523\n",
      "Validated batch 514 batch loss 6.49734592\n",
      "Validated batch 515 batch loss 5.86044025\n",
      "Validated batch 516 batch loss 6.39102077\n",
      "Validated batch 517 batch loss 7.65929794\n",
      "Validated batch 518 batch loss 6.32642746\n",
      "Validated batch 519 batch loss 6.32896805\n",
      "Validated batch 520 batch loss 6.11012411\n",
      "Validated batch 521 batch loss 5.49608898\n",
      "Validated batch 522 batch loss 5.90996408\n",
      "Validated batch 523 batch loss 6.17899609\n",
      "Validated batch 524 batch loss 6.13003635\n",
      "Validated batch 525 batch loss 6.78980684\n",
      "Validated batch 526 batch loss 6.73174381\n",
      "Validated batch 527 batch loss 6.31958771\n",
      "Validated batch 528 batch loss 6.94633102\n",
      "Validated batch 529 batch loss 6.31391478\n",
      "Validated batch 530 batch loss 6.27761078\n",
      "Validated batch 531 batch loss 5.81666088\n",
      "Validated batch 532 batch loss 6.06447077\n",
      "Validated batch 533 batch loss 6.45711613\n",
      "Validated batch 534 batch loss 6.56406784\n",
      "Validated batch 535 batch loss 6.22433853\n",
      "Validated batch 536 batch loss 6.09419537\n",
      "Validated batch 537 batch loss 6.37003899\n",
      "Validated batch 538 batch loss 6.38313484\n",
      "Validated batch 539 batch loss 6.8776865\n",
      "Validated batch 540 batch loss 6.54029465\n",
      "Validated batch 541 batch loss 6.34830427\n",
      "Validated batch 542 batch loss 6.13121128\n",
      "Validated batch 543 batch loss 7.10460186\n",
      "Validated batch 544 batch loss 5.8961525\n",
      "Validated batch 545 batch loss 5.54283428\n",
      "Validated batch 546 batch loss 6.76388264\n",
      "Validated batch 547 batch loss 6.30848312\n",
      "Validated batch 548 batch loss 6.25479937\n",
      "Validated batch 549 batch loss 5.15884\n",
      "Validated batch 550 batch loss 5.3909359\n",
      "Validated batch 551 batch loss 5.32054043\n",
      "Validated batch 552 batch loss 6.07675\n",
      "Validated batch 553 batch loss 6.1963644\n",
      "Validated batch 554 batch loss 6.39118385\n",
      "Validated batch 555 batch loss 6.3069253\n",
      "Validated batch 556 batch loss 6.41095924\n",
      "Validated batch 557 batch loss 4.7528162\n",
      "Validated batch 558 batch loss 6.7054615\n",
      "Validated batch 559 batch loss 6.10645\n",
      "Validated batch 560 batch loss 5.73478794\n",
      "Validated batch 561 batch loss 6.71256447\n",
      "Validated batch 562 batch loss 5.13375187\n",
      "Validated batch 563 batch loss 6.7320323\n",
      "Validated batch 564 batch loss 6.81458759\n",
      "Validated batch 565 batch loss 6.10126\n",
      "Validated batch 566 batch loss 5.55130911\n",
      "Validated batch 567 batch loss 5.25336266\n",
      "Validated batch 568 batch loss 5.53100681\n",
      "Validated batch 569 batch loss 5.60490274\n",
      "Validated batch 570 batch loss 5.43585873\n",
      "Validated batch 571 batch loss 6.35669613\n",
      "Validated batch 572 batch loss 5.94270372\n",
      "Validated batch 573 batch loss 5.93918943\n",
      "Validated batch 574 batch loss 6.58735085\n",
      "Validated batch 575 batch loss 6.09084845\n",
      "Validated batch 576 batch loss 6.77474117\n",
      "Validated batch 577 batch loss 6.53161192\n",
      "Validated batch 578 batch loss 5.67435932\n",
      "Validated batch 579 batch loss 6.11579514\n",
      "Validated batch 580 batch loss 5.9048872\n",
      "Validated batch 581 batch loss 5.81987667\n",
      "Validated batch 582 batch loss 6.09953547\n",
      "Validated batch 583 batch loss 6.14989519\n",
      "Validated batch 584 batch loss 6.08345222\n",
      "Validated batch 585 batch loss 6.48157454\n",
      "Validated batch 586 batch loss 5.46965218\n",
      "Validated batch 587 batch loss 5.74041843\n",
      "Validated batch 588 batch loss 6.36725235\n",
      "Validated batch 589 batch loss 6.33135605\n",
      "Validated batch 590 batch loss 6.21046352\n",
      "Validated batch 591 batch loss 5.92514896\n",
      "Validated batch 592 batch loss 5.5050478\n",
      "Validated batch 593 batch loss 5.65687466\n",
      "Validated batch 594 batch loss 5.82023525\n",
      "Validated batch 595 batch loss 7.07953835\n",
      "Validated batch 596 batch loss 6.68077183\n",
      "Validated batch 597 batch loss 5.77771854\n",
      "Validated batch 598 batch loss 5.98217773\n",
      "Validated batch 599 batch loss 7.27957249\n",
      "Validated batch 600 batch loss 7.14125967\n",
      "Validated batch 601 batch loss 6.54544592\n",
      "Validated batch 602 batch loss 6.63768768\n",
      "Validated batch 603 batch loss 6.32337332\n",
      "Validated batch 604 batch loss 6.16708946\n",
      "Validated batch 605 batch loss 5.83759\n",
      "Validated batch 606 batch loss 5.48972416\n",
      "Validated batch 607 batch loss 6.17383194\n",
      "Validated batch 608 batch loss 7.58790207\n",
      "Validated batch 609 batch loss 6.24408722\n",
      "Validated batch 610 batch loss 6.51285\n",
      "Validated batch 611 batch loss 6.48885345\n",
      "Validated batch 612 batch loss 6.00243664\n",
      "Validated batch 613 batch loss 6.05706596\n",
      "Validated batch 614 batch loss 5.71224928\n",
      "Validated batch 615 batch loss 6.20094252\n",
      "Validated batch 616 batch loss 6.21587\n",
      "Validated batch 617 batch loss 6.44561481\n",
      "Validated batch 618 batch loss 6.59364796\n",
      "Validated batch 619 batch loss 7.75294542\n",
      "Validated batch 620 batch loss 7.14916945\n",
      "Validated batch 621 batch loss 6.91002\n",
      "Validated batch 622 batch loss 6.40596867\n",
      "Validated batch 623 batch loss 6.38625431\n",
      "Validated batch 624 batch loss 5.166605\n",
      "Validated batch 625 batch loss 6.42068863\n",
      "Validated batch 626 batch loss 5.97723961\n",
      "Validated batch 627 batch loss 7.20417833\n",
      "Validated batch 628 batch loss 7.20947361\n",
      "Validated batch 629 batch loss 7.64562416\n",
      "Validated batch 630 batch loss 6.60605478\n",
      "Validated batch 631 batch loss 5.9194231\n",
      "Validated batch 632 batch loss 6.0972662\n",
      "Validated batch 633 batch loss 6.00890923\n",
      "Validated batch 634 batch loss 5.98886919\n",
      "Validated batch 635 batch loss 5.69975471\n",
      "Validated batch 636 batch loss 6.68803787\n",
      "Validated batch 637 batch loss 6.27208567\n",
      "Validated batch 638 batch loss 6.51635742\n",
      "Validated batch 639 batch loss 5.3193121\n",
      "Validated batch 640 batch loss 7.02171087\n",
      "Validated batch 641 batch loss 5.61984253\n",
      "Validated batch 642 batch loss 5.76467323\n",
      "Validated batch 643 batch loss 5.50893068\n",
      "Validated batch 644 batch loss 6.79034805\n",
      "Validated batch 645 batch loss 6.06555367\n",
      "Validated batch 646 batch loss 6.06149149\n",
      "Validated batch 647 batch loss 6.02635765\n",
      "Validated batch 648 batch loss 6.85319519\n",
      "Validated batch 649 batch loss 7.30745316\n",
      "Validated batch 650 batch loss 6.73144245\n",
      "Validated batch 651 batch loss 5.73111534\n",
      "Validated batch 652 batch loss 5.49286413\n",
      "Validated batch 653 batch loss 6.12681\n",
      "Validated batch 654 batch loss 6.73036146\n",
      "Validated batch 655 batch loss 6.54694653\n",
      "Validated batch 656 batch loss 5.82139349\n",
      "Validated batch 657 batch loss 6.34384775\n",
      "Validated batch 658 batch loss 5.44727039\n",
      "Validated batch 659 batch loss 6.4218483\n",
      "Validated batch 660 batch loss 7.0664053\n",
      "Validated batch 661 batch loss 6.73787642\n",
      "Validated batch 662 batch loss 6.81883049\n",
      "Validated batch 663 batch loss 6.65119171\n",
      "Validated batch 664 batch loss 6.4453969\n",
      "Validated batch 665 batch loss 6.20798349\n",
      "Validated batch 666 batch loss 6.18452454\n",
      "Validated batch 667 batch loss 6.56776667\n",
      "Validated batch 668 batch loss 7.01190567\n",
      "Validated batch 669 batch loss 6.77471113\n",
      "Validated batch 670 batch loss 6.29972935\n",
      "Validated batch 671 batch loss 6.65775776\n",
      "Validated batch 672 batch loss 6.48198605\n",
      "Validated batch 673 batch loss 6.23973799\n",
      "Validated batch 674 batch loss 6.49573326\n",
      "Validated batch 675 batch loss 6.3730278\n",
      "Validated batch 676 batch loss 5.43562222\n",
      "Validated batch 677 batch loss 7.01012564\n",
      "Validated batch 678 batch loss 6.43100834\n",
      "Validated batch 679 batch loss 6.08554316\n",
      "Validated batch 680 batch loss 6.5624876\n",
      "Validated batch 681 batch loss 6.51141167\n",
      "Validated batch 682 batch loss 6.27413797\n",
      "Validated batch 683 batch loss 5.29995918\n",
      "Validated batch 684 batch loss 5.91243649\n",
      "Validated batch 685 batch loss 6.76751518\n",
      "Validated batch 686 batch loss 6.62715816\n",
      "Validated batch 687 batch loss 6.51728058\n",
      "Validated batch 688 batch loss 5.69581699\n",
      "Validated batch 689 batch loss 6.14600325\n",
      "Validated batch 690 batch loss 6.48766613\n",
      "Validated batch 691 batch loss 6.25579357\n",
      "Validated batch 692 batch loss 6.82672453\n",
      "Validated batch 693 batch loss 5.91551971\n",
      "Validated batch 694 batch loss 6.62469482\n",
      "Validated batch 695 batch loss 5.90551233\n",
      "Validated batch 696 batch loss 4.3780756\n",
      "Validated batch 697 batch loss 5.66370153\n",
      "Validated batch 698 batch loss 5.30044699\n",
      "Validated batch 699 batch loss 6.12535429\n",
      "Validated batch 700 batch loss 6.17303276\n",
      "Validated batch 701 batch loss 5.87916803\n",
      "Validated batch 702 batch loss 6.62126446\n",
      "Validated batch 703 batch loss 5.793262\n",
      "Validated batch 704 batch loss 5.78064346\n",
      "Validated batch 705 batch loss 6.11394405\n",
      "Validated batch 706 batch loss 6.40470505\n",
      "Validated batch 707 batch loss 6.08942509\n",
      "Validated batch 708 batch loss 6.02112103\n",
      "Validated batch 709 batch loss 6.07181931\n",
      "Validated batch 710 batch loss 6.24611521\n",
      "Validated batch 711 batch loss 6.17668152\n",
      "Validated batch 712 batch loss 6.21479416\n",
      "Validated batch 713 batch loss 6.14768887\n",
      "Validated batch 714 batch loss 6.51017189\n",
      "Validated batch 715 batch loss 6.53770971\n",
      "Validated batch 716 batch loss 6.17755127\n",
      "Validated batch 717 batch loss 6.13844442\n",
      "Validated batch 718 batch loss 7.42554283\n",
      "Validated batch 719 batch loss 6.26412296\n",
      "Validated batch 720 batch loss 5.82868\n",
      "Validated batch 721 batch loss 6.58888626\n",
      "Validated batch 722 batch loss 5.57892323\n",
      "Validated batch 723 batch loss 6.17746305\n",
      "Validated batch 724 batch loss 5.77925\n",
      "Validated batch 725 batch loss 5.97217655\n",
      "Validated batch 726 batch loss 6.42994499\n",
      "Validated batch 727 batch loss 6.09134769\n",
      "Validated batch 728 batch loss 6.39206076\n",
      "Validated batch 729 batch loss 6.41092205\n",
      "Validated batch 730 batch loss 6.13926792\n",
      "Validated batch 731 batch loss 6.11080837\n",
      "Validated batch 732 batch loss 6.81568146\n",
      "Validated batch 733 batch loss 6.10393953\n",
      "Validated batch 734 batch loss 5.71577358\n",
      "Validated batch 735 batch loss 6.23901033\n",
      "Validated batch 736 batch loss 5.95973301\n",
      "Validated batch 737 batch loss 5.39633417\n",
      "Validated batch 738 batch loss 5.33264542\n",
      "Validated batch 739 batch loss 5.9293108\n",
      "Validated batch 740 batch loss 6.87352085\n",
      "Validated batch 741 batch loss 6.42137909\n",
      "Validated batch 742 batch loss 6.5251379\n",
      "Validated batch 743 batch loss 5.88766146\n",
      "Validated batch 744 batch loss 6.48822165\n",
      "Validated batch 745 batch loss 6.60761833\n",
      "Validated batch 746 batch loss 6.13092041\n",
      "Validated batch 747 batch loss 6.18476582\n",
      "Validated batch 748 batch loss 5.91392374\n",
      "Validated batch 749 batch loss 5.96629906\n",
      "Validated batch 750 batch loss 7.08263731\n",
      "Validated batch 751 batch loss 5.89201\n",
      "Validated batch 752 batch loss 5.83851051\n",
      "Validated batch 753 batch loss 6.06597042\n",
      "Validated batch 754 batch loss 5.60835457\n",
      "Validated batch 755 batch loss 6.74547482\n",
      "Validated batch 756 batch loss 6.57408047\n",
      "Validated batch 757 batch loss 6.41516972\n",
      "Validated batch 758 batch loss 5.86124802\n",
      "Validated batch 759 batch loss 6.76491451\n",
      "Validated batch 760 batch loss 6.2449069\n",
      "Validated batch 761 batch loss 5.9240489\n",
      "Validated batch 762 batch loss 5.37196827\n",
      "Validated batch 763 batch loss 6.06874514\n",
      "Validated batch 764 batch loss 5.50773096\n",
      "Validated batch 765 batch loss 5.95450497\n",
      "Validated batch 766 batch loss 5.86081028\n",
      "Validated batch 767 batch loss 6.21082\n",
      "Validated batch 768 batch loss 5.92580318\n",
      "Validated batch 769 batch loss 6.8737421\n",
      "Validated batch 770 batch loss 5.37064743\n",
      "Validated batch 771 batch loss 5.27122831\n",
      "Validated batch 772 batch loss 6.66988\n",
      "Validated batch 773 batch loss 6.2146368\n",
      "Validated batch 774 batch loss 5.88174152\n",
      "Validated batch 775 batch loss 6.54859543\n",
      "Validated batch 776 batch loss 6.22733593\n",
      "Validated batch 777 batch loss 5.87965965\n",
      "Validated batch 778 batch loss 6.02698374\n",
      "Validated batch 779 batch loss 6.80540371\n",
      "Validated batch 780 batch loss 6.50542927\n",
      "Validated batch 781 batch loss 6.07272148\n",
      "Validated batch 782 batch loss 6.39052629\n",
      "Validated batch 783 batch loss 5.62865448\n",
      "Validated batch 784 batch loss 5.55097961\n",
      "Validated batch 785 batch loss 6.54634476\n",
      "Validated batch 786 batch loss 6.37392426\n",
      "Validated batch 787 batch loss 6.45954084\n",
      "Validated batch 788 batch loss 6.29206657\n",
      "Validated batch 789 batch loss 6.72381973\n",
      "Validated batch 790 batch loss 6.08318901\n",
      "Validated batch 791 batch loss 6.00646\n",
      "Validated batch 792 batch loss 5.9093442\n",
      "Validated batch 793 batch loss 4.80001545\n",
      "Validated batch 794 batch loss 6.91492462\n",
      "Validated batch 795 batch loss 7.93020678\n",
      "Validated batch 796 batch loss 6.44588709\n",
      "Validated batch 797 batch loss 5.57063\n",
      "Validated batch 798 batch loss 6.45168495\n",
      "Validated batch 799 batch loss 6.55632496\n",
      "Validated batch 800 batch loss 6.18453121\n",
      "Validated batch 801 batch loss 5.90964603\n",
      "Validated batch 802 batch loss 6.03830719\n",
      "Validated batch 803 batch loss 6.26938915\n",
      "Validated batch 804 batch loss 5.30009937\n",
      "Validated batch 805 batch loss 6.55409527\n",
      "Validated batch 806 batch loss 6.94796753\n",
      "Validated batch 807 batch loss 6.01819754\n",
      "Validated batch 808 batch loss 6.47315025\n",
      "Validated batch 809 batch loss 5.81797409\n",
      "Validated batch 810 batch loss 6.56878519\n",
      "Validated batch 811 batch loss 5.12503147\n",
      "Validated batch 812 batch loss 5.79408\n",
      "Validated batch 813 batch loss 6.56470871\n",
      "Validated batch 814 batch loss 5.98242712\n",
      "Validated batch 815 batch loss 6.07933712\n",
      "Validated batch 816 batch loss 5.93677711\n",
      "Validated batch 817 batch loss 5.52394152\n",
      "Validated batch 818 batch loss 6.74846506\n",
      "Validated batch 819 batch loss 6.81534958\n",
      "Validated batch 820 batch loss 7.49119377\n",
      "Validated batch 821 batch loss 7.91658115\n",
      "Validated batch 822 batch loss 6.57121754\n",
      "Validated batch 823 batch loss 5.98173237\n",
      "Validated batch 824 batch loss 6.41026402\n",
      "Validated batch 825 batch loss 6.53344059\n",
      "Validated batch 826 batch loss 6.27173138\n",
      "Validated batch 827 batch loss 6.60631132\n",
      "Validated batch 828 batch loss 6.27074194\n",
      "Validated batch 829 batch loss 6.30500317\n",
      "Validated batch 830 batch loss 7.05600739\n",
      "Validated batch 831 batch loss 7.30054569\n",
      "Validated batch 832 batch loss 6.23181248\n",
      "Validated batch 833 batch loss 5.75084972\n",
      "Validated batch 834 batch loss 5.7627306\n",
      "Validated batch 835 batch loss 5.95088577\n",
      "Validated batch 836 batch loss 6.20365429\n",
      "Validated batch 837 batch loss 5.7433672\n",
      "Validated batch 838 batch loss 6.44068432\n",
      "Validated batch 839 batch loss 5.06472\n",
      "Validated batch 840 batch loss 6.13937569\n",
      "Validated batch 841 batch loss 6.3578577\n",
      "Validated batch 842 batch loss 6.10537148\n",
      "Validated batch 843 batch loss 7.33715534\n",
      "Validated batch 844 batch loss 6.29409695\n",
      "Validated batch 845 batch loss 6.46152782\n",
      "Validated batch 846 batch loss 6.08164072\n",
      "Validated batch 847 batch loss 5.73132324\n",
      "Validated batch 848 batch loss 5.97922611\n",
      "Validated batch 849 batch loss 5.96827173\n",
      "Validated batch 850 batch loss 6.4061451\n",
      "Validated batch 851 batch loss 6.10446548\n",
      "Validated batch 852 batch loss 6.36356211\n",
      "Validated batch 853 batch loss 6.68793774\n",
      "Validated batch 854 batch loss 6.71819592\n",
      "Validated batch 855 batch loss 6.02825451\n",
      "Validated batch 856 batch loss 5.87710285\n",
      "Validated batch 857 batch loss 6.43892765\n",
      "Validated batch 858 batch loss 6.01616383\n",
      "Validated batch 859 batch loss 6.58188772\n",
      "Validated batch 860 batch loss 5.79905701\n",
      "Validated batch 861 batch loss 6.8397541\n",
      "Validated batch 862 batch loss 6.67511368\n",
      "Validated batch 863 batch loss 5.24977684\n",
      "Validated batch 864 batch loss 5.97277927\n",
      "Validated batch 865 batch loss 6.28860188\n",
      "Validated batch 866 batch loss 5.5148983\n",
      "Validated batch 867 batch loss 6.84250927\n",
      "Validated batch 868 batch loss 6.60108757\n",
      "Validated batch 869 batch loss 6.78423738\n",
      "Validated batch 870 batch loss 7.12396717\n",
      "Validated batch 871 batch loss 6.27252197\n",
      "Validated batch 872 batch loss 6.07242823\n",
      "Validated batch 873 batch loss 6.65193319\n",
      "Validated batch 874 batch loss 6.8576\n",
      "Validated batch 875 batch loss 7.13092422\n",
      "Validated batch 876 batch loss 5.43712425\n",
      "Validated batch 877 batch loss 6.43164253\n",
      "Validated batch 878 batch loss 4.7962513\n",
      "Validated batch 879 batch loss 5.14977741\n",
      "Validated batch 880 batch loss 6.29365921\n",
      "Validated batch 881 batch loss 6.54646444\n",
      "Validated batch 882 batch loss 7.06428814\n",
      "Validated batch 883 batch loss 7.19297695\n",
      "Validated batch 884 batch loss 7.55114174\n",
      "Validated batch 885 batch loss 6.17364788\n",
      "Validated batch 886 batch loss 6.59649849\n",
      "Validated batch 887 batch loss 6.37388802\n",
      "Validated batch 888 batch loss 5.71323633\n",
      "Validated batch 889 batch loss 6.36364365\n",
      "Validated batch 890 batch loss 6.22876072\n",
      "Validated batch 891 batch loss 4.46601582\n",
      "Validated batch 892 batch loss 4.90547562\n",
      "Validated batch 893 batch loss 6.8875308\n",
      "Validated batch 894 batch loss 6.88741684\n",
      "Validated batch 895 batch loss 7.06276083\n",
      "Validated batch 896 batch loss 5.97264051\n",
      "Validated batch 897 batch loss 5.24273205\n",
      "Validated batch 898 batch loss 7.4287653\n",
      "Validated batch 899 batch loss 6.77653742\n",
      "Validated batch 900 batch loss 6.00936842\n",
      "Validated batch 901 batch loss 6.17777634\n",
      "Validated batch 902 batch loss 4.94992542\n",
      "Validated batch 903 batch loss 5.87207\n",
      "Validated batch 904 batch loss 6.64842272\n",
      "Validated batch 905 batch loss 6.25202274\n",
      "Validated batch 906 batch loss 5.78537273\n",
      "Validated batch 907 batch loss 6.40323448\n",
      "Validated batch 908 batch loss 5.98657942\n",
      "Validated batch 909 batch loss 6.05387115\n",
      "Validated batch 910 batch loss 6.7572341\n",
      "Validated batch 911 batch loss 7.22365856\n",
      "Validated batch 912 batch loss 6.85486126\n",
      "Validated batch 913 batch loss 6.42903423\n",
      "Validated batch 914 batch loss 5.48474216\n",
      "Validated batch 915 batch loss 5.40722847\n",
      "Validated batch 916 batch loss 5.40014315\n",
      "Validated batch 917 batch loss 6.10894871\n",
      "Validated batch 918 batch loss 6.5836\n",
      "Validated batch 919 batch loss 6.42870283\n",
      "Validated batch 920 batch loss 6.25712299\n",
      "Validated batch 921 batch loss 5.32599306\n",
      "Validated batch 922 batch loss 7.51811934\n",
      "Validated batch 923 batch loss 6.14961863\n",
      "Validated batch 924 batch loss 6.91358185\n",
      "Validated batch 925 batch loss 6.72318268\n",
      "Validated batch 926 batch loss 6.12378168\n",
      "Validated batch 927 batch loss 6.35961723\n",
      "Validated batch 928 batch loss 5.80167484\n",
      "Validated batch 929 batch loss 6.48478603\n",
      "Validated batch 930 batch loss 6.42148972\n",
      "Validated batch 931 batch loss 6.50794792\n",
      "Validated batch 932 batch loss 6.02228355\n",
      "Validated batch 933 batch loss 7.49356842\n",
      "Validated batch 934 batch loss 7.395854\n",
      "Validated batch 935 batch loss 7.29594803\n",
      "Validated batch 936 batch loss 6.70307446\n",
      "Validated batch 937 batch loss 6.61433315\n",
      "Validated batch 938 batch loss 5.86256409\n",
      "Validated batch 939 batch loss 7.29922152\n",
      "Validated batch 940 batch loss 6.10292\n",
      "Validated batch 941 batch loss 5.81752\n",
      "Validated batch 942 batch loss 5.32355309\n",
      "Validated batch 943 batch loss 5.26108837\n",
      "Validated batch 944 batch loss 5.55653334\n",
      "Validated batch 945 batch loss 6.59983444\n",
      "Validated batch 946 batch loss 6.63681078\n",
      "Validated batch 947 batch loss 7.62388659\n",
      "Validated batch 948 batch loss 4.93458605\n",
      "Validated batch 949 batch loss 6.68255234\n",
      "Validated batch 950 batch loss 5.61431599\n",
      "Validated batch 951 batch loss 6.23226118\n",
      "Validated batch 952 batch loss 6.06844\n",
      "Validated batch 953 batch loss 4.55375671\n",
      "Validated batch 954 batch loss 5.00947046\n",
      "Validated batch 955 batch loss 6.30065536\n",
      "Validated batch 956 batch loss 5.10847235\n",
      "Validated batch 957 batch loss 6.45686054\n",
      "Validated batch 958 batch loss 6.07290745\n",
      "Validated batch 959 batch loss 5.7929306\n",
      "Validated batch 960 batch loss 6.56064367\n",
      "Validated batch 961 batch loss 6.63051796\n",
      "Validated batch 962 batch loss 6.78270817\n",
      "Validated batch 963 batch loss 6.97606039\n",
      "Validated batch 964 batch loss 6.27525902\n",
      "Validated batch 965 batch loss 4.30711746\n",
      "Validated batch 966 batch loss 5.61769485\n",
      "Validated batch 967 batch loss 6.1751585\n",
      "Validated batch 968 batch loss 6.49673653\n",
      "Validated batch 969 batch loss 6.4404211\n",
      "Validated batch 970 batch loss 6.64462805\n",
      "Validated batch 971 batch loss 6.93981361\n",
      "Validated batch 972 batch loss 5.83490181\n",
      "Validated batch 973 batch loss 6.23059082\n",
      "Validated batch 974 batch loss 5.06966305\n",
      "Validated batch 975 batch loss 6.09030437\n",
      "Validated batch 976 batch loss 6.3433423\n",
      "Validated batch 977 batch loss 6.48052835\n",
      "Validated batch 978 batch loss 5.35918951\n",
      "Validated batch 979 batch loss 6.18829536\n",
      "Validated batch 980 batch loss 5.2241497\n",
      "Validated batch 981 batch loss 5.40499306\n",
      "Validated batch 982 batch loss 6.48635\n",
      "Validated batch 983 batch loss 5.36194324\n",
      "Validated batch 984 batch loss 5.07380772\n",
      "Validated batch 985 batch loss 6.65771294\n",
      "Validated batch 986 batch loss 6.63424158\n",
      "Validated batch 987 batch loss 6.51573944\n",
      "Validated batch 988 batch loss 6.14463663\n",
      "Validated batch 989 batch loss 5.29348183\n",
      "Validated batch 990 batch loss 4.71726\n",
      "Validated batch 991 batch loss 4.69573975\n",
      "Validated batch 992 batch loss 5.63490868\n",
      "Validated batch 993 batch loss 6.79144287\n",
      "Validated batch 994 batch loss 6.46065044\n",
      "Validated batch 995 batch loss 6.53112221\n",
      "Validated batch 996 batch loss 6.46719646\n",
      "Validated batch 997 batch loss 7.35639668\n",
      "Validated batch 998 batch loss 7.91517782\n",
      "Validated batch 999 batch loss 6.11422491\n",
      "Validated batch 1000 batch loss 5.4355917\n",
      "Validated batch 1001 batch loss 6.41710711\n",
      "Validated batch 1002 batch loss 6.47718287\n",
      "Validated batch 1003 batch loss 6.08996\n",
      "Validated batch 1004 batch loss 6.72870398\n",
      "Validated batch 1005 batch loss 5.7604475\n",
      "Validated batch 1006 batch loss 6.20685577\n",
      "Validated batch 1007 batch loss 6.3540597\n",
      "Validated batch 1008 batch loss 5.45495224\n",
      "Validated batch 1009 batch loss 6.52486134\n",
      "Validated batch 1010 batch loss 5.99605036\n",
      "Validated batch 1011 batch loss 5.07419205\n",
      "Validated batch 1012 batch loss 6.2336278\n",
      "Validated batch 1013 batch loss 6.25627899\n",
      "Validated batch 1014 batch loss 6.07553673\n",
      "Validated batch 1015 batch loss 5.31300592\n",
      "Validated batch 1016 batch loss 5.91483\n",
      "Validated batch 1017 batch loss 6.32806826\n",
      "Validated batch 1018 batch loss 6.20274496\n",
      "Validated batch 1019 batch loss 6.24706\n",
      "Validated batch 1020 batch loss 3.70959544\n",
      "Validated batch 1021 batch loss 6.78146505\n",
      "Validated batch 1022 batch loss 5.23756504\n",
      "Validated batch 1023 batch loss 6.95069647\n",
      "Validated batch 1024 batch loss 5.97582912\n",
      "Validated batch 1025 batch loss 6.16739655\n",
      "Validated batch 1026 batch loss 5.27505827\n",
      "Validated batch 1027 batch loss 6.05903721\n",
      "Validated batch 1028 batch loss 7.68766117\n",
      "Validated batch 1029 batch loss 6.21312141\n",
      "Validated batch 1030 batch loss 6.21852684\n",
      "Validated batch 1031 batch loss 7.08015537\n",
      "Validated batch 1032 batch loss 5.91649151\n",
      "Validated batch 1033 batch loss 7.43028212\n",
      "Validated batch 1034 batch loss 6.56232786\n",
      "Validated batch 1035 batch loss 6.06403351\n",
      "Validated batch 1036 batch loss 6.35622025\n",
      "Validated batch 1037 batch loss 4.9329319\n",
      "Validated batch 1038 batch loss 5.95904255\n",
      "Validated batch 1039 batch loss 5.94392633\n",
      "Validated batch 1040 batch loss 6.08837557\n",
      "Validated batch 1041 batch loss 6.84453869\n",
      "Validated batch 1042 batch loss 5.80531168\n",
      "Validated batch 1043 batch loss 6.24073601\n",
      "Validated batch 1044 batch loss 6.33757114\n",
      "Validated batch 1045 batch loss 5.78513432\n",
      "Validated batch 1046 batch loss 6.86344481\n",
      "Validated batch 1047 batch loss 7.01874638\n",
      "Validated batch 1048 batch loss 5.46101952\n",
      "Validated batch 1049 batch loss 6.39289665\n",
      "Validated batch 1050 batch loss 6.38000202\n",
      "Validated batch 1051 batch loss 7.151824\n",
      "Validated batch 1052 batch loss 6.72173595\n",
      "Validated batch 1053 batch loss 5.90519667\n",
      "Validated batch 1054 batch loss 5.98817539\n",
      "Validated batch 1055 batch loss 5.90689182\n",
      "Validated batch 1056 batch loss 5.50425911\n",
      "Validated batch 1057 batch loss 6.24162197\n",
      "Validated batch 1058 batch loss 6.13433504\n",
      "Validated batch 1059 batch loss 6.4266758\n",
      "Validated batch 1060 batch loss 6.79172945\n",
      "Validated batch 1061 batch loss 6.12809753\n",
      "Validated batch 1062 batch loss 6.58826542\n",
      "Validated batch 1063 batch loss 6.88192654\n",
      "Validated batch 1064 batch loss 6.99134445\n",
      "Validated batch 1065 batch loss 6.2638216\n",
      "Validated batch 1066 batch loss 6.41978073\n",
      "Validated batch 1067 batch loss 6.49995422\n",
      "Validated batch 1068 batch loss 6.53723907\n",
      "Validated batch 1069 batch loss 6.23526764\n",
      "Validated batch 1070 batch loss 5.4641819\n",
      "Validated batch 1071 batch loss 5.24010515\n",
      "Validated batch 1072 batch loss 6.07616806\n",
      "Validated batch 1073 batch loss 6.2294755\n",
      "Validated batch 1074 batch loss 6.30102062\n",
      "Validated batch 1075 batch loss 6.45975494\n",
      "Validated batch 1076 batch loss 6.28611469\n",
      "Validated batch 1077 batch loss 7.31105\n",
      "Validated batch 1078 batch loss 7.55236721\n",
      "Validated batch 1079 batch loss 6.77597523\n",
      "Validated batch 1080 batch loss 6.68142128\n",
      "Validated batch 1081 batch loss 5.56297\n",
      "Validated batch 1082 batch loss 4.94750595\n",
      "Validated batch 1083 batch loss 6.93507099\n",
      "Validated batch 1084 batch loss 5.68295288\n",
      "Validated batch 1085 batch loss 5.71446562\n",
      "Validated batch 1086 batch loss 6.47678375\n",
      "Validated batch 1087 batch loss 7.41548824\n",
      "Validated batch 1088 batch loss 6.65826893\n",
      "Validated batch 1089 batch loss 7.21436787\n",
      "Validated batch 1090 batch loss 6.38826418\n",
      "Validated batch 1091 batch loss 6.41526222\n",
      "Validated batch 1092 batch loss 5.64964962\n",
      "Validated batch 1093 batch loss 5.87621164\n",
      "Validated batch 1094 batch loss 5.47734642\n",
      "Validated batch 1095 batch loss 5.00599337\n",
      "Validated batch 1096 batch loss 6.06248903\n",
      "Validated batch 1097 batch loss 4.7475915\n",
      "Validated batch 1098 batch loss 5.02303028\n",
      "Validated batch 1099 batch loss 5.19387102\n",
      "Validated batch 1100 batch loss 5.45509577\n",
      "Validated batch 1101 batch loss 6.14725113\n",
      "Validated batch 1102 batch loss 6.08779144\n",
      "Validated batch 1103 batch loss 6.46946\n",
      "Validated batch 1104 batch loss 5.82163143\n",
      "Validated batch 1105 batch loss 6.26639748\n",
      "Validated batch 1106 batch loss 6.47115707\n",
      "Validated batch 1107 batch loss 7.36083078\n",
      "Validated batch 1108 batch loss 7.35035181\n",
      "Validated batch 1109 batch loss 6.06198406\n",
      "Validated batch 1110 batch loss 6.76494694\n",
      "Validated batch 1111 batch loss 7.23130751\n",
      "Validated batch 1112 batch loss 5.83967352\n",
      "Validated batch 1113 batch loss 7.0536027\n",
      "Validated batch 1114 batch loss 7.18403101\n",
      "Validated batch 1115 batch loss 6.64126921\n",
      "Validated batch 1116 batch loss 6.80615425\n",
      "Validated batch 1117 batch loss 6.84546089\n",
      "Validated batch 1118 batch loss 5.95713615\n",
      "Validated batch 1119 batch loss 5.84345\n",
      "Validated batch 1120 batch loss 6.38191223\n",
      "Validated batch 1121 batch loss 5.94979668\n",
      "Validated batch 1122 batch loss 6.21437407\n",
      "Validated batch 1123 batch loss 6.18969917\n",
      "Validated batch 1124 batch loss 6.89105797\n",
      "Validated batch 1125 batch loss 7.0041151\n",
      "Validated batch 1126 batch loss 6.64769173\n",
      "Validated batch 1127 batch loss 5.64641333\n",
      "Validated batch 1128 batch loss 7.17858028\n",
      "Validated batch 1129 batch loss 6.77525043\n",
      "Validated batch 1130 batch loss 5.39335155\n",
      "Validated batch 1131 batch loss 5.77940035\n",
      "Validated batch 1132 batch loss 6.37060308\n",
      "Validated batch 1133 batch loss 6.60166359\n",
      "Validated batch 1134 batch loss 5.8346591\n",
      "Validated batch 1135 batch loss 5.80973482\n",
      "Validated batch 1136 batch loss 5.2136097\n",
      "Validated batch 1137 batch loss 6.06340122\n",
      "Validated batch 1138 batch loss 6.23164558\n",
      "Validated batch 1139 batch loss 6.25990438\n",
      "Validated batch 1140 batch loss 6.25997877\n",
      "Validated batch 1141 batch loss 6.36592627\n",
      "Validated batch 1142 batch loss 6.83091354\n",
      "Validated batch 1143 batch loss 5.85972786\n",
      "Validated batch 1144 batch loss 5.78319263\n",
      "Validated batch 1145 batch loss 6.0899744\n",
      "Validated batch 1146 batch loss 6.23928452\n",
      "Validated batch 1147 batch loss 6.00527\n",
      "Validated batch 1148 batch loss 6.59589386\n",
      "Validated batch 1149 batch loss 6.32868\n",
      "Validated batch 1150 batch loss 6.25080299\n",
      "Validated batch 1151 batch loss 7.07269096\n",
      "Validated batch 1152 batch loss 7.01097298\n",
      "Validated batch 1153 batch loss 6.66614914\n",
      "Validated batch 1154 batch loss 5.98910666\n",
      "Validated batch 1155 batch loss 5.82414675\n",
      "Validated batch 1156 batch loss 6.40313\n",
      "Validated batch 1157 batch loss 5.4822197\n",
      "Validated batch 1158 batch loss 6.15728188\n",
      "Validated batch 1159 batch loss 6.20285559\n",
      "Validated batch 1160 batch loss 6.67173481\n",
      "Validated batch 1161 batch loss 6.50626802\n",
      "Validated batch 1162 batch loss 6.11221552\n",
      "Validated batch 1163 batch loss 5.61951637\n",
      "Validated batch 1164 batch loss 6.07232428\n",
      "Validated batch 1165 batch loss 6.61885548\n",
      "Validated batch 1166 batch loss 6.39228058\n",
      "Validated batch 1167 batch loss 5.67924595\n",
      "Validated batch 1168 batch loss 5.92757702\n",
      "Validated batch 1169 batch loss 6.14243698\n",
      "Validated batch 1170 batch loss 5.08352089\n",
      "Validated batch 1171 batch loss 5.46829891\n",
      "Validated batch 1172 batch loss 6.39189148\n",
      "Validated batch 1173 batch loss 5.24058628\n",
      "Validated batch 1174 batch loss 6.02936554\n",
      "Validated batch 1175 batch loss 5.65153027\n",
      "Validated batch 1176 batch loss 5.91934\n",
      "Validated batch 1177 batch loss 5.68985415\n",
      "Validated batch 1178 batch loss 5.9710741\n",
      "Validated batch 1179 batch loss 6.11395741\n",
      "Validated batch 1180 batch loss 6.07591486\n",
      "Validated batch 1181 batch loss 7.03935242\n",
      "Validated batch 1182 batch loss 6.40606976\n",
      "Validated batch 1183 batch loss 5.87649822\n",
      "Validated batch 1184 batch loss 6.12769604\n",
      "Validated batch 1185 batch loss 7.0744133\n",
      "Validated batch 1186 batch loss 5.00767946\n",
      "Validated batch 1187 batch loss 5.9723587\n",
      "Validated batch 1188 batch loss 5.66582489\n",
      "Validated batch 1189 batch loss 6.3131237\n",
      "Validated batch 1190 batch loss 6.78179932\n",
      "Validated batch 1191 batch loss 6.27330303\n",
      "Validated batch 1192 batch loss 6.18174362\n",
      "Validated batch 1193 batch loss 5.47595692\n",
      "Validated batch 1194 batch loss 6.33260632\n",
      "Validated batch 1195 batch loss 6.71993732\n",
      "Validated batch 1196 batch loss 6.67635298\n",
      "Validated batch 1197 batch loss 5.86091805\n",
      "Validated batch 1198 batch loss 6.64524841\n",
      "Validated batch 1199 batch loss 6.1869154\n",
      "Validated batch 1200 batch loss 6.01455545\n",
      "Validated batch 1201 batch loss 6.26937294\n",
      "Validated batch 1202 batch loss 5.5463686\n",
      "Validated batch 1203 batch loss 6.58024502\n",
      "Validated batch 1204 batch loss 6.99821186\n",
      "Validated batch 1205 batch loss 4.90058565\n",
      "Validated batch 1206 batch loss 5.86632442\n",
      "Validated batch 1207 batch loss 7.01862526\n",
      "Validated batch 1208 batch loss 6.55496597\n",
      "Validated batch 1209 batch loss 6.18376923\n",
      "Validated batch 1210 batch loss 6.19042587\n",
      "Validated batch 1211 batch loss 6.32900906\n",
      "Validated batch 1212 batch loss 5.61019516\n",
      "Validated batch 1213 batch loss 5.6650753\n",
      "Validated batch 1214 batch loss 5.87421846\n",
      "Validated batch 1215 batch loss 6.07890701\n",
      "Validated batch 1216 batch loss 5.54860783\n",
      "Validated batch 1217 batch loss 6.02313709\n",
      "Validated batch 1218 batch loss 5.97880268\n",
      "Validated batch 1219 batch loss 6.19082355\n",
      "Validated batch 1220 batch loss 5.65567446\n",
      "Validated batch 1221 batch loss 5.93810129\n",
      "Validated batch 1222 batch loss 7.01037502\n",
      "Validated batch 1223 batch loss 6.37962961\n",
      "Validated batch 1224 batch loss 6.42812538\n",
      "Validated batch 1225 batch loss 6.8333168\n",
      "Validated batch 1226 batch loss 6.29135227\n",
      "Validated batch 1227 batch loss 4.82881069\n",
      "Validated batch 1228 batch loss 5.72632074\n",
      "Validated batch 1229 batch loss 6.21844196\n",
      "Validated batch 1230 batch loss 6.22241163\n",
      "Validated batch 1231 batch loss 7.03661346\n",
      "Validated batch 1232 batch loss 6.23333645\n",
      "Validated batch 1233 batch loss 5.98379517\n",
      "Validated batch 1234 batch loss 6.18973827\n",
      "Validated batch 1235 batch loss 6.76114798\n",
      "Validated batch 1236 batch loss 6.39016342\n",
      "Validated batch 1237 batch loss 5.72283459\n",
      "Validated batch 1238 batch loss 6.3175745\n",
      "Validated batch 1239 batch loss 6.34086943\n",
      "Validated batch 1240 batch loss 6.15563583\n",
      "Validated batch 1241 batch loss 6.85670185\n",
      "Validated batch 1242 batch loss 5.85000753\n",
      "Validated batch 1243 batch loss 6.26988268\n",
      "Validated batch 1244 batch loss 6.47659206\n",
      "Validated batch 1245 batch loss 6.76065159\n",
      "Validated batch 1246 batch loss 5.68323374\n",
      "Validated batch 1247 batch loss 6.58553839\n",
      "Validated batch 1248 batch loss 7.10914135\n",
      "Validated batch 1249 batch loss 6.66460228\n",
      "Validated batch 1250 batch loss 6.31218624\n",
      "Validated batch 1251 batch loss 6.26097536\n",
      "Validated batch 1252 batch loss 6.52070189\n",
      "Validated batch 1253 batch loss 6.57493114\n",
      "Validated batch 1254 batch loss 6.56354523\n",
      "Validated batch 1255 batch loss 6.81320095\n",
      "Validated batch 1256 batch loss 6.52949524\n",
      "Validated batch 1257 batch loss 7.13399315\n",
      "Validated batch 1258 batch loss 6.73090649\n",
      "Validated batch 1259 batch loss 7.28840637\n",
      "Validated batch 1260 batch loss 7.16922665\n",
      "Validated batch 1261 batch loss 7.03162575\n",
      "Validated batch 1262 batch loss 7.2918191\n",
      "Validated batch 1263 batch loss 7.9173317\n",
      "Validated batch 1264 batch loss 6.21134949\n",
      "Validated batch 1265 batch loss 7.21827745\n",
      "Validated batch 1266 batch loss 7.39923763\n",
      "Validated batch 1267 batch loss 7.8026495\n",
      "Validated batch 1268 batch loss 6.76299906\n",
      "Validated batch 1269 batch loss 7.80260563\n",
      "Validated batch 1270 batch loss 6.78892803\n",
      "Validated batch 1271 batch loss 7.23406363\n",
      "Validated batch 1272 batch loss 7.67783976\n",
      "Validated batch 1273 batch loss 7.07547951\n",
      "Validated batch 1274 batch loss 6.2224741\n",
      "Validated batch 1275 batch loss 6.30074453\n",
      "Validated batch 1276 batch loss 6.37919188\n",
      "Validated batch 1277 batch loss 5.83227491\n",
      "Validated batch 1278 batch loss 6.60013866\n",
      "Validated batch 1279 batch loss 5.13266373\n",
      "Validated batch 1280 batch loss 6.41570807\n",
      "Validated batch 1281 batch loss 6.55315\n",
      "Validated batch 1282 batch loss 6.27064753\n",
      "Validated batch 1283 batch loss 6.73887444\n",
      "Validated batch 1284 batch loss 6.12297773\n",
      "Validated batch 1285 batch loss 7.06042099\n",
      "Validated batch 1286 batch loss 6.11188507\n",
      "Validated batch 1287 batch loss 4.35630035\n",
      "Validated batch 1288 batch loss 5.8762126\n",
      "Validated batch 1289 batch loss 5.88725758\n",
      "Validated batch 1290 batch loss 5.27825737\n",
      "Validated batch 1291 batch loss 6.43015814\n",
      "Validated batch 1292 batch loss 6.259\n",
      "Validated batch 1293 batch loss 5.97104073\n",
      "Validated batch 1294 batch loss 5.56197548\n",
      "Validated batch 1295 batch loss 4.55994129\n",
      "Validated batch 1296 batch loss 6.05474043\n",
      "Validated batch 1297 batch loss 6.45328379\n",
      "Validated batch 1298 batch loss 6.95596886\n",
      "Validated batch 1299 batch loss 5.44916916\n",
      "Validated batch 1300 batch loss 6.16439342\n",
      "Validated batch 1301 batch loss 6.69348431\n",
      "Validated batch 1302 batch loss 6.62307596\n",
      "Validated batch 1303 batch loss 6.76567841\n",
      "Validated batch 1304 batch loss 6.41984701\n",
      "Validated batch 1305 batch loss 6.03463459\n",
      "Validated batch 1306 batch loss 5.9503\n",
      "Validated batch 1307 batch loss 5.94705486\n",
      "Validated batch 1308 batch loss 6.7644434\n",
      "Validated batch 1309 batch loss 5.90024757\n",
      "Validated batch 1310 batch loss 6.28397274\n",
      "Validated batch 1311 batch loss 5.34461308\n",
      "Validated batch 1312 batch loss 6.22321939\n",
      "Validated batch 1313 batch loss 6.41801071\n",
      "Validated batch 1314 batch loss 6.42587852\n",
      "Validated batch 1315 batch loss 6.43430805\n",
      "Validated batch 1316 batch loss 4.30941439\n",
      "Validated batch 1317 batch loss 6.1540575\n",
      "Validated batch 1318 batch loss 6.32993793\n",
      "Validated batch 1319 batch loss 5.50793457\n",
      "Validated batch 1320 batch loss 6.17343712\n",
      "Validated batch 1321 batch loss 7.48496628\n",
      "Validated batch 1322 batch loss 7.38262463\n",
      "Validated batch 1323 batch loss 5.98861694\n",
      "Validated batch 1324 batch loss 6.88541\n",
      "Validated batch 1325 batch loss 7.45893145\n",
      "Validated batch 1326 batch loss 6.93874836\n",
      "Validated batch 1327 batch loss 6.87985229\n",
      "Validated batch 1328 batch loss 5.976367\n",
      "Validated batch 1329 batch loss 5.79533863\n",
      "Validated batch 1330 batch loss 6.34004784\n",
      "Validated batch 1331 batch loss 6.37964\n",
      "Validated batch 1332 batch loss 6.19307184\n",
      "Validated batch 1333 batch loss 6.51832962\n",
      "Validated batch 1334 batch loss 6.41579914\n",
      "Validated batch 1335 batch loss 5.96307325\n",
      "Validated batch 1336 batch loss 6.36825657\n",
      "Validated batch 1337 batch loss 5.93186378\n",
      "Validated batch 1338 batch loss 6.52389193\n",
      "Validated batch 1339 batch loss 5.88426399\n",
      "Validated batch 1340 batch loss 6.03092575\n",
      "Validated batch 1341 batch loss 6.4207859\n",
      "Validated batch 1342 batch loss 6.39545107\n",
      "Validated batch 1343 batch loss 7.34744787\n",
      "Validated batch 1344 batch loss 6.0301404\n",
      "Validated batch 1345 batch loss 6.87153\n",
      "Validated batch 1346 batch loss 6.01484585\n",
      "Validated batch 1347 batch loss 6.27547073\n",
      "Validated batch 1348 batch loss 5.82594299\n",
      "Validated batch 1349 batch loss 6.36215305\n",
      "Validated batch 1350 batch loss 7.05969286\n",
      "Validated batch 1351 batch loss 6.09892082\n",
      "Validated batch 1352 batch loss 4.77218819\n",
      "Validated batch 1353 batch loss 5.82219553\n",
      "Validated batch 1354 batch loss 7.5809536\n",
      "Validated batch 1355 batch loss 7.29307365\n",
      "Validated batch 1356 batch loss 6.51670933\n",
      "Validated batch 1357 batch loss 6.20607519\n",
      "Validated batch 1358 batch loss 5.61202812\n",
      "Validated batch 1359 batch loss 6.77011204\n",
      "Validated batch 1360 batch loss 6.15263033\n",
      "Validated batch 1361 batch loss 7.21974277\n",
      "Validated batch 1362 batch loss 6.54180288\n",
      "Validated batch 1363 batch loss 6.82570553\n",
      "Validated batch 1364 batch loss 6.28332663\n",
      "Validated batch 1365 batch loss 5.82066345\n",
      "Validated batch 1366 batch loss 6.79693508\n",
      "Validated batch 1367 batch loss 6.60137272\n",
      "Validated batch 1368 batch loss 5.98902416\n",
      "Validated batch 1369 batch loss 5.99095154\n",
      "Validated batch 1370 batch loss 6.33251858\n",
      "Validated batch 1371 batch loss 6.37115097\n",
      "Validated batch 1372 batch loss 5.92534399\n",
      "Validated batch 1373 batch loss 6.68825722\n",
      "Validated batch 1374 batch loss 7.23230553\n",
      "Validated batch 1375 batch loss 6.60626698\n",
      "Validated batch 1376 batch loss 6.03844929\n",
      "Validated batch 1377 batch loss 7.12963486\n",
      "Validated batch 1378 batch loss 6.13098383\n",
      "Validated batch 1379 batch loss 6.55948639\n",
      "Validated batch 1380 batch loss 6.96048784\n",
      "Validated batch 1381 batch loss 7.40155315\n",
      "Validated batch 1382 batch loss 7.06930923\n",
      "Validated batch 1383 batch loss 6.35852337\n",
      "Validated batch 1384 batch loss 7.40939522\n",
      "Validated batch 1385 batch loss 7.00193787\n",
      "Validated batch 1386 batch loss 5.91140318\n",
      "Validated batch 1387 batch loss 5.90941715\n",
      "Validated batch 1388 batch loss 6.25817394\n",
      "Validated batch 1389 batch loss 5.25745153\n",
      "Validated batch 1390 batch loss 6.62163115\n",
      "Validated batch 1391 batch loss 4.91350603\n",
      "Validated batch 1392 batch loss 5.12223816\n",
      "Validated batch 1393 batch loss 6.22331285\n",
      "Validated batch 1394 batch loss 6.94401741\n",
      "Validated batch 1395 batch loss 6.49953747\n",
      "Validated batch 1396 batch loss 6.46981335\n",
      "Validated batch 1397 batch loss 6.56418514\n",
      "Validated batch 1398 batch loss 6.43306065\n",
      "Validated batch 1399 batch loss 6.44171429\n",
      "Validated batch 1400 batch loss 7.01167202\n",
      "Validated batch 1401 batch loss 6.50780487\n",
      "Validated batch 1402 batch loss 5.29856443\n",
      "Validated batch 1403 batch loss 6.21059513\n",
      "Validated batch 1404 batch loss 7.05848503\n",
      "Validated batch 1405 batch loss 6.49582291\n",
      "Validated batch 1406 batch loss 5.87290287\n",
      "Validated batch 1407 batch loss 7.21033907\n",
      "Validated batch 1408 batch loss 6.31168\n",
      "Validated batch 1409 batch loss 5.73761129\n",
      "Validated batch 1410 batch loss 6.85634327\n",
      "Validated batch 1411 batch loss 6.08847141\n",
      "Validated batch 1412 batch loss 4.94263935\n",
      "Validated batch 1413 batch loss 6.34308243\n",
      "Validated batch 1414 batch loss 7.20303\n",
      "Validated batch 1415 batch loss 6.51649714\n",
      "Validated batch 1416 batch loss 5.90632439\n",
      "Validated batch 1417 batch loss 5.95419312\n",
      "Validated batch 1418 batch loss 6.86713886\n",
      "Validated batch 1419 batch loss 6.36516762\n",
      "Validated batch 1420 batch loss 6.66838551\n",
      "Validated batch 1421 batch loss 6.01013708\n",
      "Validated batch 1422 batch loss 5.95893\n",
      "Validated batch 1423 batch loss 5.98178911\n",
      "Validated batch 1424 batch loss 6.2783556\n",
      "Validated batch 1425 batch loss 6.49981785\n",
      "Validated batch 1426 batch loss 6.43062162\n",
      "Validated batch 1427 batch loss 7.32965755\n",
      "Validated batch 1428 batch loss 7.75289536\n",
      "Validated batch 1429 batch loss 6.79605\n",
      "Validated batch 1430 batch loss 5.91113853\n",
      "Validated batch 1431 batch loss 5.30176067\n",
      "Validated batch 1432 batch loss 5.36557627\n",
      "Validated batch 1433 batch loss 6.08615\n",
      "Validated batch 1434 batch loss 4.96428585\n",
      "Validated batch 1435 batch loss 5.62906599\n",
      "Validated batch 1436 batch loss 6.36311579\n",
      "Validated batch 1437 batch loss 5.58575583\n",
      "Validated batch 1438 batch loss 5.54401398\n",
      "Validated batch 1439 batch loss 6.77258778\n",
      "Validated batch 1440 batch loss 6.7919817\n",
      "Validated batch 1441 batch loss 5.09644556\n",
      "Validated batch 1442 batch loss 5.64708233\n",
      "Validated batch 1443 batch loss 6.20724297\n",
      "Validated batch 1444 batch loss 6.92986631\n",
      "Validated batch 1445 batch loss 6.76917362\n",
      "Validated batch 1446 batch loss 6.47250462\n",
      "Validated batch 1447 batch loss 6.28018\n",
      "Validated batch 1448 batch loss 5.69236469\n",
      "Validated batch 1449 batch loss 6.08796501\n",
      "Validated batch 1450 batch loss 6.14419556\n",
      "Validated batch 1451 batch loss 6.21774101\n",
      "Validated batch 1452 batch loss 6.59447289\n",
      "Validated batch 1453 batch loss 5.97638512\n",
      "Validated batch 1454 batch loss 6.15622854\n",
      "Validated batch 1455 batch loss 6.02955246\n",
      "Validated batch 1456 batch loss 5.53223372\n",
      "Validated batch 1457 batch loss 5.81774712\n",
      "Validated batch 1458 batch loss 6.267663\n",
      "Validated batch 1459 batch loss 7.13583755\n",
      "Validated batch 1460 batch loss 5.67589\n",
      "Validated batch 1461 batch loss 6.69923\n",
      "Validated batch 1462 batch loss 7.00631523\n",
      "Validated batch 1463 batch loss 7.85214901\n",
      "Validated batch 1464 batch loss 6.69370747\n",
      "Validated batch 1465 batch loss 6.77995586\n",
      "Validated batch 1466 batch loss 7.18222523\n",
      "Validated batch 1467 batch loss 6.24591255\n",
      "Validated batch 1468 batch loss 6.98019648\n",
      "Validated batch 1469 batch loss 7.19103193\n",
      "Validated batch 1470 batch loss 6.53710413\n",
      "Validated batch 1471 batch loss 6.13004923\n",
      "Validated batch 1472 batch loss 5.55777836\n",
      "Validated batch 1473 batch loss 5.95554686\n",
      "Validated batch 1474 batch loss 5.56102371\n",
      "Validated batch 1475 batch loss 5.99009752\n",
      "Validated batch 1476 batch loss 5.77074957\n",
      "Validated batch 1477 batch loss 6.89788389\n",
      "Validated batch 1478 batch loss 6.06771088\n",
      "Validated batch 1479 batch loss 6.52840185\n",
      "Epoch 1 val loss 6.23746919631958\n",
      "Model ./models/model-v0.0.1-epoch-1-loss-6.2375.h5 saved.\n",
      "Start epoch 2 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 5.87599468 epoch total loss 5.87599468\n",
      "Trained batch 2 batch loss 6.40770435 epoch total loss 6.14184952\n",
      "Trained batch 3 batch loss 6.92306042 epoch total loss 6.40225363\n",
      "Trained batch 4 batch loss 6.12253094 epoch total loss 6.33232307\n",
      "Trained batch 5 batch loss 6.74879551 epoch total loss 6.41561747\n",
      "Trained batch 6 batch loss 6.16996193 epoch total loss 6.37467432\n",
      "Trained batch 7 batch loss 6.47350645 epoch total loss 6.38879347\n",
      "Trained batch 8 batch loss 6.59991074 epoch total loss 6.41518307\n",
      "Trained batch 9 batch loss 5.78257227 epoch total loss 6.34489298\n",
      "Trained batch 10 batch loss 6.09232569 epoch total loss 6.31963634\n",
      "Trained batch 11 batch loss 5.9602747 epoch total loss 6.28696728\n",
      "Trained batch 12 batch loss 5.89853764 epoch total loss 6.25459814\n",
      "Trained batch 13 batch loss 6.99050188 epoch total loss 6.31120586\n",
      "Trained batch 14 batch loss 5.06088829 epoch total loss 6.2218976\n",
      "Trained batch 15 batch loss 4.71897507 epoch total loss 6.12170315\n",
      "Trained batch 16 batch loss 5.13351727 epoch total loss 6.05994129\n",
      "Trained batch 17 batch loss 5.67308474 epoch total loss 6.03718519\n",
      "Trained batch 18 batch loss 6.3508029 epoch total loss 6.05460835\n",
      "Trained batch 19 batch loss 6.31582737 epoch total loss 6.06835651\n",
      "Trained batch 20 batch loss 6.07496786 epoch total loss 6.06868696\n",
      "Trained batch 21 batch loss 6.47725582 epoch total loss 6.08814287\n",
      "Trained batch 22 batch loss 6.08050156 epoch total loss 6.08779573\n",
      "Trained batch 23 batch loss 6.13151 epoch total loss 6.08969641\n",
      "Trained batch 24 batch loss 5.73117876 epoch total loss 6.07475853\n",
      "Trained batch 25 batch loss 6.13504076 epoch total loss 6.07717\n",
      "Trained batch 26 batch loss 5.62770939 epoch total loss 6.05988312\n",
      "Trained batch 27 batch loss 6.65780926 epoch total loss 6.08202839\n",
      "Trained batch 28 batch loss 6.11422777 epoch total loss 6.08317852\n",
      "Trained batch 29 batch loss 6.52740812 epoch total loss 6.09849644\n",
      "Trained batch 30 batch loss 6.04148388 epoch total loss 6.09659624\n",
      "Trained batch 31 batch loss 6.41495514 epoch total loss 6.10686588\n",
      "Trained batch 32 batch loss 5.85712767 epoch total loss 6.09906149\n",
      "Trained batch 33 batch loss 6.49126625 epoch total loss 6.11094666\n",
      "Trained batch 34 batch loss 6.17989254 epoch total loss 6.11297417\n",
      "Trained batch 35 batch loss 6.57029438 epoch total loss 6.12604046\n",
      "Trained batch 36 batch loss 6.52155161 epoch total loss 6.13702679\n",
      "Trained batch 37 batch loss 5.79884291 epoch total loss 6.12788677\n",
      "Trained batch 38 batch loss 6.53582382 epoch total loss 6.13862228\n",
      "Trained batch 39 batch loss 4.84170771 epoch total loss 6.10536766\n",
      "Trained batch 40 batch loss 5.96942616 epoch total loss 6.10196924\n",
      "Trained batch 41 batch loss 6.94027615 epoch total loss 6.12241554\n",
      "Trained batch 42 batch loss 6.48290634 epoch total loss 6.13099909\n",
      "Trained batch 43 batch loss 5.31332493 epoch total loss 6.1119833\n",
      "Trained batch 44 batch loss 6.60280514 epoch total loss 6.12313843\n",
      "Trained batch 45 batch loss 5.94165707 epoch total loss 6.11910534\n",
      "Trained batch 46 batch loss 4.73572159 epoch total loss 6.0890317\n",
      "Trained batch 47 batch loss 6.12050962 epoch total loss 6.08970165\n",
      "Trained batch 48 batch loss 6.41480637 epoch total loss 6.09647417\n",
      "Trained batch 49 batch loss 6.4828 epoch total loss 6.1043582\n",
      "Trained batch 50 batch loss 5.46767902 epoch total loss 6.09162474\n",
      "Trained batch 51 batch loss 6.51992607 epoch total loss 6.10002279\n",
      "Trained batch 52 batch loss 6.02321243 epoch total loss 6.09854603\n",
      "Trained batch 53 batch loss 5.69144249 epoch total loss 6.09086466\n",
      "Trained batch 54 batch loss 5.70404625 epoch total loss 6.08370113\n",
      "Trained batch 55 batch loss 5.97488117 epoch total loss 6.08172274\n",
      "Trained batch 56 batch loss 5.79239 epoch total loss 6.07655621\n",
      "Trained batch 57 batch loss 5.46429 epoch total loss 6.0658145\n",
      "Trained batch 58 batch loss 5.84117603 epoch total loss 6.06194162\n",
      "Trained batch 59 batch loss 6.16730118 epoch total loss 6.06372738\n",
      "Trained batch 60 batch loss 6.21069765 epoch total loss 6.06617689\n",
      "Trained batch 61 batch loss 6.145154 epoch total loss 6.0674715\n",
      "Trained batch 62 batch loss 6.14863443 epoch total loss 6.06878042\n",
      "Trained batch 63 batch loss 6.18973589 epoch total loss 6.0707\n",
      "Trained batch 64 batch loss 6.00237703 epoch total loss 6.06963253\n",
      "Trained batch 65 batch loss 6.30025387 epoch total loss 6.07318068\n",
      "Trained batch 66 batch loss 5.75609159 epoch total loss 6.06837654\n",
      "Trained batch 67 batch loss 6.17725 epoch total loss 6.0700016\n",
      "Trained batch 68 batch loss 5.34649849 epoch total loss 6.05936146\n",
      "Trained batch 69 batch loss 6.00449371 epoch total loss 6.05856609\n",
      "Trained batch 70 batch loss 5.71806717 epoch total loss 6.05370235\n",
      "Trained batch 71 batch loss 6.21804667 epoch total loss 6.05601692\n",
      "Trained batch 72 batch loss 5.46507835 epoch total loss 6.0478096\n",
      "Trained batch 73 batch loss 6.09904 epoch total loss 6.04851103\n",
      "Trained batch 74 batch loss 5.8555913 epoch total loss 6.04590416\n",
      "Trained batch 75 batch loss 6.05289078 epoch total loss 6.04599714\n",
      "Trained batch 76 batch loss 6.17309189 epoch total loss 6.04767\n",
      "Trained batch 77 batch loss 6.0821023 epoch total loss 6.04811668\n",
      "Trained batch 78 batch loss 5.93584442 epoch total loss 6.04667759\n",
      "Trained batch 79 batch loss 6.17874527 epoch total loss 6.0483489\n",
      "Trained batch 80 batch loss 5.88287 epoch total loss 6.04628086\n",
      "Trained batch 81 batch loss 6.07579422 epoch total loss 6.04664516\n",
      "Trained batch 82 batch loss 6.17466736 epoch total loss 6.04820681\n",
      "Trained batch 83 batch loss 6.78179741 epoch total loss 6.05704498\n",
      "Trained batch 84 batch loss 6.67159176 epoch total loss 6.0643611\n",
      "Trained batch 85 batch loss 6.52496481 epoch total loss 6.06978035\n",
      "Trained batch 86 batch loss 6.44516373 epoch total loss 6.07414579\n",
      "Trained batch 87 batch loss 6.28702927 epoch total loss 6.07659292\n",
      "Trained batch 88 batch loss 6.89863873 epoch total loss 6.08593416\n",
      "Trained batch 89 batch loss 6.75599 epoch total loss 6.09346247\n",
      "Trained batch 90 batch loss 6.57554531 epoch total loss 6.09881926\n",
      "Trained batch 91 batch loss 6.24010706 epoch total loss 6.10037184\n",
      "Trained batch 92 batch loss 6.84133577 epoch total loss 6.10842562\n",
      "Trained batch 93 batch loss 6.2133832 epoch total loss 6.10955429\n",
      "Trained batch 94 batch loss 5.96397209 epoch total loss 6.10800552\n",
      "Trained batch 95 batch loss 6.17248821 epoch total loss 6.10868454\n",
      "Trained batch 96 batch loss 5.91659355 epoch total loss 6.10668325\n",
      "Trained batch 97 batch loss 7.07109499 epoch total loss 6.11662579\n",
      "Trained batch 98 batch loss 7.36815739 epoch total loss 6.12939644\n",
      "Trained batch 99 batch loss 6.79836655 epoch total loss 6.13615322\n",
      "Trained batch 100 batch loss 6.71520233 epoch total loss 6.14194393\n",
      "Trained batch 101 batch loss 6.96148157 epoch total loss 6.15005827\n",
      "Trained batch 102 batch loss 6.02423382 epoch total loss 6.14882469\n",
      "Trained batch 103 batch loss 6.41966343 epoch total loss 6.15145445\n",
      "Trained batch 104 batch loss 6.28782654 epoch total loss 6.15276575\n",
      "Trained batch 105 batch loss 6.38381481 epoch total loss 6.15496588\n",
      "Trained batch 106 batch loss 6.69137716 epoch total loss 6.16002655\n",
      "Trained batch 107 batch loss 6.36468029 epoch total loss 6.16193962\n",
      "Trained batch 108 batch loss 6.2183485 epoch total loss 6.16246128\n",
      "Trained batch 109 batch loss 6.09808207 epoch total loss 6.16187096\n",
      "Trained batch 110 batch loss 6.00918198 epoch total loss 6.16048241\n",
      "Trained batch 111 batch loss 6.36957169 epoch total loss 6.16236639\n",
      "Trained batch 112 batch loss 6.2427268 epoch total loss 6.16308355\n",
      "Trained batch 113 batch loss 6.5187788 epoch total loss 6.16623163\n",
      "Trained batch 114 batch loss 6.41057539 epoch total loss 6.16837502\n",
      "Trained batch 115 batch loss 6.15567398 epoch total loss 6.16826487\n",
      "Trained batch 116 batch loss 6.5830822 epoch total loss 6.17184067\n",
      "Trained batch 117 batch loss 5.64745808 epoch total loss 6.16735888\n",
      "Trained batch 118 batch loss 6.61907578 epoch total loss 6.17118692\n",
      "Trained batch 119 batch loss 6.47282124 epoch total loss 6.17372179\n",
      "Trained batch 120 batch loss 6.94273376 epoch total loss 6.18013048\n",
      "Trained batch 121 batch loss 6.09209871 epoch total loss 6.17940283\n",
      "Trained batch 122 batch loss 6.84587812 epoch total loss 6.18486595\n",
      "Trained batch 123 batch loss 6.2581954 epoch total loss 6.185462\n",
      "Trained batch 124 batch loss 6.38592386 epoch total loss 6.18707848\n",
      "Trained batch 125 batch loss 5.84079599 epoch total loss 6.18430853\n",
      "Trained batch 126 batch loss 6.14205503 epoch total loss 6.18397284\n",
      "Trained batch 127 batch loss 6.72647285 epoch total loss 6.18824482\n",
      "Trained batch 128 batch loss 6.55090523 epoch total loss 6.19107819\n",
      "Trained batch 129 batch loss 6.48197937 epoch total loss 6.19333315\n",
      "Trained batch 130 batch loss 6.34381533 epoch total loss 6.19449091\n",
      "Trained batch 131 batch loss 6.08693933 epoch total loss 6.19367\n",
      "Trained batch 132 batch loss 5.91146088 epoch total loss 6.19153166\n",
      "Trained batch 133 batch loss 6.19183826 epoch total loss 6.19153404\n",
      "Trained batch 134 batch loss 5.99498 epoch total loss 6.19006729\n",
      "Trained batch 135 batch loss 5.9252553 epoch total loss 6.18810558\n",
      "Trained batch 136 batch loss 6.28234291 epoch total loss 6.18879843\n",
      "Trained batch 137 batch loss 6.12257 epoch total loss 6.18831491\n",
      "Trained batch 138 batch loss 6.60768461 epoch total loss 6.1913538\n",
      "Trained batch 139 batch loss 5.46729422 epoch total loss 6.18614435\n",
      "Trained batch 140 batch loss 5.64678574 epoch total loss 6.18229198\n",
      "Trained batch 141 batch loss 6.19228268 epoch total loss 6.18236256\n",
      "Trained batch 142 batch loss 6.28195047 epoch total loss 6.18306398\n",
      "Trained batch 143 batch loss 6.23228741 epoch total loss 6.18340826\n",
      "Trained batch 144 batch loss 6.60799074 epoch total loss 6.18635654\n",
      "Trained batch 145 batch loss 7.04529762 epoch total loss 6.19228029\n",
      "Trained batch 146 batch loss 5.25022936 epoch total loss 6.18582773\n",
      "Trained batch 147 batch loss 6.33158827 epoch total loss 6.18681955\n",
      "Trained batch 148 batch loss 5.54929543 epoch total loss 6.18251181\n",
      "Trained batch 149 batch loss 5.60291767 epoch total loss 6.17862225\n",
      "Trained batch 150 batch loss 6.47728109 epoch total loss 6.18061304\n",
      "Trained batch 151 batch loss 6.05963326 epoch total loss 6.17981195\n",
      "Trained batch 152 batch loss 6.323349 epoch total loss 6.18075657\n",
      "Trained batch 153 batch loss 6.30002499 epoch total loss 6.1815362\n",
      "Trained batch 154 batch loss 6.44881248 epoch total loss 6.18327141\n",
      "Trained batch 155 batch loss 4.83572483 epoch total loss 6.17457771\n",
      "Trained batch 156 batch loss 6.00337124 epoch total loss 6.17348051\n",
      "Trained batch 157 batch loss 6.08343601 epoch total loss 6.17290688\n",
      "Trained batch 158 batch loss 5.80601311 epoch total loss 6.17058468\n",
      "Trained batch 159 batch loss 6.05034161 epoch total loss 6.16982841\n",
      "Trained batch 160 batch loss 6.1248064 epoch total loss 6.16954708\n",
      "Trained batch 161 batch loss 6.68899345 epoch total loss 6.17277336\n",
      "Trained batch 162 batch loss 6.15999317 epoch total loss 6.17269468\n",
      "Trained batch 163 batch loss 5.6281929 epoch total loss 6.16935396\n",
      "Trained batch 164 batch loss 5.71309662 epoch total loss 6.16657162\n",
      "Trained batch 165 batch loss 5.89699268 epoch total loss 6.1649375\n",
      "Trained batch 166 batch loss 5.92341423 epoch total loss 6.16348267\n",
      "Trained batch 167 batch loss 6.66469955 epoch total loss 6.16648436\n",
      "Trained batch 168 batch loss 5.30494308 epoch total loss 6.16135597\n",
      "Trained batch 169 batch loss 5.4683609 epoch total loss 6.15725565\n",
      "Trained batch 170 batch loss 5.79026604 epoch total loss 6.15509701\n",
      "Trained batch 171 batch loss 5.49712658 epoch total loss 6.15124846\n",
      "Trained batch 172 batch loss 5.96322727 epoch total loss 6.15015554\n",
      "Trained batch 173 batch loss 6.21592236 epoch total loss 6.15053606\n",
      "Trained batch 174 batch loss 7.03375196 epoch total loss 6.15561152\n",
      "Trained batch 175 batch loss 6.40306902 epoch total loss 6.15702581\n",
      "Trained batch 176 batch loss 6.84891605 epoch total loss 6.16095686\n",
      "Trained batch 177 batch loss 5.89969587 epoch total loss 6.15948057\n",
      "Trained batch 178 batch loss 6.43133259 epoch total loss 6.1610074\n",
      "Trained batch 179 batch loss 6.81687975 epoch total loss 6.16467142\n",
      "Trained batch 180 batch loss 6.24112701 epoch total loss 6.16509581\n",
      "Trained batch 181 batch loss 6.34903145 epoch total loss 6.16611195\n",
      "Trained batch 182 batch loss 6.02884293 epoch total loss 6.16535759\n",
      "Trained batch 183 batch loss 5.82494831 epoch total loss 6.16349745\n",
      "Trained batch 184 batch loss 6.41003799 epoch total loss 6.16483736\n",
      "Trained batch 185 batch loss 5.99855 epoch total loss 6.16393852\n",
      "Trained batch 186 batch loss 6.83848953 epoch total loss 6.16756535\n",
      "Trained batch 187 batch loss 6.5398469 epoch total loss 6.16955566\n",
      "Trained batch 188 batch loss 5.84381962 epoch total loss 6.16782331\n",
      "Trained batch 189 batch loss 6.02965927 epoch total loss 6.16709232\n",
      "Trained batch 190 batch loss 6.18885851 epoch total loss 6.16720676\n",
      "Trained batch 191 batch loss 6.95942688 epoch total loss 6.17135477\n",
      "Trained batch 192 batch loss 6.05173397 epoch total loss 6.17073202\n",
      "Trained batch 193 batch loss 6.06132269 epoch total loss 6.17016459\n",
      "Trained batch 194 batch loss 5.36656666 epoch total loss 6.16602278\n",
      "Trained batch 195 batch loss 5.93822289 epoch total loss 6.16485453\n",
      "Trained batch 196 batch loss 6.56640434 epoch total loss 6.16690302\n",
      "Trained batch 197 batch loss 6.70642948 epoch total loss 6.16964197\n",
      "Trained batch 198 batch loss 6.26120806 epoch total loss 6.1701045\n",
      "Trained batch 199 batch loss 6.24978256 epoch total loss 6.17050457\n",
      "Trained batch 200 batch loss 6.99684238 epoch total loss 6.17463636\n",
      "Trained batch 201 batch loss 6.61449862 epoch total loss 6.17682457\n",
      "Trained batch 202 batch loss 6.97732258 epoch total loss 6.18078756\n",
      "Trained batch 203 batch loss 6.0743289 epoch total loss 6.18026304\n",
      "Trained batch 204 batch loss 5.77374458 epoch total loss 6.17827034\n",
      "Trained batch 205 batch loss 6.17559814 epoch total loss 6.17825699\n",
      "Trained batch 206 batch loss 6.22913885 epoch total loss 6.17850399\n",
      "Trained batch 207 batch loss 5.98096752 epoch total loss 6.17755\n",
      "Trained batch 208 batch loss 5.35079 epoch total loss 6.1735754\n",
      "Trained batch 209 batch loss 5.23109818 epoch total loss 6.16906548\n",
      "Trained batch 210 batch loss 5.73215294 epoch total loss 6.16698503\n",
      "Trained batch 211 batch loss 6.26391506 epoch total loss 6.16744471\n",
      "Trained batch 212 batch loss 6.59499741 epoch total loss 6.16946125\n",
      "Trained batch 213 batch loss 6.67653179 epoch total loss 6.17184162\n",
      "Trained batch 214 batch loss 6.7234 epoch total loss 6.17441893\n",
      "Trained batch 215 batch loss 6.5019021 epoch total loss 6.17594242\n",
      "Trained batch 216 batch loss 4.43570709 epoch total loss 6.16788578\n",
      "Trained batch 217 batch loss 6.04525661 epoch total loss 6.16732073\n",
      "Trained batch 218 batch loss 5.31002903 epoch total loss 6.16338825\n",
      "Trained batch 219 batch loss 5.83811712 epoch total loss 6.1619029\n",
      "Trained batch 220 batch loss 5.85579491 epoch total loss 6.16051197\n",
      "Trained batch 221 batch loss 5.66943455 epoch total loss 6.15829\n",
      "Trained batch 222 batch loss 6.14197254 epoch total loss 6.15821648\n",
      "Trained batch 223 batch loss 6.29372215 epoch total loss 6.15882397\n",
      "Trained batch 224 batch loss 6.07293 epoch total loss 6.15844\n",
      "Trained batch 225 batch loss 5.8606534 epoch total loss 6.15711641\n",
      "Trained batch 226 batch loss 5.83103657 epoch total loss 6.1556735\n",
      "Trained batch 227 batch loss 5.82148457 epoch total loss 6.15420151\n",
      "Trained batch 228 batch loss 5.85636 epoch total loss 6.15289497\n",
      "Trained batch 229 batch loss 5.75881433 epoch total loss 6.15117407\n",
      "Trained batch 230 batch loss 5.80623388 epoch total loss 6.14967489\n",
      "Trained batch 231 batch loss 6.13347578 epoch total loss 6.14960432\n",
      "Trained batch 232 batch loss 6.1057992 epoch total loss 6.14941549\n",
      "Trained batch 233 batch loss 5.88060474 epoch total loss 6.14826202\n",
      "Trained batch 234 batch loss 6.32077456 epoch total loss 6.14899921\n",
      "Trained batch 235 batch loss 6.50895405 epoch total loss 6.15053082\n",
      "Trained batch 236 batch loss 6.75285721 epoch total loss 6.15308285\n",
      "Trained batch 237 batch loss 6.57246733 epoch total loss 6.15485287\n",
      "Trained batch 238 batch loss 6.38040638 epoch total loss 6.15580034\n",
      "Trained batch 239 batch loss 6.77391529 epoch total loss 6.15838671\n",
      "Trained batch 240 batch loss 5.87817526 epoch total loss 6.15721893\n",
      "Trained batch 241 batch loss 5.34891415 epoch total loss 6.15386486\n",
      "Trained batch 242 batch loss 6.13426 epoch total loss 6.1537838\n",
      "Trained batch 243 batch loss 5.75474167 epoch total loss 6.15214157\n",
      "Trained batch 244 batch loss 6.28239632 epoch total loss 6.15267563\n",
      "Trained batch 245 batch loss 5.45644283 epoch total loss 6.14983368\n",
      "Trained batch 246 batch loss 6.30020237 epoch total loss 6.15044451\n",
      "Trained batch 247 batch loss 6.34396315 epoch total loss 6.15122843\n",
      "Trained batch 248 batch loss 6.08177328 epoch total loss 6.15094852\n",
      "Trained batch 249 batch loss 5.86747169 epoch total loss 6.14981\n",
      "Trained batch 250 batch loss 5.88347054 epoch total loss 6.14874411\n",
      "Trained batch 251 batch loss 6.91561127 epoch total loss 6.15179968\n",
      "Trained batch 252 batch loss 5.82061958 epoch total loss 6.15048552\n",
      "Trained batch 253 batch loss 6.04458618 epoch total loss 6.15006685\n",
      "Trained batch 254 batch loss 6.26226568 epoch total loss 6.1505084\n",
      "Trained batch 255 batch loss 5.8201046 epoch total loss 6.14921236\n",
      "Trained batch 256 batch loss 6.1077919 epoch total loss 6.14905071\n",
      "Trained batch 257 batch loss 5.70953751 epoch total loss 6.14734077\n",
      "Trained batch 258 batch loss 5.87234783 epoch total loss 6.14627457\n",
      "Trained batch 259 batch loss 5.98562288 epoch total loss 6.1456542\n",
      "Trained batch 260 batch loss 6.08110094 epoch total loss 6.14540577\n",
      "Trained batch 261 batch loss 6.05067778 epoch total loss 6.1450429\n",
      "Trained batch 262 batch loss 6.34346247 epoch total loss 6.14580059\n",
      "Trained batch 263 batch loss 5.82759047 epoch total loss 6.14459085\n",
      "Trained batch 264 batch loss 5.59302807 epoch total loss 6.14250135\n",
      "Trained batch 265 batch loss 5.89848137 epoch total loss 6.14158058\n",
      "Trained batch 266 batch loss 5.89061594 epoch total loss 6.14063692\n",
      "Trained batch 267 batch loss 5.92861843 epoch total loss 6.13984251\n",
      "Trained batch 268 batch loss 5.84296751 epoch total loss 6.13873529\n",
      "Trained batch 269 batch loss 5.48478508 epoch total loss 6.1363039\n",
      "Trained batch 270 batch loss 6.32317352 epoch total loss 6.13699579\n",
      "Trained batch 271 batch loss 6.90027428 epoch total loss 6.13981247\n",
      "Trained batch 272 batch loss 5.10513973 epoch total loss 6.13600826\n",
      "Trained batch 273 batch loss 7.40739584 epoch total loss 6.14066505\n",
      "Trained batch 274 batch loss 5.92334127 epoch total loss 6.13987207\n",
      "Trained batch 275 batch loss 5.86940384 epoch total loss 6.13888836\n",
      "Trained batch 276 batch loss 5.4764061 epoch total loss 6.13648844\n",
      "Trained batch 277 batch loss 5.90976858 epoch total loss 6.13566971\n",
      "Trained batch 278 batch loss 5.70531511 epoch total loss 6.13412189\n",
      "Trained batch 279 batch loss 5.8740406 epoch total loss 6.13318968\n",
      "Trained batch 280 batch loss 6.34103346 epoch total loss 6.13393211\n",
      "Trained batch 281 batch loss 6.67021799 epoch total loss 6.13584042\n",
      "Trained batch 282 batch loss 5.79251146 epoch total loss 6.13462257\n",
      "Trained batch 283 batch loss 7.18777752 epoch total loss 6.13834429\n",
      "Trained batch 284 batch loss 5.89002275 epoch total loss 6.13747\n",
      "Trained batch 285 batch loss 6.27790451 epoch total loss 6.13796282\n",
      "Trained batch 286 batch loss 6.7688055 epoch total loss 6.14016819\n",
      "Trained batch 287 batch loss 5.79697895 epoch total loss 6.13897276\n",
      "Trained batch 288 batch loss 6.20689 epoch total loss 6.13920832\n",
      "Trained batch 289 batch loss 5.85553455 epoch total loss 6.13822699\n",
      "Trained batch 290 batch loss 5.92984867 epoch total loss 6.13750839\n",
      "Trained batch 291 batch loss 5.48337126 epoch total loss 6.13526058\n",
      "Trained batch 292 batch loss 5.8987813 epoch total loss 6.13445091\n",
      "Trained batch 293 batch loss 6.53221655 epoch total loss 6.13580847\n",
      "Trained batch 294 batch loss 6.0901823 epoch total loss 6.1356535\n",
      "Trained batch 295 batch loss 5.64299679 epoch total loss 6.13398314\n",
      "Trained batch 296 batch loss 5.90513563 epoch total loss 6.13321\n",
      "Trained batch 297 batch loss 6.28261709 epoch total loss 6.13371325\n",
      "Trained batch 298 batch loss 5.21768 epoch total loss 6.13063908\n",
      "Trained batch 299 batch loss 6.11824894 epoch total loss 6.13059759\n",
      "Trained batch 300 batch loss 5.65328074 epoch total loss 6.12900686\n",
      "Trained batch 301 batch loss 6.46012497 epoch total loss 6.13010645\n",
      "Trained batch 302 batch loss 6.37825155 epoch total loss 6.13092852\n",
      "Trained batch 303 batch loss 6.13904095 epoch total loss 6.13095522\n",
      "Trained batch 304 batch loss 6.10179901 epoch total loss 6.13085938\n",
      "Trained batch 305 batch loss 6.64139748 epoch total loss 6.13253307\n",
      "Trained batch 306 batch loss 4.29778576 epoch total loss 6.12653685\n",
      "Trained batch 307 batch loss 5.79062939 epoch total loss 6.12544298\n",
      "Trained batch 308 batch loss 6.07541084 epoch total loss 6.12528038\n",
      "Trained batch 309 batch loss 6.88566494 epoch total loss 6.12774134\n",
      "Trained batch 310 batch loss 5.82201958 epoch total loss 6.12675524\n",
      "Trained batch 311 batch loss 6.28588533 epoch total loss 6.12726688\n",
      "Trained batch 312 batch loss 6.58986855 epoch total loss 6.12874937\n",
      "Trained batch 313 batch loss 5.85872746 epoch total loss 6.12788677\n",
      "Trained batch 314 batch loss 5.1309824 epoch total loss 6.12471199\n",
      "Trained batch 315 batch loss 6.01239061 epoch total loss 6.12435532\n",
      "Trained batch 316 batch loss 6.41032219 epoch total loss 6.12526035\n",
      "Trained batch 317 batch loss 6.67854691 epoch total loss 6.12700605\n",
      "Trained batch 318 batch loss 6.63394833 epoch total loss 6.1286\n",
      "Trained batch 319 batch loss 4.97139454 epoch total loss 6.12497234\n",
      "Trained batch 320 batch loss 6.10932446 epoch total loss 6.12492371\n",
      "Trained batch 321 batch loss 5.8643465 epoch total loss 6.12411213\n",
      "Trained batch 322 batch loss 5.6925087 epoch total loss 6.12277174\n",
      "Trained batch 323 batch loss 6.57425308 epoch total loss 6.12416935\n",
      "Trained batch 324 batch loss 5.3928256 epoch total loss 6.121912\n",
      "Trained batch 325 batch loss 6.47864437 epoch total loss 6.12300968\n",
      "Trained batch 326 batch loss 4.42686892 epoch total loss 6.11780691\n",
      "Trained batch 327 batch loss 4.81952 epoch total loss 6.11383677\n",
      "Trained batch 328 batch loss 6.58122826 epoch total loss 6.11526155\n",
      "Trained batch 329 batch loss 6.24207687 epoch total loss 6.11564684\n",
      "Trained batch 330 batch loss 6.05783844 epoch total loss 6.11547184\n",
      "Trained batch 331 batch loss 5.56925583 epoch total loss 6.11382151\n",
      "Trained batch 332 batch loss 6.11241865 epoch total loss 6.11381721\n",
      "Trained batch 333 batch loss 5.38444233 epoch total loss 6.1116271\n",
      "Trained batch 334 batch loss 5.5279417 epoch total loss 6.10987949\n",
      "Trained batch 335 batch loss 4.91397476 epoch total loss 6.10630941\n",
      "Trained batch 336 batch loss 5.63256121 epoch total loss 6.10489893\n",
      "Trained batch 337 batch loss 4.90551424 epoch total loss 6.10134029\n",
      "Trained batch 338 batch loss 5.84002829 epoch total loss 6.10056734\n",
      "Trained batch 339 batch loss 6.24829483 epoch total loss 6.10100269\n",
      "Trained batch 340 batch loss 4.16347218 epoch total loss 6.09530449\n",
      "Trained batch 341 batch loss 5.4834981 epoch total loss 6.09351\n",
      "Trained batch 342 batch loss 4.8237114 epoch total loss 6.0897975\n",
      "Trained batch 343 batch loss 4.79837036 epoch total loss 6.08603239\n",
      "Trained batch 344 batch loss 4.8952508 epoch total loss 6.08257055\n",
      "Trained batch 345 batch loss 4.9410181 epoch total loss 6.0792613\n",
      "Trained batch 346 batch loss 5.18447971 epoch total loss 6.07667589\n",
      "Trained batch 347 batch loss 4.60454464 epoch total loss 6.07243299\n",
      "Trained batch 348 batch loss 6.74401474 epoch total loss 6.07436275\n",
      "Trained batch 349 batch loss 7.4446516 epoch total loss 6.07828856\n",
      "Trained batch 350 batch loss 7.42597294 epoch total loss 6.08213949\n",
      "Trained batch 351 batch loss 7.78457355 epoch total loss 6.08699\n",
      "Trained batch 352 batch loss 7.14146137 epoch total loss 6.08998537\n",
      "Trained batch 353 batch loss 7.56650162 epoch total loss 6.09416771\n",
      "Trained batch 354 batch loss 7.32723904 epoch total loss 6.09765053\n",
      "Trained batch 355 batch loss 7.02615452 epoch total loss 6.10026598\n",
      "Trained batch 356 batch loss 6.13560867 epoch total loss 6.10036516\n",
      "Trained batch 357 batch loss 6.03900719 epoch total loss 6.1001935\n",
      "Trained batch 358 batch loss 6.29853058 epoch total loss 6.10074759\n",
      "Trained batch 359 batch loss 6.79008389 epoch total loss 6.10266781\n",
      "Trained batch 360 batch loss 6.15998554 epoch total loss 6.1028266\n",
      "Trained batch 361 batch loss 6.10356092 epoch total loss 6.1028285\n",
      "Trained batch 362 batch loss 6.27938 epoch total loss 6.10331583\n",
      "Trained batch 363 batch loss 6.57283592 epoch total loss 6.10460901\n",
      "Trained batch 364 batch loss 6.4789257 epoch total loss 6.10563755\n",
      "Trained batch 365 batch loss 5.08896494 epoch total loss 6.10285187\n",
      "Trained batch 366 batch loss 5.57338238 epoch total loss 6.10140562\n",
      "Trained batch 367 batch loss 6.72922564 epoch total loss 6.10311651\n",
      "Trained batch 368 batch loss 5.9703393 epoch total loss 6.10275602\n",
      "Trained batch 369 batch loss 6.67034435 epoch total loss 6.1042943\n",
      "Trained batch 370 batch loss 5.69980335 epoch total loss 6.10320091\n",
      "Trained batch 371 batch loss 5.25937843 epoch total loss 6.10092592\n",
      "Trained batch 372 batch loss 4.7577076 epoch total loss 6.09731579\n",
      "Trained batch 373 batch loss 5.81075859 epoch total loss 6.0965476\n",
      "Trained batch 374 batch loss 5.52391148 epoch total loss 6.09501648\n",
      "Trained batch 375 batch loss 6.04581976 epoch total loss 6.09488535\n",
      "Trained batch 376 batch loss 6.27082157 epoch total loss 6.09535313\n",
      "Trained batch 377 batch loss 5.94603634 epoch total loss 6.09495687\n",
      "Trained batch 378 batch loss 6.74227524 epoch total loss 6.0966692\n",
      "Trained batch 379 batch loss 7.19922113 epoch total loss 6.09957838\n",
      "Trained batch 380 batch loss 7.25436 epoch total loss 6.10261726\n",
      "Trained batch 381 batch loss 6.30306911 epoch total loss 6.10314322\n",
      "Trained batch 382 batch loss 6.66565895 epoch total loss 6.10461617\n",
      "Trained batch 383 batch loss 5.29624891 epoch total loss 6.10250521\n",
      "Trained batch 384 batch loss 6.05471611 epoch total loss 6.10238075\n",
      "Trained batch 385 batch loss 6.42681265 epoch total loss 6.10322332\n",
      "Trained batch 386 batch loss 6.26294613 epoch total loss 6.10363722\n",
      "Trained batch 387 batch loss 6.44832706 epoch total loss 6.10452747\n",
      "Trained batch 388 batch loss 6.75759363 epoch total loss 6.10621071\n",
      "Trained batch 389 batch loss 6.19195366 epoch total loss 6.10643101\n",
      "Trained batch 390 batch loss 6.51267052 epoch total loss 6.10747242\n",
      "Trained batch 391 batch loss 6.29391766 epoch total loss 6.10794926\n",
      "Trained batch 392 batch loss 5.81334972 epoch total loss 6.10719776\n",
      "Trained batch 393 batch loss 4.73678493 epoch total loss 6.10371065\n",
      "Trained batch 394 batch loss 6.47345829 epoch total loss 6.10464907\n",
      "Trained batch 395 batch loss 6.26625347 epoch total loss 6.10505819\n",
      "Trained batch 396 batch loss 5.17465401 epoch total loss 6.10270882\n",
      "Trained batch 397 batch loss 6.04736423 epoch total loss 6.1025691\n",
      "Trained batch 398 batch loss 6.2370224 epoch total loss 6.10290718\n",
      "Trained batch 399 batch loss 5.90559292 epoch total loss 6.10241222\n",
      "Trained batch 400 batch loss 5.67858124 epoch total loss 6.10135269\n",
      "Trained batch 401 batch loss 5.82292223 epoch total loss 6.10065842\n",
      "Trained batch 402 batch loss 6.04307842 epoch total loss 6.10051489\n",
      "Trained batch 403 batch loss 5.97448301 epoch total loss 6.10020208\n",
      "Trained batch 404 batch loss 6.25160742 epoch total loss 6.10057688\n",
      "Trained batch 405 batch loss 5.85532904 epoch total loss 6.09997129\n",
      "Trained batch 406 batch loss 5.99901962 epoch total loss 6.09972239\n",
      "Trained batch 407 batch loss 5.08711958 epoch total loss 6.09723473\n",
      "Trained batch 408 batch loss 5.7264924 epoch total loss 6.09632587\n",
      "Trained batch 409 batch loss 5.92708874 epoch total loss 6.09591198\n",
      "Trained batch 410 batch loss 5.97411489 epoch total loss 6.09561491\n",
      "Trained batch 411 batch loss 5.66911459 epoch total loss 6.09457731\n",
      "Trained batch 412 batch loss 5.75011158 epoch total loss 6.09374094\n",
      "Trained batch 413 batch loss 6.12795067 epoch total loss 6.09382391\n",
      "Trained batch 414 batch loss 5.651474 epoch total loss 6.09275532\n",
      "Trained batch 415 batch loss 5.69810772 epoch total loss 6.09180403\n",
      "Trained batch 416 batch loss 6.25532532 epoch total loss 6.09219694\n",
      "Trained batch 417 batch loss 6.54633522 epoch total loss 6.09328651\n",
      "Trained batch 418 batch loss 7.48611641 epoch total loss 6.09661818\n",
      "Trained batch 419 batch loss 5.20814323 epoch total loss 6.09449816\n",
      "Trained batch 420 batch loss 4.96422958 epoch total loss 6.09180689\n",
      "Trained batch 421 batch loss 5.7014389 epoch total loss 6.09087944\n",
      "Trained batch 422 batch loss 6.03932238 epoch total loss 6.09075737\n",
      "Trained batch 423 batch loss 5.62655783 epoch total loss 6.08965969\n",
      "Trained batch 424 batch loss 6.1867609 epoch total loss 6.08988857\n",
      "Trained batch 425 batch loss 5.71334076 epoch total loss 6.08900261\n",
      "Trained batch 426 batch loss 4.76634121 epoch total loss 6.08589792\n",
      "Trained batch 427 batch loss 5.74218464 epoch total loss 6.08509302\n",
      "Trained batch 428 batch loss 6.72076797 epoch total loss 6.08657789\n",
      "Trained batch 429 batch loss 6.56975937 epoch total loss 6.08770466\n",
      "Trained batch 430 batch loss 6.58133507 epoch total loss 6.08885241\n",
      "Trained batch 431 batch loss 6.86463404 epoch total loss 6.09065247\n",
      "Trained batch 432 batch loss 6.2612896 epoch total loss 6.09104729\n",
      "Trained batch 433 batch loss 5.88997 epoch total loss 6.09058285\n",
      "Trained batch 434 batch loss 5.39581299 epoch total loss 6.08898211\n",
      "Trained batch 435 batch loss 6.03120565 epoch total loss 6.08884907\n",
      "Trained batch 436 batch loss 6.00908566 epoch total loss 6.08866596\n",
      "Trained batch 437 batch loss 6.11086702 epoch total loss 6.08871698\n",
      "Trained batch 438 batch loss 6.30359077 epoch total loss 6.08920765\n",
      "Trained batch 439 batch loss 6.16196346 epoch total loss 6.08937311\n",
      "Trained batch 440 batch loss 5.90934753 epoch total loss 6.08896446\n",
      "Trained batch 441 batch loss 6.22485161 epoch total loss 6.0892725\n",
      "Trained batch 442 batch loss 6.62237167 epoch total loss 6.09047842\n",
      "Trained batch 443 batch loss 6.18522 epoch total loss 6.09069252\n",
      "Trained batch 444 batch loss 6.54399157 epoch total loss 6.09171343\n",
      "Trained batch 445 batch loss 6.18928623 epoch total loss 6.0919323\n",
      "Trained batch 446 batch loss 6.73184919 epoch total loss 6.09336758\n",
      "Trained batch 447 batch loss 6.70028877 epoch total loss 6.09472513\n",
      "Trained batch 448 batch loss 6.35478592 epoch total loss 6.09530544\n",
      "Trained batch 449 batch loss 6.27053261 epoch total loss 6.0956955\n",
      "Trained batch 450 batch loss 6.39047623 epoch total loss 6.09635\n",
      "Trained batch 451 batch loss 6.1152792 epoch total loss 6.09639215\n",
      "Trained batch 452 batch loss 6.04186678 epoch total loss 6.09627151\n",
      "Trained batch 453 batch loss 6.36658049 epoch total loss 6.09686852\n",
      "Trained batch 454 batch loss 6.58306 epoch total loss 6.09793901\n",
      "Trained batch 455 batch loss 6.36798 epoch total loss 6.09853268\n",
      "Trained batch 456 batch loss 6.0837121 epoch total loss 6.09850025\n",
      "Trained batch 457 batch loss 6.55593729 epoch total loss 6.09950113\n",
      "Trained batch 458 batch loss 6.20759964 epoch total loss 6.09973669\n",
      "Trained batch 459 batch loss 5.98719549 epoch total loss 6.0994916\n",
      "Trained batch 460 batch loss 5.97585201 epoch total loss 6.09922314\n",
      "Trained batch 461 batch loss 5.83056211 epoch total loss 6.09864044\n",
      "Trained batch 462 batch loss 6.34558105 epoch total loss 6.09917498\n",
      "Trained batch 463 batch loss 6.13667488 epoch total loss 6.09925604\n",
      "Trained batch 464 batch loss 6.21310425 epoch total loss 6.09950161\n",
      "Trained batch 465 batch loss 6.44881821 epoch total loss 6.10025263\n",
      "Trained batch 466 batch loss 5.73816967 epoch total loss 6.09947586\n",
      "Trained batch 467 batch loss 6.89437962 epoch total loss 6.10117769\n",
      "Trained batch 468 batch loss 7.33181334 epoch total loss 6.10380745\n",
      "Trained batch 469 batch loss 7.63475084 epoch total loss 6.1070714\n",
      "Trained batch 470 batch loss 6.82851791 epoch total loss 6.10860682\n",
      "Trained batch 471 batch loss 6.5028615 epoch total loss 6.10944414\n",
      "Trained batch 472 batch loss 6.52634954 epoch total loss 6.11032724\n",
      "Trained batch 473 batch loss 5.81260967 epoch total loss 6.10969782\n",
      "Trained batch 474 batch loss 6.27813148 epoch total loss 6.11005306\n",
      "Trained batch 475 batch loss 5.47079754 epoch total loss 6.10870695\n",
      "Trained batch 476 batch loss 5.64978313 epoch total loss 6.10774279\n",
      "Trained batch 477 batch loss 5.71673298 epoch total loss 6.10692358\n",
      "Trained batch 478 batch loss 6.03676891 epoch total loss 6.10677671\n",
      "Trained batch 479 batch loss 5.59567165 epoch total loss 6.10571\n",
      "Trained batch 480 batch loss 5.94561481 epoch total loss 6.10537624\n",
      "Trained batch 481 batch loss 5.91225243 epoch total loss 6.10497475\n",
      "Trained batch 482 batch loss 5.76897717 epoch total loss 6.10427809\n",
      "Trained batch 483 batch loss 5.98692131 epoch total loss 6.1040349\n",
      "Trained batch 484 batch loss 5.84377384 epoch total loss 6.10349703\n",
      "Trained batch 485 batch loss 6.3826251 epoch total loss 6.10407257\n",
      "Trained batch 486 batch loss 5.87971687 epoch total loss 6.10361052\n",
      "Trained batch 487 batch loss 5.88444424 epoch total loss 6.10316086\n",
      "Trained batch 488 batch loss 6.61621571 epoch total loss 6.10421181\n",
      "Trained batch 489 batch loss 5.60208416 epoch total loss 6.10318518\n",
      "Trained batch 490 batch loss 6.67691517 epoch total loss 6.10435629\n",
      "Trained batch 491 batch loss 5.85485697 epoch total loss 6.10384798\n",
      "Trained batch 492 batch loss 5.23067474 epoch total loss 6.10207319\n",
      "Trained batch 493 batch loss 4.42776251 epoch total loss 6.09867668\n",
      "Trained batch 494 batch loss 4.82141876 epoch total loss 6.09609175\n",
      "Trained batch 495 batch loss 4.50666142 epoch total loss 6.09288025\n",
      "Trained batch 496 batch loss 6.34019327 epoch total loss 6.09337902\n",
      "Trained batch 497 batch loss 5.66055584 epoch total loss 6.09250832\n",
      "Trained batch 498 batch loss 6.33547449 epoch total loss 6.09299612\n",
      "Trained batch 499 batch loss 6.69353771 epoch total loss 6.09419966\n",
      "Trained batch 500 batch loss 5.42743683 epoch total loss 6.09286642\n",
      "Trained batch 501 batch loss 5.51143456 epoch total loss 6.0917058\n",
      "Trained batch 502 batch loss 4.8457942 epoch total loss 6.08922386\n",
      "Trained batch 503 batch loss 5.59292459 epoch total loss 6.08823729\n",
      "Trained batch 504 batch loss 5.96136379 epoch total loss 6.08798552\n",
      "Trained batch 505 batch loss 5.76530552 epoch total loss 6.08734655\n",
      "Trained batch 506 batch loss 6.40720367 epoch total loss 6.08797884\n",
      "Trained batch 507 batch loss 6.6565938 epoch total loss 6.08910036\n",
      "Trained batch 508 batch loss 6.68655968 epoch total loss 6.09027624\n",
      "Trained batch 509 batch loss 3.82666302 epoch total loss 6.08582926\n",
      "Trained batch 510 batch loss 5.32002163 epoch total loss 6.0843277\n",
      "Trained batch 511 batch loss 6.14572811 epoch total loss 6.08444786\n",
      "Trained batch 512 batch loss 6.1313324 epoch total loss 6.08453941\n",
      "Trained batch 513 batch loss 5.76339197 epoch total loss 6.08391333\n",
      "Trained batch 514 batch loss 6.09269428 epoch total loss 6.08393049\n",
      "Trained batch 515 batch loss 5.88567734 epoch total loss 6.08354568\n",
      "Trained batch 516 batch loss 5.8890686 epoch total loss 6.08316898\n",
      "Trained batch 517 batch loss 5.77729797 epoch total loss 6.08257771\n",
      "Trained batch 518 batch loss 5.4855051 epoch total loss 6.08142519\n",
      "Trained batch 519 batch loss 6.00208569 epoch total loss 6.0812726\n",
      "Trained batch 520 batch loss 5.79633713 epoch total loss 6.08072472\n",
      "Trained batch 521 batch loss 6.86469698 epoch total loss 6.08222961\n",
      "Trained batch 522 batch loss 6.37752247 epoch total loss 6.08279514\n",
      "Trained batch 523 batch loss 6.32124043 epoch total loss 6.083251\n",
      "Trained batch 524 batch loss 6.94436264 epoch total loss 6.08489418\n",
      "Trained batch 525 batch loss 6.28958225 epoch total loss 6.08528423\n",
      "Trained batch 526 batch loss 5.08597755 epoch total loss 6.08338404\n",
      "Trained batch 527 batch loss 5.55559731 epoch total loss 6.08238268\n",
      "Trained batch 528 batch loss 4.65503597 epoch total loss 6.07967949\n",
      "Trained batch 529 batch loss 6.35533762 epoch total loss 6.0802\n",
      "Trained batch 530 batch loss 6.52320242 epoch total loss 6.08103609\n",
      "Trained batch 531 batch loss 6.51691437 epoch total loss 6.0818572\n",
      "Trained batch 532 batch loss 5.92638063 epoch total loss 6.08156443\n",
      "Trained batch 533 batch loss 6.28136683 epoch total loss 6.08193922\n",
      "Trained batch 534 batch loss 5.99436426 epoch total loss 6.08177519\n",
      "Trained batch 535 batch loss 6.47217464 epoch total loss 6.08250475\n",
      "Trained batch 536 batch loss 5.74248314 epoch total loss 6.08187056\n",
      "Trained batch 537 batch loss 5.85101414 epoch total loss 6.08144045\n",
      "Trained batch 538 batch loss 5.54477596 epoch total loss 6.08044291\n",
      "Trained batch 539 batch loss 5.61046886 epoch total loss 6.07957077\n",
      "Trained batch 540 batch loss 6.19675446 epoch total loss 6.07978773\n",
      "Trained batch 541 batch loss 6.16058207 epoch total loss 6.07993746\n",
      "Trained batch 542 batch loss 6.20986605 epoch total loss 6.08017731\n",
      "Trained batch 543 batch loss 6.08846426 epoch total loss 6.08019209\n",
      "Trained batch 544 batch loss 5.9220438 epoch total loss 6.0799017\n",
      "Trained batch 545 batch loss 6.21869707 epoch total loss 6.08015633\n",
      "Trained batch 546 batch loss 5.6756506 epoch total loss 6.07941532\n",
      "Trained batch 547 batch loss 5.53769112 epoch total loss 6.07842493\n",
      "Trained batch 548 batch loss 6.04814434 epoch total loss 6.07836962\n",
      "Trained batch 549 batch loss 6.03079128 epoch total loss 6.07828283\n",
      "Trained batch 550 batch loss 5.80321 epoch total loss 6.07778263\n",
      "Trained batch 551 batch loss 6.1825738 epoch total loss 6.07797289\n",
      "Trained batch 552 batch loss 5.8080349 epoch total loss 6.07748413\n",
      "Trained batch 553 batch loss 5.62521648 epoch total loss 6.07666636\n",
      "Trained batch 554 batch loss 5.41119862 epoch total loss 6.0754652\n",
      "Trained batch 555 batch loss 5.86490583 epoch total loss 6.07508564\n",
      "Trained batch 556 batch loss 6.3448143 epoch total loss 6.07557058\n",
      "Trained batch 557 batch loss 5.83743143 epoch total loss 6.07514334\n",
      "Trained batch 558 batch loss 6.33769226 epoch total loss 6.0756135\n",
      "Trained batch 559 batch loss 6.59692955 epoch total loss 6.07654619\n",
      "Trained batch 560 batch loss 7.17200661 epoch total loss 6.07850266\n",
      "Trained batch 561 batch loss 6.38863468 epoch total loss 6.07905531\n",
      "Trained batch 562 batch loss 7.388381 epoch total loss 6.08138514\n",
      "Trained batch 563 batch loss 7.01711845 epoch total loss 6.08304739\n",
      "Trained batch 564 batch loss 6.91255093 epoch total loss 6.08451796\n",
      "Trained batch 565 batch loss 6.59378242 epoch total loss 6.08541918\n",
      "Trained batch 566 batch loss 6.640378 epoch total loss 6.0864\n",
      "Trained batch 567 batch loss 7.22260332 epoch total loss 6.0884037\n",
      "Trained batch 568 batch loss 6.69285 epoch total loss 6.089468\n",
      "Trained batch 569 batch loss 5.96359921 epoch total loss 6.08924675\n",
      "Trained batch 570 batch loss 5.91714239 epoch total loss 6.08894491\n",
      "Trained batch 571 batch loss 5.98361 epoch total loss 6.08876085\n",
      "Trained batch 572 batch loss 6.03712273 epoch total loss 6.08867025\n",
      "Trained batch 573 batch loss 5.95697165 epoch total loss 6.0884409\n",
      "Trained batch 574 batch loss 5.39659834 epoch total loss 6.08723497\n",
      "Trained batch 575 batch loss 4.83972931 epoch total loss 6.08506584\n",
      "Trained batch 576 batch loss 4.59040928 epoch total loss 6.08247089\n",
      "Trained batch 577 batch loss 5.89113712 epoch total loss 6.08213902\n",
      "Trained batch 578 batch loss 6.74358559 epoch total loss 6.08328342\n",
      "Trained batch 579 batch loss 4.9060297 epoch total loss 6.08125\n",
      "Trained batch 580 batch loss 4.32961273 epoch total loss 6.07823038\n",
      "Trained batch 581 batch loss 5.40198326 epoch total loss 6.07706642\n",
      "Trained batch 582 batch loss 6.9178791 epoch total loss 6.07851124\n",
      "Trained batch 583 batch loss 6.61633968 epoch total loss 6.07943392\n",
      "Trained batch 584 batch loss 6.0413475 epoch total loss 6.07936859\n",
      "Trained batch 585 batch loss 6.47656155 epoch total loss 6.08004761\n",
      "Trained batch 586 batch loss 7.0500145 epoch total loss 6.08170319\n",
      "Trained batch 587 batch loss 6.85869789 epoch total loss 6.08302641\n",
      "Trained batch 588 batch loss 5.14316511 epoch total loss 6.08142805\n",
      "Trained batch 589 batch loss 6.94607544 epoch total loss 6.08289576\n",
      "Trained batch 590 batch loss 6.66038704 epoch total loss 6.0838747\n",
      "Trained batch 591 batch loss 6.40758371 epoch total loss 6.08442211\n",
      "Trained batch 592 batch loss 6.65090466 epoch total loss 6.08537912\n",
      "Trained batch 593 batch loss 6.5548296 epoch total loss 6.08617115\n",
      "Trained batch 594 batch loss 6.62697792 epoch total loss 6.08708143\n",
      "Trained batch 595 batch loss 6.44445515 epoch total loss 6.08768177\n",
      "Trained batch 596 batch loss 6.16036129 epoch total loss 6.08780384\n",
      "Trained batch 597 batch loss 5.82613277 epoch total loss 6.08736563\n",
      "Trained batch 598 batch loss 6.0836 epoch total loss 6.08735895\n",
      "Trained batch 599 batch loss 6.26562214 epoch total loss 6.0876565\n",
      "Trained batch 600 batch loss 6.47206211 epoch total loss 6.08829737\n",
      "Trained batch 601 batch loss 6.03581047 epoch total loss 6.08821\n",
      "Trained batch 602 batch loss 6.30298519 epoch total loss 6.08856726\n",
      "Trained batch 603 batch loss 6.37494278 epoch total loss 6.08904219\n",
      "Trained batch 604 batch loss 6.31579065 epoch total loss 6.08941746\n",
      "Trained batch 605 batch loss 5.18839645 epoch total loss 6.0879283\n",
      "Trained batch 606 batch loss 4.75996256 epoch total loss 6.08573675\n",
      "Trained batch 607 batch loss 5.34213591 epoch total loss 6.08451176\n",
      "Trained batch 608 batch loss 6.37111092 epoch total loss 6.08498287\n",
      "Trained batch 609 batch loss 6.01910257 epoch total loss 6.08487463\n",
      "Trained batch 610 batch loss 5.85616684 epoch total loss 6.0845\n",
      "Trained batch 611 batch loss 6.0839572 epoch total loss 6.08449888\n",
      "Trained batch 612 batch loss 5.85698128 epoch total loss 6.08412743\n",
      "Trained batch 613 batch loss 6.09545 epoch total loss 6.08414555\n",
      "Trained batch 614 batch loss 7.03928423 epoch total loss 6.08570147\n",
      "Trained batch 615 batch loss 5.04383612 epoch total loss 6.08400726\n",
      "Trained batch 616 batch loss 4.84047651 epoch total loss 6.08198881\n",
      "Trained batch 617 batch loss 5.4038 epoch total loss 6.0808897\n",
      "Trained batch 618 batch loss 5.78079081 epoch total loss 6.0804038\n",
      "Trained batch 619 batch loss 4.55314445 epoch total loss 6.07793665\n",
      "Trained batch 620 batch loss 5.49390316 epoch total loss 6.0769949\n",
      "Trained batch 621 batch loss 5.39694548 epoch total loss 6.0758996\n",
      "Trained batch 622 batch loss 6.145473 epoch total loss 6.07601166\n",
      "Trained batch 623 batch loss 6.19878101 epoch total loss 6.07620859\n",
      "Trained batch 624 batch loss 6.3490591 epoch total loss 6.07664585\n",
      "Trained batch 625 batch loss 5.92958546 epoch total loss 6.07641077\n",
      "Trained batch 626 batch loss 5.5059185 epoch total loss 6.07549953\n",
      "Trained batch 627 batch loss 5.00383 epoch total loss 6.07379055\n",
      "Trained batch 628 batch loss 5.58066177 epoch total loss 6.0730052\n",
      "Trained batch 629 batch loss 5.78814411 epoch total loss 6.0725522\n",
      "Trained batch 630 batch loss 5.8018961 epoch total loss 6.07212257\n",
      "Trained batch 631 batch loss 6.20243073 epoch total loss 6.07232904\n",
      "Trained batch 632 batch loss 4.48742104 epoch total loss 6.06982136\n",
      "Trained batch 633 batch loss 6.20972157 epoch total loss 6.07004213\n",
      "Trained batch 634 batch loss 6.07500076 epoch total loss 6.07005\n",
      "Trained batch 635 batch loss 6.7328577 epoch total loss 6.07109356\n",
      "Trained batch 636 batch loss 6.26016235 epoch total loss 6.07139111\n",
      "Trained batch 637 batch loss 5.90418148 epoch total loss 6.07112885\n",
      "Trained batch 638 batch loss 5.76389074 epoch total loss 6.07064724\n",
      "Trained batch 639 batch loss 5.76423836 epoch total loss 6.07016754\n",
      "Trained batch 640 batch loss 6.28863716 epoch total loss 6.07050896\n",
      "Trained batch 641 batch loss 6.03365183 epoch total loss 6.07045174\n",
      "Trained batch 642 batch loss 6.44762802 epoch total loss 6.07103872\n",
      "Trained batch 643 batch loss 6.21364307 epoch total loss 6.07126045\n",
      "Trained batch 644 batch loss 6.24664783 epoch total loss 6.07153273\n",
      "Trained batch 645 batch loss 5.72642803 epoch total loss 6.07099771\n",
      "Trained batch 646 batch loss 5.61047173 epoch total loss 6.07028437\n",
      "Trained batch 647 batch loss 5.96806145 epoch total loss 6.07012653\n",
      "Trained batch 648 batch loss 6.25930309 epoch total loss 6.07041836\n",
      "Trained batch 649 batch loss 4.86767244 epoch total loss 6.06856537\n",
      "Trained batch 650 batch loss 5.94043922 epoch total loss 6.06836796\n",
      "Trained batch 651 batch loss 6.25920916 epoch total loss 6.06866121\n",
      "Trained batch 652 batch loss 6.30558205 epoch total loss 6.06902456\n",
      "Trained batch 653 batch loss 5.87383842 epoch total loss 6.06872559\n",
      "Trained batch 654 batch loss 5.1414957 epoch total loss 6.06730795\n",
      "Trained batch 655 batch loss 7.121593 epoch total loss 6.06891775\n",
      "Trained batch 656 batch loss 6.41086 epoch total loss 6.06943893\n",
      "Trained batch 657 batch loss 6.11489296 epoch total loss 6.06950855\n",
      "Trained batch 658 batch loss 5.91825533 epoch total loss 6.06927824\n",
      "Trained batch 659 batch loss 5.27412224 epoch total loss 6.06807184\n",
      "Trained batch 660 batch loss 5.3064 epoch total loss 6.0669179\n",
      "Trained batch 661 batch loss 5.83110714 epoch total loss 6.06656122\n",
      "Trained batch 662 batch loss 4.70426226 epoch total loss 6.06450319\n",
      "Trained batch 663 batch loss 5.52139664 epoch total loss 6.06368446\n",
      "Trained batch 664 batch loss 4.94430494 epoch total loss 6.06199837\n",
      "Trained batch 665 batch loss 6.10178661 epoch total loss 6.06205845\n",
      "Trained batch 666 batch loss 5.87185669 epoch total loss 6.06177282\n",
      "Trained batch 667 batch loss 5.59542894 epoch total loss 6.06107378\n",
      "Trained batch 668 batch loss 5.81050301 epoch total loss 6.06069851\n",
      "Trained batch 669 batch loss 5.94769955 epoch total loss 6.06052971\n",
      "Trained batch 670 batch loss 5.98954487 epoch total loss 6.06042385\n",
      "Trained batch 671 batch loss 6.10020494 epoch total loss 6.06048298\n",
      "Trained batch 672 batch loss 6.09795094 epoch total loss 6.06053829\n",
      "Trained batch 673 batch loss 6.51049614 epoch total loss 6.06120729\n",
      "Trained batch 674 batch loss 6.10466 epoch total loss 6.06127167\n",
      "Trained batch 675 batch loss 5.93774366 epoch total loss 6.06108856\n",
      "Trained batch 676 batch loss 5.66541 epoch total loss 6.06050348\n",
      "Trained batch 677 batch loss 5.55374527 epoch total loss 6.05975485\n",
      "Trained batch 678 batch loss 5.65889072 epoch total loss 6.05916357\n",
      "Trained batch 679 batch loss 5.72764111 epoch total loss 6.05867481\n",
      "Trained batch 680 batch loss 5.59765434 epoch total loss 6.05799723\n",
      "Trained batch 681 batch loss 5.87236881 epoch total loss 6.05772495\n",
      "Trained batch 682 batch loss 6.24288893 epoch total loss 6.0579958\n",
      "Trained batch 683 batch loss 6.14593077 epoch total loss 6.05812454\n",
      "Trained batch 684 batch loss 5.66231489 epoch total loss 6.05754566\n",
      "Trained batch 685 batch loss 6.01360035 epoch total loss 6.05748177\n",
      "Trained batch 686 batch loss 5.65092945 epoch total loss 6.05688906\n",
      "Trained batch 687 batch loss 6.2053442 epoch total loss 6.05710554\n",
      "Trained batch 688 batch loss 5.10500574 epoch total loss 6.05572176\n",
      "Trained batch 689 batch loss 4.74590826 epoch total loss 6.05382061\n",
      "Trained batch 690 batch loss 5.42484331 epoch total loss 6.05290937\n",
      "Trained batch 691 batch loss 5.46343088 epoch total loss 6.05205584\n",
      "Trained batch 692 batch loss 6.53507137 epoch total loss 6.05275393\n",
      "Trained batch 693 batch loss 5.71340466 epoch total loss 6.05226421\n",
      "Trained batch 694 batch loss 6.03980541 epoch total loss 6.05224657\n",
      "Trained batch 695 batch loss 6.98097372 epoch total loss 6.05358315\n",
      "Trained batch 696 batch loss 6.83901882 epoch total loss 6.05471134\n",
      "Trained batch 697 batch loss 5.99939966 epoch total loss 6.05463219\n",
      "Trained batch 698 batch loss 5.84184408 epoch total loss 6.05432701\n",
      "Trained batch 699 batch loss 6.38257217 epoch total loss 6.05479717\n",
      "Trained batch 700 batch loss 6.05829906 epoch total loss 6.05480194\n",
      "Trained batch 701 batch loss 5.053092 epoch total loss 6.05337334\n",
      "Trained batch 702 batch loss 5.00919056 epoch total loss 6.0518856\n",
      "Trained batch 703 batch loss 4.55813694 epoch total loss 6.04976082\n",
      "Trained batch 704 batch loss 4.79659748 epoch total loss 6.04798079\n",
      "Trained batch 705 batch loss 3.88655066 epoch total loss 6.04491472\n",
      "Trained batch 706 batch loss 4.10068 epoch total loss 6.04216099\n",
      "Trained batch 707 batch loss 5.0806489 epoch total loss 6.04080105\n",
      "Trained batch 708 batch loss 6.17359209 epoch total loss 6.04098892\n",
      "Trained batch 709 batch loss 5.78977 epoch total loss 6.04063416\n",
      "Trained batch 710 batch loss 6.20771694 epoch total loss 6.04086924\n",
      "Trained batch 711 batch loss 5.71752071 epoch total loss 6.04041386\n",
      "Trained batch 712 batch loss 6.27310944 epoch total loss 6.04074049\n",
      "Trained batch 713 batch loss 6.45094967 epoch total loss 6.04131603\n",
      "Trained batch 714 batch loss 7.22380352 epoch total loss 6.04297209\n",
      "Trained batch 715 batch loss 5.97007847 epoch total loss 6.04287052\n",
      "Trained batch 716 batch loss 5.6746788 epoch total loss 6.04235649\n",
      "Trained batch 717 batch loss 6.60432339 epoch total loss 6.04314041\n",
      "Trained batch 718 batch loss 5.23958778 epoch total loss 6.04202127\n",
      "Trained batch 719 batch loss 6.37841082 epoch total loss 6.04248953\n",
      "Trained batch 720 batch loss 5.33473539 epoch total loss 6.04150677\n",
      "Trained batch 721 batch loss 5.84146 epoch total loss 6.04122877\n",
      "Trained batch 722 batch loss 5.16484833 epoch total loss 6.04001522\n",
      "Trained batch 723 batch loss 5.72259092 epoch total loss 6.03957653\n",
      "Trained batch 724 batch loss 5.91740227 epoch total loss 6.03940773\n",
      "Trained batch 725 batch loss 6.14218521 epoch total loss 6.03954935\n",
      "Trained batch 726 batch loss 6.50630474 epoch total loss 6.0401926\n",
      "Trained batch 727 batch loss 6.53535652 epoch total loss 6.04087305\n",
      "Trained batch 728 batch loss 5.70054722 epoch total loss 6.04040575\n",
      "Trained batch 729 batch loss 6.10490608 epoch total loss 6.04049444\n",
      "Trained batch 730 batch loss 6.54773474 epoch total loss 6.04118967\n",
      "Trained batch 731 batch loss 5.76218319 epoch total loss 6.04080772\n",
      "Trained batch 732 batch loss 6.48883915 epoch total loss 6.04142\n",
      "Trained batch 733 batch loss 6.4440937 epoch total loss 6.04197\n",
      "Trained batch 734 batch loss 7.31299114 epoch total loss 6.04370117\n",
      "Trained batch 735 batch loss 6.48309803 epoch total loss 6.04429865\n",
      "Trained batch 736 batch loss 6.64444685 epoch total loss 6.04511404\n",
      "Trained batch 737 batch loss 6.16823769 epoch total loss 6.04528141\n",
      "Trained batch 738 batch loss 6.72518826 epoch total loss 6.04620266\n",
      "Trained batch 739 batch loss 6.29529333 epoch total loss 6.04654\n",
      "Trained batch 740 batch loss 5.42183733 epoch total loss 6.04569578\n",
      "Trained batch 741 batch loss 6.69839954 epoch total loss 6.0465765\n",
      "Trained batch 742 batch loss 6.40716887 epoch total loss 6.0470624\n",
      "Trained batch 743 batch loss 6.66631842 epoch total loss 6.04789639\n",
      "Trained batch 744 batch loss 6.26582527 epoch total loss 6.04818869\n",
      "Trained batch 745 batch loss 6.65910435 epoch total loss 6.04900885\n",
      "Trained batch 746 batch loss 6.26645374 epoch total loss 6.04930067\n",
      "Trained batch 747 batch loss 6.90331 epoch total loss 6.05044413\n",
      "Trained batch 748 batch loss 6.01657486 epoch total loss 6.05039883\n",
      "Trained batch 749 batch loss 5.08075237 epoch total loss 6.04910374\n",
      "Trained batch 750 batch loss 6.08630943 epoch total loss 6.0491538\n",
      "Trained batch 751 batch loss 4.93719435 epoch total loss 6.04767275\n",
      "Trained batch 752 batch loss 5.19528151 epoch total loss 6.04653931\n",
      "Trained batch 753 batch loss 5.86815739 epoch total loss 6.04630232\n",
      "Trained batch 754 batch loss 5.15317631 epoch total loss 6.04511833\n",
      "Trained batch 755 batch loss 5.30592251 epoch total loss 6.04413939\n",
      "Trained batch 756 batch loss 5.27650499 epoch total loss 6.04312372\n",
      "Trained batch 757 batch loss 4.82920837 epoch total loss 6.04152\n",
      "Trained batch 758 batch loss 5.63900375 epoch total loss 6.0409894\n",
      "Trained batch 759 batch loss 6.0199 epoch total loss 6.04096174\n",
      "Trained batch 760 batch loss 6.00516129 epoch total loss 6.04091454\n",
      "Trained batch 761 batch loss 6.26992607 epoch total loss 6.0412159\n",
      "Trained batch 762 batch loss 6.5033 epoch total loss 6.04182243\n",
      "Trained batch 763 batch loss 5.13435 epoch total loss 6.04063272\n",
      "Trained batch 764 batch loss 5.93464947 epoch total loss 6.04049397\n",
      "Trained batch 765 batch loss 5.6620903 epoch total loss 6.03999949\n",
      "Trained batch 766 batch loss 6.28405237 epoch total loss 6.04031849\n",
      "Trained batch 767 batch loss 6.24975681 epoch total loss 6.04059172\n",
      "Trained batch 768 batch loss 4.84955788 epoch total loss 6.03904104\n",
      "Trained batch 769 batch loss 6.08762646 epoch total loss 6.03910398\n",
      "Trained batch 770 batch loss 5.07831383 epoch total loss 6.03785563\n",
      "Trained batch 771 batch loss 5.0976367 epoch total loss 6.03663635\n",
      "Trained batch 772 batch loss 4.99253559 epoch total loss 6.03528404\n",
      "Trained batch 773 batch loss 4.81460905 epoch total loss 6.03370476\n",
      "Trained batch 774 batch loss 6.06088924 epoch total loss 6.03374\n",
      "Trained batch 775 batch loss 6.41725826 epoch total loss 6.034235\n",
      "Trained batch 776 batch loss 5.95459795 epoch total loss 6.03413248\n",
      "Trained batch 777 batch loss 6.34398842 epoch total loss 6.03453112\n",
      "Trained batch 778 batch loss 6.09606361 epoch total loss 6.03461027\n",
      "Trained batch 779 batch loss 5.80806446 epoch total loss 6.0343194\n",
      "Trained batch 780 batch loss 5.88065529 epoch total loss 6.03412294\n",
      "Trained batch 781 batch loss 5.88696527 epoch total loss 6.03393459\n",
      "Trained batch 782 batch loss 5.90154171 epoch total loss 6.03376532\n",
      "Trained batch 783 batch loss 6.16468716 epoch total loss 6.03393221\n",
      "Trained batch 784 batch loss 6.07112885 epoch total loss 6.03398\n",
      "Trained batch 785 batch loss 6.06522751 epoch total loss 6.03402\n",
      "Trained batch 786 batch loss 5.55020332 epoch total loss 6.03340435\n",
      "Trained batch 787 batch loss 5.89878368 epoch total loss 6.03323364\n",
      "Trained batch 788 batch loss 6.18313551 epoch total loss 6.0334239\n",
      "Trained batch 789 batch loss 6.16994858 epoch total loss 6.03359652\n",
      "Trained batch 790 batch loss 6.18703032 epoch total loss 6.03379107\n",
      "Trained batch 791 batch loss 6.29956055 epoch total loss 6.03412676\n",
      "Trained batch 792 batch loss 6.23395252 epoch total loss 6.03437901\n",
      "Trained batch 793 batch loss 6.31234837 epoch total loss 6.03472948\n",
      "Trained batch 794 batch loss 5.92904758 epoch total loss 6.03459644\n",
      "Trained batch 795 batch loss 6.53477192 epoch total loss 6.03522587\n",
      "Trained batch 796 batch loss 4.86458206 epoch total loss 6.0337553\n",
      "Trained batch 797 batch loss 6.10279655 epoch total loss 6.03384209\n",
      "Trained batch 798 batch loss 6.5022068 epoch total loss 6.03442955\n",
      "Trained batch 799 batch loss 5.58724213 epoch total loss 6.03386974\n",
      "Trained batch 800 batch loss 6.97689152 epoch total loss 6.03504896\n",
      "Trained batch 801 batch loss 6.31582451 epoch total loss 6.03539944\n",
      "Trained batch 802 batch loss 6.06897688 epoch total loss 6.0354414\n",
      "Trained batch 803 batch loss 6.31435919 epoch total loss 6.03578854\n",
      "Trained batch 804 batch loss 5.56346178 epoch total loss 6.03520107\n",
      "Trained batch 805 batch loss 6.598454 epoch total loss 6.03590107\n",
      "Trained batch 806 batch loss 6.68715906 epoch total loss 6.03670883\n",
      "Trained batch 807 batch loss 6.45113564 epoch total loss 6.03722239\n",
      "Trained batch 808 batch loss 6.68018532 epoch total loss 6.03801823\n",
      "Trained batch 809 batch loss 6.66752911 epoch total loss 6.03879642\n",
      "Trained batch 810 batch loss 6.53902531 epoch total loss 6.03941393\n",
      "Trained batch 811 batch loss 6.274683 epoch total loss 6.03970432\n",
      "Trained batch 812 batch loss 6.14962721 epoch total loss 6.03983927\n",
      "Trained batch 813 batch loss 6.66244221 epoch total loss 6.04060555\n",
      "Trained batch 814 batch loss 6.43964863 epoch total loss 6.04109526\n",
      "Trained batch 815 batch loss 6.63559532 epoch total loss 6.04182482\n",
      "Trained batch 816 batch loss 6.31710434 epoch total loss 6.04216194\n",
      "Trained batch 817 batch loss 6.33749247 epoch total loss 6.04252338\n",
      "Trained batch 818 batch loss 6.70685625 epoch total loss 6.04333591\n",
      "Trained batch 819 batch loss 6.27244854 epoch total loss 6.04361582\n",
      "Trained batch 820 batch loss 6.00193787 epoch total loss 6.0435648\n",
      "Trained batch 821 batch loss 6.69298935 epoch total loss 6.04435587\n",
      "Trained batch 822 batch loss 6.83682728 epoch total loss 6.04532\n",
      "Trained batch 823 batch loss 6.22241306 epoch total loss 6.04553556\n",
      "Trained batch 824 batch loss 6.71196 epoch total loss 6.04634428\n",
      "Trained batch 825 batch loss 6.2927351 epoch total loss 6.04664278\n",
      "Trained batch 826 batch loss 6.88792706 epoch total loss 6.0476613\n",
      "Trained batch 827 batch loss 6.34033442 epoch total loss 6.04801512\n",
      "Trained batch 828 batch loss 6.4524684 epoch total loss 6.04850388\n",
      "Trained batch 829 batch loss 6.2286396 epoch total loss 6.04872084\n",
      "Trained batch 830 batch loss 6.66710472 epoch total loss 6.04946566\n",
      "Trained batch 831 batch loss 7.14445114 epoch total loss 6.05078363\n",
      "Trained batch 832 batch loss 6.46910238 epoch total loss 6.0512867\n",
      "Trained batch 833 batch loss 6.64773321 epoch total loss 6.05200291\n",
      "Trained batch 834 batch loss 6.04471493 epoch total loss 6.05199432\n",
      "Trained batch 835 batch loss 6.18914938 epoch total loss 6.05215836\n",
      "Trained batch 836 batch loss 5.42222118 epoch total loss 6.05140495\n",
      "Trained batch 837 batch loss 6.17637444 epoch total loss 6.0515542\n",
      "Trained batch 838 batch loss 6.49006939 epoch total loss 6.05207777\n",
      "Trained batch 839 batch loss 6.44250154 epoch total loss 6.05254269\n",
      "Trained batch 840 batch loss 6.28333426 epoch total loss 6.05281734\n",
      "Trained batch 841 batch loss 6.22139215 epoch total loss 6.05301762\n",
      "Trained batch 842 batch loss 5.64991808 epoch total loss 6.05253887\n",
      "Trained batch 843 batch loss 6.27162886 epoch total loss 6.05279875\n",
      "Trained batch 844 batch loss 6.30833054 epoch total loss 6.05310106\n",
      "Trained batch 845 batch loss 6.23624659 epoch total loss 6.05331802\n",
      "Trained batch 846 batch loss 6.1284008 epoch total loss 6.05340672\n",
      "Trained batch 847 batch loss 6.40778065 epoch total loss 6.0538249\n",
      "Trained batch 848 batch loss 6.27878857 epoch total loss 6.0540905\n",
      "Trained batch 849 batch loss 6.67446518 epoch total loss 6.05482101\n",
      "Trained batch 850 batch loss 5.19730806 epoch total loss 6.05381203\n",
      "Trained batch 851 batch loss 5.65267944 epoch total loss 6.05334091\n",
      "Trained batch 852 batch loss 5.52013874 epoch total loss 6.05271482\n",
      "Trained batch 853 batch loss 6.19408703 epoch total loss 6.05288029\n",
      "Trained batch 854 batch loss 6.27840424 epoch total loss 6.05314445\n",
      "Trained batch 855 batch loss 6.27027512 epoch total loss 6.05339861\n",
      "Trained batch 856 batch loss 6.16765499 epoch total loss 6.05353165\n",
      "Trained batch 857 batch loss 6.27345848 epoch total loss 6.05378866\n",
      "Trained batch 858 batch loss 6.22381687 epoch total loss 6.05398655\n",
      "Trained batch 859 batch loss 6.47534752 epoch total loss 6.05447721\n",
      "Trained batch 860 batch loss 6.03654194 epoch total loss 6.05445623\n",
      "Trained batch 861 batch loss 5.01822 epoch total loss 6.0532527\n",
      "Trained batch 862 batch loss 5.61706257 epoch total loss 6.05274677\n",
      "Trained batch 863 batch loss 5.95947 epoch total loss 6.05263853\n",
      "Trained batch 864 batch loss 6.40256262 epoch total loss 6.05304337\n",
      "Trained batch 865 batch loss 6.4701252 epoch total loss 6.05352592\n",
      "Trained batch 866 batch loss 6.4851923 epoch total loss 6.05402422\n",
      "Trained batch 867 batch loss 5.89082241 epoch total loss 6.05383587\n",
      "Trained batch 868 batch loss 6.58172131 epoch total loss 6.05444384\n",
      "Trained batch 869 batch loss 6.26921129 epoch total loss 6.05469084\n",
      "Trained batch 870 batch loss 6.00036669 epoch total loss 6.05462837\n",
      "Trained batch 871 batch loss 6.02883816 epoch total loss 6.05459881\n",
      "Trained batch 872 batch loss 6.05466938 epoch total loss 6.05459881\n",
      "Trained batch 873 batch loss 6.14370966 epoch total loss 6.05470085\n",
      "Trained batch 874 batch loss 5.95851326 epoch total loss 6.0545907\n",
      "Trained batch 875 batch loss 6.20357704 epoch total loss 6.05476093\n",
      "Trained batch 876 batch loss 6.04784393 epoch total loss 6.0547533\n",
      "Trained batch 877 batch loss 5.98548412 epoch total loss 6.05467415\n",
      "Trained batch 878 batch loss 5.91749525 epoch total loss 6.05451775\n",
      "Trained batch 879 batch loss 5.83356047 epoch total loss 6.05426645\n",
      "Trained batch 880 batch loss 5.89772701 epoch total loss 6.05408859\n",
      "Trained batch 881 batch loss 6.020257 epoch total loss 6.05405\n",
      "Trained batch 882 batch loss 6.10470819 epoch total loss 6.05410719\n",
      "Trained batch 883 batch loss 5.80535269 epoch total loss 6.05382538\n",
      "Trained batch 884 batch loss 5.66665649 epoch total loss 6.05338717\n",
      "Trained batch 885 batch loss 6.11022425 epoch total loss 6.05345154\n",
      "Trained batch 886 batch loss 6.09571743 epoch total loss 6.05349922\n",
      "Trained batch 887 batch loss 6.54047108 epoch total loss 6.05404854\n",
      "Trained batch 888 batch loss 5.70203543 epoch total loss 6.05365229\n",
      "Trained batch 889 batch loss 6.06664944 epoch total loss 6.05366659\n",
      "Trained batch 890 batch loss 5.91891909 epoch total loss 6.05351496\n",
      "Trained batch 891 batch loss 6.65498 epoch total loss 6.05418968\n",
      "Trained batch 892 batch loss 5.41033745 epoch total loss 6.05346775\n",
      "Trained batch 893 batch loss 5.09361 epoch total loss 6.05239296\n",
      "Trained batch 894 batch loss 4.68853664 epoch total loss 6.05086756\n",
      "Trained batch 895 batch loss 5.51608658 epoch total loss 6.05027\n",
      "Trained batch 896 batch loss 4.83664036 epoch total loss 6.04891539\n",
      "Trained batch 897 batch loss 4.44066238 epoch total loss 6.047122\n",
      "Trained batch 898 batch loss 5.7156992 epoch total loss 6.04675341\n",
      "Trained batch 899 batch loss 6.08266163 epoch total loss 6.04679298\n",
      "Trained batch 900 batch loss 4.39573765 epoch total loss 6.04495811\n",
      "Trained batch 901 batch loss 4.68965816 epoch total loss 6.04345369\n",
      "Trained batch 902 batch loss 5.09119797 epoch total loss 6.04239798\n",
      "Trained batch 903 batch loss 4.55810976 epoch total loss 6.04075432\n",
      "Trained batch 904 batch loss 4.61349869 epoch total loss 6.03917551\n",
      "Trained batch 905 batch loss 4.77777195 epoch total loss 6.03778172\n",
      "Trained batch 906 batch loss 4.27130222 epoch total loss 6.03583193\n",
      "Trained batch 907 batch loss 4.52201843 epoch total loss 6.034163\n",
      "Trained batch 908 batch loss 4.93892765 epoch total loss 6.0329566\n",
      "Trained batch 909 batch loss 5.54367924 epoch total loss 6.03241825\n",
      "Trained batch 910 batch loss 6.47529 epoch total loss 6.03290462\n",
      "Trained batch 911 batch loss 5.37432957 epoch total loss 6.03218222\n",
      "Trained batch 912 batch loss 6.49860191 epoch total loss 6.03269339\n",
      "Trained batch 913 batch loss 5.96019077 epoch total loss 6.03261375\n",
      "Trained batch 914 batch loss 5.94624949 epoch total loss 6.03251934\n",
      "Trained batch 915 batch loss 6.34105968 epoch total loss 6.03285646\n",
      "Trained batch 916 batch loss 5.9761982 epoch total loss 6.03279448\n",
      "Trained batch 917 batch loss 6.39364243 epoch total loss 6.03318787\n",
      "Trained batch 918 batch loss 6.05160332 epoch total loss 6.03320789\n",
      "Trained batch 919 batch loss 5.73188496 epoch total loss 6.03288031\n",
      "Trained batch 920 batch loss 4.82232666 epoch total loss 6.03156424\n",
      "Trained batch 921 batch loss 4.81580639 epoch total loss 6.03024435\n",
      "Trained batch 922 batch loss 4.77815247 epoch total loss 6.02888632\n",
      "Trained batch 923 batch loss 6.01113796 epoch total loss 6.02886724\n",
      "Trained batch 924 batch loss 6.87992 epoch total loss 6.02978849\n",
      "Trained batch 925 batch loss 5.874053 epoch total loss 6.02962\n",
      "Trained batch 926 batch loss 4.93549633 epoch total loss 6.02843857\n",
      "Trained batch 927 batch loss 6.39924812 epoch total loss 6.02883863\n",
      "Trained batch 928 batch loss 6.13654041 epoch total loss 6.02895498\n",
      "Trained batch 929 batch loss 6.552845 epoch total loss 6.0295186\n",
      "Trained batch 930 batch loss 6.58057 epoch total loss 6.03011131\n",
      "Trained batch 931 batch loss 5.42550087 epoch total loss 6.02946138\n",
      "Trained batch 932 batch loss 6.38617563 epoch total loss 6.02984428\n",
      "Trained batch 933 batch loss 6.4550705 epoch total loss 6.0303\n",
      "Trained batch 934 batch loss 6.20141888 epoch total loss 6.03048372\n",
      "Trained batch 935 batch loss 6.0406723 epoch total loss 6.03049421\n",
      "Trained batch 936 batch loss 6.27406502 epoch total loss 6.03075457\n",
      "Trained batch 937 batch loss 5.62035894 epoch total loss 6.03031635\n",
      "Trained batch 938 batch loss 5.69349 epoch total loss 6.02995682\n",
      "Trained batch 939 batch loss 6.29519367 epoch total loss 6.03023958\n",
      "Trained batch 940 batch loss 5.9277029 epoch total loss 6.03013039\n",
      "Trained batch 941 batch loss 6.19803572 epoch total loss 6.0303092\n",
      "Trained batch 942 batch loss 6.37360191 epoch total loss 6.0306735\n",
      "Trained batch 943 batch loss 5.97291 epoch total loss 6.03061247\n",
      "Trained batch 944 batch loss 6.05460262 epoch total loss 6.03063822\n",
      "Trained batch 945 batch loss 6.1190424 epoch total loss 6.03073168\n",
      "Trained batch 946 batch loss 5.93823099 epoch total loss 6.03063393\n",
      "Trained batch 947 batch loss 6.12991619 epoch total loss 6.03073835\n",
      "Trained batch 948 batch loss 5.74812794 epoch total loss 6.03044033\n",
      "Trained batch 949 batch loss 5.87143421 epoch total loss 6.03027296\n",
      "Trained batch 950 batch loss 4.85656261 epoch total loss 6.02903748\n",
      "Trained batch 951 batch loss 5.82222462 epoch total loss 6.02882\n",
      "Trained batch 952 batch loss 5.58285046 epoch total loss 6.02835178\n",
      "Trained batch 953 batch loss 5.77551365 epoch total loss 6.02808619\n",
      "Trained batch 954 batch loss 5.9697628 epoch total loss 6.02802515\n",
      "Trained batch 955 batch loss 5.85730553 epoch total loss 6.02784634\n",
      "Trained batch 956 batch loss 5.91009712 epoch total loss 6.02772331\n",
      "Trained batch 957 batch loss 5.8125124 epoch total loss 6.02749825\n",
      "Trained batch 958 batch loss 6.22598648 epoch total loss 6.02770567\n",
      "Trained batch 959 batch loss 6.81936455 epoch total loss 6.02853107\n",
      "Trained batch 960 batch loss 6.39237261 epoch total loss 6.02891\n",
      "Trained batch 961 batch loss 6.30762243 epoch total loss 6.02920055\n",
      "Trained batch 962 batch loss 6.39154911 epoch total loss 6.02957726\n",
      "Trained batch 963 batch loss 6.62299156 epoch total loss 6.03019333\n",
      "Trained batch 964 batch loss 6.16139507 epoch total loss 6.0303297\n",
      "Trained batch 965 batch loss 6.18040276 epoch total loss 6.03048515\n",
      "Trained batch 966 batch loss 6.07333946 epoch total loss 6.03052902\n",
      "Trained batch 967 batch loss 6.30612469 epoch total loss 6.03081417\n",
      "Trained batch 968 batch loss 5.84355879 epoch total loss 6.03062105\n",
      "Trained batch 969 batch loss 6.13866 epoch total loss 6.03073263\n",
      "Trained batch 970 batch loss 6.30917501 epoch total loss 6.03101969\n",
      "Trained batch 971 batch loss 5.74992561 epoch total loss 6.03073025\n",
      "Trained batch 972 batch loss 5.76411629 epoch total loss 6.03045559\n",
      "Trained batch 973 batch loss 5.6545372 epoch total loss 6.03006935\n",
      "Trained batch 974 batch loss 6.28312302 epoch total loss 6.03032923\n",
      "Trained batch 975 batch loss 6.51790237 epoch total loss 6.03082943\n",
      "Trained batch 976 batch loss 6.52088642 epoch total loss 6.03133154\n",
      "Trained batch 977 batch loss 6.13429403 epoch total loss 6.03143692\n",
      "Trained batch 978 batch loss 6.07893896 epoch total loss 6.03148556\n",
      "Trained batch 979 batch loss 5.82618523 epoch total loss 6.03127575\n",
      "Trained batch 980 batch loss 5.87748814 epoch total loss 6.03111887\n",
      "Trained batch 981 batch loss 6.52071095 epoch total loss 6.03161764\n",
      "Trained batch 982 batch loss 5.59865379 epoch total loss 6.03117704\n",
      "Trained batch 983 batch loss 5.05087471 epoch total loss 6.0301795\n",
      "Trained batch 984 batch loss 5.65597248 epoch total loss 6.02979898\n",
      "Trained batch 985 batch loss 5.65034199 epoch total loss 6.0294137\n",
      "Trained batch 986 batch loss 6.01092243 epoch total loss 6.0293951\n",
      "Trained batch 987 batch loss 5.96570492 epoch total loss 6.02933073\n",
      "Trained batch 988 batch loss 6.20498466 epoch total loss 6.02950859\n",
      "Trained batch 989 batch loss 5.80433178 epoch total loss 6.02928066\n",
      "Trained batch 990 batch loss 5.92938232 epoch total loss 6.02917957\n",
      "Trained batch 991 batch loss 6.10116673 epoch total loss 6.02925205\n",
      "Trained batch 992 batch loss 5.94252777 epoch total loss 6.02916431\n",
      "Trained batch 993 batch loss 6.22411728 epoch total loss 6.02936077\n",
      "Trained batch 994 batch loss 5.82111931 epoch total loss 6.02915144\n",
      "Trained batch 995 batch loss 5.53551912 epoch total loss 6.02865553\n",
      "Trained batch 996 batch loss 6.11893034 epoch total loss 6.02874613\n",
      "Trained batch 997 batch loss 6.20749331 epoch total loss 6.02892542\n",
      "Trained batch 998 batch loss 5.34533215 epoch total loss 6.02824068\n",
      "Trained batch 999 batch loss 5.78280115 epoch total loss 6.02799463\n",
      "Trained batch 1000 batch loss 5.45537949 epoch total loss 6.02742243\n",
      "Trained batch 1001 batch loss 6.01050854 epoch total loss 6.02740574\n",
      "Trained batch 1002 batch loss 6.13304281 epoch total loss 6.02751112\n",
      "Trained batch 1003 batch loss 6.48020744 epoch total loss 6.02796221\n",
      "Trained batch 1004 batch loss 5.60684681 epoch total loss 6.02754259\n",
      "Trained batch 1005 batch loss 6.12950754 epoch total loss 6.02764416\n",
      "Trained batch 1006 batch loss 5.55968189 epoch total loss 6.02717876\n",
      "Trained batch 1007 batch loss 4.52537584 epoch total loss 6.02568722\n",
      "Trained batch 1008 batch loss 5.37423182 epoch total loss 6.0250411\n",
      "Trained batch 1009 batch loss 5.6588335 epoch total loss 6.02467775\n",
      "Trained batch 1010 batch loss 6.14891577 epoch total loss 6.02480078\n",
      "Trained batch 1011 batch loss 4.77994728 epoch total loss 6.02356958\n",
      "Trained batch 1012 batch loss 5.41708279 epoch total loss 6.02297\n",
      "Trained batch 1013 batch loss 5.65439224 epoch total loss 6.0226059\n",
      "Trained batch 1014 batch loss 5.80586386 epoch total loss 6.02239227\n",
      "Trained batch 1015 batch loss 6.16989899 epoch total loss 6.02253723\n",
      "Trained batch 1016 batch loss 5.4795928 epoch total loss 6.0220027\n",
      "Trained batch 1017 batch loss 5.90603 epoch total loss 6.02188921\n",
      "Trained batch 1018 batch loss 5.83893871 epoch total loss 6.02170944\n",
      "Trained batch 1019 batch loss 5.46478319 epoch total loss 6.02116299\n",
      "Trained batch 1020 batch loss 5.90258694 epoch total loss 6.02104616\n",
      "Trained batch 1021 batch loss 6.37773705 epoch total loss 6.02139568\n",
      "Trained batch 1022 batch loss 6.70788527 epoch total loss 6.02206755\n",
      "Trained batch 1023 batch loss 7.58506489 epoch total loss 6.02359533\n",
      "Trained batch 1024 batch loss 5.80116367 epoch total loss 6.02337837\n",
      "Trained batch 1025 batch loss 5.95097446 epoch total loss 6.0233078\n",
      "Trained batch 1026 batch loss 6.0814085 epoch total loss 6.02336454\n",
      "Trained batch 1027 batch loss 5.84275389 epoch total loss 6.02318907\n",
      "Trained batch 1028 batch loss 5.09357357 epoch total loss 6.02228451\n",
      "Trained batch 1029 batch loss 5.36833 epoch total loss 6.02164888\n",
      "Trained batch 1030 batch loss 4.77160788 epoch total loss 6.02043533\n",
      "Trained batch 1031 batch loss 4.9400053 epoch total loss 6.01938725\n",
      "Trained batch 1032 batch loss 5.23161316 epoch total loss 6.01862383\n",
      "Trained batch 1033 batch loss 5.9817276 epoch total loss 6.01858807\n",
      "Trained batch 1034 batch loss 6.33627892 epoch total loss 6.01889563\n",
      "Trained batch 1035 batch loss 5.61576509 epoch total loss 6.01850605\n",
      "Trained batch 1036 batch loss 4.77006912 epoch total loss 6.01730108\n",
      "Trained batch 1037 batch loss 5.45589542 epoch total loss 6.01676\n",
      "Trained batch 1038 batch loss 6.26085043 epoch total loss 6.01699495\n",
      "Trained batch 1039 batch loss 5.97596645 epoch total loss 6.01695538\n",
      "Trained batch 1040 batch loss 5.42343187 epoch total loss 6.0163846\n",
      "Trained batch 1041 batch loss 4.91140652 epoch total loss 6.01532364\n",
      "Trained batch 1042 batch loss 6.07621765 epoch total loss 6.01538181\n",
      "Trained batch 1043 batch loss 6.4395752 epoch total loss 6.01578856\n",
      "Trained batch 1044 batch loss 5.60696316 epoch total loss 6.0153966\n",
      "Trained batch 1045 batch loss 5.18776274 epoch total loss 6.01460505\n",
      "Trained batch 1046 batch loss 5.4499135 epoch total loss 6.01406479\n",
      "Trained batch 1047 batch loss 5.04230928 epoch total loss 6.01313686\n",
      "Trained batch 1048 batch loss 5.40285683 epoch total loss 6.01255465\n",
      "Trained batch 1049 batch loss 6.07135391 epoch total loss 6.01261044\n",
      "Trained batch 1050 batch loss 5.26538849 epoch total loss 6.01189899\n",
      "Trained batch 1051 batch loss 6.26241255 epoch total loss 6.01213741\n",
      "Trained batch 1052 batch loss 6.09306431 epoch total loss 6.01221466\n",
      "Trained batch 1053 batch loss 5.56650162 epoch total loss 6.01179123\n",
      "Trained batch 1054 batch loss 5.40966892 epoch total loss 6.01122\n",
      "Trained batch 1055 batch loss 5.45744801 epoch total loss 6.01069498\n",
      "Trained batch 1056 batch loss 5.36266613 epoch total loss 6.01008129\n",
      "Trained batch 1057 batch loss 5.98272896 epoch total loss 6.01005554\n",
      "Trained batch 1058 batch loss 6.1208849 epoch total loss 6.01016045\n",
      "Trained batch 1059 batch loss 6.16956949 epoch total loss 6.01031113\n",
      "Trained batch 1060 batch loss 4.47088242 epoch total loss 6.00885868\n",
      "Trained batch 1061 batch loss 4.77065849 epoch total loss 6.00769138\n",
      "Trained batch 1062 batch loss 5.42463446 epoch total loss 6.00714254\n",
      "Trained batch 1063 batch loss 5.15023232 epoch total loss 6.00633669\n",
      "Trained batch 1064 batch loss 5.61759949 epoch total loss 6.00597143\n",
      "Trained batch 1065 batch loss 5.77572393 epoch total loss 6.00575542\n",
      "Trained batch 1066 batch loss 5.63894367 epoch total loss 6.00541162\n",
      "Trained batch 1067 batch loss 6.14127731 epoch total loss 6.00553846\n",
      "Trained batch 1068 batch loss 6.43530369 epoch total loss 6.00594139\n",
      "Trained batch 1069 batch loss 6.19446373 epoch total loss 6.00611734\n",
      "Trained batch 1070 batch loss 6.37790966 epoch total loss 6.00646496\n",
      "Trained batch 1071 batch loss 5.98711967 epoch total loss 6.00644684\n",
      "Trained batch 1072 batch loss 6.56524467 epoch total loss 6.0069685\n",
      "Trained batch 1073 batch loss 6.53139687 epoch total loss 6.00745726\n",
      "Trained batch 1074 batch loss 6.04887867 epoch total loss 6.0074954\n",
      "Trained batch 1075 batch loss 6.11679554 epoch total loss 6.00759697\n",
      "Trained batch 1076 batch loss 5.43937302 epoch total loss 6.00706911\n",
      "Trained batch 1077 batch loss 4.67076588 epoch total loss 6.00582838\n",
      "Trained batch 1078 batch loss 5.38990641 epoch total loss 6.00525761\n",
      "Trained batch 1079 batch loss 5.52833652 epoch total loss 6.00481558\n",
      "Trained batch 1080 batch loss 5.64008617 epoch total loss 6.0044775\n",
      "Trained batch 1081 batch loss 5.95826244 epoch total loss 6.00443506\n",
      "Trained batch 1082 batch loss 5.09088326 epoch total loss 6.00359058\n",
      "Trained batch 1083 batch loss 6.28549671 epoch total loss 6.00385141\n",
      "Trained batch 1084 batch loss 4.76593208 epoch total loss 6.00270939\n",
      "Trained batch 1085 batch loss 5.29205751 epoch total loss 6.00205421\n",
      "Trained batch 1086 batch loss 5.77592 epoch total loss 6.00184631\n",
      "Trained batch 1087 batch loss 5.97023487 epoch total loss 6.00181723\n",
      "Trained batch 1088 batch loss 5.98947334 epoch total loss 6.00180531\n",
      "Trained batch 1089 batch loss 5.42498493 epoch total loss 6.00127554\n",
      "Trained batch 1090 batch loss 5.84375906 epoch total loss 6.00113106\n",
      "Trained batch 1091 batch loss 5.59820127 epoch total loss 6.00076151\n",
      "Trained batch 1092 batch loss 4.88211918 epoch total loss 5.99973774\n",
      "Trained batch 1093 batch loss 5.60875511 epoch total loss 5.99938\n",
      "Trained batch 1094 batch loss 5.67421913 epoch total loss 5.99908257\n",
      "Trained batch 1095 batch loss 5.56890202 epoch total loss 5.99869\n",
      "Trained batch 1096 batch loss 5.01308823 epoch total loss 5.99779081\n",
      "Trained batch 1097 batch loss 5.23424149 epoch total loss 5.99709463\n",
      "Trained batch 1098 batch loss 6.05517673 epoch total loss 5.99714756\n",
      "Trained batch 1099 batch loss 5.85301208 epoch total loss 5.99701643\n",
      "Trained batch 1100 batch loss 5.78678226 epoch total loss 5.99682522\n",
      "Trained batch 1101 batch loss 5.88123894 epoch total loss 5.99672031\n",
      "Trained batch 1102 batch loss 5.83119583 epoch total loss 5.99657\n",
      "Trained batch 1103 batch loss 5.83258867 epoch total loss 5.99642134\n",
      "Trained batch 1104 batch loss 6.22360849 epoch total loss 5.99662733\n",
      "Trained batch 1105 batch loss 6.15374 epoch total loss 5.99676943\n",
      "Trained batch 1106 batch loss 5.66308498 epoch total loss 5.99646759\n",
      "Trained batch 1107 batch loss 5.53778315 epoch total loss 5.99605322\n",
      "Trained batch 1108 batch loss 5.74445248 epoch total loss 5.99582624\n",
      "Trained batch 1109 batch loss 6.33029842 epoch total loss 5.99612761\n",
      "Trained batch 1110 batch loss 6.19424534 epoch total loss 5.99630642\n",
      "Trained batch 1111 batch loss 5.53743362 epoch total loss 5.99589348\n",
      "Trained batch 1112 batch loss 5.57762718 epoch total loss 5.99551725\n",
      "Trained batch 1113 batch loss 5.49172 epoch total loss 5.99506474\n",
      "Trained batch 1114 batch loss 4.89891911 epoch total loss 5.99408054\n",
      "Trained batch 1115 batch loss 5.39540625 epoch total loss 5.99354362\n",
      "Trained batch 1116 batch loss 5.09706497 epoch total loss 5.99274063\n",
      "Trained batch 1117 batch loss 5.27979183 epoch total loss 5.99210215\n",
      "Trained batch 1118 batch loss 6.64858437 epoch total loss 5.99268913\n",
      "Trained batch 1119 batch loss 5.60595226 epoch total loss 5.9923439\n",
      "Trained batch 1120 batch loss 6.31953526 epoch total loss 5.99263573\n",
      "Trained batch 1121 batch loss 6.60417891 epoch total loss 5.99318123\n",
      "Trained batch 1122 batch loss 6.79484653 epoch total loss 5.99389553\n",
      "Trained batch 1123 batch loss 6.11410189 epoch total loss 5.99400282\n",
      "Trained batch 1124 batch loss 6.63432932 epoch total loss 5.99457264\n",
      "Trained batch 1125 batch loss 6.16379833 epoch total loss 5.99472284\n",
      "Trained batch 1126 batch loss 6.33182478 epoch total loss 5.9950223\n",
      "Trained batch 1127 batch loss 6.33851624 epoch total loss 5.995327\n",
      "Trained batch 1128 batch loss 6.48168707 epoch total loss 5.99575806\n",
      "Trained batch 1129 batch loss 5.43573427 epoch total loss 5.99526167\n",
      "Trained batch 1130 batch loss 6.39938259 epoch total loss 5.9956193\n",
      "Trained batch 1131 batch loss 6.16266537 epoch total loss 5.99576712\n",
      "Trained batch 1132 batch loss 6.16379261 epoch total loss 5.99591494\n",
      "Trained batch 1133 batch loss 6.16212368 epoch total loss 5.9960618\n",
      "Trained batch 1134 batch loss 6.05454731 epoch total loss 5.99611378\n",
      "Trained batch 1135 batch loss 5.62799358 epoch total loss 5.99578905\n",
      "Trained batch 1136 batch loss 4.60122108 epoch total loss 5.9945612\n",
      "Trained batch 1137 batch loss 4.86154938 epoch total loss 5.99356461\n",
      "Trained batch 1138 batch loss 5.06495571 epoch total loss 5.99274874\n",
      "Trained batch 1139 batch loss 5.22573 epoch total loss 5.99207497\n",
      "Trained batch 1140 batch loss 5.36524773 epoch total loss 5.99152517\n",
      "Trained batch 1141 batch loss 4.77539062 epoch total loss 5.99045944\n",
      "Trained batch 1142 batch loss 5.0241518 epoch total loss 5.98961306\n",
      "Trained batch 1143 batch loss 5.78524637 epoch total loss 5.98943424\n",
      "Trained batch 1144 batch loss 5.92170238 epoch total loss 5.98937511\n",
      "Trained batch 1145 batch loss 6.24573898 epoch total loss 5.98959875\n",
      "Trained batch 1146 batch loss 6.3403573 epoch total loss 5.98990488\n",
      "Trained batch 1147 batch loss 6.08274078 epoch total loss 5.98998594\n",
      "Trained batch 1148 batch loss 5.94191 epoch total loss 5.98994398\n",
      "Trained batch 1149 batch loss 5.89110041 epoch total loss 5.98985767\n",
      "Trained batch 1150 batch loss 5.96958113 epoch total loss 5.98984051\n",
      "Trained batch 1151 batch loss 6.56812859 epoch total loss 5.99034309\n",
      "Trained batch 1152 batch loss 5.96927071 epoch total loss 5.9903245\n",
      "Trained batch 1153 batch loss 6.09518242 epoch total loss 5.99041557\n",
      "Trained batch 1154 batch loss 5.65361 epoch total loss 5.99012375\n",
      "Trained batch 1155 batch loss 5.97830439 epoch total loss 5.99011374\n",
      "Trained batch 1156 batch loss 5.95522118 epoch total loss 5.99008369\n",
      "Trained batch 1157 batch loss 5.65605259 epoch total loss 5.98979521\n",
      "Trained batch 1158 batch loss 6.03710556 epoch total loss 5.98983574\n",
      "Trained batch 1159 batch loss 6.06687164 epoch total loss 5.9899025\n",
      "Trained batch 1160 batch loss 5.94458199 epoch total loss 5.9898634\n",
      "Trained batch 1161 batch loss 6.31821775 epoch total loss 5.99014664\n",
      "Trained batch 1162 batch loss 6.19663286 epoch total loss 5.9903245\n",
      "Trained batch 1163 batch loss 5.91137838 epoch total loss 5.99025679\n",
      "Trained batch 1164 batch loss 5.72757626 epoch total loss 5.99003077\n",
      "Trained batch 1165 batch loss 5.67909145 epoch total loss 5.98976421\n",
      "Trained batch 1166 batch loss 5.93636036 epoch total loss 5.98971844\n",
      "Trained batch 1167 batch loss 5.64186239 epoch total loss 5.98942041\n",
      "Trained batch 1168 batch loss 5.73262405 epoch total loss 5.98920059\n",
      "Trained batch 1169 batch loss 6.0219388 epoch total loss 5.98922873\n",
      "Trained batch 1170 batch loss 5.97328472 epoch total loss 5.9892149\n",
      "Trained batch 1171 batch loss 5.99186754 epoch total loss 5.9892168\n",
      "Trained batch 1172 batch loss 6.1307807 epoch total loss 5.98933792\n",
      "Trained batch 1173 batch loss 6.11555576 epoch total loss 5.98944569\n",
      "Trained batch 1174 batch loss 5.87274265 epoch total loss 5.98934603\n",
      "Trained batch 1175 batch loss 6.11361456 epoch total loss 5.98945189\n",
      "Trained batch 1176 batch loss 5.86540604 epoch total loss 5.98934603\n",
      "Trained batch 1177 batch loss 6.11102533 epoch total loss 5.9894495\n",
      "Trained batch 1178 batch loss 6.35712862 epoch total loss 5.98976135\n",
      "Trained batch 1179 batch loss 5.92923927 epoch total loss 5.98971\n",
      "Trained batch 1180 batch loss 5.58334208 epoch total loss 5.98936558\n",
      "Trained batch 1181 batch loss 6.02392101 epoch total loss 5.98939514\n",
      "Trained batch 1182 batch loss 5.61677742 epoch total loss 5.98908\n",
      "Trained batch 1183 batch loss 5.56462669 epoch total loss 5.98872089\n",
      "Trained batch 1184 batch loss 4.89012575 epoch total loss 5.98779297\n",
      "Trained batch 1185 batch loss 5.79484367 epoch total loss 5.98763037\n",
      "Trained batch 1186 batch loss 5.17023563 epoch total loss 5.98694134\n",
      "Trained batch 1187 batch loss 4.59565163 epoch total loss 5.98576927\n",
      "Trained batch 1188 batch loss 5.77061081 epoch total loss 5.98558807\n",
      "Trained batch 1189 batch loss 6.07916164 epoch total loss 5.98566675\n",
      "Trained batch 1190 batch loss 5.3326664 epoch total loss 5.98511791\n",
      "Trained batch 1191 batch loss 5.47336674 epoch total loss 5.98468781\n",
      "Trained batch 1192 batch loss 5.85040522 epoch total loss 5.98457527\n",
      "Trained batch 1193 batch loss 5.74216938 epoch total loss 5.98437214\n",
      "Trained batch 1194 batch loss 5.50383663 epoch total loss 5.98396969\n",
      "Trained batch 1195 batch loss 5.43597269 epoch total loss 5.98351145\n",
      "Trained batch 1196 batch loss 5.55322266 epoch total loss 5.98315144\n",
      "Trained batch 1197 batch loss 5.84331703 epoch total loss 5.98303461\n",
      "Trained batch 1198 batch loss 4.87757 epoch total loss 5.98211193\n",
      "Trained batch 1199 batch loss 4.81952095 epoch total loss 5.98114204\n",
      "Trained batch 1200 batch loss 5.48472595 epoch total loss 5.98072815\n",
      "Trained batch 1201 batch loss 6.09422636 epoch total loss 5.98082304\n",
      "Trained batch 1202 batch loss 6.37978935 epoch total loss 5.98115492\n",
      "Trained batch 1203 batch loss 6.73651123 epoch total loss 5.98178244\n",
      "Trained batch 1204 batch loss 5.88437939 epoch total loss 5.98170185\n",
      "Trained batch 1205 batch loss 5.54729843 epoch total loss 5.98134136\n",
      "Trained batch 1206 batch loss 5.84194613 epoch total loss 5.98122549\n",
      "Trained batch 1207 batch loss 6.33615303 epoch total loss 5.98151922\n",
      "Trained batch 1208 batch loss 6.26682663 epoch total loss 5.98175526\n",
      "Trained batch 1209 batch loss 6.49982738 epoch total loss 5.98218393\n",
      "Trained batch 1210 batch loss 5.4341011 epoch total loss 5.98173094\n",
      "Trained batch 1211 batch loss 4.36094284 epoch total loss 5.98039246\n",
      "Trained batch 1212 batch loss 4.10831356 epoch total loss 5.97884798\n",
      "Trained batch 1213 batch loss 4.21543598 epoch total loss 5.9773941\n",
      "Trained batch 1214 batch loss 4.47833824 epoch total loss 5.97615957\n",
      "Trained batch 1215 batch loss 5.72214222 epoch total loss 5.97595024\n",
      "Trained batch 1216 batch loss 5.2914238 epoch total loss 5.97538757\n",
      "Trained batch 1217 batch loss 5.57390785 epoch total loss 5.9750576\n",
      "Trained batch 1218 batch loss 6.01477146 epoch total loss 5.97509\n",
      "Trained batch 1219 batch loss 5.26104403 epoch total loss 5.97450447\n",
      "Trained batch 1220 batch loss 5.39490843 epoch total loss 5.97402954\n",
      "Trained batch 1221 batch loss 5.57293844 epoch total loss 5.973701\n",
      "Trained batch 1222 batch loss 5.65000439 epoch total loss 5.97343588\n",
      "Trained batch 1223 batch loss 6.42809772 epoch total loss 5.97380781\n",
      "Trained batch 1224 batch loss 5.29464436 epoch total loss 5.97325277\n",
      "Trained batch 1225 batch loss 6.00340462 epoch total loss 5.97327709\n",
      "Trained batch 1226 batch loss 6.0913806 epoch total loss 5.97337341\n",
      "Trained batch 1227 batch loss 6.82034731 epoch total loss 5.97406387\n",
      "Trained batch 1228 batch loss 6.51751947 epoch total loss 5.97450638\n",
      "Trained batch 1229 batch loss 7.04560661 epoch total loss 5.97537756\n",
      "Trained batch 1230 batch loss 6.42718506 epoch total loss 5.9757452\n",
      "Trained batch 1231 batch loss 4.80388832 epoch total loss 5.97479296\n",
      "Trained batch 1232 batch loss 6.11153221 epoch total loss 5.97490406\n",
      "Trained batch 1233 batch loss 6.22864151 epoch total loss 5.97510958\n",
      "Trained batch 1234 batch loss 6.49667645 epoch total loss 5.97553205\n",
      "Trained batch 1235 batch loss 5.9206109 epoch total loss 5.97548771\n",
      "Trained batch 1236 batch loss 6.20430231 epoch total loss 5.97567225\n",
      "Trained batch 1237 batch loss 6.81285 epoch total loss 5.97634935\n",
      "Trained batch 1238 batch loss 6.85205555 epoch total loss 5.9770565\n",
      "Trained batch 1239 batch loss 5.72289181 epoch total loss 5.97685146\n",
      "Trained batch 1240 batch loss 6.49293423 epoch total loss 5.97726774\n",
      "Trained batch 1241 batch loss 6.84996462 epoch total loss 5.97797108\n",
      "Trained batch 1242 batch loss 6.75056076 epoch total loss 5.97859287\n",
      "Trained batch 1243 batch loss 6.61375618 epoch total loss 5.97910404\n",
      "Trained batch 1244 batch loss 6.92831612 epoch total loss 5.97986698\n",
      "Trained batch 1245 batch loss 7.11054802 epoch total loss 5.98077488\n",
      "Trained batch 1246 batch loss 7.191329 epoch total loss 5.98174667\n",
      "Trained batch 1247 batch loss 6.89755154 epoch total loss 5.982481\n",
      "Trained batch 1248 batch loss 7.47835064 epoch total loss 5.98368\n",
      "Trained batch 1249 batch loss 6.90233803 epoch total loss 5.98441505\n",
      "Trained batch 1250 batch loss 6.79945183 epoch total loss 5.98506737\n",
      "Trained batch 1251 batch loss 6.60863972 epoch total loss 5.98556566\n",
      "Trained batch 1252 batch loss 6.90769053 epoch total loss 5.9863019\n",
      "Trained batch 1253 batch loss 6.85530376 epoch total loss 5.9869957\n",
      "Trained batch 1254 batch loss 6.83392191 epoch total loss 5.9876709\n",
      "Trained batch 1255 batch loss 6.87985897 epoch total loss 5.98838186\n",
      "Trained batch 1256 batch loss 6.48753357 epoch total loss 5.98877907\n",
      "Trained batch 1257 batch loss 6.64867401 epoch total loss 5.98930407\n",
      "Trained batch 1258 batch loss 6.46815825 epoch total loss 5.98968458\n",
      "Trained batch 1259 batch loss 6.35567665 epoch total loss 5.98997545\n",
      "Trained batch 1260 batch loss 5.69979382 epoch total loss 5.98974514\n",
      "Trained batch 1261 batch loss 6.3636961 epoch total loss 5.99004173\n",
      "Trained batch 1262 batch loss 6.56742859 epoch total loss 5.99049902\n",
      "Trained batch 1263 batch loss 7.12305784 epoch total loss 5.99139595\n",
      "Trained batch 1264 batch loss 6.43623972 epoch total loss 5.99174738\n",
      "Trained batch 1265 batch loss 6.80833054 epoch total loss 5.99239302\n",
      "Trained batch 1266 batch loss 6.50096321 epoch total loss 5.99279451\n",
      "Trained batch 1267 batch loss 6.76072598 epoch total loss 5.99340057\n",
      "Trained batch 1268 batch loss 5.88310909 epoch total loss 5.99331379\n",
      "Trained batch 1269 batch loss 5.93200731 epoch total loss 5.99326563\n",
      "Trained batch 1270 batch loss 5.92614841 epoch total loss 5.9932127\n",
      "Trained batch 1271 batch loss 5.57386637 epoch total loss 5.99288273\n",
      "Trained batch 1272 batch loss 6.11701632 epoch total loss 5.99298048\n",
      "Trained batch 1273 batch loss 6.51770353 epoch total loss 5.99339247\n",
      "Trained batch 1274 batch loss 6.46463 epoch total loss 5.99376249\n",
      "Trained batch 1275 batch loss 6.64984703 epoch total loss 5.99427748\n",
      "Trained batch 1276 batch loss 5.96298647 epoch total loss 5.99425268\n",
      "Trained batch 1277 batch loss 5.58981323 epoch total loss 5.99393606\n",
      "Trained batch 1278 batch loss 6.7523 epoch total loss 5.99452972\n",
      "Trained batch 1279 batch loss 6.85529423 epoch total loss 5.99520254\n",
      "Trained batch 1280 batch loss 6.80538034 epoch total loss 5.9958353\n",
      "Trained batch 1281 batch loss 6.42684698 epoch total loss 5.99617195\n",
      "Trained batch 1282 batch loss 6.49294853 epoch total loss 5.99655962\n",
      "Trained batch 1283 batch loss 5.39366722 epoch total loss 5.99608946\n",
      "Trained batch 1284 batch loss 6.13248444 epoch total loss 5.99619579\n",
      "Trained batch 1285 batch loss 6.41982412 epoch total loss 5.99652529\n",
      "Trained batch 1286 batch loss 5.19432878 epoch total loss 5.99590158\n",
      "Trained batch 1287 batch loss 5.82945251 epoch total loss 5.99577236\n",
      "Trained batch 1288 batch loss 4.88671637 epoch total loss 5.99491119\n",
      "Trained batch 1289 batch loss 5.22823238 epoch total loss 5.99431658\n",
      "Trained batch 1290 batch loss 4.96997452 epoch total loss 5.99352264\n",
      "Trained batch 1291 batch loss 4.01733637 epoch total loss 5.991992\n",
      "Trained batch 1292 batch loss 4.83916473 epoch total loss 5.9911\n",
      "Trained batch 1293 batch loss 4.65670347 epoch total loss 5.99006796\n",
      "Trained batch 1294 batch loss 4.421731 epoch total loss 5.98885584\n",
      "Trained batch 1295 batch loss 6.12711859 epoch total loss 5.98896265\n",
      "Trained batch 1296 batch loss 6.38155651 epoch total loss 5.98926544\n",
      "Trained batch 1297 batch loss 6.29124403 epoch total loss 5.98949814\n",
      "Trained batch 1298 batch loss 6.3351264 epoch total loss 5.98976421\n",
      "Trained batch 1299 batch loss 6.2474432 epoch total loss 5.98996258\n",
      "Trained batch 1300 batch loss 6.05512285 epoch total loss 5.99001265\n",
      "Trained batch 1301 batch loss 6.15743637 epoch total loss 5.99014139\n",
      "Trained batch 1302 batch loss 6.39456606 epoch total loss 5.99045181\n",
      "Trained batch 1303 batch loss 6.42723942 epoch total loss 5.99078703\n",
      "Trained batch 1304 batch loss 6.38329315 epoch total loss 5.99108791\n",
      "Trained batch 1305 batch loss 6.15582514 epoch total loss 5.99121428\n",
      "Trained batch 1306 batch loss 6.32888079 epoch total loss 5.9914732\n",
      "Trained batch 1307 batch loss 5.81201553 epoch total loss 5.99133587\n",
      "Trained batch 1308 batch loss 6.27231503 epoch total loss 5.99155045\n",
      "Trained batch 1309 batch loss 6.53015041 epoch total loss 5.99196243\n",
      "Trained batch 1310 batch loss 6.66622114 epoch total loss 5.99247694\n",
      "Trained batch 1311 batch loss 6.87260914 epoch total loss 5.99314785\n",
      "Trained batch 1312 batch loss 6.93466377 epoch total loss 5.99386549\n",
      "Trained batch 1313 batch loss 6.70394707 epoch total loss 5.9944067\n",
      "Trained batch 1314 batch loss 6.23677349 epoch total loss 5.99459124\n",
      "Trained batch 1315 batch loss 6.63756371 epoch total loss 5.99508\n",
      "Trained batch 1316 batch loss 7.55079746 epoch total loss 5.99626207\n",
      "Trained batch 1317 batch loss 5.20887 epoch total loss 5.9956646\n",
      "Trained batch 1318 batch loss 5.66953754 epoch total loss 5.99541712\n",
      "Trained batch 1319 batch loss 6.20999813 epoch total loss 5.99557972\n",
      "Trained batch 1320 batch loss 5.93529892 epoch total loss 5.99553394\n",
      "Trained batch 1321 batch loss 6.1781373 epoch total loss 5.99567223\n",
      "Trained batch 1322 batch loss 5.49085093 epoch total loss 5.99529028\n",
      "Trained batch 1323 batch loss 5.33096123 epoch total loss 5.99478817\n",
      "Trained batch 1324 batch loss 5.5360055 epoch total loss 5.99444151\n",
      "Trained batch 1325 batch loss 6.42178154 epoch total loss 5.99476433\n",
      "Trained batch 1326 batch loss 6.16412067 epoch total loss 5.99489164\n",
      "Trained batch 1327 batch loss 5.24807167 epoch total loss 5.99432898\n",
      "Trained batch 1328 batch loss 5.94540119 epoch total loss 5.99429226\n",
      "Trained batch 1329 batch loss 5.98954964 epoch total loss 5.99428892\n",
      "Trained batch 1330 batch loss 5.87973785 epoch total loss 5.99420261\n",
      "Trained batch 1331 batch loss 5.82846165 epoch total loss 5.99407816\n",
      "Trained batch 1332 batch loss 6.05518389 epoch total loss 5.99412394\n",
      "Trained batch 1333 batch loss 4.63232708 epoch total loss 5.99310255\n",
      "Trained batch 1334 batch loss 5.05333805 epoch total loss 5.99239779\n",
      "Trained batch 1335 batch loss 6.06982088 epoch total loss 5.99245596\n",
      "Trained batch 1336 batch loss 5.59958506 epoch total loss 5.99216175\n",
      "Trained batch 1337 batch loss 5.49759197 epoch total loss 5.9917922\n",
      "Trained batch 1338 batch loss 5.56186 epoch total loss 5.99147081\n",
      "Trained batch 1339 batch loss 6.39122581 epoch total loss 5.99176931\n",
      "Trained batch 1340 batch loss 5.36148834 epoch total loss 5.99129868\n",
      "Trained batch 1341 batch loss 5.99938822 epoch total loss 5.99130487\n",
      "Trained batch 1342 batch loss 6.2117033 epoch total loss 5.99146938\n",
      "Trained batch 1343 batch loss 5.91253662 epoch total loss 5.99141073\n",
      "Trained batch 1344 batch loss 5.67194843 epoch total loss 5.99117279\n",
      "Trained batch 1345 batch loss 5.90808773 epoch total loss 5.99111128\n",
      "Trained batch 1346 batch loss 5.63626 epoch total loss 5.99084759\n",
      "Trained batch 1347 batch loss 5.93616867 epoch total loss 5.99080658\n",
      "Trained batch 1348 batch loss 6.18544197 epoch total loss 5.99095106\n",
      "Trained batch 1349 batch loss 5.74487591 epoch total loss 5.99076891\n",
      "Trained batch 1350 batch loss 5.65852356 epoch total loss 5.99052286\n",
      "Trained batch 1351 batch loss 4.50168705 epoch total loss 5.98942089\n",
      "Trained batch 1352 batch loss 6.1036315 epoch total loss 5.98950529\n",
      "Trained batch 1353 batch loss 5.992136 epoch total loss 5.9895072\n",
      "Trained batch 1354 batch loss 4.83170319 epoch total loss 5.98865175\n",
      "Trained batch 1355 batch loss 6.31193352 epoch total loss 5.98889065\n",
      "Trained batch 1356 batch loss 5.67995739 epoch total loss 5.98866272\n",
      "Trained batch 1357 batch loss 5.0980978 epoch total loss 5.98800659\n",
      "Trained batch 1358 batch loss 5.12766027 epoch total loss 5.98737288\n",
      "Trained batch 1359 batch loss 5.36057854 epoch total loss 5.98691177\n",
      "Trained batch 1360 batch loss 5.51041555 epoch total loss 5.9865613\n",
      "Trained batch 1361 batch loss 5.20068073 epoch total loss 5.98598385\n",
      "Trained batch 1362 batch loss 5.95272827 epoch total loss 5.98595905\n",
      "Trained batch 1363 batch loss 5.93517303 epoch total loss 5.98592186\n",
      "Trained batch 1364 batch loss 6.40996027 epoch total loss 5.98623276\n",
      "Trained batch 1365 batch loss 5.69466925 epoch total loss 5.98601961\n",
      "Trained batch 1366 batch loss 6.51622295 epoch total loss 5.98640728\n",
      "Trained batch 1367 batch loss 6.26254082 epoch total loss 5.98660946\n",
      "Trained batch 1368 batch loss 5.52509069 epoch total loss 5.98627186\n",
      "Trained batch 1369 batch loss 6.1392889 epoch total loss 5.98638391\n",
      "Trained batch 1370 batch loss 5.29311562 epoch total loss 5.98587751\n",
      "Trained batch 1371 batch loss 5.31348467 epoch total loss 5.98538733\n",
      "Trained batch 1372 batch loss 5.27669477 epoch total loss 5.98487043\n",
      "Trained batch 1373 batch loss 5.25766563 epoch total loss 5.98434067\n",
      "Trained batch 1374 batch loss 5.30639458 epoch total loss 5.98384762\n",
      "Trained batch 1375 batch loss 5.87585497 epoch total loss 5.98376894\n",
      "Trained batch 1376 batch loss 6.06032276 epoch total loss 5.98382521\n",
      "Trained batch 1377 batch loss 5.64247084 epoch total loss 5.98357725\n",
      "Trained batch 1378 batch loss 5.3579092 epoch total loss 5.98312283\n",
      "Trained batch 1379 batch loss 5.87648344 epoch total loss 5.98304558\n",
      "Trained batch 1380 batch loss 6.18334484 epoch total loss 5.98319101\n",
      "Trained batch 1381 batch loss 6.0695529 epoch total loss 5.98325348\n",
      "Trained batch 1382 batch loss 5.92882395 epoch total loss 5.9832139\n",
      "Trained batch 1383 batch loss 5.88579369 epoch total loss 5.98314333\n",
      "Trained batch 1384 batch loss 6.47300768 epoch total loss 5.98349714\n",
      "Trained batch 1385 batch loss 6.05604458 epoch total loss 5.98354912\n",
      "Trained batch 1386 batch loss 6.22111273 epoch total loss 5.9837203\n",
      "Trained batch 1387 batch loss 5.48571396 epoch total loss 5.98336124\n",
      "Trained batch 1388 batch loss 6.02105331 epoch total loss 5.98338842\n",
      "Trained batch 1389 batch loss 6.18940926 epoch total loss 5.98353672\n",
      "Trained batch 1390 batch loss 6.84734535 epoch total loss 5.98415852\n",
      "Trained batch 1391 batch loss 6.2168889 epoch total loss 5.98432589\n",
      "Trained batch 1392 batch loss 5.79467869 epoch total loss 5.98419\n",
      "Trained batch 1393 batch loss 6.63444948 epoch total loss 5.98465681\n",
      "Trained batch 1394 batch loss 6.77317572 epoch total loss 5.98522282\n",
      "Trained batch 1395 batch loss 6.25599861 epoch total loss 5.98541689\n",
      "Trained batch 1396 batch loss 6.60345268 epoch total loss 5.98585939\n",
      "Trained batch 1397 batch loss 4.87704372 epoch total loss 5.98506546\n",
      "Trained batch 1398 batch loss 5.58337402 epoch total loss 5.98477793\n",
      "Trained batch 1399 batch loss 5.70624495 epoch total loss 5.98457861\n",
      "Trained batch 1400 batch loss 6.01868153 epoch total loss 5.98460293\n",
      "Trained batch 1401 batch loss 5.61267662 epoch total loss 5.98433733\n",
      "Trained batch 1402 batch loss 5.94981623 epoch total loss 5.98431301\n",
      "Trained batch 1403 batch loss 6.34429264 epoch total loss 5.98457\n",
      "Trained batch 1404 batch loss 6.63132477 epoch total loss 5.98503\n",
      "Trained batch 1405 batch loss 5.7591238 epoch total loss 5.984869\n",
      "Trained batch 1406 batch loss 5.84012032 epoch total loss 5.98476601\n",
      "Trained batch 1407 batch loss 5.5862751 epoch total loss 5.98448277\n",
      "Trained batch 1408 batch loss 5.09844255 epoch total loss 5.98385334\n",
      "Trained batch 1409 batch loss 6.18594646 epoch total loss 5.98399639\n",
      "Trained batch 1410 batch loss 6.57435751 epoch total loss 5.98441505\n",
      "Trained batch 1411 batch loss 5.74711704 epoch total loss 5.98424673\n",
      "Trained batch 1412 batch loss 5.96900463 epoch total loss 5.98423576\n",
      "Trained batch 1413 batch loss 6.19123173 epoch total loss 5.98438263\n",
      "Trained batch 1414 batch loss 6.13767052 epoch total loss 5.98449087\n",
      "Trained batch 1415 batch loss 5.66680193 epoch total loss 5.98426676\n",
      "Trained batch 1416 batch loss 6.52257919 epoch total loss 5.9846468\n",
      "Trained batch 1417 batch loss 6.17784691 epoch total loss 5.98478317\n",
      "Trained batch 1418 batch loss 6.11606741 epoch total loss 5.98487568\n",
      "Trained batch 1419 batch loss 6.93876457 epoch total loss 5.98554754\n",
      "Trained batch 1420 batch loss 5.67584133 epoch total loss 5.98532963\n",
      "Trained batch 1421 batch loss 5.29854774 epoch total loss 5.98484659\n",
      "Trained batch 1422 batch loss 6.17332554 epoch total loss 5.98497868\n",
      "Trained batch 1423 batch loss 5.84234619 epoch total loss 5.98487854\n",
      "Trained batch 1424 batch loss 4.0344696 epoch total loss 5.98350906\n",
      "Trained batch 1425 batch loss 4.87043953 epoch total loss 5.98272753\n",
      "Trained batch 1426 batch loss 4.51755905 epoch total loss 5.9817\n",
      "Trained batch 1427 batch loss 4.91685 epoch total loss 5.98095417\n",
      "Trained batch 1428 batch loss 4.70430088 epoch total loss 5.98005962\n",
      "Trained batch 1429 batch loss 5.69467878 epoch total loss 5.97986\n",
      "Trained batch 1430 batch loss 5.70936966 epoch total loss 5.97967052\n",
      "Trained batch 1431 batch loss 5.6980896 epoch total loss 5.97947359\n",
      "Trained batch 1432 batch loss 6.51103687 epoch total loss 5.97984457\n",
      "Trained batch 1433 batch loss 5.71493149 epoch total loss 5.97966\n",
      "Trained batch 1434 batch loss 5.725564 epoch total loss 5.97948265\n",
      "Trained batch 1435 batch loss 5.98448467 epoch total loss 5.97948599\n",
      "Trained batch 1436 batch loss 6.458498 epoch total loss 5.97982\n",
      "Trained batch 1437 batch loss 6.3145709 epoch total loss 5.98005295\n",
      "Trained batch 1438 batch loss 5.8257513 epoch total loss 5.97994566\n",
      "Trained batch 1439 batch loss 5.52124405 epoch total loss 5.97962713\n",
      "Trained batch 1440 batch loss 6.17738867 epoch total loss 5.97976494\n",
      "Trained batch 1441 batch loss 6.05631208 epoch total loss 5.97981834\n",
      "Trained batch 1442 batch loss 6.10858965 epoch total loss 5.97990751\n",
      "Trained batch 1443 batch loss 5.88742447 epoch total loss 5.97984362\n",
      "Trained batch 1444 batch loss 6.32240295 epoch total loss 5.9800806\n",
      "Trained batch 1445 batch loss 5.78871727 epoch total loss 5.97994852\n",
      "Trained batch 1446 batch loss 6.86404848 epoch total loss 5.98056\n",
      "Trained batch 1447 batch loss 5.42878914 epoch total loss 5.98017836\n",
      "Trained batch 1448 batch loss 4.76856852 epoch total loss 5.97934198\n",
      "Trained batch 1449 batch loss 6.14893341 epoch total loss 5.97945929\n",
      "Trained batch 1450 batch loss 5.92032 epoch total loss 5.97941828\n",
      "Trained batch 1451 batch loss 6.08786964 epoch total loss 5.97949266\n",
      "Trained batch 1452 batch loss 6.5724473 epoch total loss 5.97990131\n",
      "Trained batch 1453 batch loss 5.64392376 epoch total loss 5.97966957\n",
      "Trained batch 1454 batch loss 6.28174925 epoch total loss 5.97987747\n",
      "Trained batch 1455 batch loss 6.14006615 epoch total loss 5.97998762\n",
      "Trained batch 1456 batch loss 6.06236076 epoch total loss 5.98004436\n",
      "Trained batch 1457 batch loss 6.20598555 epoch total loss 5.98019934\n",
      "Trained batch 1458 batch loss 5.65153313 epoch total loss 5.97997379\n",
      "Trained batch 1459 batch loss 5.60015488 epoch total loss 5.97971392\n",
      "Trained batch 1460 batch loss 5.56166553 epoch total loss 5.97942734\n",
      "Trained batch 1461 batch loss 4.82982302 epoch total loss 5.97864056\n",
      "Trained batch 1462 batch loss 5.77710962 epoch total loss 5.97850275\n",
      "Trained batch 1463 batch loss 5.49899864 epoch total loss 5.97817516\n",
      "Trained batch 1464 batch loss 5.43077278 epoch total loss 5.97780132\n",
      "Trained batch 1465 batch loss 5.59389305 epoch total loss 5.97753906\n",
      "Trained batch 1466 batch loss 5.46670055 epoch total loss 5.97719049\n",
      "Trained batch 1467 batch loss 5.45550585 epoch total loss 5.97683477\n",
      "Trained batch 1468 batch loss 5.4783659 epoch total loss 5.97649527\n",
      "Trained batch 1469 batch loss 5.67611599 epoch total loss 5.9762907\n",
      "Trained batch 1470 batch loss 5.91093159 epoch total loss 5.97624636\n",
      "Trained batch 1471 batch loss 5.26598 epoch total loss 5.97576332\n",
      "Trained batch 1472 batch loss 6.252666 epoch total loss 5.97595167\n",
      "Trained batch 1473 batch loss 5.18682861 epoch total loss 5.97541571\n",
      "Trained batch 1474 batch loss 5.57846212 epoch total loss 5.97514582\n",
      "Trained batch 1475 batch loss 5.49702311 epoch total loss 5.97482204\n",
      "Trained batch 1476 batch loss 6.89072609 epoch total loss 5.97544241\n",
      "Trained batch 1477 batch loss 5.823452 epoch total loss 5.97533941\n",
      "Trained batch 1478 batch loss 6.30275679 epoch total loss 5.97556067\n",
      "Trained batch 1479 batch loss 4.60261917 epoch total loss 5.97463226\n",
      "Trained batch 1480 batch loss 5.37729931 epoch total loss 5.97422886\n",
      "Trained batch 1481 batch loss 4.93402481 epoch total loss 5.973526\n",
      "Trained batch 1482 batch loss 6.02416515 epoch total loss 5.97356033\n",
      "Trained batch 1483 batch loss 6.25544214 epoch total loss 5.97375059\n",
      "Trained batch 1484 batch loss 6.00684786 epoch total loss 5.973773\n",
      "Trained batch 1485 batch loss 6.07301044 epoch total loss 5.97383976\n",
      "Trained batch 1486 batch loss 6.32036352 epoch total loss 5.97407293\n",
      "Trained batch 1487 batch loss 6.65350628 epoch total loss 5.97452974\n",
      "Trained batch 1488 batch loss 6.17351341 epoch total loss 5.97466373\n",
      "Trained batch 1489 batch loss 4.69043684 epoch total loss 5.97380114\n",
      "Trained batch 1490 batch loss 6.24436569 epoch total loss 5.97398281\n",
      "Trained batch 1491 batch loss 5.80307 epoch total loss 5.97386789\n",
      "Trained batch 1492 batch loss 5.21553326 epoch total loss 5.97336\n",
      "Trained batch 1493 batch loss 4.5512991 epoch total loss 5.97240782\n",
      "Trained batch 1494 batch loss 4.91303158 epoch total loss 5.97169876\n",
      "Trained batch 1495 batch loss 5.58069897 epoch total loss 5.97143745\n",
      "Trained batch 1496 batch loss 6.25365973 epoch total loss 5.97162628\n",
      "Trained batch 1497 batch loss 5.67163515 epoch total loss 5.97142601\n",
      "Trained batch 1498 batch loss 5.01856852 epoch total loss 5.97079\n",
      "Trained batch 1499 batch loss 5.4093523 epoch total loss 5.97041512\n",
      "Trained batch 1500 batch loss 5.26007652 epoch total loss 5.96994162\n",
      "Trained batch 1501 batch loss 5.68696499 epoch total loss 5.96975279\n",
      "Trained batch 1502 batch loss 6.6026 epoch total loss 5.97017384\n",
      "Trained batch 1503 batch loss 5.83672142 epoch total loss 5.97008514\n",
      "Trained batch 1504 batch loss 5.30435562 epoch total loss 5.96964264\n",
      "Trained batch 1505 batch loss 5.82162476 epoch total loss 5.96954441\n",
      "Trained batch 1506 batch loss 6.08617687 epoch total loss 5.96962166\n",
      "Trained batch 1507 batch loss 5.34903336 epoch total loss 5.96920967\n",
      "Trained batch 1508 batch loss 4.86149883 epoch total loss 5.96847486\n",
      "Trained batch 1509 batch loss 5.67397261 epoch total loss 5.96827936\n",
      "Trained batch 1510 batch loss 6.11943 epoch total loss 5.9683795\n",
      "Trained batch 1511 batch loss 6.08684683 epoch total loss 5.9684577\n",
      "Trained batch 1512 batch loss 6.23147583 epoch total loss 5.96863174\n",
      "Trained batch 1513 batch loss 6.30944681 epoch total loss 5.96885729\n",
      "Trained batch 1514 batch loss 5.95764446 epoch total loss 5.96885\n",
      "Trained batch 1515 batch loss 5.50052357 epoch total loss 5.96854115\n",
      "Trained batch 1516 batch loss 5.89808559 epoch total loss 5.96849489\n",
      "Trained batch 1517 batch loss 5.96431351 epoch total loss 5.96849203\n",
      "Trained batch 1518 batch loss 5.66254759 epoch total loss 5.96829\n",
      "Trained batch 1519 batch loss 5.26026154 epoch total loss 5.96782446\n",
      "Trained batch 1520 batch loss 5.80826902 epoch total loss 5.96771955\n",
      "Trained batch 1521 batch loss 6.23588228 epoch total loss 5.96789598\n",
      "Trained batch 1522 batch loss 6.33349323 epoch total loss 5.96813583\n",
      "Trained batch 1523 batch loss 6.18039417 epoch total loss 5.96827555\n",
      "Trained batch 1524 batch loss 6.52388859 epoch total loss 5.96864\n",
      "Trained batch 1525 batch loss 8.02451897 epoch total loss 5.96998787\n",
      "Trained batch 1526 batch loss 7.16644287 epoch total loss 5.97077179\n",
      "Trained batch 1527 batch loss 5.43574333 epoch total loss 5.97042131\n",
      "Trained batch 1528 batch loss 7.78127956 epoch total loss 5.97160625\n",
      "Trained batch 1529 batch loss 7.03588343 epoch total loss 5.97230244\n",
      "Trained batch 1530 batch loss 6.93413162 epoch total loss 5.97293139\n",
      "Trained batch 1531 batch loss 7.17678452 epoch total loss 5.97371769\n",
      "Trained batch 1532 batch loss 6.27531624 epoch total loss 5.97391462\n",
      "Trained batch 1533 batch loss 6.2825985 epoch total loss 5.97411585\n",
      "Trained batch 1534 batch loss 6.38092422 epoch total loss 5.97438097\n",
      "Trained batch 1535 batch loss 6.72145 epoch total loss 5.97486782\n",
      "Trained batch 1536 batch loss 5.62898 epoch total loss 5.97464228\n",
      "Trained batch 1537 batch loss 6.45560884 epoch total loss 5.97495556\n",
      "Trained batch 1538 batch loss 6.52601242 epoch total loss 5.97531414\n",
      "Trained batch 1539 batch loss 6.71278667 epoch total loss 5.97579336\n",
      "Trained batch 1540 batch loss 6.58939743 epoch total loss 5.976192\n",
      "Trained batch 1541 batch loss 6.58429384 epoch total loss 5.97658682\n",
      "Trained batch 1542 batch loss 6.94105339 epoch total loss 5.97721243\n",
      "Trained batch 1543 batch loss 6.54899406 epoch total loss 5.97758293\n",
      "Trained batch 1544 batch loss 6.42141342 epoch total loss 5.97787046\n",
      "Trained batch 1545 batch loss 6.22215796 epoch total loss 5.9780283\n",
      "Trained batch 1546 batch loss 6.24896717 epoch total loss 5.97820377\n",
      "Trained batch 1547 batch loss 5.6350832 epoch total loss 5.97798157\n",
      "Trained batch 1548 batch loss 5.37478542 epoch total loss 5.97759199\n",
      "Trained batch 1549 batch loss 5.96921396 epoch total loss 5.97758627\n",
      "Trained batch 1550 batch loss 5.70365477 epoch total loss 5.97741\n",
      "Trained batch 1551 batch loss 5.8440237 epoch total loss 5.97732353\n",
      "Trained batch 1552 batch loss 6.06915855 epoch total loss 5.97738314\n",
      "Trained batch 1553 batch loss 6.41249132 epoch total loss 5.97766304\n",
      "Trained batch 1554 batch loss 6.69142294 epoch total loss 5.97812223\n",
      "Trained batch 1555 batch loss 6.27617645 epoch total loss 5.97831392\n",
      "Trained batch 1556 batch loss 6.04079151 epoch total loss 5.97835445\n",
      "Trained batch 1557 batch loss 6.3716321 epoch total loss 5.97860718\n",
      "Trained batch 1558 batch loss 6.1515336 epoch total loss 5.97871828\n",
      "Trained batch 1559 batch loss 6.17480373 epoch total loss 5.97884369\n",
      "Trained batch 1560 batch loss 3.92344666 epoch total loss 5.97752666\n",
      "Trained batch 1561 batch loss 4.70328283 epoch total loss 5.97671032\n",
      "Trained batch 1562 batch loss 5.85392189 epoch total loss 5.97663116\n",
      "Trained batch 1563 batch loss 5.0545826 epoch total loss 5.97604132\n",
      "Trained batch 1564 batch loss 4.51417542 epoch total loss 5.97510719\n",
      "Trained batch 1565 batch loss 5.8040967 epoch total loss 5.97499752\n",
      "Trained batch 1566 batch loss 6.67133 epoch total loss 5.97544193\n",
      "Trained batch 1567 batch loss 5.27691889 epoch total loss 5.97499657\n",
      "Trained batch 1568 batch loss 6.5693059 epoch total loss 5.97537565\n",
      "Trained batch 1569 batch loss 6.3897543 epoch total loss 5.97563934\n",
      "Trained batch 1570 batch loss 6.41371727 epoch total loss 5.97591877\n",
      "Trained batch 1571 batch loss 5.57972145 epoch total loss 5.97566652\n",
      "Trained batch 1572 batch loss 6.25544453 epoch total loss 5.97584486\n",
      "Trained batch 1573 batch loss 6.00492382 epoch total loss 5.97586346\n",
      "Trained batch 1574 batch loss 6.41174364 epoch total loss 5.9761405\n",
      "Trained batch 1575 batch loss 6.51716518 epoch total loss 5.9764843\n",
      "Trained batch 1576 batch loss 5.43966198 epoch total loss 5.97614384\n",
      "Trained batch 1577 batch loss 5.80229712 epoch total loss 5.97603369\n",
      "Trained batch 1578 batch loss 5.9864006 epoch total loss 5.97604036\n",
      "Trained batch 1579 batch loss 6.16637421 epoch total loss 5.97616053\n",
      "Trained batch 1580 batch loss 5.92801952 epoch total loss 5.97613\n",
      "Trained batch 1581 batch loss 6.00451088 epoch total loss 5.97614813\n",
      "Trained batch 1582 batch loss 5.96170616 epoch total loss 5.97613907\n",
      "Trained batch 1583 batch loss 6.17687893 epoch total loss 5.97626591\n",
      "Trained batch 1584 batch loss 6.03011799 epoch total loss 5.9763\n",
      "Trained batch 1585 batch loss 5.54740143 epoch total loss 5.9760294\n",
      "Trained batch 1586 batch loss 6.46577263 epoch total loss 5.97633839\n",
      "Trained batch 1587 batch loss 6.50389385 epoch total loss 5.97667074\n",
      "Trained batch 1588 batch loss 5.73357105 epoch total loss 5.97651768\n",
      "Trained batch 1589 batch loss 6.391119 epoch total loss 5.97677898\n",
      "Trained batch 1590 batch loss 6.39496517 epoch total loss 5.97704172\n",
      "Trained batch 1591 batch loss 5.73492813 epoch total loss 5.97688961\n",
      "Trained batch 1592 batch loss 5.87727547 epoch total loss 5.97682667\n",
      "Trained batch 1593 batch loss 5.77946949 epoch total loss 5.97670269\n",
      "Trained batch 1594 batch loss 6.14391327 epoch total loss 5.97680759\n",
      "Trained batch 1595 batch loss 6.40989542 epoch total loss 5.97707939\n",
      "Trained batch 1596 batch loss 6.30020094 epoch total loss 5.97728157\n",
      "Trained batch 1597 batch loss 6.0694437 epoch total loss 5.97733927\n",
      "Trained batch 1598 batch loss 5.71157837 epoch total loss 5.97717285\n",
      "Trained batch 1599 batch loss 5.80109596 epoch total loss 5.9770627\n",
      "Trained batch 1600 batch loss 5.11431265 epoch total loss 5.9765234\n",
      "Trained batch 1601 batch loss 5.17981434 epoch total loss 5.97602558\n",
      "Trained batch 1602 batch loss 4.87037611 epoch total loss 5.9753356\n",
      "Trained batch 1603 batch loss 4.48945427 epoch total loss 5.97440815\n",
      "Trained batch 1604 batch loss 4.20942211 epoch total loss 5.97330761\n",
      "Trained batch 1605 batch loss 4.93630791 epoch total loss 5.9726615\n",
      "Trained batch 1606 batch loss 4.86085558 epoch total loss 5.9719696\n",
      "Trained batch 1607 batch loss 5.33508873 epoch total loss 5.97157335\n",
      "Trained batch 1608 batch loss 4.99741 epoch total loss 5.97096729\n",
      "Trained batch 1609 batch loss 4.92638 epoch total loss 5.97031832\n",
      "Trained batch 1610 batch loss 5.18031693 epoch total loss 5.96982765\n",
      "Trained batch 1611 batch loss 5.22079945 epoch total loss 5.96936274\n",
      "Trained batch 1612 batch loss 5.17084217 epoch total loss 5.9688673\n",
      "Trained batch 1613 batch loss 5.49004316 epoch total loss 5.96857071\n",
      "Trained batch 1614 batch loss 5.48845387 epoch total loss 5.96827316\n",
      "Trained batch 1615 batch loss 6.31393242 epoch total loss 5.96848679\n",
      "Trained batch 1616 batch loss 5.30203676 epoch total loss 5.96807432\n",
      "Trained batch 1617 batch loss 5.43606043 epoch total loss 5.96774578\n",
      "Trained batch 1618 batch loss 6.21135044 epoch total loss 5.96789598\n",
      "Trained batch 1619 batch loss 6.64556 epoch total loss 5.96831465\n",
      "Trained batch 1620 batch loss 6.11616373 epoch total loss 5.96840572\n",
      "Trained batch 1621 batch loss 6.45876789 epoch total loss 5.96870852\n",
      "Trained batch 1622 batch loss 6.74033165 epoch total loss 5.96918392\n",
      "Trained batch 1623 batch loss 6.29723263 epoch total loss 5.9693861\n",
      "Trained batch 1624 batch loss 6.80995 epoch total loss 5.96990347\n",
      "Trained batch 1625 batch loss 6.3780365 epoch total loss 5.97015429\n",
      "Trained batch 1626 batch loss 7.12693167 epoch total loss 5.97086573\n",
      "Trained batch 1627 batch loss 6.36238432 epoch total loss 5.97110653\n",
      "Trained batch 1628 batch loss 6.48742867 epoch total loss 5.97142363\n",
      "Trained batch 1629 batch loss 6.48854256 epoch total loss 5.97174072\n",
      "Trained batch 1630 batch loss 6.41264725 epoch total loss 5.97201157\n",
      "Trained batch 1631 batch loss 6.52790117 epoch total loss 5.9723525\n",
      "Trained batch 1632 batch loss 5.69492626 epoch total loss 5.97218275\n",
      "Trained batch 1633 batch loss 6.14571142 epoch total loss 5.97228909\n",
      "Trained batch 1634 batch loss 6.42122126 epoch total loss 5.97256374\n",
      "Trained batch 1635 batch loss 5.79468632 epoch total loss 5.97245502\n",
      "Trained batch 1636 batch loss 7.53973293 epoch total loss 5.97341299\n",
      "Trained batch 1637 batch loss 6.18701506 epoch total loss 5.97354412\n",
      "Trained batch 1638 batch loss 6.01333618 epoch total loss 5.97356844\n",
      "Trained batch 1639 batch loss 6.48628044 epoch total loss 5.97388124\n",
      "Trained batch 1640 batch loss 5.52177286 epoch total loss 5.97360563\n",
      "Trained batch 1641 batch loss 6.08335543 epoch total loss 5.97367191\n",
      "Trained batch 1642 batch loss 5.9904027 epoch total loss 5.9736824\n",
      "Trained batch 1643 batch loss 7.26036167 epoch total loss 5.97446537\n",
      "Trained batch 1644 batch loss 6.2379179 epoch total loss 5.97462606\n",
      "Trained batch 1645 batch loss 6.73682499 epoch total loss 5.97508955\n",
      "Trained batch 1646 batch loss 5.82478428 epoch total loss 5.97499847\n",
      "Trained batch 1647 batch loss 6.5215106 epoch total loss 5.97533035\n",
      "Trained batch 1648 batch loss 6.1429472 epoch total loss 5.97543192\n",
      "Trained batch 1649 batch loss 6.57328129 epoch total loss 5.97579432\n",
      "Trained batch 1650 batch loss 6.72370577 epoch total loss 5.97624779\n",
      "Trained batch 1651 batch loss 6.01549625 epoch total loss 5.97627163\n",
      "Trained batch 1652 batch loss 6.75327 epoch total loss 5.97674179\n",
      "Trained batch 1653 batch loss 6.93001175 epoch total loss 5.97731829\n",
      "Trained batch 1654 batch loss 6.94792938 epoch total loss 5.97790527\n",
      "Trained batch 1655 batch loss 6.66644382 epoch total loss 5.97832108\n",
      "Trained batch 1656 batch loss 6.27520466 epoch total loss 5.97850037\n",
      "Trained batch 1657 batch loss 5.77642488 epoch total loss 5.9783783\n",
      "Trained batch 1658 batch loss 6.40341 epoch total loss 5.97863483\n",
      "Trained batch 1659 batch loss 6.92731428 epoch total loss 5.97920656\n",
      "Trained batch 1660 batch loss 6.21918297 epoch total loss 5.97935104\n",
      "Trained batch 1661 batch loss 6.59351635 epoch total loss 5.97972107\n",
      "Trained batch 1662 batch loss 5.82917738 epoch total loss 5.97963047\n",
      "Trained batch 1663 batch loss 5.93705511 epoch total loss 5.97960472\n",
      "Trained batch 1664 batch loss 6.15539455 epoch total loss 5.97971058\n",
      "Trained batch 1665 batch loss 6.09740067 epoch total loss 5.97978115\n",
      "Trained batch 1666 batch loss 6.07946205 epoch total loss 5.97984076\n",
      "Trained batch 1667 batch loss 6.10399818 epoch total loss 5.97991514\n",
      "Trained batch 1668 batch loss 6.21468163 epoch total loss 5.98005581\n",
      "Trained batch 1669 batch loss 6.11657238 epoch total loss 5.98013735\n",
      "Trained batch 1670 batch loss 6.18666172 epoch total loss 5.98026133\n",
      "Trained batch 1671 batch loss 6.18074894 epoch total loss 5.98038101\n",
      "Trained batch 1672 batch loss 6.59260464 epoch total loss 5.98074722\n",
      "Trained batch 1673 batch loss 6.25234 epoch total loss 5.98090935\n",
      "Trained batch 1674 batch loss 5.91832638 epoch total loss 5.98087168\n",
      "Trained batch 1675 batch loss 6.39938402 epoch total loss 5.98112154\n",
      "Trained batch 1676 batch loss 5.63857174 epoch total loss 5.98091745\n",
      "Trained batch 1677 batch loss 6.47607803 epoch total loss 5.98121309\n",
      "Trained batch 1678 batch loss 7.3237896 epoch total loss 5.98201323\n",
      "Trained batch 1679 batch loss 5.80311203 epoch total loss 5.98190641\n",
      "Trained batch 1680 batch loss 6.31865168 epoch total loss 5.98210669\n",
      "Trained batch 1681 batch loss 6.51162291 epoch total loss 5.98242188\n",
      "Trained batch 1682 batch loss 4.47929144 epoch total loss 5.98152828\n",
      "Trained batch 1683 batch loss 5.93634701 epoch total loss 5.98150158\n",
      "Trained batch 1684 batch loss 5.94898272 epoch total loss 5.98148251\n",
      "Trained batch 1685 batch loss 5.89342308 epoch total loss 5.98143\n",
      "Trained batch 1686 batch loss 5.96987724 epoch total loss 5.98142338\n",
      "Trained batch 1687 batch loss 6.22790289 epoch total loss 5.98156929\n",
      "Trained batch 1688 batch loss 5.9472971 epoch total loss 5.98154879\n",
      "Trained batch 1689 batch loss 5.82960033 epoch total loss 5.98145914\n",
      "Trained batch 1690 batch loss 6.93626738 epoch total loss 5.98202419\n",
      "Trained batch 1691 batch loss 5.69297552 epoch total loss 5.98185349\n",
      "Trained batch 1692 batch loss 6.18312407 epoch total loss 5.98197269\n",
      "Trained batch 1693 batch loss 5.0846076 epoch total loss 5.98144293\n",
      "Trained batch 1694 batch loss 6.66189098 epoch total loss 5.9818449\n",
      "Trained batch 1695 batch loss 6.2883 epoch total loss 5.98202562\n",
      "Trained batch 1696 batch loss 6.73434639 epoch total loss 5.98246908\n",
      "Trained batch 1697 batch loss 6.34338951 epoch total loss 5.98268175\n",
      "Trained batch 1698 batch loss 6.58545494 epoch total loss 5.98303747\n",
      "Trained batch 1699 batch loss 5.22206068 epoch total loss 5.98258924\n",
      "Trained batch 1700 batch loss 6.24979734 epoch total loss 5.9827466\n",
      "Trained batch 1701 batch loss 5.95770025 epoch total loss 5.98273182\n",
      "Trained batch 1702 batch loss 5.63337517 epoch total loss 5.98252678\n",
      "Trained batch 1703 batch loss 5.92981529 epoch total loss 5.98249578\n",
      "Trained batch 1704 batch loss 5.9186964 epoch total loss 5.98245859\n",
      "Trained batch 1705 batch loss 5.48835278 epoch total loss 5.98216867\n",
      "Trained batch 1706 batch loss 6.4658308 epoch total loss 5.98245239\n",
      "Trained batch 1707 batch loss 6.46280241 epoch total loss 5.98273373\n",
      "Trained batch 1708 batch loss 7.04804516 epoch total loss 5.98335743\n",
      "Trained batch 1709 batch loss 6.91719627 epoch total loss 5.98390341\n",
      "Trained batch 1710 batch loss 5.36765575 epoch total loss 5.98354292\n",
      "Trained batch 1711 batch loss 5.93064785 epoch total loss 5.98351192\n",
      "Trained batch 1712 batch loss 7.19853497 epoch total loss 5.98422146\n",
      "Trained batch 1713 batch loss 6.41592503 epoch total loss 5.98447371\n",
      "Trained batch 1714 batch loss 6.05243158 epoch total loss 5.98451328\n",
      "Trained batch 1715 batch loss 6.37127876 epoch total loss 5.98473883\n",
      "Trained batch 1716 batch loss 6.30675507 epoch total loss 5.98492622\n",
      "Trained batch 1717 batch loss 6.31504393 epoch total loss 5.98511887\n",
      "Trained batch 1718 batch loss 5.92078352 epoch total loss 5.98508167\n",
      "Trained batch 1719 batch loss 6.35800362 epoch total loss 5.98529863\n",
      "Trained batch 1720 batch loss 5.91318512 epoch total loss 5.98525667\n",
      "Trained batch 1721 batch loss 5.92479658 epoch total loss 5.98522139\n",
      "Trained batch 1722 batch loss 6.01437187 epoch total loss 5.98523855\n",
      "Trained batch 1723 batch loss 6.00269365 epoch total loss 5.98524904\n",
      "Trained batch 1724 batch loss 5.63297272 epoch total loss 5.98504448\n",
      "Trained batch 1725 batch loss 5.85932255 epoch total loss 5.98497152\n",
      "Trained batch 1726 batch loss 5.55934429 epoch total loss 5.984725\n",
      "Trained batch 1727 batch loss 6.56720734 epoch total loss 5.9850626\n",
      "Trained batch 1728 batch loss 6.33516312 epoch total loss 5.98526525\n",
      "Trained batch 1729 batch loss 6.041049 epoch total loss 5.9852972\n",
      "Trained batch 1730 batch loss 6.13290882 epoch total loss 5.98538256\n",
      "Trained batch 1731 batch loss 6.04419756 epoch total loss 5.98541641\n",
      "Trained batch 1732 batch loss 6.72906733 epoch total loss 5.98584604\n",
      "Trained batch 1733 batch loss 5.99346542 epoch total loss 5.98585033\n",
      "Trained batch 1734 batch loss 5.10947609 epoch total loss 5.98534489\n",
      "Trained batch 1735 batch loss 5.75044632 epoch total loss 5.98520899\n",
      "Trained batch 1736 batch loss 6.50331879 epoch total loss 5.98550749\n",
      "Trained batch 1737 batch loss 6.39070702 epoch total loss 5.98574066\n",
      "Trained batch 1738 batch loss 6.64096451 epoch total loss 5.98611736\n",
      "Trained batch 1739 batch loss 6.76342154 epoch total loss 5.98656464\n",
      "Trained batch 1740 batch loss 6.11935759 epoch total loss 5.98664093\n",
      "Trained batch 1741 batch loss 6.43453407 epoch total loss 5.98689795\n",
      "Trained batch 1742 batch loss 6.32103252 epoch total loss 5.98709\n",
      "Trained batch 1743 batch loss 6.47820568 epoch total loss 5.98737192\n",
      "Trained batch 1744 batch loss 6.51733 epoch total loss 5.98767614\n",
      "Trained batch 1745 batch loss 6.7009654 epoch total loss 5.98808479\n",
      "Trained batch 1746 batch loss 6.01492119 epoch total loss 5.9881\n",
      "Trained batch 1747 batch loss 6.48799372 epoch total loss 5.98838615\n",
      "Trained batch 1748 batch loss 6.19162607 epoch total loss 5.9885025\n",
      "Trained batch 1749 batch loss 6.02076244 epoch total loss 5.98852062\n",
      "Trained batch 1750 batch loss 6.02071285 epoch total loss 5.98853922\n",
      "Trained batch 1751 batch loss 5.28895283 epoch total loss 5.98813963\n",
      "Trained batch 1752 batch loss 5.11332273 epoch total loss 5.98764038\n",
      "Trained batch 1753 batch loss 4.90016222 epoch total loss 5.98702\n",
      "Trained batch 1754 batch loss 5.08813095 epoch total loss 5.98650742\n",
      "Trained batch 1755 batch loss 5.18569565 epoch total loss 5.98605108\n",
      "Trained batch 1756 batch loss 4.42740345 epoch total loss 5.98516369\n",
      "Trained batch 1757 batch loss 4.86702633 epoch total loss 5.98452711\n",
      "Trained batch 1758 batch loss 5.16294718 epoch total loss 5.98406\n",
      "Trained batch 1759 batch loss 4.49143219 epoch total loss 5.98321152\n",
      "Trained batch 1760 batch loss 4.8043294 epoch total loss 5.98254156\n",
      "Trained batch 1761 batch loss 5.66852283 epoch total loss 5.9823637\n",
      "Trained batch 1762 batch loss 5.78036213 epoch total loss 5.98224878\n",
      "Trained batch 1763 batch loss 4.45008659 epoch total loss 5.98138\n",
      "Trained batch 1764 batch loss 4.56591654 epoch total loss 5.98057747\n",
      "Trained batch 1765 batch loss 4.26854277 epoch total loss 5.97960711\n",
      "Trained batch 1766 batch loss 4.35076809 epoch total loss 5.9786849\n",
      "Trained batch 1767 batch loss 4.6065836 epoch total loss 5.97790813\n",
      "Trained batch 1768 batch loss 4.75827599 epoch total loss 5.97721815\n",
      "Trained batch 1769 batch loss 4.19535351 epoch total loss 5.97621107\n",
      "Trained batch 1770 batch loss 6.28024673 epoch total loss 5.97638273\n",
      "Trained batch 1771 batch loss 5.72967196 epoch total loss 5.97624302\n",
      "Trained batch 1772 batch loss 6.22023487 epoch total loss 5.9763813\n",
      "Trained batch 1773 batch loss 6.35960102 epoch total loss 5.97659731\n",
      "Trained batch 1774 batch loss 6.13515 epoch total loss 5.97668648\n",
      "Trained batch 1775 batch loss 7.18954659 epoch total loss 5.97737\n",
      "Trained batch 1776 batch loss 6.63761139 epoch total loss 5.97774124\n",
      "Trained batch 1777 batch loss 5.73715305 epoch total loss 5.9776063\n",
      "Trained batch 1778 batch loss 5.55336952 epoch total loss 5.97736788\n",
      "Trained batch 1779 batch loss 6.28806496 epoch total loss 5.9775424\n",
      "Trained batch 1780 batch loss 5.88924885 epoch total loss 5.97749281\n",
      "Trained batch 1781 batch loss 6.34133148 epoch total loss 5.97769737\n",
      "Trained batch 1782 batch loss 6.35515738 epoch total loss 5.97790956\n",
      "Trained batch 1783 batch loss 6.30727625 epoch total loss 5.97809458\n",
      "Trained batch 1784 batch loss 6.42697716 epoch total loss 5.97834587\n",
      "Trained batch 1785 batch loss 6.39892387 epoch total loss 5.97858143\n",
      "Trained batch 1786 batch loss 6.19919491 epoch total loss 5.97870493\n",
      "Trained batch 1787 batch loss 6.8712759 epoch total loss 5.97920418\n",
      "Trained batch 1788 batch loss 5.51109362 epoch total loss 5.97894239\n",
      "Trained batch 1789 batch loss 6.34828472 epoch total loss 5.97914886\n",
      "Trained batch 1790 batch loss 5.84255743 epoch total loss 5.97907257\n",
      "Trained batch 1791 batch loss 6.17803478 epoch total loss 5.97918367\n",
      "Trained batch 1792 batch loss 5.05800629 epoch total loss 5.97866917\n",
      "Trained batch 1793 batch loss 5.74592781 epoch total loss 5.97853947\n",
      "Trained batch 1794 batch loss 6.36516762 epoch total loss 5.978755\n",
      "Trained batch 1795 batch loss 5.74544 epoch total loss 5.97862482\n",
      "Trained batch 1796 batch loss 6.06752396 epoch total loss 5.97867441\n",
      "Trained batch 1797 batch loss 6.10124302 epoch total loss 5.9787426\n",
      "Trained batch 1798 batch loss 5.81069851 epoch total loss 5.97864914\n",
      "Trained batch 1799 batch loss 4.80903673 epoch total loss 5.97799873\n",
      "Trained batch 1800 batch loss 6.05835056 epoch total loss 5.97804356\n",
      "Trained batch 1801 batch loss 5.82668591 epoch total loss 5.97795963\n",
      "Trained batch 1802 batch loss 5.89656353 epoch total loss 5.97791481\n",
      "Trained batch 1803 batch loss 5.71049929 epoch total loss 5.97776651\n",
      "Trained batch 1804 batch loss 6.15036583 epoch total loss 5.97786236\n",
      "Trained batch 1805 batch loss 6.58058453 epoch total loss 5.97819662\n",
      "Trained batch 1806 batch loss 6.24426746 epoch total loss 5.97834349\n",
      "Trained batch 1807 batch loss 4.86632824 epoch total loss 5.97772837\n",
      "Trained batch 1808 batch loss 6.10447025 epoch total loss 5.97779846\n",
      "Trained batch 1809 batch loss 5.82288265 epoch total loss 5.97771311\n",
      "Trained batch 1810 batch loss 6.18911266 epoch total loss 5.97783\n",
      "Trained batch 1811 batch loss 6.18054914 epoch total loss 5.97794199\n",
      "Trained batch 1812 batch loss 5.88968515 epoch total loss 5.97789335\n",
      "Trained batch 1813 batch loss 5.82700539 epoch total loss 5.97781\n",
      "Trained batch 1814 batch loss 5.98318958 epoch total loss 5.97781324\n",
      "Trained batch 1815 batch loss 6.05413723 epoch total loss 5.97785473\n",
      "Trained batch 1816 batch loss 6.39700699 epoch total loss 5.97808599\n",
      "Trained batch 1817 batch loss 5.67511654 epoch total loss 5.9779191\n",
      "Trained batch 1818 batch loss 6.31943274 epoch total loss 5.97810698\n",
      "Trained batch 1819 batch loss 5.71499443 epoch total loss 5.97796202\n",
      "Trained batch 1820 batch loss 6.03709126 epoch total loss 5.97799444\n",
      "Trained batch 1821 batch loss 5.71785641 epoch total loss 5.97785187\n",
      "Trained batch 1822 batch loss 5.70055676 epoch total loss 5.97769928\n",
      "Trained batch 1823 batch loss 5.95778322 epoch total loss 5.97768831\n",
      "Trained batch 1824 batch loss 5.97747278 epoch total loss 5.97768831\n",
      "Trained batch 1825 batch loss 5.93556499 epoch total loss 5.97766542\n",
      "Trained batch 1826 batch loss 5.37937164 epoch total loss 5.97733736\n",
      "Trained batch 1827 batch loss 5.26745605 epoch total loss 5.97694874\n",
      "Trained batch 1828 batch loss 5.9864893 epoch total loss 5.97695398\n",
      "Trained batch 1829 batch loss 6.35337639 epoch total loss 5.97716\n",
      "Trained batch 1830 batch loss 6.50655174 epoch total loss 5.97744942\n",
      "Trained batch 1831 batch loss 5.60557604 epoch total loss 5.97724628\n",
      "Trained batch 1832 batch loss 6.33313942 epoch total loss 5.97744036\n",
      "Trained batch 1833 batch loss 6.55599308 epoch total loss 5.97775602\n",
      "Trained batch 1834 batch loss 4.00362778 epoch total loss 5.97668\n",
      "Trained batch 1835 batch loss 5.46212673 epoch total loss 5.97639894\n",
      "Trained batch 1836 batch loss 6.28027248 epoch total loss 5.97656441\n",
      "Trained batch 1837 batch loss 5.80245 epoch total loss 5.97647\n",
      "Trained batch 1838 batch loss 5.49796867 epoch total loss 5.97620964\n",
      "Trained batch 1839 batch loss 6.04713535 epoch total loss 5.97624826\n",
      "Trained batch 1840 batch loss 4.43836546 epoch total loss 5.97541237\n",
      "Trained batch 1841 batch loss 5.70915318 epoch total loss 5.97526789\n",
      "Trained batch 1842 batch loss 5.11083317 epoch total loss 5.9747982\n",
      "Trained batch 1843 batch loss 5.90549 epoch total loss 5.97476053\n",
      "Trained batch 1844 batch loss 5.9564662 epoch total loss 5.97475\n",
      "Trained batch 1845 batch loss 6.01756382 epoch total loss 5.97477341\n",
      "Trained batch 1846 batch loss 5.81527758 epoch total loss 5.9746871\n",
      "Trained batch 1847 batch loss 5.88973618 epoch total loss 5.97464132\n",
      "Trained batch 1848 batch loss 5.89397764 epoch total loss 5.97459745\n",
      "Trained batch 1849 batch loss 6.12727451 epoch total loss 5.97467947\n",
      "Trained batch 1850 batch loss 5.98165751 epoch total loss 5.97468328\n",
      "Trained batch 1851 batch loss 6.14527798 epoch total loss 5.97477579\n",
      "Trained batch 1852 batch loss 5.78542805 epoch total loss 5.97467327\n",
      "Trained batch 1853 batch loss 5.76341152 epoch total loss 5.97455931\n",
      "Trained batch 1854 batch loss 5.99121237 epoch total loss 5.97456837\n",
      "Trained batch 1855 batch loss 6.30867195 epoch total loss 5.97474813\n",
      "Trained batch 1856 batch loss 5.78041458 epoch total loss 5.97464371\n",
      "Trained batch 1857 batch loss 5.59132576 epoch total loss 5.97443724\n",
      "Trained batch 1858 batch loss 5.87391663 epoch total loss 5.97438335\n",
      "Trained batch 1859 batch loss 5.42824554 epoch total loss 5.97408962\n",
      "Trained batch 1860 batch loss 5.48407221 epoch total loss 5.97382641\n",
      "Trained batch 1861 batch loss 6.24177504 epoch total loss 5.97397089\n",
      "Trained batch 1862 batch loss 6.03831768 epoch total loss 5.97400522\n",
      "Trained batch 1863 batch loss 5.71261787 epoch total loss 5.97386503\n",
      "Trained batch 1864 batch loss 5.43914795 epoch total loss 5.97357845\n",
      "Trained batch 1865 batch loss 5.82990408 epoch total loss 5.97350121\n",
      "Trained batch 1866 batch loss 5.62353945 epoch total loss 5.97331429\n",
      "Trained batch 1867 batch loss 6.31014919 epoch total loss 5.97349453\n",
      "Trained batch 1868 batch loss 6.32172632 epoch total loss 5.97368097\n",
      "Trained batch 1869 batch loss 5.71548557 epoch total loss 5.97354317\n",
      "Trained batch 1870 batch loss 6.41645908 epoch total loss 5.97377968\n",
      "Trained batch 1871 batch loss 6.00644732 epoch total loss 5.97379732\n",
      "Trained batch 1872 batch loss 5.95097542 epoch total loss 5.97378492\n",
      "Trained batch 1873 batch loss 5.20688963 epoch total loss 5.9733758\n",
      "Trained batch 1874 batch loss 5.90869474 epoch total loss 5.97334146\n",
      "Trained batch 1875 batch loss 5.93272591 epoch total loss 5.97332\n",
      "Trained batch 1876 batch loss 6.37482548 epoch total loss 5.97353411\n",
      "Trained batch 1877 batch loss 5.35349655 epoch total loss 5.97320366\n",
      "Trained batch 1878 batch loss 6.41367435 epoch total loss 5.97343826\n",
      "Trained batch 1879 batch loss 5.66354084 epoch total loss 5.97327328\n",
      "Trained batch 1880 batch loss 6.05107355 epoch total loss 5.97331429\n",
      "Trained batch 1881 batch loss 6.01551247 epoch total loss 5.9733367\n",
      "Trained batch 1882 batch loss 5.97614765 epoch total loss 5.9733386\n",
      "Trained batch 1883 batch loss 6.1006484 epoch total loss 5.97340631\n",
      "Trained batch 1884 batch loss 5.96382809 epoch total loss 5.97340107\n",
      "Trained batch 1885 batch loss 5.18145514 epoch total loss 5.97298098\n",
      "Trained batch 1886 batch loss 5.3660965 epoch total loss 5.97265959\n",
      "Trained batch 1887 batch loss 5.86060905 epoch total loss 5.9726\n",
      "Trained batch 1888 batch loss 6.14073 epoch total loss 5.97268867\n",
      "Trained batch 1889 batch loss 5.26613188 epoch total loss 5.97231483\n",
      "Trained batch 1890 batch loss 6.15099907 epoch total loss 5.97240973\n",
      "Trained batch 1891 batch loss 5.28536892 epoch total loss 5.97204638\n",
      "Trained batch 1892 batch loss 6.16821766 epoch total loss 5.97215\n",
      "Trained batch 1893 batch loss 5.76413631 epoch total loss 5.9720397\n",
      "Trained batch 1894 batch loss 5.88097191 epoch total loss 5.97199154\n",
      "Trained batch 1895 batch loss 5.88410282 epoch total loss 5.97194529\n",
      "Trained batch 1896 batch loss 5.86479712 epoch total loss 5.97188902\n",
      "Trained batch 1897 batch loss 5.81235266 epoch total loss 5.97180462\n",
      "Trained batch 1898 batch loss 6.41068 epoch total loss 5.97203636\n",
      "Trained batch 1899 batch loss 5.77482033 epoch total loss 5.97193241\n",
      "Trained batch 1900 batch loss 5.88984585 epoch total loss 5.97188902\n",
      "Trained batch 1901 batch loss 6.45071316 epoch total loss 5.97214079\n",
      "Trained batch 1902 batch loss 6.05162239 epoch total loss 5.97218275\n",
      "Trained batch 1903 batch loss 5.7542119 epoch total loss 5.97206831\n",
      "Trained batch 1904 batch loss 5.69158077 epoch total loss 5.97192097\n",
      "Trained batch 1905 batch loss 5.8878355 epoch total loss 5.97187662\n",
      "Trained batch 1906 batch loss 5.59883499 epoch total loss 5.97168064\n",
      "Trained batch 1907 batch loss 6.13421059 epoch total loss 5.97176552\n",
      "Trained batch 1908 batch loss 5.98179 epoch total loss 5.97177076\n",
      "Trained batch 1909 batch loss 6.96628094 epoch total loss 5.97229147\n",
      "Trained batch 1910 batch loss 6.70947647 epoch total loss 5.97267771\n",
      "Trained batch 1911 batch loss 7.14930964 epoch total loss 5.9732933\n",
      "Trained batch 1912 batch loss 6.90136909 epoch total loss 5.97377872\n",
      "Trained batch 1913 batch loss 7.23719597 epoch total loss 5.97443962\n",
      "Trained batch 1914 batch loss 6.60263824 epoch total loss 5.97476768\n",
      "Trained batch 1915 batch loss 5.93227768 epoch total loss 5.97474575\n",
      "Trained batch 1916 batch loss 5.73995399 epoch total loss 5.9746232\n",
      "Trained batch 1917 batch loss 6.19132423 epoch total loss 5.97473621\n",
      "Trained batch 1918 batch loss 6.38972044 epoch total loss 5.9749527\n",
      "Trained batch 1919 batch loss 6.43703079 epoch total loss 5.9751935\n",
      "Trained batch 1920 batch loss 7.15953541 epoch total loss 5.97581\n",
      "Trained batch 1921 batch loss 6.98312378 epoch total loss 5.97633457\n",
      "Trained batch 1922 batch loss 6.38012409 epoch total loss 5.97654486\n",
      "Trained batch 1923 batch loss 5.68248844 epoch total loss 5.97639179\n",
      "Trained batch 1924 batch loss 5.74724627 epoch total loss 5.97627258\n",
      "Trained batch 1925 batch loss 5.99527025 epoch total loss 5.9762826\n",
      "Trained batch 1926 batch loss 5.85488701 epoch total loss 5.97621918\n",
      "Trained batch 1927 batch loss 6.01646519 epoch total loss 5.97624\n",
      "Trained batch 1928 batch loss 6.04195881 epoch total loss 5.97627449\n",
      "Trained batch 1929 batch loss 6.00526953 epoch total loss 5.97628927\n",
      "Trained batch 1930 batch loss 5.73421 epoch total loss 5.97616386\n",
      "Trained batch 1931 batch loss 6.93843317 epoch total loss 5.97666216\n",
      "Trained batch 1932 batch loss 6.18351746 epoch total loss 5.97676945\n",
      "Trained batch 1933 batch loss 6.28724289 epoch total loss 5.97692966\n",
      "Trained batch 1934 batch loss 6.15983963 epoch total loss 5.97702456\n",
      "Trained batch 1935 batch loss 6.13253307 epoch total loss 5.97710514\n",
      "Trained batch 1936 batch loss 6.01287746 epoch total loss 5.97712326\n",
      "Trained batch 1937 batch loss 5.88851166 epoch total loss 5.97707796\n",
      "Trained batch 1938 batch loss 6.11877823 epoch total loss 5.97715092\n",
      "Trained batch 1939 batch loss 6.34472084 epoch total loss 5.9773407\n",
      "Trained batch 1940 batch loss 6.65308619 epoch total loss 5.97768927\n",
      "Trained batch 1941 batch loss 7.14859 epoch total loss 5.97829247\n",
      "Trained batch 1942 batch loss 6.89051437 epoch total loss 5.97876215\n",
      "Trained batch 1943 batch loss 6.75069618 epoch total loss 5.97915936\n",
      "Trained batch 1944 batch loss 5.64378071 epoch total loss 5.97898674\n",
      "Trained batch 1945 batch loss 6.31466961 epoch total loss 5.97915936\n",
      "Trained batch 1946 batch loss 5.74726963 epoch total loss 5.97904\n",
      "Trained batch 1947 batch loss 5.36533451 epoch total loss 5.97872496\n",
      "Trained batch 1948 batch loss 5.9162097 epoch total loss 5.97869253\n",
      "Trained batch 1949 batch loss 5.96019363 epoch total loss 5.97868299\n",
      "Trained batch 1950 batch loss 5.9643054 epoch total loss 5.97867537\n",
      "Trained batch 1951 batch loss 6.55392838 epoch total loss 5.97897\n",
      "Trained batch 1952 batch loss 5.13902283 epoch total loss 5.97853947\n",
      "Trained batch 1953 batch loss 6.05097961 epoch total loss 5.97857666\n",
      "Trained batch 1954 batch loss 4.9067831 epoch total loss 5.9780283\n",
      "Trained batch 1955 batch loss 6.39338 epoch total loss 5.97824097\n",
      "Trained batch 1956 batch loss 6.78621769 epoch total loss 5.97865391\n",
      "Trained batch 1957 batch loss 6.36907959 epoch total loss 5.97885323\n",
      "Trained batch 1958 batch loss 6.88576078 epoch total loss 5.97931671\n",
      "Trained batch 1959 batch loss 6.16158152 epoch total loss 5.97940922\n",
      "Trained batch 1960 batch loss 6.81100559 epoch total loss 5.9798336\n",
      "Trained batch 1961 batch loss 6.67051315 epoch total loss 5.98018599\n",
      "Trained batch 1962 batch loss 5.30574799 epoch total loss 5.97984219\n",
      "Trained batch 1963 batch loss 4.74452305 epoch total loss 5.97921276\n",
      "Trained batch 1964 batch loss 5.65501308 epoch total loss 5.97904778\n",
      "Trained batch 1965 batch loss 5.6780014 epoch total loss 5.97889423\n",
      "Trained batch 1966 batch loss 5.26391745 epoch total loss 5.97853041\n",
      "Trained batch 1967 batch loss 5.00534821 epoch total loss 5.97803545\n",
      "Trained batch 1968 batch loss 6.24749231 epoch total loss 5.9781723\n",
      "Trained batch 1969 batch loss 6.42721748 epoch total loss 5.9784\n",
      "Trained batch 1970 batch loss 6.29793739 epoch total loss 5.97856236\n",
      "Trained batch 1971 batch loss 6.4825182 epoch total loss 5.97881794\n",
      "Trained batch 1972 batch loss 7.00765276 epoch total loss 5.9793396\n",
      "Trained batch 1973 batch loss 6.18493366 epoch total loss 5.97944355\n",
      "Trained batch 1974 batch loss 5.50639534 epoch total loss 5.97920418\n",
      "Trained batch 1975 batch loss 6.10664463 epoch total loss 5.97926855\n",
      "Trained batch 1976 batch loss 6.62310219 epoch total loss 5.97959471\n",
      "Trained batch 1977 batch loss 6.33533239 epoch total loss 5.97977448\n",
      "Trained batch 1978 batch loss 6.4844532 epoch total loss 5.98002958\n",
      "Trained batch 1979 batch loss 5.39678478 epoch total loss 5.97973442\n",
      "Trained batch 1980 batch loss 5.84364223 epoch total loss 5.97966576\n",
      "Trained batch 1981 batch loss 5.82705927 epoch total loss 5.97958899\n",
      "Trained batch 1982 batch loss 4.88994884 epoch total loss 5.97903872\n",
      "Trained batch 1983 batch loss 5.67764759 epoch total loss 5.97888708\n",
      "Trained batch 1984 batch loss 6.05987453 epoch total loss 5.97892761\n",
      "Trained batch 1985 batch loss 4.83215809 epoch total loss 5.97834969\n",
      "Trained batch 1986 batch loss 5.93808842 epoch total loss 5.97832966\n",
      "Trained batch 1987 batch loss 6.09206486 epoch total loss 5.97838688\n",
      "Trained batch 1988 batch loss 4.43156433 epoch total loss 5.97760868\n",
      "Trained batch 1989 batch loss 5.86667585 epoch total loss 5.97755289\n",
      "Trained batch 1990 batch loss 6.15033627 epoch total loss 5.97763968\n",
      "Trained batch 1991 batch loss 6.40880251 epoch total loss 5.97785664\n",
      "Trained batch 1992 batch loss 6.32033 epoch total loss 5.9780283\n",
      "Trained batch 1993 batch loss 5.68028736 epoch total loss 5.97787905\n",
      "Trained batch 1994 batch loss 5.34849358 epoch total loss 5.97756338\n",
      "Trained batch 1995 batch loss 5.87337685 epoch total loss 5.97751093\n",
      "Trained batch 1996 batch loss 6.03436375 epoch total loss 5.97753954\n",
      "Trained batch 1997 batch loss 5.95376253 epoch total loss 5.97752762\n",
      "Trained batch 1998 batch loss 5.82684422 epoch total loss 5.97745275\n",
      "Trained batch 1999 batch loss 6.1193552 epoch total loss 5.97752333\n",
      "Trained batch 2000 batch loss 5.9987421 epoch total loss 5.97753429\n",
      "Trained batch 2001 batch loss 5.79097557 epoch total loss 5.97744083\n",
      "Trained batch 2002 batch loss 5.14587 epoch total loss 5.97702551\n",
      "Trained batch 2003 batch loss 5.83222485 epoch total loss 5.97695303\n",
      "Trained batch 2004 batch loss 6.22112083 epoch total loss 5.97707462\n",
      "Trained batch 2005 batch loss 5.81156349 epoch total loss 5.97699213\n",
      "Trained batch 2006 batch loss 5.36648417 epoch total loss 5.97668743\n",
      "Trained batch 2007 batch loss 5.69714785 epoch total loss 5.97654819\n",
      "Trained batch 2008 batch loss 5.90599489 epoch total loss 5.97651339\n",
      "Trained batch 2009 batch loss 6.06774616 epoch total loss 5.97655869\n",
      "Trained batch 2010 batch loss 6.38580704 epoch total loss 5.97676229\n",
      "Trained batch 2011 batch loss 6.22741508 epoch total loss 5.97688675\n",
      "Trained batch 2012 batch loss 6.35455084 epoch total loss 5.97707462\n",
      "Trained batch 2013 batch loss 5.63222551 epoch total loss 5.97690296\n",
      "Trained batch 2014 batch loss 5.84955454 epoch total loss 5.97684\n",
      "Trained batch 2015 batch loss 5.78052235 epoch total loss 5.97674227\n",
      "Trained batch 2016 batch loss 5.44151783 epoch total loss 5.97647667\n",
      "Trained batch 2017 batch loss 5.85017109 epoch total loss 5.9764142\n",
      "Trained batch 2018 batch loss 5.46617174 epoch total loss 5.97616148\n",
      "Trained batch 2019 batch loss 5.62687492 epoch total loss 5.97598839\n",
      "Trained batch 2020 batch loss 5.05469 epoch total loss 5.97553205\n",
      "Trained batch 2021 batch loss 5.36944914 epoch total loss 5.97523212\n",
      "Trained batch 2022 batch loss 5.67598629 epoch total loss 5.9750843\n",
      "Trained batch 2023 batch loss 5.043293 epoch total loss 5.9746232\n",
      "Trained batch 2024 batch loss 5.89373064 epoch total loss 5.97458315\n",
      "Trained batch 2025 batch loss 5.7743721 epoch total loss 5.97448444\n",
      "Trained batch 2026 batch loss 5.59915638 epoch total loss 5.97429943\n",
      "Trained batch 2027 batch loss 5.96067715 epoch total loss 5.97429276\n",
      "Trained batch 2028 batch loss 5.89670753 epoch total loss 5.97425461\n",
      "Trained batch 2029 batch loss 6.08586454 epoch total loss 5.97430944\n",
      "Trained batch 2030 batch loss 5.10783195 epoch total loss 5.97388268\n",
      "Trained batch 2031 batch loss 5.91240215 epoch total loss 5.97385216\n",
      "Trained batch 2032 batch loss 5.87552357 epoch total loss 5.973804\n",
      "Trained batch 2033 batch loss 5.7630558 epoch total loss 5.9737\n",
      "Trained batch 2034 batch loss 5.74349833 epoch total loss 5.97358656\n",
      "Trained batch 2035 batch loss 5.68223 epoch total loss 5.97344351\n",
      "Trained batch 2036 batch loss 5.08058643 epoch total loss 5.97300529\n",
      "Trained batch 2037 batch loss 5.13888645 epoch total loss 5.97259569\n",
      "Trained batch 2038 batch loss 5.73722792 epoch total loss 5.9724803\n",
      "Trained batch 2039 batch loss 5.38145 epoch total loss 5.97219086\n",
      "Trained batch 2040 batch loss 5.64914179 epoch total loss 5.97203255\n",
      "Trained batch 2041 batch loss 5.13301659 epoch total loss 5.97162151\n",
      "Trained batch 2042 batch loss 5.54858112 epoch total loss 5.97141409\n",
      "Trained batch 2043 batch loss 6.21791077 epoch total loss 5.97153473\n",
      "Trained batch 2044 batch loss 5.11702919 epoch total loss 5.97111702\n",
      "Trained batch 2045 batch loss 5.44018459 epoch total loss 5.97085714\n",
      "Trained batch 2046 batch loss 5.28130341 epoch total loss 5.9705205\n",
      "Trained batch 2047 batch loss 6.27078629 epoch total loss 5.97066689\n",
      "Trained batch 2048 batch loss 5.74440956 epoch total loss 5.97055626\n",
      "Trained batch 2049 batch loss 6.250247 epoch total loss 5.97069263\n",
      "Trained batch 2050 batch loss 5.42027092 epoch total loss 5.97042418\n",
      "Trained batch 2051 batch loss 5.39743376 epoch total loss 5.97014475\n",
      "Trained batch 2052 batch loss 5.84300566 epoch total loss 5.97008276\n",
      "Trained batch 2053 batch loss 5.74586 epoch total loss 5.96997356\n",
      "Trained batch 2054 batch loss 5.61892223 epoch total loss 5.96980286\n",
      "Trained batch 2055 batch loss 5.71608353 epoch total loss 5.96967888\n",
      "Trained batch 2056 batch loss 4.8996172 epoch total loss 5.96915865\n",
      "Trained batch 2057 batch loss 4.6440382 epoch total loss 5.96851397\n",
      "Trained batch 2058 batch loss 6.19943476 epoch total loss 5.96862602\n",
      "Trained batch 2059 batch loss 6.24236917 epoch total loss 5.96875906\n",
      "Trained batch 2060 batch loss 6.20725107 epoch total loss 5.96887445\n",
      "Trained batch 2061 batch loss 6.22064686 epoch total loss 5.968997\n",
      "Trained batch 2062 batch loss 5.81339741 epoch total loss 5.96892166\n",
      "Trained batch 2063 batch loss 5.85422134 epoch total loss 5.96886587\n",
      "Trained batch 2064 batch loss 5.36084604 epoch total loss 5.96857166\n",
      "Trained batch 2065 batch loss 5.04629803 epoch total loss 5.96812487\n",
      "Trained batch 2066 batch loss 5.03833294 epoch total loss 5.96767473\n",
      "Trained batch 2067 batch loss 4.8932271 epoch total loss 5.96715498\n",
      "Trained batch 2068 batch loss 5.69000387 epoch total loss 5.96702099\n",
      "Trained batch 2069 batch loss 4.61732864 epoch total loss 5.96636868\n",
      "Trained batch 2070 batch loss 5.95976973 epoch total loss 5.96636581\n",
      "Trained batch 2071 batch loss 5.35475349 epoch total loss 5.96607\n",
      "Trained batch 2072 batch loss 4.45192337 epoch total loss 5.96533966\n",
      "Trained batch 2073 batch loss 4.83531857 epoch total loss 5.96479416\n",
      "Trained batch 2074 batch loss 5.20288944 epoch total loss 5.96442699\n",
      "Trained batch 2075 batch loss 4.91087294 epoch total loss 5.96391964\n",
      "Trained batch 2076 batch loss 5.54158258 epoch total loss 5.96371603\n",
      "Trained batch 2077 batch loss 6.2903347 epoch total loss 5.96387339\n",
      "Trained batch 2078 batch loss 5.61618328 epoch total loss 5.96370602\n",
      "Trained batch 2079 batch loss 5.62571907 epoch total loss 5.96354342\n",
      "Trained batch 2080 batch loss 5.66261148 epoch total loss 5.96339893\n",
      "Trained batch 2081 batch loss 5.0958147 epoch total loss 5.96298218\n",
      "Trained batch 2082 batch loss 5.5077219 epoch total loss 5.96276331\n",
      "Trained batch 2083 batch loss 5.70453 epoch total loss 5.96263933\n",
      "Trained batch 2084 batch loss 5.9715004 epoch total loss 5.96264362\n",
      "Trained batch 2085 batch loss 6.18189144 epoch total loss 5.96274853\n",
      "Trained batch 2086 batch loss 6.66019821 epoch total loss 5.96308327\n",
      "Trained batch 2087 batch loss 5.47447634 epoch total loss 5.96284914\n",
      "Trained batch 2088 batch loss 5.54190254 epoch total loss 5.96264744\n",
      "Trained batch 2089 batch loss 5.73656 epoch total loss 5.9625392\n",
      "Trained batch 2090 batch loss 5.38782883 epoch total loss 5.96226406\n",
      "Trained batch 2091 batch loss 6.00417376 epoch total loss 5.96228409\n",
      "Trained batch 2092 batch loss 5.96629381 epoch total loss 5.96228552\n",
      "Trained batch 2093 batch loss 5.74909639 epoch total loss 5.96218395\n",
      "Trained batch 2094 batch loss 5.33974028 epoch total loss 5.96188641\n",
      "Trained batch 2095 batch loss 5.85205317 epoch total loss 5.96183443\n",
      "Trained batch 2096 batch loss 6.05225611 epoch total loss 5.96187782\n",
      "Trained batch 2097 batch loss 5.23082256 epoch total loss 5.96152878\n",
      "Trained batch 2098 batch loss 6.02334642 epoch total loss 5.96155834\n",
      "Trained batch 2099 batch loss 5.47084141 epoch total loss 5.96132469\n",
      "Trained batch 2100 batch loss 4.81442118 epoch total loss 5.96077824\n",
      "Trained batch 2101 batch loss 5.58684444 epoch total loss 5.96060038\n",
      "Trained batch 2102 batch loss 5.11649132 epoch total loss 5.96019888\n",
      "Trained batch 2103 batch loss 5.55875874 epoch total loss 5.96000767\n",
      "Trained batch 2104 batch loss 5.65406179 epoch total loss 5.95986271\n",
      "Trained batch 2105 batch loss 5.16345119 epoch total loss 5.9594841\n",
      "Trained batch 2106 batch loss 5.93198919 epoch total loss 5.95947075\n",
      "Trained batch 2107 batch loss 6.22448063 epoch total loss 5.95959663\n",
      "Trained batch 2108 batch loss 5.78848743 epoch total loss 5.95951509\n",
      "Trained batch 2109 batch loss 5.54782057 epoch total loss 5.95932\n",
      "Trained batch 2110 batch loss 5.62559843 epoch total loss 5.95916224\n",
      "Trained batch 2111 batch loss 4.92673922 epoch total loss 5.958673\n",
      "Trained batch 2112 batch loss 5.82752419 epoch total loss 5.95861053\n",
      "Trained batch 2113 batch loss 5.88044453 epoch total loss 5.95857382\n",
      "Trained batch 2114 batch loss 5.57762909 epoch total loss 5.95839357\n",
      "Trained batch 2115 batch loss 4.32018375 epoch total loss 5.95761919\n",
      "Trained batch 2116 batch loss 3.70841551 epoch total loss 5.95655584\n",
      "Trained batch 2117 batch loss 4.54668045 epoch total loss 5.95589\n",
      "Trained batch 2118 batch loss 7.22003937 epoch total loss 5.9564867\n",
      "Trained batch 2119 batch loss 5.63076401 epoch total loss 5.95633316\n",
      "Trained batch 2120 batch loss 6.15783 epoch total loss 5.95642805\n",
      "Trained batch 2121 batch loss 6.72949 epoch total loss 5.95679283\n",
      "Trained batch 2122 batch loss 6.330966 epoch total loss 5.95696926\n",
      "Trained batch 2123 batch loss 6.55677891 epoch total loss 5.95725155\n",
      "Trained batch 2124 batch loss 7.27154875 epoch total loss 5.95787048\n",
      "Trained batch 2125 batch loss 6.11955929 epoch total loss 5.9579463\n",
      "Trained batch 2126 batch loss 7.02387857 epoch total loss 5.95844746\n",
      "Trained batch 2127 batch loss 6.16810036 epoch total loss 5.95854568\n",
      "Trained batch 2128 batch loss 5.86366463 epoch total loss 5.95850134\n",
      "Trained batch 2129 batch loss 4.91880941 epoch total loss 5.95801306\n",
      "Trained batch 2130 batch loss 6.70474386 epoch total loss 5.95836353\n",
      "Trained batch 2131 batch loss 5.87169647 epoch total loss 5.958323\n",
      "Trained batch 2132 batch loss 7.06728315 epoch total loss 5.95884323\n",
      "Trained batch 2133 batch loss 6.67789125 epoch total loss 5.95918036\n",
      "Trained batch 2134 batch loss 7.95748425 epoch total loss 5.96011639\n",
      "Trained batch 2135 batch loss 6.5192 epoch total loss 5.96037865\n",
      "Trained batch 2136 batch loss 7.96523285 epoch total loss 5.96131706\n",
      "Trained batch 2137 batch loss 6.98213768 epoch total loss 5.96179485\n",
      "Trained batch 2138 batch loss 7.03547573 epoch total loss 5.96229696\n",
      "Trained batch 2139 batch loss 6.22067308 epoch total loss 5.9624176\n",
      "Trained batch 2140 batch loss 6.76274 epoch total loss 5.96279144\n",
      "Trained batch 2141 batch loss 5.36177397 epoch total loss 5.96251059\n",
      "Trained batch 2142 batch loss 6.50156069 epoch total loss 5.96276236\n",
      "Trained batch 2143 batch loss 6.03488302 epoch total loss 5.96279621\n",
      "Trained batch 2144 batch loss 6.09878922 epoch total loss 5.96285963\n",
      "Trained batch 2145 batch loss 5.64235497 epoch total loss 5.96271038\n",
      "Trained batch 2146 batch loss 6.03422642 epoch total loss 5.96274376\n",
      "Trained batch 2147 batch loss 6.54799366 epoch total loss 5.96301603\n",
      "Trained batch 2148 batch loss 6.17930698 epoch total loss 5.96311712\n",
      "Trained batch 2149 batch loss 6.71292877 epoch total loss 5.96346569\n",
      "Trained batch 2150 batch loss 6.60969687 epoch total loss 5.96376657\n",
      "Trained batch 2151 batch loss 6.14422846 epoch total loss 5.9638505\n",
      "Trained batch 2152 batch loss 6.49710941 epoch total loss 5.96409798\n",
      "Trained batch 2153 batch loss 6.53700829 epoch total loss 5.96436453\n",
      "Trained batch 2154 batch loss 6.42654896 epoch total loss 5.96457911\n",
      "Trained batch 2155 batch loss 6.33314896 epoch total loss 5.96475\n",
      "Trained batch 2156 batch loss 6.4605732 epoch total loss 5.96498\n",
      "Trained batch 2157 batch loss 5.60667276 epoch total loss 5.96481371\n",
      "Trained batch 2158 batch loss 6.11756277 epoch total loss 5.96488428\n",
      "Trained batch 2159 batch loss 5.75904274 epoch total loss 5.96478891\n",
      "Trained batch 2160 batch loss 6.25919914 epoch total loss 5.96492529\n",
      "Trained batch 2161 batch loss 5.53195095 epoch total loss 5.96472502\n",
      "Trained batch 2162 batch loss 5.42796135 epoch total loss 5.96447659\n",
      "Trained batch 2163 batch loss 5.85252523 epoch total loss 5.96442461\n",
      "Trained batch 2164 batch loss 5.41577625 epoch total loss 5.96417141\n",
      "Trained batch 2165 batch loss 6.45919323 epoch total loss 5.9644\n",
      "Trained batch 2166 batch loss 5.95965147 epoch total loss 5.96439791\n",
      "Trained batch 2167 batch loss 6.83027363 epoch total loss 5.9647975\n",
      "Trained batch 2168 batch loss 6.80273 epoch total loss 5.96518373\n",
      "Trained batch 2169 batch loss 6.44443846 epoch total loss 5.96540451\n",
      "Trained batch 2170 batch loss 6.94761467 epoch total loss 5.96585703\n",
      "Trained batch 2171 batch loss 6.39561 epoch total loss 5.96605492\n",
      "Trained batch 2172 batch loss 6.45346451 epoch total loss 5.96627951\n",
      "Trained batch 2173 batch loss 6.14672661 epoch total loss 5.96636248\n",
      "Trained batch 2174 batch loss 4.75853 epoch total loss 5.96580696\n",
      "Trained batch 2175 batch loss 5.93175936 epoch total loss 5.96579123\n",
      "Trained batch 2176 batch loss 5.96677256 epoch total loss 5.9657917\n",
      "Trained batch 2177 batch loss 6.14028263 epoch total loss 5.96587181\n",
      "Trained batch 2178 batch loss 6.40858793 epoch total loss 5.96607494\n",
      "Trained batch 2179 batch loss 5.57117605 epoch total loss 5.96589375\n",
      "Trained batch 2180 batch loss 6.9272356 epoch total loss 5.96633434\n",
      "Trained batch 2181 batch loss 5.96054935 epoch total loss 5.96633196\n",
      "Trained batch 2182 batch loss 5.88732433 epoch total loss 5.9662962\n",
      "Trained batch 2183 batch loss 6.46464634 epoch total loss 5.9665246\n",
      "Trained batch 2184 batch loss 6.70075512 epoch total loss 5.96686077\n",
      "Trained batch 2185 batch loss 5.16890335 epoch total loss 5.96649551\n",
      "Trained batch 2186 batch loss 4.94280624 epoch total loss 5.96602726\n",
      "Trained batch 2187 batch loss 5.97571325 epoch total loss 5.96603155\n",
      "Trained batch 2188 batch loss 5.43869495 epoch total loss 5.96579027\n",
      "Trained batch 2189 batch loss 5.68172503 epoch total loss 5.96566057\n",
      "Trained batch 2190 batch loss 6.37307167 epoch total loss 5.96584654\n",
      "Trained batch 2191 batch loss 6.15741205 epoch total loss 5.9659338\n",
      "Trained batch 2192 batch loss 6.28997612 epoch total loss 5.96608162\n",
      "Trained batch 2193 batch loss 6.62170315 epoch total loss 5.96638107\n",
      "Trained batch 2194 batch loss 5.86089468 epoch total loss 5.96633291\n",
      "Trained batch 2195 batch loss 5.79420519 epoch total loss 5.96625471\n",
      "Trained batch 2196 batch loss 6.15971947 epoch total loss 5.96634293\n",
      "Trained batch 2197 batch loss 5.5379715 epoch total loss 5.9661479\n",
      "Trained batch 2198 batch loss 5.56361628 epoch total loss 5.96596479\n",
      "Trained batch 2199 batch loss 5.57492065 epoch total loss 5.96578693\n",
      "Trained batch 2200 batch loss 6.39636898 epoch total loss 5.96598291\n",
      "Trained batch 2201 batch loss 5.08319139 epoch total loss 5.96558142\n",
      "Trained batch 2202 batch loss 6.15991306 epoch total loss 5.96567\n",
      "Trained batch 2203 batch loss 5.96223116 epoch total loss 5.9656682\n",
      "Trained batch 2204 batch loss 6.04316139 epoch total loss 5.96570349\n",
      "Trained batch 2205 batch loss 5.68612432 epoch total loss 5.96557665\n",
      "Trained batch 2206 batch loss 5.61813354 epoch total loss 5.96541929\n",
      "Trained batch 2207 batch loss 6.17903423 epoch total loss 5.96551609\n",
      "Trained batch 2208 batch loss 5.94309044 epoch total loss 5.96550608\n",
      "Trained batch 2209 batch loss 6.53828955 epoch total loss 5.965765\n",
      "Trained batch 2210 batch loss 6.12062788 epoch total loss 5.96583557\n",
      "Trained batch 2211 batch loss 6.41022968 epoch total loss 5.96603632\n",
      "Trained batch 2212 batch loss 5.34569645 epoch total loss 5.96575594\n",
      "Trained batch 2213 batch loss 6.67858124 epoch total loss 5.9660778\n",
      "Trained batch 2214 batch loss 7.22754574 epoch total loss 5.96664762\n",
      "Trained batch 2215 batch loss 7.38279724 epoch total loss 5.96728706\n",
      "Trained batch 2216 batch loss 6.5200758 epoch total loss 5.96753693\n",
      "Trained batch 2217 batch loss 7.13883 epoch total loss 5.96806526\n",
      "Trained batch 2218 batch loss 6.15751076 epoch total loss 5.96815\n",
      "Trained batch 2219 batch loss 6.50137615 epoch total loss 5.96839046\n",
      "Trained batch 2220 batch loss 6.42083 epoch total loss 5.96859407\n",
      "Trained batch 2221 batch loss 5.71404791 epoch total loss 5.96847963\n",
      "Trained batch 2222 batch loss 6.3313489 epoch total loss 5.96864271\n",
      "Trained batch 2223 batch loss 5.7015 epoch total loss 5.96852255\n",
      "Trained batch 2224 batch loss 6.00760221 epoch total loss 5.96854\n",
      "Trained batch 2225 batch loss 6.0445261 epoch total loss 5.96857452\n",
      "Trained batch 2226 batch loss 6.23236656 epoch total loss 5.96869278\n",
      "Trained batch 2227 batch loss 6.18227196 epoch total loss 5.9687891\n",
      "Trained batch 2228 batch loss 5.62958384 epoch total loss 5.96863699\n",
      "Trained batch 2229 batch loss 6.09882832 epoch total loss 5.96869516\n",
      "Trained batch 2230 batch loss 5.37479162 epoch total loss 5.96842909\n",
      "Trained batch 2231 batch loss 5.73208237 epoch total loss 5.96832323\n",
      "Trained batch 2232 batch loss 5.77385139 epoch total loss 5.96823597\n",
      "Trained batch 2233 batch loss 4.90236759 epoch total loss 5.96775866\n",
      "Trained batch 2234 batch loss 4.97207594 epoch total loss 5.96731281\n",
      "Trained batch 2235 batch loss 5.09880781 epoch total loss 5.96692419\n",
      "Trained batch 2236 batch loss 5.5451746 epoch total loss 5.96673536\n",
      "Trained batch 2237 batch loss 6.20286894 epoch total loss 5.96684074\n",
      "Trained batch 2238 batch loss 6.39020109 epoch total loss 5.96703053\n",
      "Trained batch 2239 batch loss 6.15528 epoch total loss 5.96711445\n",
      "Trained batch 2240 batch loss 5.64973211 epoch total loss 5.96697235\n",
      "Trained batch 2241 batch loss 5.06741619 epoch total loss 5.96657133\n",
      "Trained batch 2242 batch loss 5.23070288 epoch total loss 5.96624279\n",
      "Trained batch 2243 batch loss 5.67606974 epoch total loss 5.96611309\n",
      "Trained batch 2244 batch loss 6.42975712 epoch total loss 5.96632\n",
      "Trained batch 2245 batch loss 6.08288908 epoch total loss 5.96637201\n",
      "Trained batch 2246 batch loss 6.39701939 epoch total loss 5.9665637\n",
      "Trained batch 2247 batch loss 6.09127665 epoch total loss 5.96661901\n",
      "Trained batch 2248 batch loss 6.23362112 epoch total loss 5.96673775\n",
      "Trained batch 2249 batch loss 6.04337788 epoch total loss 5.9667716\n",
      "Trained batch 2250 batch loss 5.83888817 epoch total loss 5.96671486\n",
      "Trained batch 2251 batch loss 5.83188057 epoch total loss 5.96665478\n",
      "Trained batch 2252 batch loss 5.57850742 epoch total loss 5.96648264\n",
      "Trained batch 2253 batch loss 4.92762566 epoch total loss 5.96602154\n",
      "Trained batch 2254 batch loss 5.1706953 epoch total loss 5.96566868\n",
      "Trained batch 2255 batch loss 5.00388145 epoch total loss 5.96524239\n",
      "Trained batch 2256 batch loss 5.09124947 epoch total loss 5.96485472\n",
      "Trained batch 2257 batch loss 4.71746349 epoch total loss 5.96430206\n",
      "Trained batch 2258 batch loss 5.77666 epoch total loss 5.96421862\n",
      "Trained batch 2259 batch loss 5.56325245 epoch total loss 5.96404123\n",
      "Trained batch 2260 batch loss 6.54384899 epoch total loss 5.96429777\n",
      "Trained batch 2261 batch loss 5.84646845 epoch total loss 5.9642458\n",
      "Trained batch 2262 batch loss 6.83997 epoch total loss 5.96463299\n",
      "Trained batch 2263 batch loss 5.87293196 epoch total loss 5.96459246\n",
      "Trained batch 2264 batch loss 5.85639572 epoch total loss 5.96454477\n",
      "Trained batch 2265 batch loss 6.02242374 epoch total loss 5.96457052\n",
      "Trained batch 2266 batch loss 6.87989187 epoch total loss 5.9649744\n",
      "Trained batch 2267 batch loss 6.6399641 epoch total loss 5.96527195\n",
      "Trained batch 2268 batch loss 4.88856745 epoch total loss 5.96479702\n",
      "Trained batch 2269 batch loss 6.76517487 epoch total loss 5.96515036\n",
      "Trained batch 2270 batch loss 5.29356098 epoch total loss 5.96485472\n",
      "Trained batch 2271 batch loss 5.70701361 epoch total loss 5.96474075\n",
      "Trained batch 2272 batch loss 5.02025175 epoch total loss 5.96432543\n",
      "Trained batch 2273 batch loss 4.51379395 epoch total loss 5.96368694\n",
      "Trained batch 2274 batch loss 4.79659843 epoch total loss 5.96317387\n",
      "Trained batch 2275 batch loss 4.57131147 epoch total loss 5.96256208\n",
      "Trained batch 2276 batch loss 5.4772048 epoch total loss 5.96234894\n",
      "Trained batch 2277 batch loss 4.68026495 epoch total loss 5.96178627\n",
      "Trained batch 2278 batch loss 4.69806433 epoch total loss 5.96123171\n",
      "Trained batch 2279 batch loss 4.6080389 epoch total loss 5.96063805\n",
      "Trained batch 2280 batch loss 4.79728699 epoch total loss 5.96012735\n",
      "Trained batch 2281 batch loss 4.57747 epoch total loss 5.95952129\n",
      "Trained batch 2282 batch loss 4.96210241 epoch total loss 5.95908403\n",
      "Trained batch 2283 batch loss 4.46319389 epoch total loss 5.95842886\n",
      "Trained batch 2284 batch loss 4.7504921 epoch total loss 5.9579\n",
      "Trained batch 2285 batch loss 4.5695138 epoch total loss 5.95729256\n",
      "Trained batch 2286 batch loss 4.98163033 epoch total loss 5.95686531\n",
      "Trained batch 2287 batch loss 5.19036 epoch total loss 5.95653057\n",
      "Trained batch 2288 batch loss 4.24516249 epoch total loss 5.95578241\n",
      "Trained batch 2289 batch loss 4.85664845 epoch total loss 5.95530224\n",
      "Trained batch 2290 batch loss 4.82692766 epoch total loss 5.95480967\n",
      "Trained batch 2291 batch loss 5.01157808 epoch total loss 5.95439768\n",
      "Trained batch 2292 batch loss 4.78057051 epoch total loss 5.95388556\n",
      "Trained batch 2293 batch loss 4.44029236 epoch total loss 5.95322561\n",
      "Trained batch 2294 batch loss 4.41947412 epoch total loss 5.95255709\n",
      "Trained batch 2295 batch loss 4.4655447 epoch total loss 5.95190907\n",
      "Trained batch 2296 batch loss 4.90557814 epoch total loss 5.95145321\n",
      "Trained batch 2297 batch loss 4.8806572 epoch total loss 5.95098734\n",
      "Trained batch 2298 batch loss 4.8848753 epoch total loss 5.95052338\n",
      "Trained batch 2299 batch loss 4.90383101 epoch total loss 5.95006847\n",
      "Trained batch 2300 batch loss 6.02286053 epoch total loss 5.9501\n",
      "Trained batch 2301 batch loss 7.13854933 epoch total loss 5.95061636\n",
      "Trained batch 2302 batch loss 6.65165472 epoch total loss 5.95092058\n",
      "Trained batch 2303 batch loss 6.54144621 epoch total loss 5.95117712\n",
      "Trained batch 2304 batch loss 6.2128315 epoch total loss 5.95129061\n",
      "Trained batch 2305 batch loss 6.90971279 epoch total loss 5.95170641\n",
      "Trained batch 2306 batch loss 6.72442436 epoch total loss 5.95204163\n",
      "Trained batch 2307 batch loss 4.63833332 epoch total loss 5.95147228\n",
      "Trained batch 2308 batch loss 6.65594 epoch total loss 5.95177794\n",
      "Trained batch 2309 batch loss 5.48877096 epoch total loss 5.95157766\n",
      "Trained batch 2310 batch loss 5.70496321 epoch total loss 5.95147085\n",
      "Trained batch 2311 batch loss 5.36817074 epoch total loss 5.95121813\n",
      "Trained batch 2312 batch loss 6.24209166 epoch total loss 5.95134401\n",
      "Trained batch 2313 batch loss 6.33196068 epoch total loss 5.951509\n",
      "Trained batch 2314 batch loss 5.22243309 epoch total loss 5.95119381\n",
      "Trained batch 2315 batch loss 5.37782764 epoch total loss 5.95094633\n",
      "Trained batch 2316 batch loss 6.04473305 epoch total loss 5.95098686\n",
      "Trained batch 2317 batch loss 7.33910894 epoch total loss 5.95158577\n",
      "Trained batch 2318 batch loss 6.84341431 epoch total loss 5.95197058\n",
      "Trained batch 2319 batch loss 6.35682344 epoch total loss 5.9521451\n",
      "Trained batch 2320 batch loss 7.27809048 epoch total loss 5.95271683\n",
      "Trained batch 2321 batch loss 5.99830246 epoch total loss 5.95273638\n",
      "Trained batch 2322 batch loss 7.4838047 epoch total loss 5.95339537\n",
      "Trained batch 2323 batch loss 6.58407688 epoch total loss 5.95366669\n",
      "Trained batch 2324 batch loss 5.50492668 epoch total loss 5.95347357\n",
      "Trained batch 2325 batch loss 7.26797724 epoch total loss 5.9540391\n",
      "Trained batch 2326 batch loss 6.92658806 epoch total loss 5.95445728\n",
      "Trained batch 2327 batch loss 6.368958 epoch total loss 5.95463514\n",
      "Trained batch 2328 batch loss 6.62799883 epoch total loss 5.95492458\n",
      "Trained batch 2329 batch loss 7.32510614 epoch total loss 5.955513\n",
      "Trained batch 2330 batch loss 5.99451828 epoch total loss 5.95552969\n",
      "Trained batch 2331 batch loss 6.11258507 epoch total loss 5.95559692\n",
      "Trained batch 2332 batch loss 5.8435545 epoch total loss 5.95554876\n",
      "Trained batch 2333 batch loss 4.91552067 epoch total loss 5.95510292\n",
      "Trained batch 2334 batch loss 5.70538044 epoch total loss 5.95499563\n",
      "Trained batch 2335 batch loss 5.6872735 epoch total loss 5.95488119\n",
      "Trained batch 2336 batch loss 5.53300095 epoch total loss 5.95470047\n",
      "Trained batch 2337 batch loss 5.84767056 epoch total loss 5.95465469\n",
      "Trained batch 2338 batch loss 5.85863876 epoch total loss 5.95461369\n",
      "Trained batch 2339 batch loss 6.64101601 epoch total loss 5.95490694\n",
      "Trained batch 2340 batch loss 6.33376789 epoch total loss 5.95506907\n",
      "Trained batch 2341 batch loss 6.35426712 epoch total loss 5.95524\n",
      "Trained batch 2342 batch loss 5.73080063 epoch total loss 5.95514345\n",
      "Trained batch 2343 batch loss 5.75150967 epoch total loss 5.95505667\n",
      "Trained batch 2344 batch loss 6.4315896 epoch total loss 5.95526028\n",
      "Trained batch 2345 batch loss 5.43934107 epoch total loss 5.95504045\n",
      "Trained batch 2346 batch loss 5.62727451 epoch total loss 5.95490026\n",
      "Trained batch 2347 batch loss 5.60246181 epoch total loss 5.95475\n",
      "Trained batch 2348 batch loss 6.31828117 epoch total loss 5.95490503\n",
      "Trained batch 2349 batch loss 6.40348387 epoch total loss 5.95509577\n",
      "Trained batch 2350 batch loss 6.68415546 epoch total loss 5.95540619\n",
      "Trained batch 2351 batch loss 6.31805325 epoch total loss 5.95556068\n",
      "Trained batch 2352 batch loss 7.38390636 epoch total loss 5.95616817\n",
      "Trained batch 2353 batch loss 6.83491707 epoch total loss 5.95654154\n",
      "Trained batch 2354 batch loss 6.48089218 epoch total loss 5.95676422\n",
      "Trained batch 2355 batch loss 5.93168 epoch total loss 5.95675325\n",
      "Trained batch 2356 batch loss 6.88522625 epoch total loss 5.95714712\n",
      "Trained batch 2357 batch loss 6.35891485 epoch total loss 5.95731783\n",
      "Trained batch 2358 batch loss 6.16545534 epoch total loss 5.95740604\n",
      "Trained batch 2359 batch loss 6.37897 epoch total loss 5.95758486\n",
      "Trained batch 2360 batch loss 5.13844395 epoch total loss 5.95723772\n",
      "Trained batch 2361 batch loss 5.68834591 epoch total loss 5.95712376\n",
      "Trained batch 2362 batch loss 6.28013611 epoch total loss 5.95726061\n",
      "Trained batch 2363 batch loss 6.01706886 epoch total loss 5.95728588\n",
      "Trained batch 2364 batch loss 6.07627869 epoch total loss 5.95733595\n",
      "Trained batch 2365 batch loss 5.63080406 epoch total loss 5.95719814\n",
      "Trained batch 2366 batch loss 6.18048191 epoch total loss 5.95729256\n",
      "Trained batch 2367 batch loss 6.81452417 epoch total loss 5.95765448\n",
      "Trained batch 2368 batch loss 6.71855402 epoch total loss 5.95797586\n",
      "Trained batch 2369 batch loss 6.42029905 epoch total loss 5.95817089\n",
      "Trained batch 2370 batch loss 6.46581364 epoch total loss 5.95838547\n",
      "Trained batch 2371 batch loss 6.70794296 epoch total loss 5.95870161\n",
      "Trained batch 2372 batch loss 6.5356822 epoch total loss 5.9589448\n",
      "Trained batch 2373 batch loss 6.5144062 epoch total loss 5.95917892\n",
      "Trained batch 2374 batch loss 5.76415157 epoch total loss 5.95909691\n",
      "Trained batch 2375 batch loss 6.73580647 epoch total loss 5.95942354\n",
      "Trained batch 2376 batch loss 5.37074947 epoch total loss 5.95917606\n",
      "Trained batch 2377 batch loss 5.84326744 epoch total loss 5.95912743\n",
      "Trained batch 2378 batch loss 5.72480106 epoch total loss 5.95902872\n",
      "Trained batch 2379 batch loss 5.9563756 epoch total loss 5.95902729\n",
      "Trained batch 2380 batch loss 5.95260334 epoch total loss 5.95902443\n",
      "Trained batch 2381 batch loss 5.93389416 epoch total loss 5.95901394\n",
      "Trained batch 2382 batch loss 5.643641 epoch total loss 5.95888138\n",
      "Trained batch 2383 batch loss 6.3213644 epoch total loss 5.95903349\n",
      "Trained batch 2384 batch loss 6.82611609 epoch total loss 5.95939732\n",
      "Trained batch 2385 batch loss 6.25791407 epoch total loss 5.95952225\n",
      "Trained batch 2386 batch loss 6.69229221 epoch total loss 5.95982933\n",
      "Trained batch 2387 batch loss 6.07359076 epoch total loss 5.95987701\n",
      "Trained batch 2388 batch loss 6.84152794 epoch total loss 5.96024656\n",
      "Trained batch 2389 batch loss 6.67925596 epoch total loss 5.96054745\n",
      "Trained batch 2390 batch loss 6.91893578 epoch total loss 5.96094847\n",
      "Trained batch 2391 batch loss 6.7444582 epoch total loss 5.96127605\n",
      "Trained batch 2392 batch loss 6.53675461 epoch total loss 5.96151686\n",
      "Trained batch 2393 batch loss 6.51245117 epoch total loss 5.96174717\n",
      "Trained batch 2394 batch loss 6.3741436 epoch total loss 5.96191931\n",
      "Trained batch 2395 batch loss 6.35572624 epoch total loss 5.96208382\n",
      "Trained batch 2396 batch loss 6.1454649 epoch total loss 5.96216\n",
      "Trained batch 2397 batch loss 6.06680632 epoch total loss 5.9622035\n",
      "Trained batch 2398 batch loss 4.92970467 epoch total loss 5.9617734\n",
      "Trained batch 2399 batch loss 5.46124887 epoch total loss 5.96156454\n",
      "Trained batch 2400 batch loss 5.54704094 epoch total loss 5.96139145\n",
      "Trained batch 2401 batch loss 5.46637821 epoch total loss 5.96118546\n",
      "Trained batch 2402 batch loss 6.25024366 epoch total loss 5.96130562\n",
      "Trained batch 2403 batch loss 6.14172697 epoch total loss 5.96138096\n",
      "Trained batch 2404 batch loss 6.11127567 epoch total loss 5.96144342\n",
      "Trained batch 2405 batch loss 6.33523941 epoch total loss 5.9615984\n",
      "Trained batch 2406 batch loss 5.80584335 epoch total loss 5.96153355\n",
      "Trained batch 2407 batch loss 5.23859501 epoch total loss 5.96123314\n",
      "Trained batch 2408 batch loss 5.62914371 epoch total loss 5.96109533\n",
      "Trained batch 2409 batch loss 6.39625549 epoch total loss 5.96127605\n",
      "Trained batch 2410 batch loss 6.25204945 epoch total loss 5.96139669\n",
      "Trained batch 2411 batch loss 6.70561838 epoch total loss 5.96170568\n",
      "Trained batch 2412 batch loss 5.90690708 epoch total loss 5.9616828\n",
      "Trained batch 2413 batch loss 5.42330074 epoch total loss 5.96145964\n",
      "Trained batch 2414 batch loss 6.05629539 epoch total loss 5.96149921\n",
      "Trained batch 2415 batch loss 6.02157402 epoch total loss 5.96152401\n",
      "Trained batch 2416 batch loss 4.60252762 epoch total loss 5.96096134\n",
      "Trained batch 2417 batch loss 5.08716965 epoch total loss 5.9606\n",
      "Trained batch 2418 batch loss 4.71287 epoch total loss 5.96008348\n",
      "Trained batch 2419 batch loss 4.67093086 epoch total loss 5.95955086\n",
      "Trained batch 2420 batch loss 4.71436596 epoch total loss 5.95903635\n",
      "Trained batch 2421 batch loss 5.12050581 epoch total loss 5.95868969\n",
      "Trained batch 2422 batch loss 4.66814232 epoch total loss 5.95815706\n",
      "Trained batch 2423 batch loss 5.28401852 epoch total loss 5.95787907\n",
      "Trained batch 2424 batch loss 5.3529377 epoch total loss 5.9576292\n",
      "Trained batch 2425 batch loss 5.28006 epoch total loss 5.95735\n",
      "Trained batch 2426 batch loss 5.89432812 epoch total loss 5.95732403\n",
      "Trained batch 2427 batch loss 5.33768845 epoch total loss 5.95706844\n",
      "Trained batch 2428 batch loss 5.01096821 epoch total loss 5.95667887\n",
      "Trained batch 2429 batch loss 5.86643696 epoch total loss 5.95664167\n",
      "Trained batch 2430 batch loss 5.58852959 epoch total loss 5.95649052\n",
      "Trained batch 2431 batch loss 6.02964401 epoch total loss 5.95652\n",
      "Trained batch 2432 batch loss 6.55659342 epoch total loss 5.95676708\n",
      "Trained batch 2433 batch loss 6.07870483 epoch total loss 5.95681715\n",
      "Trained batch 2434 batch loss 6.21982574 epoch total loss 5.95692539\n",
      "Trained batch 2435 batch loss 5.18022776 epoch total loss 5.95660639\n",
      "Trained batch 2436 batch loss 6.27864742 epoch total loss 5.95673847\n",
      "Trained batch 2437 batch loss 4.71604252 epoch total loss 5.95622921\n",
      "Trained batch 2438 batch loss 5.11448288 epoch total loss 5.95588398\n",
      "Trained batch 2439 batch loss 6.56848907 epoch total loss 5.95613527\n",
      "Trained batch 2440 batch loss 5.96730042 epoch total loss 5.95614\n",
      "Trained batch 2441 batch loss 5.75052738 epoch total loss 5.95605612\n",
      "Trained batch 2442 batch loss 5.90442562 epoch total loss 5.95603466\n",
      "Trained batch 2443 batch loss 6.42989779 epoch total loss 5.95622873\n",
      "Trained batch 2444 batch loss 5.77312517 epoch total loss 5.95615387\n",
      "Trained batch 2445 batch loss 5.35415745 epoch total loss 5.95590782\n",
      "Trained batch 2446 batch loss 6.38647842 epoch total loss 5.95608377\n",
      "Trained batch 2447 batch loss 5.92586231 epoch total loss 5.95607138\n",
      "Trained batch 2448 batch loss 5.74212933 epoch total loss 5.95598412\n",
      "Trained batch 2449 batch loss 6.07020092 epoch total loss 5.95603085\n",
      "Trained batch 2450 batch loss 5.68127489 epoch total loss 5.95591879\n",
      "Trained batch 2451 batch loss 5.91529083 epoch total loss 5.9559021\n",
      "Trained batch 2452 batch loss 4.75816679 epoch total loss 5.95541334\n",
      "Trained batch 2453 batch loss 5.32576942 epoch total loss 5.9551568\n",
      "Trained batch 2454 batch loss 5.4702282 epoch total loss 5.95495939\n",
      "Trained batch 2455 batch loss 5.78426695 epoch total loss 5.95489\n",
      "Trained batch 2456 batch loss 6.24494934 epoch total loss 5.95500803\n",
      "Trained batch 2457 batch loss 4.83070707 epoch total loss 5.95455074\n",
      "Trained batch 2458 batch loss 5.04153728 epoch total loss 5.95417929\n",
      "Trained batch 2459 batch loss 6.70358276 epoch total loss 5.95448399\n",
      "Trained batch 2460 batch loss 5.90649748 epoch total loss 5.95446444\n",
      "Trained batch 2461 batch loss 6.7000103 epoch total loss 5.95476723\n",
      "Trained batch 2462 batch loss 4.82187462 epoch total loss 5.95430756\n",
      "Trained batch 2463 batch loss 4.81963825 epoch total loss 5.95384645\n",
      "Trained batch 2464 batch loss 5.58092213 epoch total loss 5.9536953\n",
      "Trained batch 2465 batch loss 5.29466772 epoch total loss 5.95342827\n",
      "Trained batch 2466 batch loss 5.98105145 epoch total loss 5.95343924\n",
      "Trained batch 2467 batch loss 5.58899355 epoch total loss 5.95329142\n",
      "Trained batch 2468 batch loss 6.63436556 epoch total loss 5.95356798\n",
      "Trained batch 2469 batch loss 5.91466379 epoch total loss 5.95355225\n",
      "Trained batch 2470 batch loss 6.44399738 epoch total loss 5.95375109\n",
      "Trained batch 2471 batch loss 6.34840965 epoch total loss 5.95391083\n",
      "Trained batch 2472 batch loss 5.17566 epoch total loss 5.95359612\n",
      "Trained batch 2473 batch loss 7.08738804 epoch total loss 5.95405436\n",
      "Trained batch 2474 batch loss 4.90686 epoch total loss 5.95363092\n",
      "Trained batch 2475 batch loss 5.73654747 epoch total loss 5.95354319\n",
      "Trained batch 2476 batch loss 6.42380714 epoch total loss 5.95373297\n",
      "Trained batch 2477 batch loss 6.81417084 epoch total loss 5.95408058\n",
      "Trained batch 2478 batch loss 7.07973242 epoch total loss 5.95453501\n",
      "Trained batch 2479 batch loss 5.33593035 epoch total loss 5.95428562\n",
      "Trained batch 2480 batch loss 5.79612207 epoch total loss 5.95422173\n",
      "Trained batch 2481 batch loss 5.6006794 epoch total loss 5.95407915\n",
      "Trained batch 2482 batch loss 6.11993599 epoch total loss 5.95414591\n",
      "Trained batch 2483 batch loss 6.59432411 epoch total loss 5.95440388\n",
      "Trained batch 2484 batch loss 5.92124748 epoch total loss 5.95439053\n",
      "Trained batch 2485 batch loss 6.23801231 epoch total loss 5.95450497\n",
      "Trained batch 2486 batch loss 5.94149113 epoch total loss 5.95449972\n",
      "Trained batch 2487 batch loss 6.31095266 epoch total loss 5.95464277\n",
      "Trained batch 2488 batch loss 6.06295633 epoch total loss 5.95468616\n",
      "Trained batch 2489 batch loss 6.2813406 epoch total loss 5.9548173\n",
      "Trained batch 2490 batch loss 6.51282215 epoch total loss 5.95504141\n",
      "Trained batch 2491 batch loss 6.94495249 epoch total loss 5.95543861\n",
      "Trained batch 2492 batch loss 5.7890954 epoch total loss 5.95537186\n",
      "Trained batch 2493 batch loss 5.1281991 epoch total loss 5.95504\n",
      "Trained batch 2494 batch loss 4.57573223 epoch total loss 5.95448732\n",
      "Trained batch 2495 batch loss 5.83742714 epoch total loss 5.95444059\n",
      "Trained batch 2496 batch loss 5.42480659 epoch total loss 5.9542284\n",
      "Trained batch 2497 batch loss 5.95196342 epoch total loss 5.95422745\n",
      "Trained batch 2498 batch loss 5.40781403 epoch total loss 5.95400906\n",
      "Trained batch 2499 batch loss 5.24259853 epoch total loss 5.95372391\n",
      "Trained batch 2500 batch loss 5.94504642 epoch total loss 5.95372057\n",
      "Trained batch 2501 batch loss 6.10915947 epoch total loss 5.95378304\n",
      "Trained batch 2502 batch loss 6.54314661 epoch total loss 5.95401859\n",
      "Trained batch 2503 batch loss 5.71313477 epoch total loss 5.95392227\n",
      "Trained batch 2504 batch loss 6.31541157 epoch total loss 5.95406628\n",
      "Trained batch 2505 batch loss 6.06172514 epoch total loss 5.95410919\n",
      "Trained batch 2506 batch loss 5.95472813 epoch total loss 5.95410967\n",
      "Trained batch 2507 batch loss 6.03499126 epoch total loss 5.95414209\n",
      "Trained batch 2508 batch loss 5.95145893 epoch total loss 5.95414066\n",
      "Trained batch 2509 batch loss 5.21486 epoch total loss 5.95384645\n",
      "Trained batch 2510 batch loss 4.89769268 epoch total loss 5.95342541\n",
      "Trained batch 2511 batch loss 4.06581497 epoch total loss 5.95267344\n",
      "Trained batch 2512 batch loss 6.32300901 epoch total loss 5.95282078\n",
      "Trained batch 2513 batch loss 6.37724113 epoch total loss 5.95298958\n",
      "Trained batch 2514 batch loss 6.70559263 epoch total loss 5.95328951\n",
      "Trained batch 2515 batch loss 6.12947178 epoch total loss 5.9533596\n",
      "Trained batch 2516 batch loss 5.51361847 epoch total loss 5.9531846\n",
      "Trained batch 2517 batch loss 6.36339188 epoch total loss 5.95334768\n",
      "Trained batch 2518 batch loss 5.95045757 epoch total loss 5.95334625\n",
      "Trained batch 2519 batch loss 5.4280138 epoch total loss 5.95313787\n",
      "Trained batch 2520 batch loss 5.95003414 epoch total loss 5.95313644\n",
      "Trained batch 2521 batch loss 5.8042469 epoch total loss 5.95307779\n",
      "Trained batch 2522 batch loss 5.68478584 epoch total loss 5.95297146\n",
      "Trained batch 2523 batch loss 5.55306435 epoch total loss 5.95281267\n",
      "Trained batch 2524 batch loss 5.77368212 epoch total loss 5.95274162\n",
      "Trained batch 2525 batch loss 6.41868782 epoch total loss 5.95292616\n",
      "Trained batch 2526 batch loss 6.33622694 epoch total loss 5.95307779\n",
      "Trained batch 2527 batch loss 6.20070934 epoch total loss 5.95317602\n",
      "Trained batch 2528 batch loss 6.15479183 epoch total loss 5.95325613\n",
      "Trained batch 2529 batch loss 5.59289169 epoch total loss 5.95311356\n",
      "Trained batch 2530 batch loss 5.09104252 epoch total loss 5.95277262\n",
      "Trained batch 2531 batch loss 6.13640976 epoch total loss 5.9528451\n",
      "Trained batch 2532 batch loss 5.52508 epoch total loss 5.9526763\n",
      "Trained batch 2533 batch loss 5.49281836 epoch total loss 5.9524951\n",
      "Trained batch 2534 batch loss 5.61864758 epoch total loss 5.95236301\n",
      "Trained batch 2535 batch loss 5.56742668 epoch total loss 5.95221138\n",
      "Trained batch 2536 batch loss 6.50980186 epoch total loss 5.9524312\n",
      "Trained batch 2537 batch loss 5.27691746 epoch total loss 5.95216513\n",
      "Trained batch 2538 batch loss 5.17583275 epoch total loss 5.951859\n",
      "Trained batch 2539 batch loss 4.81235504 epoch total loss 5.95141029\n",
      "Trained batch 2540 batch loss 5.05441284 epoch total loss 5.95105743\n",
      "Trained batch 2541 batch loss 5.67623281 epoch total loss 5.95094919\n",
      "Trained batch 2542 batch loss 5.00849771 epoch total loss 5.95057821\n",
      "Trained batch 2543 batch loss 6.04013634 epoch total loss 5.9506135\n",
      "Trained batch 2544 batch loss 5.70914078 epoch total loss 5.95051861\n",
      "Trained batch 2545 batch loss 6.1885829 epoch total loss 5.95061207\n",
      "Trained batch 2546 batch loss 6.01249218 epoch total loss 5.95063639\n",
      "Trained batch 2547 batch loss 5.95196724 epoch total loss 5.95063686\n",
      "Trained batch 2548 batch loss 6.16721582 epoch total loss 5.95072174\n",
      "Trained batch 2549 batch loss 6.36028 epoch total loss 5.95088243\n",
      "Trained batch 2550 batch loss 5.95571804 epoch total loss 5.95088482\n",
      "Trained batch 2551 batch loss 6.27311 epoch total loss 5.95101118\n",
      "Trained batch 2552 batch loss 6.74691057 epoch total loss 5.95132303\n",
      "Trained batch 2553 batch loss 6.17447 epoch total loss 5.95141077\n",
      "Trained batch 2554 batch loss 6.38169289 epoch total loss 5.95157909\n",
      "Trained batch 2555 batch loss 6.23573494 epoch total loss 5.95169\n",
      "Trained batch 2556 batch loss 5.08515 epoch total loss 5.95135117\n",
      "Trained batch 2557 batch loss 5.79251814 epoch total loss 5.95128918\n",
      "Trained batch 2558 batch loss 4.99012566 epoch total loss 5.95091343\n",
      "Trained batch 2559 batch loss 4.45863533 epoch total loss 5.95033026\n",
      "Trained batch 2560 batch loss 6.17900467 epoch total loss 5.95041943\n",
      "Trained batch 2561 batch loss 4.97055292 epoch total loss 5.950037\n",
      "Trained batch 2562 batch loss 7.00665236 epoch total loss 5.95044947\n",
      "Trained batch 2563 batch loss 7.23090839 epoch total loss 5.95094919\n",
      "Trained batch 2564 batch loss 6.99506378 epoch total loss 5.95135641\n",
      "Trained batch 2565 batch loss 6.32150078 epoch total loss 5.95150042\n",
      "Trained batch 2566 batch loss 5.64877605 epoch total loss 5.95138216\n",
      "Trained batch 2567 batch loss 6.37506437 epoch total loss 5.95154715\n",
      "Trained batch 2568 batch loss 5.49740124 epoch total loss 5.95137024\n",
      "Trained batch 2569 batch loss 5.54527378 epoch total loss 5.95121241\n",
      "Trained batch 2570 batch loss 5.91961956 epoch total loss 5.9512\n",
      "Trained batch 2571 batch loss 5.55021667 epoch total loss 5.95104408\n",
      "Trained batch 2572 batch loss 5.93654346 epoch total loss 5.95103836\n",
      "Trained batch 2573 batch loss 5.97649431 epoch total loss 5.95104837\n",
      "Trained batch 2574 batch loss 5.71296 epoch total loss 5.95095539\n",
      "Trained batch 2575 batch loss 5.51138639 epoch total loss 5.95078516\n",
      "Trained batch 2576 batch loss 6.23349714 epoch total loss 5.95089483\n",
      "Trained batch 2577 batch loss 4.69492817 epoch total loss 5.95040751\n",
      "Trained batch 2578 batch loss 5.81219578 epoch total loss 5.9503541\n",
      "Trained batch 2579 batch loss 6.67152929 epoch total loss 5.950634\n",
      "Trained batch 2580 batch loss 5.9940176 epoch total loss 5.95065069\n",
      "Trained batch 2581 batch loss 6.67346907 epoch total loss 5.95093107\n",
      "Trained batch 2582 batch loss 6.55090809 epoch total loss 5.95116329\n",
      "Trained batch 2583 batch loss 4.64916897 epoch total loss 5.95065928\n",
      "Trained batch 2584 batch loss 5.88748741 epoch total loss 5.95063496\n",
      "Trained batch 2585 batch loss 6.70924664 epoch total loss 5.95092821\n",
      "Trained batch 2586 batch loss 6.65398216 epoch total loss 5.9512\n",
      "Trained batch 2587 batch loss 4.77692127 epoch total loss 5.95074654\n",
      "Trained batch 2588 batch loss 5.87957287 epoch total loss 5.95071888\n",
      "Trained batch 2589 batch loss 4.52594185 epoch total loss 5.95016909\n",
      "Trained batch 2590 batch loss 6.8124485 epoch total loss 5.95050192\n",
      "Trained batch 2591 batch loss 7.38886833 epoch total loss 5.95105696\n",
      "Trained batch 2592 batch loss 5.97064972 epoch total loss 5.95106459\n",
      "Trained batch 2593 batch loss 5.98891735 epoch total loss 5.95107937\n",
      "Trained batch 2594 batch loss 6.25298405 epoch total loss 5.95119572\n",
      "Trained batch 2595 batch loss 6.47573137 epoch total loss 5.9513979\n",
      "Trained batch 2596 batch loss 5.53387547 epoch total loss 5.95123672\n",
      "Trained batch 2597 batch loss 5.39559269 epoch total loss 5.9510231\n",
      "Trained batch 2598 batch loss 5.72043896 epoch total loss 5.95093441\n",
      "Trained batch 2599 batch loss 5.23165417 epoch total loss 5.95065737\n",
      "Trained batch 2600 batch loss 5.13025141 epoch total loss 5.9503417\n",
      "Trained batch 2601 batch loss 5.16626453 epoch total loss 5.95004034\n",
      "Trained batch 2602 batch loss 4.40007 epoch total loss 5.94944477\n",
      "Trained batch 2603 batch loss 5.06174 epoch total loss 5.94910336\n",
      "Trained batch 2604 batch loss 4.8318119 epoch total loss 5.94867468\n",
      "Trained batch 2605 batch loss 5.40753078 epoch total loss 5.94846678\n",
      "Trained batch 2606 batch loss 5.28973866 epoch total loss 5.94821405\n",
      "Trained batch 2607 batch loss 4.08052158 epoch total loss 5.94749737\n",
      "Trained batch 2608 batch loss 4.54493237 epoch total loss 5.9469595\n",
      "Trained batch 2609 batch loss 6.06737089 epoch total loss 5.94700575\n",
      "Trained batch 2610 batch loss 6.40988779 epoch total loss 5.94718313\n",
      "Trained batch 2611 batch loss 5.98983383 epoch total loss 5.9472\n",
      "Trained batch 2612 batch loss 6.37866211 epoch total loss 5.94736528\n",
      "Trained batch 2613 batch loss 6.29584169 epoch total loss 5.94749832\n",
      "Trained batch 2614 batch loss 5.77145863 epoch total loss 5.94743109\n",
      "Trained batch 2615 batch loss 5.5685606 epoch total loss 5.94728613\n",
      "Trained batch 2616 batch loss 5.23404312 epoch total loss 5.94701385\n",
      "Trained batch 2617 batch loss 6.16495705 epoch total loss 5.94709682\n",
      "Trained batch 2618 batch loss 6.14813042 epoch total loss 5.94717407\n",
      "Trained batch 2619 batch loss 6.39721251 epoch total loss 5.94734573\n",
      "Trained batch 2620 batch loss 6.69960308 epoch total loss 5.94763279\n",
      "Trained batch 2621 batch loss 6.67689419 epoch total loss 5.94791079\n",
      "Trained batch 2622 batch loss 7.52639484 epoch total loss 5.94851303\n",
      "Trained batch 2623 batch loss 5.4814024 epoch total loss 5.94833469\n",
      "Trained batch 2624 batch loss 6.9621 epoch total loss 5.94872093\n",
      "Trained batch 2625 batch loss 6.6009388 epoch total loss 5.94896936\n",
      "Trained batch 2626 batch loss 6.51435375 epoch total loss 5.94918489\n",
      "Trained batch 2627 batch loss 6.22474432 epoch total loss 5.94929\n",
      "Trained batch 2628 batch loss 6.21387863 epoch total loss 5.94939041\n",
      "Trained batch 2629 batch loss 6.10455561 epoch total loss 5.94944954\n",
      "Trained batch 2630 batch loss 6.32852888 epoch total loss 5.94959354\n",
      "Trained batch 2631 batch loss 4.76140833 epoch total loss 5.94914198\n",
      "Trained batch 2632 batch loss 5.63463879 epoch total loss 5.94902229\n",
      "Trained batch 2633 batch loss 5.43483829 epoch total loss 5.94882727\n",
      "Trained batch 2634 batch loss 5.59361362 epoch total loss 5.94869232\n",
      "Trained batch 2635 batch loss 4.77423191 epoch total loss 5.94824648\n",
      "Trained batch 2636 batch loss 5.01318884 epoch total loss 5.94789219\n",
      "Trained batch 2637 batch loss 5.667799 epoch total loss 5.94778585\n",
      "Trained batch 2638 batch loss 6.06179905 epoch total loss 5.94782925\n",
      "Trained batch 2639 batch loss 5.69416237 epoch total loss 5.94773293\n",
      "Trained batch 2640 batch loss 6.486485 epoch total loss 5.94793701\n",
      "Trained batch 2641 batch loss 6.21519 epoch total loss 5.9480381\n",
      "Trained batch 2642 batch loss 6.31431389 epoch total loss 5.94817686\n",
      "Trained batch 2643 batch loss 5.74441051 epoch total loss 5.94809961\n",
      "Trained batch 2644 batch loss 6.42827606 epoch total loss 5.94828129\n",
      "Trained batch 2645 batch loss 6.1352 epoch total loss 5.94835186\n",
      "Trained batch 2646 batch loss 6.5075469 epoch total loss 5.9485631\n",
      "Trained batch 2647 batch loss 5.4822979 epoch total loss 5.94838715\n",
      "Trained batch 2648 batch loss 6.34447861 epoch total loss 5.94853687\n",
      "Trained batch 2649 batch loss 6.00949812 epoch total loss 5.94855976\n",
      "Trained batch 2650 batch loss 5.98015118 epoch total loss 5.94857216\n",
      "Trained batch 2651 batch loss 5.89160538 epoch total loss 5.9485507\n",
      "Trained batch 2652 batch loss 5.94805098 epoch total loss 5.94855\n",
      "Trained batch 2653 batch loss 5.48444462 epoch total loss 5.94837523\n",
      "Trained batch 2654 batch loss 6.29370785 epoch total loss 5.94850588\n",
      "Trained batch 2655 batch loss 6.94463062 epoch total loss 5.94888067\n",
      "Trained batch 2656 batch loss 6.97134686 epoch total loss 5.94926596\n",
      "Trained batch 2657 batch loss 6.09190941 epoch total loss 5.94931936\n",
      "Trained batch 2658 batch loss 5.94897366 epoch total loss 5.94931936\n",
      "Trained batch 2659 batch loss 5.8612504 epoch total loss 5.94928646\n",
      "Trained batch 2660 batch loss 5.4287219 epoch total loss 5.94909048\n",
      "Trained batch 2661 batch loss 6.08948946 epoch total loss 5.94914341\n",
      "Trained batch 2662 batch loss 5.55797958 epoch total loss 5.94899654\n",
      "Trained batch 2663 batch loss 5.53359652 epoch total loss 5.94884\n",
      "Trained batch 2664 batch loss 6.62343693 epoch total loss 5.94909334\n",
      "Trained batch 2665 batch loss 5.6028 epoch total loss 5.94896317\n",
      "Trained batch 2666 batch loss 5.72608328 epoch total loss 5.94887972\n",
      "Trained batch 2667 batch loss 6.09485626 epoch total loss 5.94893456\n",
      "Trained batch 2668 batch loss 6.4975071 epoch total loss 5.94914\n",
      "Trained batch 2669 batch loss 5.76894188 epoch total loss 5.94907236\n",
      "Trained batch 2670 batch loss 5.74795341 epoch total loss 5.94899702\n",
      "Trained batch 2671 batch loss 5.9864397 epoch total loss 5.94901085\n",
      "Trained batch 2672 batch loss 5.83034706 epoch total loss 5.9489665\n",
      "Trained batch 2673 batch loss 5.90102339 epoch total loss 5.94894886\n",
      "Trained batch 2674 batch loss 5.8829565 epoch total loss 5.94892406\n",
      "Trained batch 2675 batch loss 5.92801571 epoch total loss 5.94891596\n",
      "Trained batch 2676 batch loss 5.8282156 epoch total loss 5.94887114\n",
      "Trained batch 2677 batch loss 6.71627569 epoch total loss 5.94915724\n",
      "Trained batch 2678 batch loss 6.21171093 epoch total loss 5.94925547\n",
      "Trained batch 2679 batch loss 6.20882416 epoch total loss 5.94935274\n",
      "Trained batch 2680 batch loss 5.96056461 epoch total loss 5.94935703\n",
      "Trained batch 2681 batch loss 6.48232555 epoch total loss 5.94955587\n",
      "Trained batch 2682 batch loss 6.06529522 epoch total loss 5.94959879\n",
      "Trained batch 2683 batch loss 6.54624224 epoch total loss 5.949821\n",
      "Trained batch 2684 batch loss 5.59681892 epoch total loss 5.94968939\n",
      "Trained batch 2685 batch loss 5.94579697 epoch total loss 5.94968796\n",
      "Trained batch 2686 batch loss 5.72131538 epoch total loss 5.94960308\n",
      "Trained batch 2687 batch loss 5.26971674 epoch total loss 5.94935\n",
      "Trained batch 2688 batch loss 5.75580025 epoch total loss 5.94927788\n",
      "Trained batch 2689 batch loss 4.83679485 epoch total loss 5.94886446\n",
      "Trained batch 2690 batch loss 4.63451576 epoch total loss 5.9483757\n",
      "Trained batch 2691 batch loss 4.52821445 epoch total loss 5.94784784\n",
      "Trained batch 2692 batch loss 4.77632332 epoch total loss 5.94741297\n",
      "Trained batch 2693 batch loss 5.31428337 epoch total loss 5.94717789\n",
      "Trained batch 2694 batch loss 5.07042742 epoch total loss 5.94685221\n",
      "Trained batch 2695 batch loss 6.04645634 epoch total loss 5.9468894\n",
      "Trained batch 2696 batch loss 6.08112717 epoch total loss 5.94693947\n",
      "Trained batch 2697 batch loss 6.19885826 epoch total loss 5.94703293\n",
      "Trained batch 2698 batch loss 6.079144 epoch total loss 5.94708157\n",
      "Trained batch 2699 batch loss 6.8980279 epoch total loss 5.94743443\n",
      "Trained batch 2700 batch loss 6.90046501 epoch total loss 5.94778728\n",
      "Trained batch 2701 batch loss 6.08748722 epoch total loss 5.94783926\n",
      "Trained batch 2702 batch loss 5.42524242 epoch total loss 5.94764566\n",
      "Trained batch 2703 batch loss 6.37629271 epoch total loss 5.94780397\n",
      "Trained batch 2704 batch loss 6.59963179 epoch total loss 5.94804478\n",
      "Trained batch 2705 batch loss 6.21500874 epoch total loss 5.94814348\n",
      "Trained batch 2706 batch loss 5.87418652 epoch total loss 5.9481163\n",
      "Trained batch 2707 batch loss 6.12854147 epoch total loss 5.94818306\n",
      "Trained batch 2708 batch loss 6.22930908 epoch total loss 5.94828701\n",
      "Trained batch 2709 batch loss 6.25887203 epoch total loss 5.94840145\n",
      "Trained batch 2710 batch loss 5.6380949 epoch total loss 5.94828701\n",
      "Trained batch 2711 batch loss 5.6756 epoch total loss 5.9481864\n",
      "Trained batch 2712 batch loss 5.23096943 epoch total loss 5.94792223\n",
      "Trained batch 2713 batch loss 5.79459858 epoch total loss 5.94786549\n",
      "Trained batch 2714 batch loss 5.78958702 epoch total loss 5.94780731\n",
      "Trained batch 2715 batch loss 6.03716612 epoch total loss 5.94784\n",
      "Trained batch 2716 batch loss 6.20175076 epoch total loss 5.94793415\n",
      "Trained batch 2717 batch loss 5.84135103 epoch total loss 5.94789505\n",
      "Trained batch 2718 batch loss 7.21192074 epoch total loss 5.94836\n",
      "Trained batch 2719 batch loss 6.50366354 epoch total loss 5.94856453\n",
      "Trained batch 2720 batch loss 4.78975 epoch total loss 5.94813824\n",
      "Trained batch 2721 batch loss 7.12923384 epoch total loss 5.94857216\n",
      "Trained batch 2722 batch loss 6.43287182 epoch total loss 5.94875\n",
      "Trained batch 2723 batch loss 6.106493 epoch total loss 5.94880819\n",
      "Trained batch 2724 batch loss 5.59945965 epoch total loss 5.94868\n",
      "Trained batch 2725 batch loss 5.771 epoch total loss 5.94861507\n",
      "Trained batch 2726 batch loss 5.45060492 epoch total loss 5.94843197\n",
      "Trained batch 2727 batch loss 5.61695766 epoch total loss 5.94831038\n",
      "Trained batch 2728 batch loss 6.23327732 epoch total loss 5.94841528\n",
      "Trained batch 2729 batch loss 6.45128059 epoch total loss 5.94859934\n",
      "Trained batch 2730 batch loss 6.05737782 epoch total loss 5.94863939\n",
      "Trained batch 2731 batch loss 6.29669666 epoch total loss 5.94876671\n",
      "Trained batch 2732 batch loss 6.1918087 epoch total loss 5.9488554\n",
      "Trained batch 2733 batch loss 5.54613209 epoch total loss 5.94870806\n",
      "Trained batch 2734 batch loss 6.09848595 epoch total loss 5.94876289\n",
      "Trained batch 2735 batch loss 6.32973671 epoch total loss 5.94890261\n",
      "Trained batch 2736 batch loss 5.57638597 epoch total loss 5.94876623\n",
      "Trained batch 2737 batch loss 5.68394232 epoch total loss 5.94866943\n",
      "Trained batch 2738 batch loss 5.80870628 epoch total loss 5.94861794\n",
      "Trained batch 2739 batch loss 6.05187511 epoch total loss 5.94865561\n",
      "Trained batch 2740 batch loss 4.94471359 epoch total loss 5.94828939\n",
      "Trained batch 2741 batch loss 5.64205456 epoch total loss 5.94817734\n",
      "Trained batch 2742 batch loss 5.83151722 epoch total loss 5.94813442\n",
      "Trained batch 2743 batch loss 5.83215094 epoch total loss 5.94809246\n",
      "Trained batch 2744 batch loss 7.18221092 epoch total loss 5.94854212\n",
      "Trained batch 2745 batch loss 6.59240818 epoch total loss 5.94877672\n",
      "Trained batch 2746 batch loss 6.21349049 epoch total loss 5.94887352\n",
      "Trained batch 2747 batch loss 6.10801029 epoch total loss 5.94893169\n",
      "Trained batch 2748 batch loss 5.94921446 epoch total loss 5.94893169\n",
      "Trained batch 2749 batch loss 6.09787178 epoch total loss 5.94898558\n",
      "Trained batch 2750 batch loss 5.62539673 epoch total loss 5.9488678\n",
      "Trained batch 2751 batch loss 4.10684204 epoch total loss 5.94819832\n",
      "Trained batch 2752 batch loss 5.37231493 epoch total loss 5.94798899\n",
      "Trained batch 2753 batch loss 5.80421591 epoch total loss 5.94793653\n",
      "Trained batch 2754 batch loss 5.28746462 epoch total loss 5.94769669\n",
      "Trained batch 2755 batch loss 6.89664459 epoch total loss 5.94804144\n",
      "Trained batch 2756 batch loss 6.6076622 epoch total loss 5.94828033\n",
      "Trained batch 2757 batch loss 6.37626934 epoch total loss 5.94843578\n",
      "Trained batch 2758 batch loss 6.83902168 epoch total loss 5.94875908\n",
      "Trained batch 2759 batch loss 6.54332447 epoch total loss 5.94897461\n",
      "Trained batch 2760 batch loss 6.32185459 epoch total loss 5.94910955\n",
      "Trained batch 2761 batch loss 6.33206606 epoch total loss 5.94924831\n",
      "Trained batch 2762 batch loss 6.15566444 epoch total loss 5.94932318\n",
      "Trained batch 2763 batch loss 6.61799145 epoch total loss 5.94956493\n",
      "Trained batch 2764 batch loss 5.1157341 epoch total loss 5.9492631\n",
      "Trained batch 2765 batch loss 6.33416224 epoch total loss 5.94940233\n",
      "Trained batch 2766 batch loss 6.64571285 epoch total loss 5.94965458\n",
      "Trained batch 2767 batch loss 5.38863039 epoch total loss 5.94945192\n",
      "Trained batch 2768 batch loss 6.16326237 epoch total loss 5.94952917\n",
      "Trained batch 2769 batch loss 6.72817421 epoch total loss 5.9498105\n",
      "Trained batch 2770 batch loss 5.6304245 epoch total loss 5.94969559\n",
      "Trained batch 2771 batch loss 6.35668 epoch total loss 5.94984245\n",
      "Trained batch 2772 batch loss 6.80154705 epoch total loss 5.95014954\n",
      "Trained batch 2773 batch loss 6.34451389 epoch total loss 5.95029163\n",
      "Trained batch 2774 batch loss 4.50767708 epoch total loss 5.9497714\n",
      "Trained batch 2775 batch loss 4.42503643 epoch total loss 5.94922209\n",
      "Trained batch 2776 batch loss 4.22154093 epoch total loss 5.9486\n",
      "Trained batch 2777 batch loss 5.62755299 epoch total loss 5.94848394\n",
      "Trained batch 2778 batch loss 6.03051805 epoch total loss 5.94851351\n",
      "Trained batch 2779 batch loss 5.86685228 epoch total loss 5.94848442\n",
      "Trained batch 2780 batch loss 6.19095087 epoch total loss 5.94857168\n",
      "Trained batch 2781 batch loss 5.26673079 epoch total loss 5.94832659\n",
      "Trained batch 2782 batch loss 6.19776249 epoch total loss 5.94841623\n",
      "Trained batch 2783 batch loss 6.08506107 epoch total loss 5.94846582\n",
      "Trained batch 2784 batch loss 6.00260162 epoch total loss 5.9484849\n",
      "Trained batch 2785 batch loss 6.53194046 epoch total loss 5.94869423\n",
      "Trained batch 2786 batch loss 6.25573063 epoch total loss 5.94880438\n",
      "Trained batch 2787 batch loss 5.71632814 epoch total loss 5.94872141\n",
      "Trained batch 2788 batch loss 5.81585026 epoch total loss 5.94867373\n",
      "Trained batch 2789 batch loss 6.52043343 epoch total loss 5.94887829\n",
      "Trained batch 2790 batch loss 5.92510319 epoch total loss 5.94887\n",
      "Trained batch 2791 batch loss 6.19155598 epoch total loss 5.94895697\n",
      "Trained batch 2792 batch loss 6.02289295 epoch total loss 5.94898367\n",
      "Trained batch 2793 batch loss 6.1284914 epoch total loss 5.94904804\n",
      "Trained batch 2794 batch loss 5.86256886 epoch total loss 5.94901752\n",
      "Trained batch 2795 batch loss 6.79087639 epoch total loss 5.94931889\n",
      "Trained batch 2796 batch loss 6.25533676 epoch total loss 5.94942808\n",
      "Trained batch 2797 batch loss 6.28483105 epoch total loss 5.94954824\n",
      "Trained batch 2798 batch loss 6.23154736 epoch total loss 5.94964933\n",
      "Trained batch 2799 batch loss 5.81008 epoch total loss 5.94959974\n",
      "Trained batch 2800 batch loss 5.89024878 epoch total loss 5.94957876\n",
      "Trained batch 2801 batch loss 6.29031467 epoch total loss 5.94970036\n",
      "Trained batch 2802 batch loss 6.43092108 epoch total loss 5.94987249\n",
      "Trained batch 2803 batch loss 6.40058708 epoch total loss 5.95003319\n",
      "Trained batch 2804 batch loss 5.55338764 epoch total loss 5.94989157\n",
      "Trained batch 2805 batch loss 5.36323881 epoch total loss 5.94968271\n",
      "Trained batch 2806 batch loss 6.05431032 epoch total loss 5.94972\n",
      "Trained batch 2807 batch loss 5.89188 epoch total loss 5.9496994\n",
      "Trained batch 2808 batch loss 5.58434105 epoch total loss 5.94956923\n",
      "Trained batch 2809 batch loss 6.02548 epoch total loss 5.94959641\n",
      "Trained batch 2810 batch loss 6.14834213 epoch total loss 5.94966698\n",
      "Trained batch 2811 batch loss 5.81111956 epoch total loss 5.94961739\n",
      "Trained batch 2812 batch loss 6.28652859 epoch total loss 5.94973755\n",
      "Trained batch 2813 batch loss 5.91536093 epoch total loss 5.94972563\n",
      "Trained batch 2814 batch loss 5.55787182 epoch total loss 5.94958639\n",
      "Trained batch 2815 batch loss 4.77124214 epoch total loss 5.94916821\n",
      "Trained batch 2816 batch loss 5.11667347 epoch total loss 5.94887257\n",
      "Trained batch 2817 batch loss 5.86884785 epoch total loss 5.94884443\n",
      "Trained batch 2818 batch loss 6.00848341 epoch total loss 5.94886541\n",
      "Trained batch 2819 batch loss 5.98884487 epoch total loss 5.94887924\n",
      "Trained batch 2820 batch loss 5.84637165 epoch total loss 5.94884253\n",
      "Trained batch 2821 batch loss 6.02188492 epoch total loss 5.94886827\n",
      "Trained batch 2822 batch loss 5.86713791 epoch total loss 5.94883966\n",
      "Trained batch 2823 batch loss 5.81107903 epoch total loss 5.94879055\n",
      "Trained batch 2824 batch loss 5.55690384 epoch total loss 5.94865179\n",
      "Trained batch 2825 batch loss 5.6275177 epoch total loss 5.94853783\n",
      "Trained batch 2826 batch loss 5.62730408 epoch total loss 5.94842386\n",
      "Trained batch 2827 batch loss 5.74835873 epoch total loss 5.94835329\n",
      "Trained batch 2828 batch loss 5.97161913 epoch total loss 5.94836092\n",
      "Trained batch 2829 batch loss 5.22273731 epoch total loss 5.94810438\n",
      "Trained batch 2830 batch loss 5.71465492 epoch total loss 5.94802189\n",
      "Trained batch 2831 batch loss 5.98939514 epoch total loss 5.94803715\n",
      "Trained batch 2832 batch loss 5.60789394 epoch total loss 5.94791651\n",
      "Trained batch 2833 batch loss 5.65993404 epoch total loss 5.94781494\n",
      "Trained batch 2834 batch loss 5.90325832 epoch total loss 5.94779921\n",
      "Trained batch 2835 batch loss 5.25404 epoch total loss 5.94755411\n",
      "Trained batch 2836 batch loss 4.58976698 epoch total loss 5.94707537\n",
      "Trained batch 2837 batch loss 4.53947449 epoch total loss 5.94657946\n",
      "Trained batch 2838 batch loss 5.72082043 epoch total loss 5.9465\n",
      "Trained batch 2839 batch loss 5.66194344 epoch total loss 5.94639969\n",
      "Trained batch 2840 batch loss 5.71890736 epoch total loss 5.9463191\n",
      "Trained batch 2841 batch loss 5.95413113 epoch total loss 5.94632244\n",
      "Trained batch 2842 batch loss 5.13064575 epoch total loss 5.94603539\n",
      "Trained batch 2843 batch loss 6.29636192 epoch total loss 5.94615889\n",
      "Trained batch 2844 batch loss 6.40833378 epoch total loss 5.94632149\n",
      "Trained batch 2845 batch loss 6.17895794 epoch total loss 5.9464035\n",
      "Trained batch 2846 batch loss 6.12924957 epoch total loss 5.9464674\n",
      "Trained batch 2847 batch loss 6.23391247 epoch total loss 5.94656849\n",
      "Trained batch 2848 batch loss 6.4162693 epoch total loss 5.94673347\n",
      "Trained batch 2849 batch loss 5.43169212 epoch total loss 5.94655275\n",
      "Trained batch 2850 batch loss 4.70056772 epoch total loss 5.94611549\n",
      "Trained batch 2851 batch loss 5.25819397 epoch total loss 5.94587421\n",
      "Trained batch 2852 batch loss 5.22024584 epoch total loss 5.94562\n",
      "Trained batch 2853 batch loss 5.30211163 epoch total loss 5.94539452\n",
      "Trained batch 2854 batch loss 5.3140192 epoch total loss 5.94517374\n",
      "Trained batch 2855 batch loss 6.17949057 epoch total loss 5.94525576\n",
      "Trained batch 2856 batch loss 5.95336342 epoch total loss 5.94525862\n",
      "Trained batch 2857 batch loss 5.82203484 epoch total loss 5.94521523\n",
      "Trained batch 2858 batch loss 5.16453028 epoch total loss 5.944942\n",
      "Trained batch 2859 batch loss 4.39244175 epoch total loss 5.94439936\n",
      "Trained batch 2860 batch loss 5.01557541 epoch total loss 5.94407415\n",
      "Trained batch 2861 batch loss 4.8358984 epoch total loss 5.94368696\n",
      "Trained batch 2862 batch loss 5.08633757 epoch total loss 5.94338751\n",
      "Trained batch 2863 batch loss 5.34418392 epoch total loss 5.9431777\n",
      "Trained batch 2864 batch loss 6.69000816 epoch total loss 5.94343853\n",
      "Trained batch 2865 batch loss 6.38781548 epoch total loss 5.94359398\n",
      "Trained batch 2866 batch loss 5.87386274 epoch total loss 5.94356918\n",
      "Trained batch 2867 batch loss 6.06360626 epoch total loss 5.94361162\n",
      "Trained batch 2868 batch loss 6.07241344 epoch total loss 5.94365644\n",
      "Trained batch 2869 batch loss 5.74673176 epoch total loss 5.9435873\n",
      "Trained batch 2870 batch loss 6.13170052 epoch total loss 5.94365263\n",
      "Trained batch 2871 batch loss 6.00662518 epoch total loss 5.94367456\n",
      "Trained batch 2872 batch loss 5.80519247 epoch total loss 5.94362593\n",
      "Trained batch 2873 batch loss 5.97164345 epoch total loss 5.94363546\n",
      "Trained batch 2874 batch loss 5.5113678 epoch total loss 5.94348526\n",
      "Trained batch 2875 batch loss 5.72167683 epoch total loss 5.94340754\n",
      "Trained batch 2876 batch loss 6.08508921 epoch total loss 5.94345713\n",
      "Trained batch 2877 batch loss 5.03629589 epoch total loss 5.94314194\n",
      "Trained batch 2878 batch loss 5.96374226 epoch total loss 5.94314909\n",
      "Trained batch 2879 batch loss 5.98952198 epoch total loss 5.9431653\n",
      "Trained batch 2880 batch loss 5.95609093 epoch total loss 5.94317\n",
      "Trained batch 2881 batch loss 5.70052719 epoch total loss 5.94308615\n",
      "Trained batch 2882 batch loss 5.16526031 epoch total loss 5.94281673\n",
      "Trained batch 2883 batch loss 6.25336123 epoch total loss 5.9429245\n",
      "Trained batch 2884 batch loss 5.8380003 epoch total loss 5.94288826\n",
      "Trained batch 2885 batch loss 5.21055794 epoch total loss 5.94263411\n",
      "Trained batch 2886 batch loss 5.2622757 epoch total loss 5.94239855\n",
      "Trained batch 2887 batch loss 5.43878078 epoch total loss 5.94222403\n",
      "Trained batch 2888 batch loss 6.17771339 epoch total loss 5.94230556\n",
      "Trained batch 2889 batch loss 6.08327436 epoch total loss 5.94235468\n",
      "Trained batch 2890 batch loss 6.04414463 epoch total loss 5.94239044\n",
      "Trained batch 2891 batch loss 6.13474369 epoch total loss 5.94245672\n",
      "Trained batch 2892 batch loss 6.24065685 epoch total loss 5.94255972\n",
      "Trained batch 2893 batch loss 5.75647068 epoch total loss 5.94249535\n",
      "Trained batch 2894 batch loss 5.63874865 epoch total loss 5.94239044\n",
      "Trained batch 2895 batch loss 5.45115852 epoch total loss 5.94222069\n",
      "Trained batch 2896 batch loss 4.65125 epoch total loss 5.94177437\n",
      "Trained batch 2897 batch loss 5.47313929 epoch total loss 5.94161272\n",
      "Trained batch 2898 batch loss 6.70509815 epoch total loss 5.94187593\n",
      "Trained batch 2899 batch loss 5.50689268 epoch total loss 5.94172621\n",
      "Trained batch 2900 batch loss 5.69601917 epoch total loss 5.94164133\n",
      "Trained batch 2901 batch loss 5.39094162 epoch total loss 5.94145155\n",
      "Trained batch 2902 batch loss 5.74169922 epoch total loss 5.94138288\n",
      "Trained batch 2903 batch loss 5.35132217 epoch total loss 5.94117975\n",
      "Trained batch 2904 batch loss 6.18014526 epoch total loss 5.94126177\n",
      "Trained batch 2905 batch loss 5.71034622 epoch total loss 5.94118214\n",
      "Trained batch 2906 batch loss 5.06834221 epoch total loss 5.94088221\n",
      "Trained batch 2907 batch loss 4.99636173 epoch total loss 5.940557\n",
      "Trained batch 2908 batch loss 6.57974339 epoch total loss 5.94077682\n",
      "Trained batch 2909 batch loss 6.04477406 epoch total loss 5.94081259\n",
      "Trained batch 2910 batch loss 6.09288406 epoch total loss 5.94086552\n",
      "Trained batch 2911 batch loss 5.74736786 epoch total loss 5.94079924\n",
      "Trained batch 2912 batch loss 4.64218473 epoch total loss 5.94035339\n",
      "Trained batch 2913 batch loss 4.62687874 epoch total loss 5.93990231\n",
      "Trained batch 2914 batch loss 4.5152688 epoch total loss 5.93941355\n",
      "Trained batch 2915 batch loss 5.77049351 epoch total loss 5.93935537\n",
      "Trained batch 2916 batch loss 4.91201448 epoch total loss 5.93900299\n",
      "Trained batch 2917 batch loss 5.95716238 epoch total loss 5.93900919\n",
      "Trained batch 2918 batch loss 6.09137774 epoch total loss 5.93906164\n",
      "Trained batch 2919 batch loss 5.64070702 epoch total loss 5.93895912\n",
      "Trained batch 2920 batch loss 6.06998873 epoch total loss 5.93900442\n",
      "Trained batch 2921 batch loss 5.99013281 epoch total loss 5.93902206\n",
      "Trained batch 2922 batch loss 5.75439119 epoch total loss 5.93895864\n",
      "Trained batch 2923 batch loss 5.59860516 epoch total loss 5.93884182\n",
      "Trained batch 2924 batch loss 5.43528748 epoch total loss 5.93866968\n",
      "Trained batch 2925 batch loss 6.08783865 epoch total loss 5.9387207\n",
      "Trained batch 2926 batch loss 6.63902521 epoch total loss 5.93896\n",
      "Trained batch 2927 batch loss 6.02406216 epoch total loss 5.93898869\n",
      "Trained batch 2928 batch loss 5.15721703 epoch total loss 5.93872118\n",
      "Trained batch 2929 batch loss 5.88489246 epoch total loss 5.93870306\n",
      "Trained batch 2930 batch loss 6.05163479 epoch total loss 5.93874121\n",
      "Trained batch 2931 batch loss 5.3500185 epoch total loss 5.93854\n",
      "Trained batch 2932 batch loss 4.98485184 epoch total loss 5.93821478\n",
      "Trained batch 2933 batch loss 3.96016121 epoch total loss 5.93754053\n",
      "Trained batch 2934 batch loss 5.74905539 epoch total loss 5.93747663\n",
      "Trained batch 2935 batch loss 6.43566704 epoch total loss 5.93764639\n",
      "Trained batch 2936 batch loss 6.49738741 epoch total loss 5.93783712\n",
      "Trained batch 2937 batch loss 6.44616 epoch total loss 5.93801\n",
      "Trained batch 2938 batch loss 5.8618 epoch total loss 5.93798399\n",
      "Trained batch 2939 batch loss 5.89346457 epoch total loss 5.93796873\n",
      "Trained batch 2940 batch loss 5.52242041 epoch total loss 5.93782663\n",
      "Trained batch 2941 batch loss 5.41651821 epoch total loss 5.93764925\n",
      "Trained batch 2942 batch loss 6.9481945 epoch total loss 5.93799257\n",
      "Trained batch 2943 batch loss 6.52828407 epoch total loss 5.93819284\n",
      "Trained batch 2944 batch loss 6.09774971 epoch total loss 5.9382472\n",
      "Trained batch 2945 batch loss 7.05948734 epoch total loss 5.93862724\n",
      "Trained batch 2946 batch loss 6.17869759 epoch total loss 5.93870878\n",
      "Trained batch 2947 batch loss 6.08175 epoch total loss 5.93875742\n",
      "Trained batch 2948 batch loss 5.92198944 epoch total loss 5.9387517\n",
      "Trained batch 2949 batch loss 5.64826 epoch total loss 5.93865299\n",
      "Trained batch 2950 batch loss 5.49377537 epoch total loss 5.93850231\n",
      "Trained batch 2951 batch loss 4.50762272 epoch total loss 5.93801737\n",
      "Trained batch 2952 batch loss 4.43318844 epoch total loss 5.93750811\n",
      "Trained batch 2953 batch loss 4.40841818 epoch total loss 5.93699026\n",
      "Trained batch 2954 batch loss 4.57684231 epoch total loss 5.93652916\n",
      "Trained batch 2955 batch loss 4.61773586 epoch total loss 5.93608284\n",
      "Trained batch 2956 batch loss 4.83503151 epoch total loss 5.93571091\n",
      "Trained batch 2957 batch loss 4.56603622 epoch total loss 5.9352479\n",
      "Trained batch 2958 batch loss 4.63333225 epoch total loss 5.9348073\n",
      "Trained batch 2959 batch loss 4.9363718 epoch total loss 5.9344697\n",
      "Trained batch 2960 batch loss 4.07328081 epoch total loss 5.93384123\n",
      "Trained batch 2961 batch loss 4.05497932 epoch total loss 5.93320656\n",
      "Trained batch 2962 batch loss 4.76642561 epoch total loss 5.93281221\n",
      "Trained batch 2963 batch loss 3.72870898 epoch total loss 5.93206835\n",
      "Trained batch 2964 batch loss 4.504951 epoch total loss 5.93158722\n",
      "Trained batch 2965 batch loss 4.6680727 epoch total loss 5.93116093\n",
      "Trained batch 2966 batch loss 4.59248543 epoch total loss 5.93070936\n",
      "Trained batch 2967 batch loss 4.55153799 epoch total loss 5.93024445\n",
      "Trained batch 2968 batch loss 4.50349712 epoch total loss 5.92976379\n",
      "Trained batch 2969 batch loss 3.66368937 epoch total loss 5.92900085\n",
      "Trained batch 2970 batch loss 5.56217194 epoch total loss 5.92887735\n",
      "Trained batch 2971 batch loss 6.3641777 epoch total loss 5.92902374\n",
      "Trained batch 2972 batch loss 5.01457357 epoch total loss 5.92871571\n",
      "Trained batch 2973 batch loss 6.05180883 epoch total loss 5.92875719\n",
      "Trained batch 2974 batch loss 5.75771046 epoch total loss 5.9287\n",
      "Trained batch 2975 batch loss 6.33944321 epoch total loss 5.92883778\n",
      "Trained batch 2976 batch loss 6.44614601 epoch total loss 5.92901134\n",
      "Trained batch 2977 batch loss 5.62924623 epoch total loss 5.92891073\n",
      "Trained batch 2978 batch loss 6.30219173 epoch total loss 5.92903614\n",
      "Trained batch 2979 batch loss 6.43855238 epoch total loss 5.9292078\n",
      "Trained batch 2980 batch loss 6.27822161 epoch total loss 5.92932463\n",
      "Trained batch 2981 batch loss 5.52252293 epoch total loss 5.92918825\n",
      "Trained batch 2982 batch loss 5.99726152 epoch total loss 5.92921114\n",
      "Trained batch 2983 batch loss 6.78095913 epoch total loss 5.92949677\n",
      "Trained batch 2984 batch loss 6.20729685 epoch total loss 5.92958975\n",
      "Trained batch 2985 batch loss 6.22067785 epoch total loss 5.9296875\n",
      "Trained batch 2986 batch loss 6.46352768 epoch total loss 5.92986584\n",
      "Trained batch 2987 batch loss 6.06633329 epoch total loss 5.92991161\n",
      "Trained batch 2988 batch loss 6.7140522 epoch total loss 5.93017435\n",
      "Trained batch 2989 batch loss 6.8966918 epoch total loss 5.93049765\n",
      "Trained batch 2990 batch loss 6.80364752 epoch total loss 5.93078947\n",
      "Trained batch 2991 batch loss 7.36353493 epoch total loss 5.93126822\n",
      "Trained batch 2992 batch loss 7.38155 epoch total loss 5.93175268\n",
      "Trained batch 2993 batch loss 7.36354256 epoch total loss 5.93223143\n",
      "Trained batch 2994 batch loss 7.24786329 epoch total loss 5.93267059\n",
      "Trained batch 2995 batch loss 6.97373581 epoch total loss 5.93301868\n",
      "Trained batch 2996 batch loss 7.46190834 epoch total loss 5.93352842\n",
      "Trained batch 2997 batch loss 7.21389675 epoch total loss 5.93395615\n",
      "Trained batch 2998 batch loss 5.54032373 epoch total loss 5.93382502\n",
      "Trained batch 2999 batch loss 6.23595715 epoch total loss 5.93392611\n",
      "Trained batch 3000 batch loss 5.28924894 epoch total loss 5.93371105\n",
      "Trained batch 3001 batch loss 4.95911694 epoch total loss 5.93338633\n",
      "Trained batch 3002 batch loss 4.68321514 epoch total loss 5.93297\n",
      "Trained batch 3003 batch loss 4.91221523 epoch total loss 5.93263\n",
      "Trained batch 3004 batch loss 6.35939598 epoch total loss 5.93277216\n",
      "Trained batch 3005 batch loss 5.76376343 epoch total loss 5.93271589\n",
      "Trained batch 3006 batch loss 6.49461079 epoch total loss 5.93290234\n",
      "Trained batch 3007 batch loss 6.34020329 epoch total loss 5.93303776\n",
      "Trained batch 3008 batch loss 4.72764349 epoch total loss 5.93263721\n",
      "Trained batch 3009 batch loss 6.11091805 epoch total loss 5.93269682\n",
      "Trained batch 3010 batch loss 4.76178837 epoch total loss 5.93230772\n",
      "Trained batch 3011 batch loss 5.91000652 epoch total loss 5.9323\n",
      "Trained batch 3012 batch loss 4.64756298 epoch total loss 5.93187428\n",
      "Trained batch 3013 batch loss 4.58269072 epoch total loss 5.93142605\n",
      "Trained batch 3014 batch loss 5.91599274 epoch total loss 5.9314208\n",
      "Trained batch 3015 batch loss 5.24289656 epoch total loss 5.9311924\n",
      "Trained batch 3016 batch loss 6.96157169 epoch total loss 5.93153381\n",
      "Trained batch 3017 batch loss 6.13503885 epoch total loss 5.93160105\n",
      "Trained batch 3018 batch loss 6.25320625 epoch total loss 5.93170786\n",
      "Trained batch 3019 batch loss 6.24020672 epoch total loss 5.93181\n",
      "Trained batch 3020 batch loss 5.92203331 epoch total loss 5.93180704\n",
      "Trained batch 3021 batch loss 6.17137575 epoch total loss 5.9318862\n",
      "Trained batch 3022 batch loss 6.12366486 epoch total loss 5.93194962\n",
      "Trained batch 3023 batch loss 6.46853638 epoch total loss 5.932127\n",
      "Trained batch 3024 batch loss 6.04088879 epoch total loss 5.93216324\n",
      "Trained batch 3025 batch loss 6.35063553 epoch total loss 5.932302\n",
      "Trained batch 3026 batch loss 6.0394 epoch total loss 5.93233728\n",
      "Trained batch 3027 batch loss 6.10958767 epoch total loss 5.93239546\n",
      "Trained batch 3028 batch loss 6.08776569 epoch total loss 5.93244696\n",
      "Trained batch 3029 batch loss 6.10120583 epoch total loss 5.93250275\n",
      "Trained batch 3030 batch loss 5.76366329 epoch total loss 5.93244696\n",
      "Trained batch 3031 batch loss 6.11181784 epoch total loss 5.93250608\n",
      "Trained batch 3032 batch loss 6.03372765 epoch total loss 5.93253946\n",
      "Trained batch 3033 batch loss 5.85540152 epoch total loss 5.93251371\n",
      "Trained batch 3034 batch loss 6.45708704 epoch total loss 5.93268681\n",
      "Trained batch 3035 batch loss 5.93878603 epoch total loss 5.93268871\n",
      "Trained batch 3036 batch loss 5.88412809 epoch total loss 5.93267298\n",
      "Trained batch 3037 batch loss 6.1616478 epoch total loss 5.93274879\n",
      "Trained batch 3038 batch loss 5.94251871 epoch total loss 5.93275213\n",
      "Trained batch 3039 batch loss 5.98449707 epoch total loss 5.9327693\n",
      "Trained batch 3040 batch loss 5.87878227 epoch total loss 5.93275166\n",
      "Trained batch 3041 batch loss 6.17338467 epoch total loss 5.93283081\n",
      "Trained batch 3042 batch loss 6.21465111 epoch total loss 5.93292332\n",
      "Trained batch 3043 batch loss 5.90885925 epoch total loss 5.93291521\n",
      "Trained batch 3044 batch loss 6.12358952 epoch total loss 5.93297768\n",
      "Trained batch 3045 batch loss 5.76900482 epoch total loss 5.93292427\n",
      "Trained batch 3046 batch loss 5.88864803 epoch total loss 5.93290949\n",
      "Trained batch 3047 batch loss 6.00348759 epoch total loss 5.93293285\n",
      "Trained batch 3048 batch loss 5.68360758 epoch total loss 5.93285084\n",
      "Trained batch 3049 batch loss 6.18674278 epoch total loss 5.93293476\n",
      "Trained batch 3050 batch loss 5.78595638 epoch total loss 5.93288612\n",
      "Trained batch 3051 batch loss 6.02864027 epoch total loss 5.93291759\n",
      "Trained batch 3052 batch loss 5.77213573 epoch total loss 5.93286467\n",
      "Trained batch 3053 batch loss 5.62071323 epoch total loss 5.93276262\n",
      "Trained batch 3054 batch loss 5.99997 epoch total loss 5.93278456\n",
      "Trained batch 3055 batch loss 6.20396233 epoch total loss 5.93287325\n",
      "Trained batch 3056 batch loss 6.6146965 epoch total loss 5.93309641\n",
      "Trained batch 3057 batch loss 6.48738432 epoch total loss 5.93327808\n",
      "Trained batch 3058 batch loss 6.40215302 epoch total loss 5.93343163\n",
      "Trained batch 3059 batch loss 6.26045322 epoch total loss 5.93353844\n",
      "Trained batch 3060 batch loss 6.24474525 epoch total loss 5.93363953\n",
      "Trained batch 3061 batch loss 6.53437805 epoch total loss 5.93383598\n",
      "Trained batch 3062 batch loss 6.67260742 epoch total loss 5.93407726\n",
      "Trained batch 3063 batch loss 5.89018106 epoch total loss 5.93406296\n",
      "Trained batch 3064 batch loss 5.95668888 epoch total loss 5.93407059\n",
      "Trained batch 3065 batch loss 5.26742029 epoch total loss 5.93385315\n",
      "Trained batch 3066 batch loss 5.7876606 epoch total loss 5.93380547\n",
      "Trained batch 3067 batch loss 5.76540279 epoch total loss 5.93375063\n",
      "Trained batch 3068 batch loss 6.25423288 epoch total loss 5.93385458\n",
      "Trained batch 3069 batch loss 5.11783504 epoch total loss 5.9335885\n",
      "Trained batch 3070 batch loss 5.97522449 epoch total loss 5.93360186\n",
      "Trained batch 3071 batch loss 6.10277176 epoch total loss 5.93365717\n",
      "Trained batch 3072 batch loss 6.18150806 epoch total loss 5.93373823\n",
      "Trained batch 3073 batch loss 6.31011057 epoch total loss 5.93386078\n",
      "Trained batch 3074 batch loss 6.36083364 epoch total loss 5.93399954\n",
      "Trained batch 3075 batch loss 6.21087742 epoch total loss 5.93408966\n",
      "Trained batch 3076 batch loss 6.15908289 epoch total loss 5.93416262\n",
      "Trained batch 3077 batch loss 5.87693834 epoch total loss 5.93414402\n",
      "Trained batch 3078 batch loss 5.88430309 epoch total loss 5.93412781\n",
      "Trained batch 3079 batch loss 5.82213 epoch total loss 5.93409157\n",
      "Trained batch 3080 batch loss 5.92603827 epoch total loss 5.93408918\n",
      "Trained batch 3081 batch loss 6.0559864 epoch total loss 5.93412876\n",
      "Trained batch 3082 batch loss 5.78997946 epoch total loss 5.93408155\n",
      "Trained batch 3083 batch loss 6.04307556 epoch total loss 5.93411684\n",
      "Trained batch 3084 batch loss 6.10819817 epoch total loss 5.93417311\n",
      "Trained batch 3085 batch loss 5.10889292 epoch total loss 5.93390608\n",
      "Trained batch 3086 batch loss 4.96162701 epoch total loss 5.93359041\n",
      "Trained batch 3087 batch loss 5.37455082 epoch total loss 5.93340969\n",
      "Trained batch 3088 batch loss 5.43951321 epoch total loss 5.93324947\n",
      "Trained batch 3089 batch loss 5.66383839 epoch total loss 5.93316269\n",
      "Trained batch 3090 batch loss 5.70961952 epoch total loss 5.93309\n",
      "Trained batch 3091 batch loss 6.23293114 epoch total loss 5.93318701\n",
      "Trained batch 3092 batch loss 5.59638643 epoch total loss 5.93307781\n",
      "Trained batch 3093 batch loss 5.56801939 epoch total loss 5.93295956\n",
      "Trained batch 3094 batch loss 6.23576641 epoch total loss 5.93305779\n",
      "Trained batch 3095 batch loss 6.41998482 epoch total loss 5.93321514\n",
      "Trained batch 3096 batch loss 6.58751 epoch total loss 5.93342638\n",
      "Trained batch 3097 batch loss 5.75078869 epoch total loss 5.93336725\n",
      "Trained batch 3098 batch loss 6.70467043 epoch total loss 5.93361664\n",
      "Trained batch 3099 batch loss 5.88137817 epoch total loss 5.93359947\n",
      "Trained batch 3100 batch loss 7.10331678 epoch total loss 5.93397665\n",
      "Trained batch 3101 batch loss 5.51195621 epoch total loss 5.93384075\n",
      "Trained batch 3102 batch loss 6.05967522 epoch total loss 5.93388128\n",
      "Trained batch 3103 batch loss 6.13767815 epoch total loss 5.93394709\n",
      "Trained batch 3104 batch loss 7.11276197 epoch total loss 5.93432665\n",
      "Trained batch 3105 batch loss 5.77248573 epoch total loss 5.93427515\n",
      "Trained batch 3106 batch loss 7.03658533 epoch total loss 5.93463\n",
      "Trained batch 3107 batch loss 5.89772654 epoch total loss 5.93461847\n",
      "Trained batch 3108 batch loss 6.24501324 epoch total loss 5.93471813\n",
      "Trained batch 3109 batch loss 5.35636854 epoch total loss 5.93453169\n",
      "Trained batch 3110 batch loss 6.09623528 epoch total loss 5.93458366\n",
      "Trained batch 3111 batch loss 5.48860455 epoch total loss 5.93444\n",
      "Trained batch 3112 batch loss 5.712255 epoch total loss 5.93436909\n",
      "Trained batch 3113 batch loss 5.75809526 epoch total loss 5.93431234\n",
      "Trained batch 3114 batch loss 5.56173897 epoch total loss 5.93419266\n",
      "Trained batch 3115 batch loss 6.74360657 epoch total loss 5.93445253\n",
      "Trained batch 3116 batch loss 6.21373701 epoch total loss 5.93454218\n",
      "Trained batch 3117 batch loss 3.60387659 epoch total loss 5.9337945\n",
      "Trained batch 3118 batch loss 5.00690413 epoch total loss 5.93349743\n",
      "Trained batch 3119 batch loss 3.72673273 epoch total loss 5.93279\n",
      "Trained batch 3120 batch loss 3.90024376 epoch total loss 5.93213844\n",
      "Trained batch 3121 batch loss 4.1637392 epoch total loss 5.93157196\n",
      "Trained batch 3122 batch loss 4.7484 epoch total loss 5.93119287\n",
      "Trained batch 3123 batch loss 5.56716347 epoch total loss 5.93107605\n",
      "Trained batch 3124 batch loss 5.84746695 epoch total loss 5.93104935\n",
      "Trained batch 3125 batch loss 5.4992795 epoch total loss 5.93091106\n",
      "Trained batch 3126 batch loss 5.55659246 epoch total loss 5.93079138\n",
      "Trained batch 3127 batch loss 5.66852093 epoch total loss 5.93070745\n",
      "Trained batch 3128 batch loss 6.25968266 epoch total loss 5.93081284\n",
      "Trained batch 3129 batch loss 6.97456741 epoch total loss 5.93114614\n",
      "Trained batch 3130 batch loss 6.43807507 epoch total loss 5.93130827\n",
      "Trained batch 3131 batch loss 5.41304493 epoch total loss 5.93114233\n",
      "Trained batch 3132 batch loss 6.64156628 epoch total loss 5.93136883\n",
      "Trained batch 3133 batch loss 5.78970623 epoch total loss 5.93132353\n",
      "Trained batch 3134 batch loss 5.64164257 epoch total loss 5.93123102\n",
      "Trained batch 3135 batch loss 6.69393635 epoch total loss 5.93147421\n",
      "Trained batch 3136 batch loss 4.66809845 epoch total loss 5.93107128\n",
      "Trained batch 3137 batch loss 6.03556728 epoch total loss 5.93110466\n",
      "Trained batch 3138 batch loss 5.61452579 epoch total loss 5.93100405\n",
      "Trained batch 3139 batch loss 5.26067162 epoch total loss 5.93079\n",
      "Trained batch 3140 batch loss 5.92835617 epoch total loss 5.93078899\n",
      "Trained batch 3141 batch loss 5.72353745 epoch total loss 5.93072271\n",
      "Trained batch 3142 batch loss 4.98203564 epoch total loss 5.93042088\n",
      "Trained batch 3143 batch loss 6.17671633 epoch total loss 5.93049908\n",
      "Trained batch 3144 batch loss 5.50330734 epoch total loss 5.93036318\n",
      "Trained batch 3145 batch loss 5.65613079 epoch total loss 5.93027639\n",
      "Trained batch 3146 batch loss 5.52048683 epoch total loss 5.93014574\n",
      "Trained batch 3147 batch loss 5.75798082 epoch total loss 5.9300909\n",
      "Trained batch 3148 batch loss 5.66420889 epoch total loss 5.9300065\n",
      "Trained batch 3149 batch loss 5.4537077 epoch total loss 5.92985487\n",
      "Trained batch 3150 batch loss 5.27630949 epoch total loss 5.92964697\n",
      "Trained batch 3151 batch loss 5.34855175 epoch total loss 5.92946243\n",
      "Trained batch 3152 batch loss 5.55880165 epoch total loss 5.92934465\n",
      "Trained batch 3153 batch loss 5.06343269 epoch total loss 5.92907\n",
      "Trained batch 3154 batch loss 4.93330431 epoch total loss 5.92875433\n",
      "Trained batch 3155 batch loss 5.3134346 epoch total loss 5.92855883\n",
      "Trained batch 3156 batch loss 5.53786135 epoch total loss 5.92843485\n",
      "Trained batch 3157 batch loss 6.08244228 epoch total loss 5.92848349\n",
      "Trained batch 3158 batch loss 5.81175423 epoch total loss 5.92844677\n",
      "Trained batch 3159 batch loss 5.86612368 epoch total loss 5.92842674\n",
      "Trained batch 3160 batch loss 5.27987957 epoch total loss 5.92822123\n",
      "Trained batch 3161 batch loss 5.50808048 epoch total loss 5.92808819\n",
      "Trained batch 3162 batch loss 5.19376183 epoch total loss 5.92785597\n",
      "Trained batch 3163 batch loss 6.02885056 epoch total loss 5.92788792\n",
      "Trained batch 3164 batch loss 5.64757586 epoch total loss 5.9277997\n",
      "Trained batch 3165 batch loss 6.24798679 epoch total loss 5.92790079\n",
      "Trained batch 3166 batch loss 5.4527936 epoch total loss 5.92775106\n",
      "Trained batch 3167 batch loss 5.62830925 epoch total loss 5.92765665\n",
      "Trained batch 3168 batch loss 5.34456778 epoch total loss 5.92747259\n",
      "Trained batch 3169 batch loss 5.8893733 epoch total loss 5.92746\n",
      "Trained batch 3170 batch loss 6.2304039 epoch total loss 5.92755556\n",
      "Trained batch 3171 batch loss 5.91636801 epoch total loss 5.92755222\n",
      "Trained batch 3172 batch loss 5.87196684 epoch total loss 5.9275341\n",
      "Trained batch 3173 batch loss 5.63282585 epoch total loss 5.9274416\n",
      "Trained batch 3174 batch loss 5.68552399 epoch total loss 5.9273653\n",
      "Trained batch 3175 batch loss 5.09939098 epoch total loss 5.92710447\n",
      "Trained batch 3176 batch loss 7.01972198 epoch total loss 5.92744827\n",
      "Trained batch 3177 batch loss 6.79255486 epoch total loss 5.92772102\n",
      "Trained batch 3178 batch loss 5.53789568 epoch total loss 5.927598\n",
      "Trained batch 3179 batch loss 5.79346561 epoch total loss 5.92755556\n",
      "Trained batch 3180 batch loss 6.07783604 epoch total loss 5.92760277\n",
      "Trained batch 3181 batch loss 6.02850771 epoch total loss 5.92763472\n",
      "Trained batch 3182 batch loss 5.90508699 epoch total loss 5.92762756\n",
      "Trained batch 3183 batch loss 5.72497177 epoch total loss 5.92756367\n",
      "Trained batch 3184 batch loss 6.21724844 epoch total loss 5.92765474\n",
      "Trained batch 3185 batch loss 5.53126192 epoch total loss 5.92753029\n",
      "Trained batch 3186 batch loss 5.69803524 epoch total loss 5.92745781\n",
      "Trained batch 3187 batch loss 5.84152794 epoch total loss 5.92743111\n",
      "Trained batch 3188 batch loss 7.08169937 epoch total loss 5.92779303\n",
      "Trained batch 3189 batch loss 4.52873802 epoch total loss 5.92735481\n",
      "Trained batch 3190 batch loss 4.68154 epoch total loss 5.92696428\n",
      "Trained batch 3191 batch loss 5.65884638 epoch total loss 5.92688\n",
      "Trained batch 3192 batch loss 5.58515072 epoch total loss 5.92677307\n",
      "Trained batch 3193 batch loss 4.56168175 epoch total loss 5.92634583\n",
      "Trained batch 3194 batch loss 3.81610727 epoch total loss 5.92568541\n",
      "Trained batch 3195 batch loss 5.53324699 epoch total loss 5.92556238\n",
      "Trained batch 3196 batch loss 5.82933617 epoch total loss 5.92553234\n",
      "Trained batch 3197 batch loss 5.71387529 epoch total loss 5.92546654\n",
      "Trained batch 3198 batch loss 6.05355835 epoch total loss 5.92550659\n",
      "Trained batch 3199 batch loss 5.92898083 epoch total loss 5.92550755\n",
      "Trained batch 3200 batch loss 5.9443512 epoch total loss 5.92551374\n",
      "Trained batch 3201 batch loss 5.60172558 epoch total loss 5.92541265\n",
      "Trained batch 3202 batch loss 5.79999828 epoch total loss 5.92537355\n",
      "Trained batch 3203 batch loss 5.80970144 epoch total loss 5.92533779\n",
      "Trained batch 3204 batch loss 5.69309521 epoch total loss 5.92526531\n",
      "Trained batch 3205 batch loss 5.91464329 epoch total loss 5.92526197\n",
      "Trained batch 3206 batch loss 6.26900196 epoch total loss 5.92536926\n",
      "Trained batch 3207 batch loss 6.02840471 epoch total loss 5.92540169\n",
      "Trained batch 3208 batch loss 6.4211669 epoch total loss 5.92555666\n",
      "Trained batch 3209 batch loss 6.38128757 epoch total loss 5.92569828\n",
      "Trained batch 3210 batch loss 4.83510303 epoch total loss 5.92535877\n",
      "Trained batch 3211 batch loss 6.15594244 epoch total loss 5.92543077\n",
      "Trained batch 3212 batch loss 5.68207645 epoch total loss 5.92535496\n",
      "Trained batch 3213 batch loss 5.99672699 epoch total loss 5.92537689\n",
      "Trained batch 3214 batch loss 5.96092415 epoch total loss 5.92538786\n",
      "Trained batch 3215 batch loss 5.71771812 epoch total loss 5.92532301\n",
      "Trained batch 3216 batch loss 5.66206741 epoch total loss 5.92524147\n",
      "Trained batch 3217 batch loss 4.54908657 epoch total loss 5.92481327\n",
      "Trained batch 3218 batch loss 6.42327 epoch total loss 5.92496872\n",
      "Trained batch 3219 batch loss 5.72278118 epoch total loss 5.92490578\n",
      "Trained batch 3220 batch loss 6.21444798 epoch total loss 5.9249959\n",
      "Trained batch 3221 batch loss 6.32458544 epoch total loss 5.92512\n",
      "Trained batch 3222 batch loss 5.58692694 epoch total loss 5.92501497\n",
      "Trained batch 3223 batch loss 5.19167614 epoch total loss 5.92478752\n",
      "Trained batch 3224 batch loss 6.84791088 epoch total loss 5.92507362\n",
      "Trained batch 3225 batch loss 6.03486252 epoch total loss 5.92510796\n",
      "Trained batch 3226 batch loss 6.53938055 epoch total loss 5.92529821\n",
      "Trained batch 3227 batch loss 6.25275707 epoch total loss 5.9253993\n",
      "Trained batch 3228 batch loss 6.21174288 epoch total loss 5.925488\n",
      "Trained batch 3229 batch loss 5.23450041 epoch total loss 5.9252739\n",
      "Trained batch 3230 batch loss 5.4337225 epoch total loss 5.92512131\n",
      "Trained batch 3231 batch loss 3.61333752 epoch total loss 5.92440605\n",
      "Trained batch 3232 batch loss 3.94178295 epoch total loss 5.92379236\n",
      "Trained batch 3233 batch loss 4.25272369 epoch total loss 5.92327547\n",
      "Trained batch 3234 batch loss 5.48594 epoch total loss 5.92314\n",
      "Trained batch 3235 batch loss 4.75904608 epoch total loss 5.92278051\n",
      "Trained batch 3236 batch loss 4.7862196 epoch total loss 5.92242956\n",
      "Trained batch 3237 batch loss 5.6435051 epoch total loss 5.92234325\n",
      "Trained batch 3238 batch loss 5.81255817 epoch total loss 5.9223094\n",
      "Trained batch 3239 batch loss 5.74069 epoch total loss 5.92225313\n",
      "Trained batch 3240 batch loss 5.82819319 epoch total loss 5.92222404\n",
      "Trained batch 3241 batch loss 5.69640255 epoch total loss 5.92215443\n",
      "Trained batch 3242 batch loss 5.42785 epoch total loss 5.92200232\n",
      "Trained batch 3243 batch loss 5.36670303 epoch total loss 5.92183113\n",
      "Trained batch 3244 batch loss 5.91790295 epoch total loss 5.9218297\n",
      "Trained batch 3245 batch loss 5.867939 epoch total loss 5.92181301\n",
      "Trained batch 3246 batch loss 5.89406729 epoch total loss 5.92180443\n",
      "Trained batch 3247 batch loss 5.58868408 epoch total loss 5.92170191\n",
      "Trained batch 3248 batch loss 5.74051666 epoch total loss 5.92164612\n",
      "Trained batch 3249 batch loss 6.26335478 epoch total loss 5.92175102\n",
      "Trained batch 3250 batch loss 5.66637087 epoch total loss 5.92167234\n",
      "Trained batch 3251 batch loss 6.03989601 epoch total loss 5.92170858\n",
      "Trained batch 3252 batch loss 5.30560398 epoch total loss 5.9215188\n",
      "Trained batch 3253 batch loss 5.9570837 epoch total loss 5.92153\n",
      "Trained batch 3254 batch loss 6.06565094 epoch total loss 5.92157412\n",
      "Trained batch 3255 batch loss 6.48524666 epoch total loss 5.92174721\n",
      "Trained batch 3256 batch loss 6.46799088 epoch total loss 5.92191505\n",
      "Trained batch 3257 batch loss 6.57365561 epoch total loss 5.92211533\n",
      "Trained batch 3258 batch loss 6.73658276 epoch total loss 5.92236519\n",
      "Trained batch 3259 batch loss 6.62090492 epoch total loss 5.92258\n",
      "Trained batch 3260 batch loss 5.85920238 epoch total loss 5.92256\n",
      "Trained batch 3261 batch loss 6.28546 epoch total loss 5.92267179\n",
      "Trained batch 3262 batch loss 6.66373348 epoch total loss 5.92289877\n",
      "Trained batch 3263 batch loss 6.72969151 epoch total loss 5.92314625\n",
      "Trained batch 3264 batch loss 5.56175947 epoch total loss 5.9230361\n",
      "Trained batch 3265 batch loss 6.4123764 epoch total loss 5.92318583\n",
      "Trained batch 3266 batch loss 6.55580759 epoch total loss 5.92337942\n",
      "Trained batch 3267 batch loss 6.37470722 epoch total loss 5.9235177\n",
      "Trained batch 3268 batch loss 6.20010567 epoch total loss 5.9236021\n",
      "Trained batch 3269 batch loss 7.06489182 epoch total loss 5.92395115\n",
      "Trained batch 3270 batch loss 6.21271229 epoch total loss 5.92403936\n",
      "Trained batch 3271 batch loss 6.48514462 epoch total loss 5.92421103\n",
      "Trained batch 3272 batch loss 5.37251282 epoch total loss 5.92404222\n",
      "Trained batch 3273 batch loss 5.58981609 epoch total loss 5.92394\n",
      "Trained batch 3274 batch loss 6.05135536 epoch total loss 5.92397928\n",
      "Trained batch 3275 batch loss 5.88454151 epoch total loss 5.92396688\n",
      "Trained batch 3276 batch loss 6.05895805 epoch total loss 5.92400837\n",
      "Trained batch 3277 batch loss 5.9876585 epoch total loss 5.92402792\n",
      "Trained batch 3278 batch loss 5.93120289 epoch total loss 5.9240303\n",
      "Trained batch 3279 batch loss 5.98072577 epoch total loss 5.92404747\n",
      "Trained batch 3280 batch loss 6.03648376 epoch total loss 5.9240818\n",
      "Trained batch 3281 batch loss 6.46562767 epoch total loss 5.92424679\n",
      "Trained batch 3282 batch loss 6.25097609 epoch total loss 5.92434597\n",
      "Trained batch 3283 batch loss 6.00975466 epoch total loss 5.92437172\n",
      "Trained batch 3284 batch loss 6.40500355 epoch total loss 5.92451811\n",
      "Trained batch 3285 batch loss 5.15470695 epoch total loss 5.9242835\n",
      "Trained batch 3286 batch loss 4.77323055 epoch total loss 5.92393351\n",
      "Trained batch 3287 batch loss 5.98596859 epoch total loss 5.9239521\n",
      "Trained batch 3288 batch loss 5.41564322 epoch total loss 5.92379761\n",
      "Trained batch 3289 batch loss 4.85192394 epoch total loss 5.92347193\n",
      "Trained batch 3290 batch loss 5.1747694 epoch total loss 5.923244\n",
      "Trained batch 3291 batch loss 4.98057938 epoch total loss 5.92295742\n",
      "Trained batch 3292 batch loss 5.91231728 epoch total loss 5.92295408\n",
      "Trained batch 3293 batch loss 5.51167774 epoch total loss 5.92282915\n",
      "Trained batch 3294 batch loss 4.89322 epoch total loss 5.92251635\n",
      "Trained batch 3295 batch loss 5.19174099 epoch total loss 5.92229462\n",
      "Trained batch 3296 batch loss 4.98134136 epoch total loss 5.92200899\n",
      "Trained batch 3297 batch loss 5.88466835 epoch total loss 5.92199755\n",
      "Trained batch 3298 batch loss 6.14914227 epoch total loss 5.92206621\n",
      "Trained batch 3299 batch loss 5.41919947 epoch total loss 5.9219141\n",
      "Trained batch 3300 batch loss 5.92612171 epoch total loss 5.92191505\n",
      "Trained batch 3301 batch loss 5.52247953 epoch total loss 5.92179441\n",
      "Trained batch 3302 batch loss 5.48389673 epoch total loss 5.92166185\n",
      "Trained batch 3303 batch loss 5.35493755 epoch total loss 5.92149067\n",
      "Trained batch 3304 batch loss 5.68481302 epoch total loss 5.92141914\n",
      "Trained batch 3305 batch loss 5.81335402 epoch total loss 5.92138624\n",
      "Trained batch 3306 batch loss 5.64082909 epoch total loss 5.92130136\n",
      "Trained batch 3307 batch loss 5.64114 epoch total loss 5.92121649\n",
      "Trained batch 3308 batch loss 4.44638062 epoch total loss 5.92077112\n",
      "Trained batch 3309 batch loss 6.02819443 epoch total loss 5.92080307\n",
      "Trained batch 3310 batch loss 5.42976093 epoch total loss 5.92065477\n",
      "Trained batch 3311 batch loss 6.12123775 epoch total loss 5.92071533\n",
      "Trained batch 3312 batch loss 4.90773582 epoch total loss 5.92040968\n",
      "Trained batch 3313 batch loss 5.86705208 epoch total loss 5.92039347\n",
      "Trained batch 3314 batch loss 5.59506416 epoch total loss 5.92029572\n",
      "Trained batch 3315 batch loss 5.73929 epoch total loss 5.92024136\n",
      "Trained batch 3316 batch loss 6.87008715 epoch total loss 5.92052746\n",
      "Trained batch 3317 batch loss 7.03497028 epoch total loss 5.92086363\n",
      "Trained batch 3318 batch loss 5.56103086 epoch total loss 5.92075491\n",
      "Trained batch 3319 batch loss 5.2986784 epoch total loss 5.92056751\n",
      "Trained batch 3320 batch loss 6.13576412 epoch total loss 5.92063236\n",
      "Trained batch 3321 batch loss 7.65127087 epoch total loss 5.92115355\n",
      "Trained batch 3322 batch loss 5.21447325 epoch total loss 5.92094088\n",
      "Trained batch 3323 batch loss 5.43907928 epoch total loss 5.92079592\n",
      "Trained batch 3324 batch loss 4.96273422 epoch total loss 5.92050791\n",
      "Trained batch 3325 batch loss 4.76113939 epoch total loss 5.92015934\n",
      "Trained batch 3326 batch loss 5.91973686 epoch total loss 5.92015934\n",
      "Trained batch 3327 batch loss 5.52314234 epoch total loss 5.92003965\n",
      "Trained batch 3328 batch loss 6.2324934 epoch total loss 5.92013359\n",
      "Trained batch 3329 batch loss 5.88620806 epoch total loss 5.92012358\n",
      "Trained batch 3330 batch loss 5.8711 epoch total loss 5.9201088\n",
      "Trained batch 3331 batch loss 6.55019474 epoch total loss 5.9202981\n",
      "Trained batch 3332 batch loss 6.12932396 epoch total loss 5.92036104\n",
      "Trained batch 3333 batch loss 6.69275379 epoch total loss 5.92059278\n",
      "Trained batch 3334 batch loss 6.28384256 epoch total loss 5.9207015\n",
      "Trained batch 3335 batch loss 6.68536949 epoch total loss 5.92093086\n",
      "Trained batch 3336 batch loss 6.49239063 epoch total loss 5.92110205\n",
      "Trained batch 3337 batch loss 6.07580376 epoch total loss 5.92114878\n",
      "Trained batch 3338 batch loss 5.87124157 epoch total loss 5.92113352\n",
      "Trained batch 3339 batch loss 5.0300436 epoch total loss 5.92086649\n",
      "Trained batch 3340 batch loss 6.39845943 epoch total loss 5.92100954\n",
      "Trained batch 3341 batch loss 6.42089796 epoch total loss 5.92115879\n",
      "Trained batch 3342 batch loss 5.77450609 epoch total loss 5.9211154\n",
      "Trained batch 3343 batch loss 5.61709595 epoch total loss 5.92102432\n",
      "Trained batch 3344 batch loss 5.7691288 epoch total loss 5.92097902\n",
      "Trained batch 3345 batch loss 5.71028519 epoch total loss 5.92091608\n",
      "Trained batch 3346 batch loss 6.49112844 epoch total loss 5.92108631\n",
      "Trained batch 3347 batch loss 5.4340086 epoch total loss 5.92094088\n",
      "Trained batch 3348 batch loss 5.64157152 epoch total loss 5.92085695\n",
      "Trained batch 3349 batch loss 6.20627689 epoch total loss 5.92094231\n",
      "Trained batch 3350 batch loss 5.85421562 epoch total loss 5.92092228\n",
      "Trained batch 3351 batch loss 6.35367393 epoch total loss 5.9210515\n",
      "Trained batch 3352 batch loss 5.63672924 epoch total loss 5.92096663\n",
      "Trained batch 3353 batch loss 6.19419 epoch total loss 5.92104769\n",
      "Trained batch 3354 batch loss 5.43672943 epoch total loss 5.92090368\n",
      "Trained batch 3355 batch loss 5.93979168 epoch total loss 5.9209094\n",
      "Trained batch 3356 batch loss 6.01442051 epoch total loss 5.92093706\n",
      "Trained batch 3357 batch loss 6.34133863 epoch total loss 5.92106199\n",
      "Trained batch 3358 batch loss 5.69143 epoch total loss 5.9209938\n",
      "Trained batch 3359 batch loss 6.10454464 epoch total loss 5.92104864\n",
      "Trained batch 3360 batch loss 6.18425322 epoch total loss 5.92112684\n",
      "Trained batch 3361 batch loss 6.0882473 epoch total loss 5.92117643\n",
      "Trained batch 3362 batch loss 4.78284454 epoch total loss 5.92083788\n",
      "Trained batch 3363 batch loss 5.69613838 epoch total loss 5.92077112\n",
      "Trained batch 3364 batch loss 5.70087671 epoch total loss 5.9207058\n",
      "Trained batch 3365 batch loss 7.4752636 epoch total loss 5.92116737\n",
      "Trained batch 3366 batch loss 7.72205067 epoch total loss 5.92170286\n",
      "Trained batch 3367 batch loss 6.6974597 epoch total loss 5.92193317\n",
      "Trained batch 3368 batch loss 6.53182507 epoch total loss 5.9221139\n",
      "Trained batch 3369 batch loss 5.90833664 epoch total loss 5.9221096\n",
      "Trained batch 3370 batch loss 5.83833122 epoch total loss 5.92208481\n",
      "Trained batch 3371 batch loss 6.10733128 epoch total loss 5.92213964\n",
      "Trained batch 3372 batch loss 6.33057785 epoch total loss 5.92226076\n",
      "Trained batch 3373 batch loss 4.94543839 epoch total loss 5.92197132\n",
      "Trained batch 3374 batch loss 5.61873579 epoch total loss 5.9218812\n",
      "Trained batch 3375 batch loss 6.45394516 epoch total loss 5.92203856\n",
      "Trained batch 3376 batch loss 5.73869562 epoch total loss 5.9219842\n",
      "Trained batch 3377 batch loss 4.70546865 epoch total loss 5.92162418\n",
      "Trained batch 3378 batch loss 5.51414585 epoch total loss 5.92150307\n",
      "Trained batch 3379 batch loss 6.23583412 epoch total loss 5.92159653\n",
      "Trained batch 3380 batch loss 5.89765739 epoch total loss 5.92158937\n",
      "Trained batch 3381 batch loss 6.11707067 epoch total loss 5.92164755\n",
      "Trained batch 3382 batch loss 6.54510832 epoch total loss 5.92183161\n",
      "Trained batch 3383 batch loss 6.40678883 epoch total loss 5.92197466\n",
      "Trained batch 3384 batch loss 6.46990728 epoch total loss 5.92213726\n",
      "Trained batch 3385 batch loss 6.66049242 epoch total loss 5.92235518\n",
      "Trained batch 3386 batch loss 6.30007648 epoch total loss 5.92246675\n",
      "Trained batch 3387 batch loss 6.21280956 epoch total loss 5.92255259\n",
      "Trained batch 3388 batch loss 6.09648657 epoch total loss 5.92260361\n",
      "Trained batch 3389 batch loss 5.46877766 epoch total loss 5.92246962\n",
      "Trained batch 3390 batch loss 5.31751919 epoch total loss 5.92229176\n",
      "Trained batch 3391 batch loss 4.44375515 epoch total loss 5.92185545\n",
      "Trained batch 3392 batch loss 5.22603893 epoch total loss 5.92165041\n",
      "Trained batch 3393 batch loss 5.48594 epoch total loss 5.92152214\n",
      "Trained batch 3394 batch loss 5.74427176 epoch total loss 5.92146969\n",
      "Trained batch 3395 batch loss 5.94331455 epoch total loss 5.92147636\n",
      "Trained batch 3396 batch loss 4.15425253 epoch total loss 5.92095613\n",
      "Trained batch 3397 batch loss 5.40389824 epoch total loss 5.92080402\n",
      "Trained batch 3398 batch loss 5.37717867 epoch total loss 5.92064381\n",
      "Trained batch 3399 batch loss 5.19321632 epoch total loss 5.92042971\n",
      "Trained batch 3400 batch loss 4.67350483 epoch total loss 5.92006302\n",
      "Trained batch 3401 batch loss 5.0807991 epoch total loss 5.91981602\n",
      "Trained batch 3402 batch loss 4.62040615 epoch total loss 5.91943455\n",
      "Trained batch 3403 batch loss 4.37877083 epoch total loss 5.91898155\n",
      "Trained batch 3404 batch loss 5.73851967 epoch total loss 5.91892862\n",
      "Trained batch 3405 batch loss 5.63735485 epoch total loss 5.91884565\n",
      "Trained batch 3406 batch loss 5.7799077 epoch total loss 5.91880465\n",
      "Trained batch 3407 batch loss 4.56417561 epoch total loss 5.91840744\n",
      "Trained batch 3408 batch loss 4.92813 epoch total loss 5.91811657\n",
      "Trained batch 3409 batch loss 4.6008091 epoch total loss 5.91773033\n",
      "Trained batch 3410 batch loss 4.40062523 epoch total loss 5.91728544\n",
      "Trained batch 3411 batch loss 4.57578278 epoch total loss 5.91689205\n",
      "Trained batch 3412 batch loss 6.41124582 epoch total loss 5.91703749\n",
      "Trained batch 3413 batch loss 6.85970974 epoch total loss 5.91731358\n",
      "Trained batch 3414 batch loss 6.57455492 epoch total loss 5.91750574\n",
      "Trained batch 3415 batch loss 7.22995758 epoch total loss 5.91789055\n",
      "Trained batch 3416 batch loss 6.00927925 epoch total loss 5.91791725\n",
      "Trained batch 3417 batch loss 5.80631685 epoch total loss 5.91788483\n",
      "Trained batch 3418 batch loss 4.99640942 epoch total loss 5.91761494\n",
      "Trained batch 3419 batch loss 6.90975046 epoch total loss 5.91790533\n",
      "Trained batch 3420 batch loss 6.33259296 epoch total loss 5.91802645\n",
      "Trained batch 3421 batch loss 7.05779886 epoch total loss 5.91835976\n",
      "Trained batch 3422 batch loss 5.95142698 epoch total loss 5.91836929\n",
      "Trained batch 3423 batch loss 5.20803833 epoch total loss 5.91816235\n",
      "Trained batch 3424 batch loss 5.28450108 epoch total loss 5.91797733\n",
      "Trained batch 3425 batch loss 4.62868309 epoch total loss 5.91760111\n",
      "Trained batch 3426 batch loss 5.05262184 epoch total loss 5.91734838\n",
      "Trained batch 3427 batch loss 5.1597929 epoch total loss 5.91712761\n",
      "Trained batch 3428 batch loss 6.72023201 epoch total loss 5.91736174\n",
      "Trained batch 3429 batch loss 6.04050112 epoch total loss 5.91739798\n",
      "Trained batch 3430 batch loss 6.59766197 epoch total loss 5.91759634\n",
      "Trained batch 3431 batch loss 6.32970142 epoch total loss 5.9177165\n",
      "Trained batch 3432 batch loss 7.13128805 epoch total loss 5.91807\n",
      "Trained batch 3433 batch loss 6.74623775 epoch total loss 5.91831112\n",
      "Trained batch 3434 batch loss 6.6741786 epoch total loss 5.91853142\n",
      "Trained batch 3435 batch loss 6.27039576 epoch total loss 5.91863346\n",
      "Trained batch 3436 batch loss 6.13012266 epoch total loss 5.91869497\n",
      "Trained batch 3437 batch loss 5.5378809 epoch total loss 5.91858435\n",
      "Trained batch 3438 batch loss 5.47510481 epoch total loss 5.91845512\n",
      "Trained batch 3439 batch loss 5.99654961 epoch total loss 5.91847754\n",
      "Trained batch 3440 batch loss 5.9658761 epoch total loss 5.91849184\n",
      "Trained batch 3441 batch loss 6.0583992 epoch total loss 5.91853237\n",
      "Trained batch 3442 batch loss 6.58048725 epoch total loss 5.91872454\n",
      "Trained batch 3443 batch loss 6.14025784 epoch total loss 5.91878891\n",
      "Trained batch 3444 batch loss 5.95909882 epoch total loss 5.91880083\n",
      "Trained batch 3445 batch loss 6.18216324 epoch total loss 5.91887712\n",
      "Trained batch 3446 batch loss 5.47890043 epoch total loss 5.91874933\n",
      "Trained batch 3447 batch loss 5.68323612 epoch total loss 5.91868114\n",
      "Trained batch 3448 batch loss 6.58857822 epoch total loss 5.91887522\n",
      "Trained batch 3449 batch loss 6.572052 epoch total loss 5.91906452\n",
      "Trained batch 3450 batch loss 5.9925828 epoch total loss 5.9190855\n",
      "Trained batch 3451 batch loss 5.60025072 epoch total loss 5.918993\n",
      "Trained batch 3452 batch loss 5.68263817 epoch total loss 5.91892481\n",
      "Trained batch 3453 batch loss 4.57213688 epoch total loss 5.91853476\n",
      "Trained batch 3454 batch loss 4.67937279 epoch total loss 5.91817617\n",
      "Trained batch 3455 batch loss 4.12404537 epoch total loss 5.91765738\n",
      "Trained batch 3456 batch loss 4.60248756 epoch total loss 5.91727638\n",
      "Trained batch 3457 batch loss 4.71740866 epoch total loss 5.91692924\n",
      "Trained batch 3458 batch loss 4.14342451 epoch total loss 5.91641617\n",
      "Trained batch 3459 batch loss 5.6461463 epoch total loss 5.91633797\n",
      "Trained batch 3460 batch loss 6.2325 epoch total loss 5.91642952\n",
      "Trained batch 3461 batch loss 6.19031382 epoch total loss 5.9165082\n",
      "Trained batch 3462 batch loss 6.04106522 epoch total loss 5.91654444\n",
      "Trained batch 3463 batch loss 6.73081398 epoch total loss 5.91677904\n",
      "Trained batch 3464 batch loss 6.90644646 epoch total loss 5.91706514\n",
      "Trained batch 3465 batch loss 5.5756731 epoch total loss 5.91696644\n",
      "Trained batch 3466 batch loss 6.47748947 epoch total loss 5.91712809\n",
      "Trained batch 3467 batch loss 5.66447353 epoch total loss 5.91705513\n",
      "Trained batch 3468 batch loss 6.32655811 epoch total loss 5.91717291\n",
      "Trained batch 3469 batch loss 6.25624371 epoch total loss 5.91727066\n",
      "Trained batch 3470 batch loss 6.4954195 epoch total loss 5.91743755\n",
      "Trained batch 3471 batch loss 6.453866 epoch total loss 5.91759157\n",
      "Trained batch 3472 batch loss 6.17834282 epoch total loss 5.91766644\n",
      "Trained batch 3473 batch loss 5.71883965 epoch total loss 5.91760921\n",
      "Trained batch 3474 batch loss 6.19839764 epoch total loss 5.91769028\n",
      "Trained batch 3475 batch loss 5.93804121 epoch total loss 5.917696\n",
      "Trained batch 3476 batch loss 6.05165815 epoch total loss 5.91773462\n",
      "Trained batch 3477 batch loss 5.91172 epoch total loss 5.91773272\n",
      "Trained batch 3478 batch loss 5.69108057 epoch total loss 5.91766787\n",
      "Trained batch 3479 batch loss 6.02326965 epoch total loss 5.91769838\n",
      "Trained batch 3480 batch loss 5.49032068 epoch total loss 5.91757536\n",
      "Trained batch 3481 batch loss 4.73142624 epoch total loss 5.91723442\n",
      "Trained batch 3482 batch loss 4.69300556 epoch total loss 5.91688299\n",
      "Trained batch 3483 batch loss 5.42578411 epoch total loss 5.91674185\n",
      "Trained batch 3484 batch loss 5.81865168 epoch total loss 5.91671371\n",
      "Trained batch 3485 batch loss 4.16937637 epoch total loss 5.91621256\n",
      "Trained batch 3486 batch loss 5.72250938 epoch total loss 5.91615677\n",
      "Trained batch 3487 batch loss 6.42650414 epoch total loss 5.91630316\n",
      "Trained batch 3488 batch loss 5.86585236 epoch total loss 5.91628838\n",
      "Trained batch 3489 batch loss 5.24537468 epoch total loss 5.91609621\n",
      "Trained batch 3490 batch loss 5.84261417 epoch total loss 5.91607475\n",
      "Trained batch 3491 batch loss 6.27547932 epoch total loss 5.91617775\n",
      "Trained batch 3492 batch loss 5.98009205 epoch total loss 5.91619635\n",
      "Trained batch 3493 batch loss 5.46982813 epoch total loss 5.91606855\n",
      "Trained batch 3494 batch loss 4.73453617 epoch total loss 5.91573048\n",
      "Trained batch 3495 batch loss 6.14836311 epoch total loss 5.91579723\n",
      "Trained batch 3496 batch loss 6.63466406 epoch total loss 5.91600275\n",
      "Trained batch 3497 batch loss 6.47845 epoch total loss 5.91616344\n",
      "Trained batch 3498 batch loss 7.27975082 epoch total loss 5.9165535\n",
      "Trained batch 3499 batch loss 6.22231054 epoch total loss 5.91664076\n",
      "Trained batch 3500 batch loss 6.0206809 epoch total loss 5.9166708\n",
      "Trained batch 3501 batch loss 5.55757332 epoch total loss 5.9165678\n",
      "Trained batch 3502 batch loss 6.13150692 epoch total loss 5.91662931\n",
      "Trained batch 3503 batch loss 5.14607334 epoch total loss 5.91640949\n",
      "Trained batch 3504 batch loss 6.22495079 epoch total loss 5.91649723\n",
      "Trained batch 3505 batch loss 5.80627441 epoch total loss 5.91646576\n",
      "Trained batch 3506 batch loss 5.94789219 epoch total loss 5.91647482\n",
      "Trained batch 3507 batch loss 5.08983088 epoch total loss 5.91623878\n",
      "Trained batch 3508 batch loss 4.93160534 epoch total loss 5.9159584\n",
      "Trained batch 3509 batch loss 5.92685795 epoch total loss 5.91596174\n",
      "Trained batch 3510 batch loss 5.87708187 epoch total loss 5.9159503\n",
      "Trained batch 3511 batch loss 5.95095682 epoch total loss 5.91596079\n",
      "Trained batch 3512 batch loss 3.67967272 epoch total loss 5.91532373\n",
      "Trained batch 3513 batch loss 5.96431637 epoch total loss 5.91533804\n",
      "Trained batch 3514 batch loss 4.90107059 epoch total loss 5.91504908\n",
      "Trained batch 3515 batch loss 5.05535603 epoch total loss 5.91480446\n",
      "Trained batch 3516 batch loss 5.07292366 epoch total loss 5.91456461\n",
      "Trained batch 3517 batch loss 4.98716831 epoch total loss 5.91430092\n",
      "Trained batch 3518 batch loss 5.57619381 epoch total loss 5.9142046\n",
      "Trained batch 3519 batch loss 5.42697716 epoch total loss 5.91406631\n",
      "Trained batch 3520 batch loss 5.65049314 epoch total loss 5.91399145\n",
      "Trained batch 3521 batch loss 5.35240746 epoch total loss 5.91383171\n",
      "Trained batch 3522 batch loss 5.30468845 epoch total loss 5.91365862\n",
      "Trained batch 3523 batch loss 5.28773832 epoch total loss 5.91348076\n",
      "Trained batch 3524 batch loss 5.22936058 epoch total loss 5.91328669\n",
      "Trained batch 3525 batch loss 5.00784874 epoch total loss 5.91302967\n",
      "Trained batch 3526 batch loss 5.97586632 epoch total loss 5.91304779\n",
      "Trained batch 3527 batch loss 5.22702312 epoch total loss 5.91285324\n",
      "Trained batch 3528 batch loss 4.22674847 epoch total loss 5.91237497\n",
      "Trained batch 3529 batch loss 4.46464777 epoch total loss 5.91196489\n",
      "Trained batch 3530 batch loss 5.94398832 epoch total loss 5.91197395\n",
      "Trained batch 3531 batch loss 6.04177952 epoch total loss 5.91201\n",
      "Trained batch 3532 batch loss 5.02626419 epoch total loss 5.91175938\n",
      "Trained batch 3533 batch loss 5.65750647 epoch total loss 5.91168737\n",
      "Trained batch 3534 batch loss 5.9161005 epoch total loss 5.9116888\n",
      "Trained batch 3535 batch loss 3.90935016 epoch total loss 5.9111228\n",
      "Trained batch 3536 batch loss 6.07248211 epoch total loss 5.9111681\n",
      "Trained batch 3537 batch loss 5.57156277 epoch total loss 5.91107225\n",
      "Trained batch 3538 batch loss 4.75952911 epoch total loss 5.91074705\n",
      "Trained batch 3539 batch loss 4.98865604 epoch total loss 5.91048622\n",
      "Trained batch 3540 batch loss 4.97072268 epoch total loss 5.91022062\n",
      "Trained batch 3541 batch loss 5.52645206 epoch total loss 5.91011286\n",
      "Trained batch 3542 batch loss 5.89493179 epoch total loss 5.91010809\n",
      "Trained batch 3543 batch loss 6.01127815 epoch total loss 5.91013718\n",
      "Trained batch 3544 batch loss 6.22457266 epoch total loss 5.91022587\n",
      "Trained batch 3545 batch loss 5.56656599 epoch total loss 5.91012859\n",
      "Trained batch 3546 batch loss 6.38035 epoch total loss 5.91026163\n",
      "Trained batch 3547 batch loss 5.84266424 epoch total loss 5.91024208\n",
      "Trained batch 3548 batch loss 6.39329195 epoch total loss 5.91037798\n",
      "Trained batch 3549 batch loss 5.93217087 epoch total loss 5.91038418\n",
      "Trained batch 3550 batch loss 6.68576622 epoch total loss 5.91060257\n",
      "Trained batch 3551 batch loss 5.85504532 epoch total loss 5.91058683\n",
      "Trained batch 3552 batch loss 5.98733282 epoch total loss 5.91060877\n",
      "Trained batch 3553 batch loss 6.02641869 epoch total loss 5.91064167\n",
      "Trained batch 3554 batch loss 5.66110897 epoch total loss 5.9105711\n",
      "Trained batch 3555 batch loss 6.07047081 epoch total loss 5.91061592\n",
      "Trained batch 3556 batch loss 6.34527874 epoch total loss 5.91073847\n",
      "Trained batch 3557 batch loss 5.81587839 epoch total loss 5.91071177\n",
      "Trained batch 3558 batch loss 6.1847887 epoch total loss 5.91078901\n",
      "Trained batch 3559 batch loss 5.78094482 epoch total loss 5.91075277\n",
      "Trained batch 3560 batch loss 5.94023371 epoch total loss 5.91076088\n",
      "Trained batch 3561 batch loss 6.35830736 epoch total loss 5.91088629\n",
      "Trained batch 3562 batch loss 6.11979294 epoch total loss 5.91094494\n",
      "Trained batch 3563 batch loss 5.51926756 epoch total loss 5.91083479\n",
      "Trained batch 3564 batch loss 5.92654133 epoch total loss 5.91083908\n",
      "Trained batch 3565 batch loss 5.45763493 epoch total loss 5.91071177\n",
      "Trained batch 3566 batch loss 5.81791115 epoch total loss 5.91068602\n",
      "Trained batch 3567 batch loss 6.64211226 epoch total loss 5.91089106\n",
      "Trained batch 3568 batch loss 5.13921261 epoch total loss 5.91067457\n",
      "Trained batch 3569 batch loss 5.31421661 epoch total loss 5.91050768\n",
      "Trained batch 3570 batch loss 4.76032209 epoch total loss 5.91018534\n",
      "Trained batch 3571 batch loss 5.57900572 epoch total loss 5.91009235\n",
      "Trained batch 3572 batch loss 5.79223156 epoch total loss 5.91005945\n",
      "Trained batch 3573 batch loss 5.59869385 epoch total loss 5.90997267\n",
      "Trained batch 3574 batch loss 5.82712269 epoch total loss 5.9099493\n",
      "Trained batch 3575 batch loss 5.19952 epoch total loss 5.90975046\n",
      "Trained batch 3576 batch loss 5.09076834 epoch total loss 5.9095211\n",
      "Trained batch 3577 batch loss 4.81411552 epoch total loss 5.90921497\n",
      "Trained batch 3578 batch loss 4.51462936 epoch total loss 5.90882492\n",
      "Trained batch 3579 batch loss 6.16533852 epoch total loss 5.90889692\n",
      "Trained batch 3580 batch loss 5.41226768 epoch total loss 5.90875816\n",
      "Trained batch 3581 batch loss 5.50114441 epoch total loss 5.9086442\n",
      "Trained batch 3582 batch loss 5.48678493 epoch total loss 5.90852642\n",
      "Trained batch 3583 batch loss 5.10234928 epoch total loss 5.90830135\n",
      "Trained batch 3584 batch loss 6.37868214 epoch total loss 5.90843248\n",
      "Trained batch 3585 batch loss 6.13214588 epoch total loss 5.90849495\n",
      "Trained batch 3586 batch loss 6.34922218 epoch total loss 5.90861797\n",
      "Trained batch 3587 batch loss 5.32840443 epoch total loss 5.90845633\n",
      "Trained batch 3588 batch loss 5.55905867 epoch total loss 5.90835857\n",
      "Trained batch 3589 batch loss 5.80504513 epoch total loss 5.90833\n",
      "Trained batch 3590 batch loss 6.64066172 epoch total loss 5.90853405\n",
      "Trained batch 3591 batch loss 5.97171497 epoch total loss 5.90855169\n",
      "Trained batch 3592 batch loss 7.06252193 epoch total loss 5.90887308\n",
      "Trained batch 3593 batch loss 6.36939812 epoch total loss 5.90900135\n",
      "Trained batch 3594 batch loss 5.84086466 epoch total loss 5.90898228\n",
      "Trained batch 3595 batch loss 6.25914478 epoch total loss 5.90908\n",
      "Trained batch 3596 batch loss 6.60023785 epoch total loss 5.90927219\n",
      "Trained batch 3597 batch loss 5.89931774 epoch total loss 5.90926886\n",
      "Trained batch 3598 batch loss 6.59855318 epoch total loss 5.90946054\n",
      "Trained batch 3599 batch loss 5.54586172 epoch total loss 5.90935898\n",
      "Trained batch 3600 batch loss 5.95055676 epoch total loss 5.90937042\n",
      "Trained batch 3601 batch loss 5.57014894 epoch total loss 5.90927649\n",
      "Trained batch 3602 batch loss 5.45880604 epoch total loss 5.90915155\n",
      "Trained batch 3603 batch loss 5.80048561 epoch total loss 5.90912151\n",
      "Trained batch 3604 batch loss 6.01504087 epoch total loss 5.90915108\n",
      "Trained batch 3605 batch loss 5.56418133 epoch total loss 5.90905523\n",
      "Trained batch 3606 batch loss 5.68930149 epoch total loss 5.90899467\n",
      "Trained batch 3607 batch loss 5.90335655 epoch total loss 5.90899324\n",
      "Trained batch 3608 batch loss 6.16160822 epoch total loss 5.90906334\n",
      "Trained batch 3609 batch loss 6.19801903 epoch total loss 5.90914297\n",
      "Trained batch 3610 batch loss 5.94414711 epoch total loss 5.90915251\n",
      "Trained batch 3611 batch loss 5.95641804 epoch total loss 5.90916586\n",
      "Trained batch 3612 batch loss 6.22639179 epoch total loss 5.9092536\n",
      "Trained batch 3613 batch loss 6.13905811 epoch total loss 5.90931749\n",
      "Trained batch 3614 batch loss 5.70644093 epoch total loss 5.90926123\n",
      "Trained batch 3615 batch loss 4.77657413 epoch total loss 5.90894794\n",
      "Trained batch 3616 batch loss 5.90414143 epoch total loss 5.90894699\n",
      "Trained batch 3617 batch loss 5.43528843 epoch total loss 5.90881586\n",
      "Trained batch 3618 batch loss 5.41343498 epoch total loss 5.90867949\n",
      "Trained batch 3619 batch loss 5.50908709 epoch total loss 5.90856886\n",
      "Trained batch 3620 batch loss 5.12924576 epoch total loss 5.90835381\n",
      "Trained batch 3621 batch loss 5.22188139 epoch total loss 5.9081645\n",
      "Trained batch 3622 batch loss 5.81301 epoch total loss 5.9081378\n",
      "Trained batch 3623 batch loss 5.56228828 epoch total loss 5.90804243\n",
      "Trained batch 3624 batch loss 5.18139601 epoch total loss 5.90784216\n",
      "Trained batch 3625 batch loss 5.09572411 epoch total loss 5.90761805\n",
      "Trained batch 3626 batch loss 5.38259506 epoch total loss 5.90747309\n",
      "Trained batch 3627 batch loss 5.43751431 epoch total loss 5.90734386\n",
      "Trained batch 3628 batch loss 5.15020657 epoch total loss 5.90713501\n",
      "Trained batch 3629 batch loss 5.06055307 epoch total loss 5.90690184\n",
      "Trained batch 3630 batch loss 5.29782534 epoch total loss 5.90673351\n",
      "Trained batch 3631 batch loss 5.68123102 epoch total loss 5.90667152\n",
      "Trained batch 3632 batch loss 5.23556137 epoch total loss 5.90648699\n",
      "Trained batch 3633 batch loss 5.20614433 epoch total loss 5.90629482\n",
      "Trained batch 3634 batch loss 5.76408386 epoch total loss 5.90625525\n",
      "Trained batch 3635 batch loss 5.52609158 epoch total loss 5.90615082\n",
      "Trained batch 3636 batch loss 5.2097187 epoch total loss 5.90595865\n",
      "Trained batch 3637 batch loss 5.85987377 epoch total loss 5.90594625\n",
      "Trained batch 3638 batch loss 5.30535173 epoch total loss 5.90578079\n",
      "Trained batch 3639 batch loss 5.6962719 epoch total loss 5.90572309\n",
      "Trained batch 3640 batch loss 5.73302555 epoch total loss 5.90567541\n",
      "Trained batch 3641 batch loss 5.54280043 epoch total loss 5.90557575\n",
      "Trained batch 3642 batch loss 4.87879848 epoch total loss 5.90529394\n",
      "Trained batch 3643 batch loss 5.45031404 epoch total loss 5.90516901\n",
      "Trained batch 3644 batch loss 5.08532143 epoch total loss 5.90494442\n",
      "Trained batch 3645 batch loss 5.31539059 epoch total loss 5.9047823\n",
      "Trained batch 3646 batch loss 6.01541615 epoch total loss 5.90481281\n",
      "Trained batch 3647 batch loss 5.38123608 epoch total loss 5.90466928\n",
      "Trained batch 3648 batch loss 5.95185423 epoch total loss 5.90468168\n",
      "Trained batch 3649 batch loss 5.65452814 epoch total loss 5.90461302\n",
      "Trained batch 3650 batch loss 5.07705212 epoch total loss 5.90438604\n",
      "Trained batch 3651 batch loss 5.71535587 epoch total loss 5.90433455\n",
      "Trained batch 3652 batch loss 5.69947195 epoch total loss 5.90427828\n",
      "Trained batch 3653 batch loss 5.5293746 epoch total loss 5.90417528\n",
      "Trained batch 3654 batch loss 5.994277 epoch total loss 5.9042\n",
      "Trained batch 3655 batch loss 6.36317348 epoch total loss 5.90432596\n",
      "Trained batch 3656 batch loss 5.43816185 epoch total loss 5.90419817\n",
      "Trained batch 3657 batch loss 5.9678 epoch total loss 5.90421581\n",
      "Trained batch 3658 batch loss 6.5281992 epoch total loss 5.90438604\n",
      "Trained batch 3659 batch loss 6.44579458 epoch total loss 5.90453386\n",
      "Trained batch 3660 batch loss 6.35241318 epoch total loss 5.90465593\n",
      "Trained batch 3661 batch loss 6.05067158 epoch total loss 5.90469599\n",
      "Trained batch 3662 batch loss 6.11712 epoch total loss 5.90475416\n",
      "Trained batch 3663 batch loss 5.88242435 epoch total loss 5.90474796\n",
      "Trained batch 3664 batch loss 5.76900959 epoch total loss 5.90471125\n",
      "Trained batch 3665 batch loss 6.35491467 epoch total loss 5.90483427\n",
      "Trained batch 3666 batch loss 5.85359621 epoch total loss 5.90482\n",
      "Trained batch 3667 batch loss 5.79976225 epoch total loss 5.90479136\n",
      "Trained batch 3668 batch loss 6.17020607 epoch total loss 5.90486336\n",
      "Trained batch 3669 batch loss 5.74320555 epoch total loss 5.90481949\n",
      "Trained batch 3670 batch loss 5.69206381 epoch total loss 5.90476131\n",
      "Trained batch 3671 batch loss 5.99556684 epoch total loss 5.90478659\n",
      "Trained batch 3672 batch loss 5.84298563 epoch total loss 5.90477\n",
      "Trained batch 3673 batch loss 5.47429657 epoch total loss 5.9046526\n",
      "Trained batch 3674 batch loss 5.7721014 epoch total loss 5.90461636\n",
      "Trained batch 3675 batch loss 5.37005615 epoch total loss 5.90447044\n",
      "Trained batch 3676 batch loss 7.29408169 epoch total loss 5.90484905\n",
      "Trained batch 3677 batch loss 6.81024742 epoch total loss 5.9050951\n",
      "Trained batch 3678 batch loss 6.45021534 epoch total loss 5.90524387\n",
      "Trained batch 3679 batch loss 6.73746681 epoch total loss 5.90547\n",
      "Trained batch 3680 batch loss 6.26152897 epoch total loss 5.90556717\n",
      "Trained batch 3681 batch loss 6.77924 epoch total loss 5.90580416\n",
      "Trained batch 3682 batch loss 6.26311207 epoch total loss 5.90590143\n",
      "Trained batch 3683 batch loss 6.33752346 epoch total loss 5.90601873\n",
      "Trained batch 3684 batch loss 6.39209557 epoch total loss 5.90615082\n",
      "Trained batch 3685 batch loss 5.79362488 epoch total loss 5.9061203\n",
      "Trained batch 3686 batch loss 6.20388222 epoch total loss 5.90620089\n",
      "Trained batch 3687 batch loss 6.25065804 epoch total loss 5.90629387\n",
      "Trained batch 3688 batch loss 6.00566959 epoch total loss 5.90632105\n",
      "Trained batch 3689 batch loss 6.19249249 epoch total loss 5.90639877\n",
      "Trained batch 3690 batch loss 6.050951 epoch total loss 5.90643787\n",
      "Trained batch 3691 batch loss 6.21742773 epoch total loss 5.9065218\n",
      "Trained batch 3692 batch loss 6.03056717 epoch total loss 5.90655565\n",
      "Trained batch 3693 batch loss 6.20063591 epoch total loss 5.90663576\n",
      "Trained batch 3694 batch loss 6.63749266 epoch total loss 5.90683317\n",
      "Trained batch 3695 batch loss 6.04919529 epoch total loss 5.9068718\n",
      "Trained batch 3696 batch loss 5.76463509 epoch total loss 5.9068327\n",
      "Trained batch 3697 batch loss 5.95790577 epoch total loss 5.90684652\n",
      "Trained batch 3698 batch loss 5.82425117 epoch total loss 5.90682411\n",
      "Trained batch 3699 batch loss 6.21941662 epoch total loss 5.90690851\n",
      "Trained batch 3700 batch loss 5.95111465 epoch total loss 5.90692043\n",
      "Trained batch 3701 batch loss 6.07603931 epoch total loss 5.90696621\n",
      "Trained batch 3702 batch loss 6.13982439 epoch total loss 5.90702915\n",
      "Trained batch 3703 batch loss 5.16450644 epoch total loss 5.9068284\n",
      "Trained batch 3704 batch loss 6.11396599 epoch total loss 5.90688419\n",
      "Trained batch 3705 batch loss 5.9652586 epoch total loss 5.9069\n",
      "Trained batch 3706 batch loss 5.76647282 epoch total loss 5.90686178\n",
      "Trained batch 3707 batch loss 5.7531004 epoch total loss 5.90682077\n",
      "Trained batch 3708 batch loss 5.28873062 epoch total loss 5.90665388\n",
      "Trained batch 3709 batch loss 5.24351406 epoch total loss 5.90647554\n",
      "Trained batch 3710 batch loss 5.8512249 epoch total loss 5.90646076\n",
      "Trained batch 3711 batch loss 5.5621376 epoch total loss 5.90636778\n",
      "Trained batch 3712 batch loss 6.08118296 epoch total loss 5.90641499\n",
      "Trained batch 3713 batch loss 5.50593615 epoch total loss 5.90630722\n",
      "Trained batch 3714 batch loss 5.07801199 epoch total loss 5.90608454\n",
      "Trained batch 3715 batch loss 5.90706 epoch total loss 5.90608454\n",
      "Trained batch 3716 batch loss 6.3744297 epoch total loss 5.90621042\n",
      "Trained batch 3717 batch loss 6.19814205 epoch total loss 5.9062891\n",
      "Trained batch 3718 batch loss 6.25783539 epoch total loss 5.90638351\n",
      "Trained batch 3719 batch loss 4.84851456 epoch total loss 5.90609884\n",
      "Trained batch 3720 batch loss 6.42933369 epoch total loss 5.90623951\n",
      "Trained batch 3721 batch loss 5.37998104 epoch total loss 5.90609837\n",
      "Trained batch 3722 batch loss 6.70035839 epoch total loss 5.90631199\n",
      "Trained batch 3723 batch loss 4.9838028 epoch total loss 5.90606451\n",
      "Trained batch 3724 batch loss 6.37015057 epoch total loss 5.90618896\n",
      "Trained batch 3725 batch loss 5.99586105 epoch total loss 5.90621328\n",
      "Trained batch 3726 batch loss 5.66404152 epoch total loss 5.90614843\n",
      "Trained batch 3727 batch loss 5.73264599 epoch total loss 5.9061017\n",
      "Trained batch 3728 batch loss 5.82887554 epoch total loss 5.90608072\n",
      "Trained batch 3729 batch loss 5.27663136 epoch total loss 5.9059124\n",
      "Trained batch 3730 batch loss 6.19618034 epoch total loss 5.90598965\n",
      "Trained batch 3731 batch loss 6.70619106 epoch total loss 5.90620422\n",
      "Trained batch 3732 batch loss 6.61712646 epoch total loss 5.90639496\n",
      "Trained batch 3733 batch loss 6.91236 epoch total loss 5.90666437\n",
      "Trained batch 3734 batch loss 5.90539074 epoch total loss 5.90666437\n",
      "Trained batch 3735 batch loss 6.70709801 epoch total loss 5.90687847\n",
      "Trained batch 3736 batch loss 6.76781464 epoch total loss 5.90710878\n",
      "Trained batch 3737 batch loss 6.27036476 epoch total loss 5.90720606\n",
      "Trained batch 3738 batch loss 6.27720118 epoch total loss 5.90730476\n",
      "Trained batch 3739 batch loss 6.46060181 epoch total loss 5.90745306\n",
      "Trained batch 3740 batch loss 5.86630535 epoch total loss 5.90744209\n",
      "Trained batch 3741 batch loss 5.95720482 epoch total loss 5.90745544\n",
      "Trained batch 3742 batch loss 5.5763607 epoch total loss 5.90736675\n",
      "Trained batch 3743 batch loss 6.12598705 epoch total loss 5.9074254\n",
      "Trained batch 3744 batch loss 6.03596401 epoch total loss 5.90745974\n",
      "Trained batch 3745 batch loss 6.49227095 epoch total loss 5.90761566\n",
      "Trained batch 3746 batch loss 6.76930237 epoch total loss 5.90784597\n",
      "Trained batch 3747 batch loss 6.4445734 epoch total loss 5.9079895\n",
      "Trained batch 3748 batch loss 6.22228336 epoch total loss 5.90807343\n",
      "Trained batch 3749 batch loss 7.40745544 epoch total loss 5.90847349\n",
      "Trained batch 3750 batch loss 7.52959061 epoch total loss 5.90890551\n",
      "Trained batch 3751 batch loss 6.0004549 epoch total loss 5.90893\n",
      "Trained batch 3752 batch loss 5.83885 epoch total loss 5.90891123\n",
      "Trained batch 3753 batch loss 6.37041092 epoch total loss 5.90903425\n",
      "Trained batch 3754 batch loss 7.61221504 epoch total loss 5.90948772\n",
      "Trained batch 3755 batch loss 6.53378201 epoch total loss 5.90965366\n",
      "Trained batch 3756 batch loss 7.03156376 epoch total loss 5.90995264\n",
      "Trained batch 3757 batch loss 7.38055229 epoch total loss 5.91034412\n",
      "Trained batch 3758 batch loss 6.09314251 epoch total loss 5.91039276\n",
      "Trained batch 3759 batch loss 6.47122383 epoch total loss 5.91054201\n",
      "Trained batch 3760 batch loss 5.7439003 epoch total loss 5.91049767\n",
      "Trained batch 3761 batch loss 5.54829788 epoch total loss 5.91040134\n",
      "Trained batch 3762 batch loss 6.20047855 epoch total loss 5.91047859\n",
      "Trained batch 3763 batch loss 6.24318027 epoch total loss 5.91056728\n",
      "Trained batch 3764 batch loss 6.09144163 epoch total loss 5.91061544\n",
      "Trained batch 3765 batch loss 6.70287609 epoch total loss 5.91082573\n",
      "Trained batch 3766 batch loss 6.48924351 epoch total loss 5.91097927\n",
      "Trained batch 3767 batch loss 6.57111931 epoch total loss 5.91115427\n",
      "Trained batch 3768 batch loss 7.21109772 epoch total loss 5.9114995\n",
      "Trained batch 3769 batch loss 6.41955853 epoch total loss 5.91163397\n",
      "Trained batch 3770 batch loss 5.20296907 epoch total loss 5.91144609\n",
      "Trained batch 3771 batch loss 6.91636705 epoch total loss 5.91171265\n",
      "Trained batch 3772 batch loss 5.57516956 epoch total loss 5.911623\n",
      "Trained batch 3773 batch loss 7.22608185 epoch total loss 5.91197157\n",
      "Trained batch 3774 batch loss 6.73141527 epoch total loss 5.91218853\n",
      "Trained batch 3775 batch loss 6.8348217 epoch total loss 5.91243267\n",
      "Trained batch 3776 batch loss 6.83848763 epoch total loss 5.91267776\n",
      "Trained batch 3777 batch loss 6.32346106 epoch total loss 5.91278696\n",
      "Trained batch 3778 batch loss 5.3191843 epoch total loss 5.9126296\n",
      "Trained batch 3779 batch loss 5.66822386 epoch total loss 5.91256475\n",
      "Trained batch 3780 batch loss 6.79522133 epoch total loss 5.91279793\n",
      "Trained batch 3781 batch loss 6.43208122 epoch total loss 5.91293526\n",
      "Trained batch 3782 batch loss 6.75496483 epoch total loss 5.91315842\n",
      "Trained batch 3783 batch loss 6.69822121 epoch total loss 5.91336536\n",
      "Trained batch 3784 batch loss 6.79536057 epoch total loss 5.91359854\n",
      "Trained batch 3785 batch loss 6.62187 epoch total loss 5.91378546\n",
      "Trained batch 3786 batch loss 6.35838032 epoch total loss 5.91390276\n",
      "Trained batch 3787 batch loss 6.8983779 epoch total loss 5.91416264\n",
      "Trained batch 3788 batch loss 6.64938831 epoch total loss 5.91435623\n",
      "Trained batch 3789 batch loss 6.54243183 epoch total loss 5.91452217\n",
      "Trained batch 3790 batch loss 6.43400192 epoch total loss 5.91465902\n",
      "Trained batch 3791 batch loss 6.8041172 epoch total loss 5.9148941\n",
      "Trained batch 3792 batch loss 6.48391056 epoch total loss 5.91504431\n",
      "Trained batch 3793 batch loss 6.34062195 epoch total loss 5.91515636\n",
      "Trained batch 3794 batch loss 6.32334137 epoch total loss 5.91526413\n",
      "Trained batch 3795 batch loss 6.71871805 epoch total loss 5.91547585\n",
      "Trained batch 3796 batch loss 6.30404758 epoch total loss 5.91557837\n",
      "Trained batch 3797 batch loss 6.31753349 epoch total loss 5.91568422\n",
      "Trained batch 3798 batch loss 6.58537149 epoch total loss 5.91586065\n",
      "Trained batch 3799 batch loss 6.0681076 epoch total loss 5.91590118\n",
      "Trained batch 3800 batch loss 6.74257 epoch total loss 5.91611862\n",
      "Trained batch 3801 batch loss 6.70894432 epoch total loss 5.916327\n",
      "Trained batch 3802 batch loss 5.94764042 epoch total loss 5.91633511\n",
      "Trained batch 3803 batch loss 6.49099255 epoch total loss 5.91648626\n",
      "Trained batch 3804 batch loss 6.37480736 epoch total loss 5.91660643\n",
      "Trained batch 3805 batch loss 5.93816566 epoch total loss 5.91661215\n",
      "Trained batch 3806 batch loss 5.40692806 epoch total loss 5.91647816\n",
      "Trained batch 3807 batch loss 5.62348795 epoch total loss 5.91640091\n",
      "Trained batch 3808 batch loss 5.73922396 epoch total loss 5.91635418\n",
      "Trained batch 3809 batch loss 5.93473434 epoch total loss 5.91635895\n",
      "Trained batch 3810 batch loss 5.8355484 epoch total loss 5.91633797\n",
      "Trained batch 3811 batch loss 5.93599892 epoch total loss 5.91634321\n",
      "Trained batch 3812 batch loss 5.61926031 epoch total loss 5.91626501\n",
      "Trained batch 3813 batch loss 5.71709633 epoch total loss 5.91621304\n",
      "Trained batch 3814 batch loss 4.32026482 epoch total loss 5.91579437\n",
      "Trained batch 3815 batch loss 5.42821836 epoch total loss 5.91566658\n",
      "Trained batch 3816 batch loss 5.55422211 epoch total loss 5.91557169\n",
      "Trained batch 3817 batch loss 6.51789427 epoch total loss 5.91572952\n",
      "Trained batch 3818 batch loss 5.70163774 epoch total loss 5.91567326\n",
      "Trained batch 3819 batch loss 6.21601439 epoch total loss 5.91575241\n",
      "Trained batch 3820 batch loss 5.58662224 epoch total loss 5.9156661\n",
      "Trained batch 3821 batch loss 5.86703777 epoch total loss 5.91565323\n",
      "Trained batch 3822 batch loss 6.19197 epoch total loss 5.91572523\n",
      "Trained batch 3823 batch loss 6.26356411 epoch total loss 5.91581631\n",
      "Trained batch 3824 batch loss 4.20076561 epoch total loss 5.91536808\n",
      "Trained batch 3825 batch loss 5.87949419 epoch total loss 5.91535854\n",
      "Trained batch 3826 batch loss 5.71480513 epoch total loss 5.91530609\n",
      "Trained batch 3827 batch loss 6.19526863 epoch total loss 5.91537905\n",
      "Trained batch 3828 batch loss 6.1339283 epoch total loss 5.91543674\n",
      "Trained batch 3829 batch loss 6.1992445 epoch total loss 5.91551065\n",
      "Trained batch 3830 batch loss 5.99042702 epoch total loss 5.91553\n",
      "Trained batch 3831 batch loss 6.01195908 epoch total loss 5.91555548\n",
      "Trained batch 3832 batch loss 5.39503765 epoch total loss 5.9154191\n",
      "Trained batch 3833 batch loss 5.29497051 epoch total loss 5.91525745\n",
      "Trained batch 3834 batch loss 4.81651354 epoch total loss 5.91497087\n",
      "Trained batch 3835 batch loss 5.3535738 epoch total loss 5.91482449\n",
      "Trained batch 3836 batch loss 6.01261044 epoch total loss 5.91484976\n",
      "Trained batch 3837 batch loss 6.27347851 epoch total loss 5.91494322\n",
      "Trained batch 3838 batch loss 6.10432148 epoch total loss 5.91499233\n",
      "Trained batch 3839 batch loss 5.79224682 epoch total loss 5.91496038\n",
      "Trained batch 3840 batch loss 6.1929121 epoch total loss 5.91503286\n",
      "Trained batch 3841 batch loss 5.98368645 epoch total loss 5.91505098\n",
      "Trained batch 3842 batch loss 5.79161167 epoch total loss 5.91501856\n",
      "Trained batch 3843 batch loss 5.6738205 epoch total loss 5.91495609\n",
      "Trained batch 3844 batch loss 5.67236137 epoch total loss 5.91489267\n",
      "Trained batch 3845 batch loss 6.02710152 epoch total loss 5.91492176\n",
      "Trained batch 3846 batch loss 5.70122957 epoch total loss 5.91486645\n",
      "Trained batch 3847 batch loss 5.07638645 epoch total loss 5.91464853\n",
      "Trained batch 3848 batch loss 4.49929237 epoch total loss 5.91428089\n",
      "Trained batch 3849 batch loss 5.60398912 epoch total loss 5.9142\n",
      "Trained batch 3850 batch loss 5.37175083 epoch total loss 5.91405916\n",
      "Trained batch 3851 batch loss 5.3903718 epoch total loss 5.91392326\n",
      "Trained batch 3852 batch loss 6.44387293 epoch total loss 5.91406059\n",
      "Trained batch 3853 batch loss 6.08033562 epoch total loss 5.91410351\n",
      "Trained batch 3854 batch loss 4.64404678 epoch total loss 5.91377401\n",
      "Trained batch 3855 batch loss 5.55477333 epoch total loss 5.91368103\n",
      "Trained batch 3856 batch loss 5.76433659 epoch total loss 5.91364193\n",
      "Trained batch 3857 batch loss 5.03694725 epoch total loss 5.91341496\n",
      "Trained batch 3858 batch loss 4.67398071 epoch total loss 5.91309357\n",
      "Trained batch 3859 batch loss 4.72674179 epoch total loss 5.91278601\n",
      "Trained batch 3860 batch loss 4.78841877 epoch total loss 5.91249514\n",
      "Trained batch 3861 batch loss 6.26903629 epoch total loss 5.91258764\n",
      "Trained batch 3862 batch loss 4.7638588 epoch total loss 5.91229\n",
      "Trained batch 3863 batch loss 4.99889612 epoch total loss 5.91205311\n",
      "Trained batch 3864 batch loss 7.05849838 epoch total loss 5.91235\n",
      "Trained batch 3865 batch loss 6.76026392 epoch total loss 5.91256905\n",
      "Trained batch 3866 batch loss 5.9304533 epoch total loss 5.91257381\n",
      "Trained batch 3867 batch loss 6.73980045 epoch total loss 5.91278791\n",
      "Trained batch 3868 batch loss 5.6259675 epoch total loss 5.91271305\n",
      "Trained batch 3869 batch loss 5.24997187 epoch total loss 5.91254187\n",
      "Trained batch 3870 batch loss 5.47486877 epoch total loss 5.91242886\n",
      "Trained batch 3871 batch loss 5.56298256 epoch total loss 5.91233826\n",
      "Trained batch 3872 batch loss 5.67035866 epoch total loss 5.91227579\n",
      "Trained batch 3873 batch loss 4.62217426 epoch total loss 5.91194296\n",
      "Trained batch 3874 batch loss 5.69885063 epoch total loss 5.91188812\n",
      "Trained batch 3875 batch loss 5.85472202 epoch total loss 5.91187334\n",
      "Trained batch 3876 batch loss 5.91845322 epoch total loss 5.91187525\n",
      "Trained batch 3877 batch loss 5.2928853 epoch total loss 5.91171551\n",
      "Trained batch 3878 batch loss 4.56022358 epoch total loss 5.91136694\n",
      "Trained batch 3879 batch loss 4.36266613 epoch total loss 5.91096783\n",
      "Trained batch 3880 batch loss 5.5635252 epoch total loss 5.91087866\n",
      "Trained batch 3881 batch loss 4.54422283 epoch total loss 5.91052675\n",
      "Trained batch 3882 batch loss 5.93298721 epoch total loss 5.91053247\n",
      "Trained batch 3883 batch loss 5.9161253 epoch total loss 5.91053391\n",
      "Trained batch 3884 batch loss 5.9168582 epoch total loss 5.91053534\n",
      "Trained batch 3885 batch loss 5.72387743 epoch total loss 5.91048765\n",
      "Trained batch 3886 batch loss 6.48628426 epoch total loss 5.91063595\n",
      "Trained batch 3887 batch loss 5.70495129 epoch total loss 5.91058302\n",
      "Trained batch 3888 batch loss 5.52522182 epoch total loss 5.91048384\n",
      "Trained batch 3889 batch loss 6.21645164 epoch total loss 5.91056252\n",
      "Trained batch 3890 batch loss 5.85137558 epoch total loss 5.91054726\n",
      "Trained batch 3891 batch loss 6.09879494 epoch total loss 5.91059589\n",
      "Trained batch 3892 batch loss 5.77472591 epoch total loss 5.91056108\n",
      "Trained batch 3893 batch loss 4.84955788 epoch total loss 5.91028881\n",
      "Trained batch 3894 batch loss 4.79016495 epoch total loss 5.91000128\n",
      "Trained batch 3895 batch loss 4.55451488 epoch total loss 5.90965319\n",
      "Trained batch 3896 batch loss 6.02732038 epoch total loss 5.9096837\n",
      "Trained batch 3897 batch loss 5.70881128 epoch total loss 5.90963221\n",
      "Trained batch 3898 batch loss 5.27429771 epoch total loss 5.90946865\n",
      "Trained batch 3899 batch loss 5.71305752 epoch total loss 5.90941858\n",
      "Trained batch 3900 batch loss 5.87398338 epoch total loss 5.90940905\n",
      "Trained batch 3901 batch loss 5.04795933 epoch total loss 5.90918827\n",
      "Trained batch 3902 batch loss 4.57720184 epoch total loss 5.90884733\n",
      "Trained batch 3903 batch loss 5.55610371 epoch total loss 5.90875721\n",
      "Trained batch 3904 batch loss 5.29932451 epoch total loss 5.90860081\n",
      "Trained batch 3905 batch loss 5.05142498 epoch total loss 5.90838099\n",
      "Trained batch 3906 batch loss 5.17312288 epoch total loss 5.90819311\n",
      "Trained batch 3907 batch loss 5.88288116 epoch total loss 5.90818644\n",
      "Trained batch 3908 batch loss 4.68631 epoch total loss 5.90787363\n",
      "Trained batch 3909 batch loss 4.98150444 epoch total loss 5.90763712\n",
      "Trained batch 3910 batch loss 5.3327527 epoch total loss 5.90749\n",
      "Trained batch 3911 batch loss 5.53077793 epoch total loss 5.90739346\n",
      "Trained batch 3912 batch loss 4.4094429 epoch total loss 5.90701103\n",
      "Trained batch 3913 batch loss 6.11413574 epoch total loss 5.90706348\n",
      "Trained batch 3914 batch loss 6.13299751 epoch total loss 5.90712118\n",
      "Trained batch 3915 batch loss 5.21244621 epoch total loss 5.9069438\n",
      "Trained batch 3916 batch loss 4.84267426 epoch total loss 5.906672\n",
      "Trained batch 3917 batch loss 5.1527977 epoch total loss 5.90647936\n",
      "Trained batch 3918 batch loss 4.6919136 epoch total loss 5.90616941\n",
      "Trained batch 3919 batch loss 4.51503706 epoch total loss 5.90581465\n",
      "Trained batch 3920 batch loss 4.6750083 epoch total loss 5.90550041\n",
      "Trained batch 3921 batch loss 4.88695192 epoch total loss 5.90524101\n",
      "Trained batch 3922 batch loss 5.98194027 epoch total loss 5.90526056\n",
      "Trained batch 3923 batch loss 5.58131933 epoch total loss 5.90517807\n",
      "Trained batch 3924 batch loss 5.14045238 epoch total loss 5.90498304\n",
      "Trained batch 3925 batch loss 5.44564104 epoch total loss 5.90486622\n",
      "Trained batch 3926 batch loss 5.02485847 epoch total loss 5.90464211\n",
      "Trained batch 3927 batch loss 5.28897476 epoch total loss 5.90448523\n",
      "Trained batch 3928 batch loss 5.98609257 epoch total loss 5.90450621\n",
      "Trained batch 3929 batch loss 6.80787802 epoch total loss 5.90473652\n",
      "Trained batch 3930 batch loss 5.74886847 epoch total loss 5.90469646\n",
      "Trained batch 3931 batch loss 6.24673653 epoch total loss 5.90478325\n",
      "Trained batch 3932 batch loss 6.05161047 epoch total loss 5.90482044\n",
      "Trained batch 3933 batch loss 6.02580738 epoch total loss 5.90485096\n",
      "Trained batch 3934 batch loss 5.89493799 epoch total loss 5.90484858\n",
      "Trained batch 3935 batch loss 5.58134556 epoch total loss 5.90476656\n",
      "Trained batch 3936 batch loss 5.73348904 epoch total loss 5.90472317\n",
      "Trained batch 3937 batch loss 6.06154633 epoch total loss 5.90476322\n",
      "Trained batch 3938 batch loss 5.92129898 epoch total loss 5.90476751\n",
      "Trained batch 3939 batch loss 5.82963276 epoch total loss 5.90474844\n",
      "Trained batch 3940 batch loss 5.68534088 epoch total loss 5.90469313\n",
      "Trained batch 3941 batch loss 5.36497974 epoch total loss 5.90455627\n",
      "Trained batch 3942 batch loss 5.58993435 epoch total loss 5.90447617\n",
      "Trained batch 3943 batch loss 5.83136082 epoch total loss 5.90445805\n",
      "Trained batch 3944 batch loss 5.4913559 epoch total loss 5.90435314\n",
      "Trained batch 3945 batch loss 5.86937523 epoch total loss 5.90434456\n",
      "Trained batch 3946 batch loss 5.80937672 epoch total loss 5.90432024\n",
      "Trained batch 3947 batch loss 6.15987349 epoch total loss 5.90438509\n",
      "Trained batch 3948 batch loss 5.46533585 epoch total loss 5.90427351\n",
      "Trained batch 3949 batch loss 5.52958679 epoch total loss 5.90417862\n",
      "Trained batch 3950 batch loss 6.12610245 epoch total loss 5.90423489\n",
      "Trained batch 3951 batch loss 5.61685562 epoch total loss 5.90416241\n",
      "Trained batch 3952 batch loss 6.17258072 epoch total loss 5.90423\n",
      "Trained batch 3953 batch loss 5.50463295 epoch total loss 5.90412903\n",
      "Trained batch 3954 batch loss 5.98633099 epoch total loss 5.90414953\n",
      "Trained batch 3955 batch loss 5.88159561 epoch total loss 5.90414381\n",
      "Trained batch 3956 batch loss 6.01451826 epoch total loss 5.90417147\n",
      "Trained batch 3957 batch loss 5.8763876 epoch total loss 5.90416479\n",
      "Trained batch 3958 batch loss 6.17754698 epoch total loss 5.90423393\n",
      "Trained batch 3959 batch loss 6.09325027 epoch total loss 5.90428162\n",
      "Trained batch 3960 batch loss 6.11111641 epoch total loss 5.90433407\n",
      "Trained batch 3961 batch loss 5.53046 epoch total loss 5.90423965\n",
      "Trained batch 3962 batch loss 6.20891285 epoch total loss 5.90431643\n",
      "Trained batch 3963 batch loss 5.36720037 epoch total loss 5.904181\n",
      "Trained batch 3964 batch loss 5.93946171 epoch total loss 5.90419\n",
      "Trained batch 3965 batch loss 6.49456358 epoch total loss 5.90433884\n",
      "Trained batch 3966 batch loss 6.57907867 epoch total loss 5.90450859\n",
      "Trained batch 3967 batch loss 6.5777483 epoch total loss 5.90467834\n",
      "Trained batch 3968 batch loss 6.86679268 epoch total loss 5.90492105\n",
      "Trained batch 3969 batch loss 5.81768847 epoch total loss 5.90489912\n",
      "Trained batch 3970 batch loss 5.56007385 epoch total loss 5.90481234\n",
      "Trained batch 3971 batch loss 6.08396721 epoch total loss 5.90485764\n",
      "Trained batch 3972 batch loss 6.26156 epoch total loss 5.90494728\n",
      "Trained batch 3973 batch loss 6.34256 epoch total loss 5.90505743\n",
      "Trained batch 3974 batch loss 6.52021074 epoch total loss 5.90521193\n",
      "Trained batch 3975 batch loss 5.76325226 epoch total loss 5.90517616\n",
      "Trained batch 3976 batch loss 6.3529377 epoch total loss 5.90528917\n",
      "Trained batch 3977 batch loss 5.9569788 epoch total loss 5.90530205\n",
      "Trained batch 3978 batch loss 5.89861202 epoch total loss 5.90530062\n",
      "Trained batch 3979 batch loss 6.29943848 epoch total loss 5.90539932\n",
      "Trained batch 3980 batch loss 5.83958149 epoch total loss 5.90538263\n",
      "Trained batch 3981 batch loss 5.96238136 epoch total loss 5.90539742\n",
      "Trained batch 3982 batch loss 6.14055157 epoch total loss 5.90545654\n",
      "Trained batch 3983 batch loss 5.7807889 epoch total loss 5.90542507\n",
      "Trained batch 3984 batch loss 5.91256046 epoch total loss 5.90542698\n",
      "Trained batch 3985 batch loss 6.33542633 epoch total loss 5.90553474\n",
      "Trained batch 3986 batch loss 6.08316565 epoch total loss 5.90557957\n",
      "Trained batch 3987 batch loss 6.06757545 epoch total loss 5.90562057\n",
      "Trained batch 3988 batch loss 6.50334644 epoch total loss 5.9057703\n",
      "Trained batch 3989 batch loss 5.05327797 epoch total loss 5.90555668\n",
      "Trained batch 3990 batch loss 6.0629425 epoch total loss 5.90559626\n",
      "Trained batch 3991 batch loss 4.85435247 epoch total loss 5.90533257\n",
      "Trained batch 3992 batch loss 5.47901726 epoch total loss 5.90522528\n",
      "Trained batch 3993 batch loss 6.2959156 epoch total loss 5.90532351\n",
      "Trained batch 3994 batch loss 5.68211 epoch total loss 5.90526772\n",
      "Trained batch 3995 batch loss 5.744627 epoch total loss 5.90522718\n",
      "Trained batch 3996 batch loss 5.93593025 epoch total loss 5.90523481\n",
      "Trained batch 3997 batch loss 5.72812843 epoch total loss 5.90519047\n",
      "Trained batch 3998 batch loss 5.81518173 epoch total loss 5.90516806\n",
      "Trained batch 3999 batch loss 5.91548443 epoch total loss 5.90517044\n",
      "Trained batch 4000 batch loss 5.77770758 epoch total loss 5.90513849\n",
      "Trained batch 4001 batch loss 5.33546066 epoch total loss 5.9049964\n",
      "Trained batch 4002 batch loss 5.57618 epoch total loss 5.90491438\n",
      "Trained batch 4003 batch loss 6.04000759 epoch total loss 5.90494776\n",
      "Trained batch 4004 batch loss 5.90024376 epoch total loss 5.9049468\n",
      "Trained batch 4005 batch loss 5.4457221 epoch total loss 5.90483189\n",
      "Trained batch 4006 batch loss 5.48615932 epoch total loss 5.90472746\n",
      "Trained batch 4007 batch loss 5.65917 epoch total loss 5.90466595\n",
      "Trained batch 4008 batch loss 6.17591667 epoch total loss 5.90473366\n",
      "Trained batch 4009 batch loss 6.50565147 epoch total loss 5.90488338\n",
      "Trained batch 4010 batch loss 6.11656666 epoch total loss 5.90493631\n",
      "Trained batch 4011 batch loss 5.53741837 epoch total loss 5.90484476\n",
      "Trained batch 4012 batch loss 6.873559 epoch total loss 5.90508604\n",
      "Trained batch 4013 batch loss 6.42707253 epoch total loss 5.90521622\n",
      "Trained batch 4014 batch loss 6.17237234 epoch total loss 5.9052825\n",
      "Trained batch 4015 batch loss 6.65769482 epoch total loss 5.90547037\n",
      "Trained batch 4016 batch loss 6.05846 epoch total loss 5.90550852\n",
      "Trained batch 4017 batch loss 5.94207382 epoch total loss 5.9055171\n",
      "Trained batch 4018 batch loss 6.29599 epoch total loss 5.90561485\n",
      "Trained batch 4019 batch loss 6.07900858 epoch total loss 5.90565777\n",
      "Trained batch 4020 batch loss 5.46486759 epoch total loss 5.9055481\n",
      "Trained batch 4021 batch loss 5.67572355 epoch total loss 5.90549088\n",
      "Trained batch 4022 batch loss 5.69550228 epoch total loss 5.90543842\n",
      "Trained batch 4023 batch loss 5.62238407 epoch total loss 5.90536833\n",
      "Trained batch 4024 batch loss 5.71134949 epoch total loss 5.90532\n",
      "Trained batch 4025 batch loss 5.34699774 epoch total loss 5.90518141\n",
      "Trained batch 4026 batch loss 5.67073059 epoch total loss 5.90512323\n",
      "Trained batch 4027 batch loss 5.27072954 epoch total loss 5.90496588\n",
      "Trained batch 4028 batch loss 6.30276 epoch total loss 5.90506458\n",
      "Trained batch 4029 batch loss 4.80183411 epoch total loss 5.90479088\n",
      "Trained batch 4030 batch loss 5.8194747 epoch total loss 5.90477\n",
      "Trained batch 4031 batch loss 5.92471313 epoch total loss 5.90477467\n",
      "Trained batch 4032 batch loss 6.02480555 epoch total loss 5.90480471\n",
      "Trained batch 4033 batch loss 5.42351437 epoch total loss 5.9046855\n",
      "Trained batch 4034 batch loss 5.49344921 epoch total loss 5.90458345\n",
      "Trained batch 4035 batch loss 4.93313122 epoch total loss 5.90434265\n",
      "Trained batch 4036 batch loss 4.12596655 epoch total loss 5.90390205\n",
      "Trained batch 4037 batch loss 5.8736372 epoch total loss 5.90389442\n",
      "Trained batch 4038 batch loss 5.94416714 epoch total loss 5.90390396\n",
      "Trained batch 4039 batch loss 6.43015 epoch total loss 5.90403414\n",
      "Trained batch 4040 batch loss 6.39405441 epoch total loss 5.90415573\n",
      "Trained batch 4041 batch loss 5.71444702 epoch total loss 5.904109\n",
      "Trained batch 4042 batch loss 6.35657215 epoch total loss 5.90422106\n",
      "Trained batch 4043 batch loss 6.44194746 epoch total loss 5.9043541\n",
      "Trained batch 4044 batch loss 6.67136288 epoch total loss 5.90454388\n",
      "Trained batch 4045 batch loss 5.44602394 epoch total loss 5.90443\n",
      "Trained batch 4046 batch loss 5.86190319 epoch total loss 5.90441942\n",
      "Trained batch 4047 batch loss 5.15863419 epoch total loss 5.90423489\n",
      "Trained batch 4048 batch loss 4.8532095 epoch total loss 5.90397549\n",
      "Trained batch 4049 batch loss 5.53839207 epoch total loss 5.90388536\n",
      "Trained batch 4050 batch loss 6.61785173 epoch total loss 5.90406132\n",
      "Trained batch 4051 batch loss 5.8967309 epoch total loss 5.90406\n",
      "Trained batch 4052 batch loss 5.95039129 epoch total loss 5.90407133\n",
      "Trained batch 4053 batch loss 5.21723652 epoch total loss 5.90390158\n",
      "Trained batch 4054 batch loss 5.50405693 epoch total loss 5.90380287\n",
      "Trained batch 4055 batch loss 5.56252956 epoch total loss 5.90371895\n",
      "Trained batch 4056 batch loss 5.55994034 epoch total loss 5.90363407\n",
      "Trained batch 4057 batch loss 5.9746356 epoch total loss 5.90365171\n",
      "Trained batch 4058 batch loss 5.61193371 epoch total loss 5.90357971\n",
      "Trained batch 4059 batch loss 5.21429968 epoch total loss 5.90341\n",
      "Trained batch 4060 batch loss 5.62069464 epoch total loss 5.90334034\n",
      "Trained batch 4061 batch loss 5.93149376 epoch total loss 5.90334749\n",
      "Trained batch 4062 batch loss 6.29465389 epoch total loss 5.90344381\n",
      "Trained batch 4063 batch loss 5.4741993 epoch total loss 5.90333843\n",
      "Trained batch 4064 batch loss 5.64096451 epoch total loss 5.90327358\n",
      "Trained batch 4065 batch loss 5.99687576 epoch total loss 5.90329647\n",
      "Trained batch 4066 batch loss 5.57234955 epoch total loss 5.90321493\n",
      "Trained batch 4067 batch loss 5.6171627 epoch total loss 5.90314484\n",
      "Trained batch 4068 batch loss 5.64686108 epoch total loss 5.90308189\n",
      "Trained batch 4069 batch loss 5.14057779 epoch total loss 5.9028945\n",
      "Trained batch 4070 batch loss 6.32655287 epoch total loss 5.90299845\n",
      "Trained batch 4071 batch loss 4.83638859 epoch total loss 5.90273619\n",
      "Trained batch 4072 batch loss 4.17547035 epoch total loss 5.90231228\n",
      "Trained batch 4073 batch loss 6.08923578 epoch total loss 5.90235806\n",
      "Trained batch 4074 batch loss 5.25298691 epoch total loss 5.90219879\n",
      "Trained batch 4075 batch loss 4.41550732 epoch total loss 5.90183449\n",
      "Trained batch 4076 batch loss 4.899 epoch total loss 5.90158796\n",
      "Trained batch 4077 batch loss 6.67339134 epoch total loss 5.90177727\n",
      "Trained batch 4078 batch loss 5.5138092 epoch total loss 5.90168238\n",
      "Trained batch 4079 batch loss 6.09789467 epoch total loss 5.90173054\n",
      "Trained batch 4080 batch loss 6.64764738 epoch total loss 5.90191317\n",
      "Trained batch 4081 batch loss 5.59901428 epoch total loss 5.90183926\n",
      "Trained batch 4082 batch loss 6.46251297 epoch total loss 5.90197659\n",
      "Trained batch 4083 batch loss 5.78037167 epoch total loss 5.90194702\n",
      "Trained batch 4084 batch loss 6.34586716 epoch total loss 5.90205574\n",
      "Trained batch 4085 batch loss 5.79284 epoch total loss 5.90202904\n",
      "Trained batch 4086 batch loss 7.16965389 epoch total loss 5.90233946\n",
      "Trained batch 4087 batch loss 4.89292812 epoch total loss 5.90209246\n",
      "Trained batch 4088 batch loss 6.69299173 epoch total loss 5.90228605\n",
      "Trained batch 4089 batch loss 6.90489388 epoch total loss 5.90253115\n",
      "Trained batch 4090 batch loss 7.41198778 epoch total loss 5.9029\n",
      "Trained batch 4091 batch loss 6.81308317 epoch total loss 5.90312243\n",
      "Trained batch 4092 batch loss 6.66607475 epoch total loss 5.90330887\n",
      "Trained batch 4093 batch loss 5.53781319 epoch total loss 5.90321922\n",
      "Trained batch 4094 batch loss 5.05468 epoch total loss 5.90301228\n",
      "Trained batch 4095 batch loss 5.58121157 epoch total loss 5.9029336\n",
      "Trained batch 4096 batch loss 5.85178661 epoch total loss 5.9029212\n",
      "Trained batch 4097 batch loss 5.86300468 epoch total loss 5.90291166\n",
      "Trained batch 4098 batch loss 6.06110859 epoch total loss 5.90295\n",
      "Trained batch 4099 batch loss 6.29998159 epoch total loss 5.90304708\n",
      "Trained batch 4100 batch loss 5.95974779 epoch total loss 5.90306091\n",
      "Trained batch 4101 batch loss 5.7359457 epoch total loss 5.90302\n",
      "Trained batch 4102 batch loss 4.31432438 epoch total loss 5.90263271\n",
      "Trained batch 4103 batch loss 6.01767159 epoch total loss 5.90266085\n",
      "Trained batch 4104 batch loss 6.13068581 epoch total loss 5.90271616\n",
      "Trained batch 4105 batch loss 5.63134 epoch total loss 5.90265036\n",
      "Trained batch 4106 batch loss 5.94972181 epoch total loss 5.90266132\n",
      "Trained batch 4107 batch loss 6.57799911 epoch total loss 5.90282583\n",
      "Trained batch 4108 batch loss 6.0915947 epoch total loss 5.90287209\n",
      "Trained batch 4109 batch loss 6.17440271 epoch total loss 5.90293789\n",
      "Trained batch 4110 batch loss 6.01042461 epoch total loss 5.90296412\n",
      "Trained batch 4111 batch loss 6.06924629 epoch total loss 5.90300417\n",
      "Trained batch 4112 batch loss 5.93509388 epoch total loss 5.90301228\n",
      "Trained batch 4113 batch loss 6.07748175 epoch total loss 5.90305471\n",
      "Trained batch 4114 batch loss 5.95098877 epoch total loss 5.90306616\n",
      "Trained batch 4115 batch loss 4.79570866 epoch total loss 5.90279722\n",
      "Trained batch 4116 batch loss 4.9430337 epoch total loss 5.90256405\n",
      "Trained batch 4117 batch loss 4.67037201 epoch total loss 5.9022646\n",
      "Trained batch 4118 batch loss 5.53711128 epoch total loss 5.9021759\n",
      "Trained batch 4119 batch loss 3.64559364 epoch total loss 5.90162802\n",
      "Trained batch 4120 batch loss 4.61559296 epoch total loss 5.90131617\n",
      "Trained batch 4121 batch loss 4.46111679 epoch total loss 5.90096664\n",
      "Trained batch 4122 batch loss 5.71483707 epoch total loss 5.90092134\n",
      "Trained batch 4123 batch loss 5.9765377 epoch total loss 5.90093946\n",
      "Trained batch 4124 batch loss 5.48590183 epoch total loss 5.90083933\n",
      "Trained batch 4125 batch loss 5.27561712 epoch total loss 5.90068769\n",
      "Trained batch 4126 batch loss 6.10090208 epoch total loss 5.90073633\n",
      "Trained batch 4127 batch loss 5.79406261 epoch total loss 5.90071058\n",
      "Trained batch 4128 batch loss 5.40499353 epoch total loss 5.90059042\n",
      "Trained batch 4129 batch loss 5.69325447 epoch total loss 5.90054\n",
      "Trained batch 4130 batch loss 5.4438343 epoch total loss 5.90042925\n",
      "Trained batch 4131 batch loss 5.64821482 epoch total loss 5.90036821\n",
      "Trained batch 4132 batch loss 5.63801956 epoch total loss 5.90030527\n",
      "Trained batch 4133 batch loss 5.93575859 epoch total loss 5.90031338\n",
      "Trained batch 4134 batch loss 5.80112839 epoch total loss 5.90028954\n",
      "Trained batch 4135 batch loss 5.3502841 epoch total loss 5.9001565\n",
      "Trained batch 4136 batch loss 5.50993538 epoch total loss 5.90006208\n",
      "Trained batch 4137 batch loss 4.97530651 epoch total loss 5.89983845\n",
      "Trained batch 4138 batch loss 6.41664696 epoch total loss 5.8999629\n",
      "Trained batch 4139 batch loss 5.16193867 epoch total loss 5.89978456\n",
      "Trained batch 4140 batch loss 5.92753792 epoch total loss 5.89979124\n",
      "Trained batch 4141 batch loss 6.30779028 epoch total loss 5.89989042\n",
      "Trained batch 4142 batch loss 5.61955166 epoch total loss 5.89982224\n",
      "Trained batch 4143 batch loss 5.60436916 epoch total loss 5.89975071\n",
      "Trained batch 4144 batch loss 4.00259304 epoch total loss 5.89929295\n",
      "Trained batch 4145 batch loss 5.481318 epoch total loss 5.89919186\n",
      "Trained batch 4146 batch loss 5.70633 epoch total loss 5.8991456\n",
      "Trained batch 4147 batch loss 5.80303669 epoch total loss 5.89912224\n",
      "Trained batch 4148 batch loss 5.86699581 epoch total loss 5.89911461\n",
      "Trained batch 4149 batch loss 6.53033 epoch total loss 5.89926672\n",
      "Trained batch 4150 batch loss 6.85780573 epoch total loss 5.89949799\n",
      "Trained batch 4151 batch loss 6.92797184 epoch total loss 5.89974546\n",
      "Trained batch 4152 batch loss 5.60413 epoch total loss 5.89967442\n",
      "Trained batch 4153 batch loss 6.3465786 epoch total loss 5.8997817\n",
      "Trained batch 4154 batch loss 5.89586735 epoch total loss 5.89978075\n",
      "Trained batch 4155 batch loss 6.27534485 epoch total loss 5.89987135\n",
      "Trained batch 4156 batch loss 6.88534307 epoch total loss 5.90010834\n",
      "Trained batch 4157 batch loss 4.79402208 epoch total loss 5.89984226\n",
      "Trained batch 4158 batch loss 4.90240669 epoch total loss 5.89960241\n",
      "Trained batch 4159 batch loss 5.68330336 epoch total loss 5.89955044\n",
      "Trained batch 4160 batch loss 5.65273046 epoch total loss 5.89949083\n",
      "Trained batch 4161 batch loss 6.08541203 epoch total loss 5.89953566\n",
      "Trained batch 4162 batch loss 5.96402836 epoch total loss 5.89955139\n",
      "Trained batch 4163 batch loss 6.16439056 epoch total loss 5.89961529\n",
      "Trained batch 4164 batch loss 6.46881151 epoch total loss 5.89975166\n",
      "Trained batch 4165 batch loss 5.11749172 epoch total loss 5.89956379\n",
      "Trained batch 4166 batch loss 5.74039459 epoch total loss 5.89952564\n",
      "Trained batch 4167 batch loss 5.74752903 epoch total loss 5.8994894\n",
      "Trained batch 4168 batch loss 3.45868564 epoch total loss 5.89890385\n",
      "Trained batch 4169 batch loss 5.59544516 epoch total loss 5.89883089\n",
      "Trained batch 4170 batch loss 4.57001 epoch total loss 5.89851236\n",
      "Trained batch 4171 batch loss 5.55981445 epoch total loss 5.8984313\n",
      "Trained batch 4172 batch loss 5.47567797 epoch total loss 5.89833\n",
      "Trained batch 4173 batch loss 5.7416811 epoch total loss 5.89829302\n",
      "Trained batch 4174 batch loss 5.41029787 epoch total loss 5.89817572\n",
      "Trained batch 4175 batch loss 6.43397903 epoch total loss 5.89830399\n",
      "Trained batch 4176 batch loss 5.28963661 epoch total loss 5.89815807\n",
      "Trained batch 4177 batch loss 5.85806656 epoch total loss 5.89814854\n",
      "Trained batch 4178 batch loss 6.16030407 epoch total loss 5.898211\n",
      "Trained batch 4179 batch loss 5.64000416 epoch total loss 5.89814949\n",
      "Trained batch 4180 batch loss 6.35603476 epoch total loss 5.89825916\n",
      "Trained batch 4181 batch loss 6.85771465 epoch total loss 5.89848852\n",
      "Trained batch 4182 batch loss 6.93471241 epoch total loss 5.89873648\n",
      "Trained batch 4183 batch loss 5.96301 epoch total loss 5.89875174\n",
      "Trained batch 4184 batch loss 6.88695669 epoch total loss 5.89898777\n",
      "Trained batch 4185 batch loss 7.04811287 epoch total loss 5.89926243\n",
      "Trained batch 4186 batch loss 7.16985035 epoch total loss 5.89956617\n",
      "Trained batch 4187 batch loss 7.04672146 epoch total loss 5.89984035\n",
      "Trained batch 4188 batch loss 6.09934235 epoch total loss 5.89988804\n",
      "Trained batch 4189 batch loss 6.73755264 epoch total loss 5.90008783\n",
      "Trained batch 4190 batch loss 6.43891287 epoch total loss 5.90021658\n",
      "Trained batch 4191 batch loss 6.41940403 epoch total loss 5.90034056\n",
      "Trained batch 4192 batch loss 6.44306517 epoch total loss 5.90047026\n",
      "Trained batch 4193 batch loss 6.33016586 epoch total loss 5.90057278\n",
      "Trained batch 4194 batch loss 6.15489531 epoch total loss 5.90063334\n",
      "Trained batch 4195 batch loss 6.3352685 epoch total loss 5.90073681\n",
      "Trained batch 4196 batch loss 6.56930971 epoch total loss 5.90089607\n",
      "Trained batch 4197 batch loss 6.68756247 epoch total loss 5.90108347\n",
      "Trained batch 4198 batch loss 6.4868927 epoch total loss 5.90122318\n",
      "Trained batch 4199 batch loss 6.34676361 epoch total loss 5.90132904\n",
      "Trained batch 4200 batch loss 6.13940811 epoch total loss 5.90138578\n",
      "Trained batch 4201 batch loss 6.32874966 epoch total loss 5.90148735\n",
      "Trained batch 4202 batch loss 6.2333889 epoch total loss 5.90156603\n",
      "Trained batch 4203 batch loss 6.21470833 epoch total loss 5.90164089\n",
      "Trained batch 4204 batch loss 5.13391352 epoch total loss 5.90145826\n",
      "Trained batch 4205 batch loss 6.93074894 epoch total loss 5.90170336\n",
      "Trained batch 4206 batch loss 6.79890919 epoch total loss 5.9019165\n",
      "Trained batch 4207 batch loss 6.52386475 epoch total loss 5.90206432\n",
      "Trained batch 4208 batch loss 6.36728096 epoch total loss 5.90217495\n",
      "Trained batch 4209 batch loss 5.94898701 epoch total loss 5.90218592\n",
      "Trained batch 4210 batch loss 6.70840549 epoch total loss 5.90237761\n",
      "Trained batch 4211 batch loss 6.04159737 epoch total loss 5.90241051\n",
      "Trained batch 4212 batch loss 6.12973833 epoch total loss 5.90246439\n",
      "Trained batch 4213 batch loss 6.48387432 epoch total loss 5.90260267\n",
      "Trained batch 4214 batch loss 6.66106081 epoch total loss 5.90278244\n",
      "Trained batch 4215 batch loss 6.29542732 epoch total loss 5.90287542\n",
      "Trained batch 4216 batch loss 6.64084721 epoch total loss 5.90305042\n",
      "Trained batch 4217 batch loss 6.29333 epoch total loss 5.90314293\n",
      "Trained batch 4218 batch loss 6.36770248 epoch total loss 5.9032526\n",
      "Trained batch 4219 batch loss 6.10438728 epoch total loss 5.90330029\n",
      "Trained batch 4220 batch loss 6.10475349 epoch total loss 5.90334797\n",
      "Trained batch 4221 batch loss 6.18117523 epoch total loss 5.90341425\n",
      "Trained batch 4222 batch loss 5.94202518 epoch total loss 5.90342283\n",
      "Trained batch 4223 batch loss 5.53872538 epoch total loss 5.90333652\n",
      "Trained batch 4224 batch loss 5.47426796 epoch total loss 5.90323544\n",
      "Trained batch 4225 batch loss 5.6991663 epoch total loss 5.9031868\n",
      "Trained batch 4226 batch loss 5.68375731 epoch total loss 5.90313482\n",
      "Trained batch 4227 batch loss 5.67828655 epoch total loss 5.90308189\n",
      "Trained batch 4228 batch loss 5.11388254 epoch total loss 5.90289497\n",
      "Trained batch 4229 batch loss 5.34831619 epoch total loss 5.90276337\n",
      "Trained batch 4230 batch loss 5.39994144 epoch total loss 5.90264463\n",
      "Trained batch 4231 batch loss 5.56707954 epoch total loss 5.90256548\n",
      "Trained batch 4232 batch loss 5.1746273 epoch total loss 5.90239334\n",
      "Trained batch 4233 batch loss 5.08160448 epoch total loss 5.90219927\n",
      "Trained batch 4234 batch loss 5.16747189 epoch total loss 5.9020257\n",
      "Trained batch 4235 batch loss 4.90027332 epoch total loss 5.90178919\n",
      "Trained batch 4236 batch loss 5.64575291 epoch total loss 5.90172911\n",
      "Trained batch 4237 batch loss 6.00375319 epoch total loss 5.90175343\n",
      "Trained batch 4238 batch loss 6.01262093 epoch total loss 5.90177917\n",
      "Trained batch 4239 batch loss 6.01437378 epoch total loss 5.9018054\n",
      "Trained batch 4240 batch loss 6.46965885 epoch total loss 5.90193939\n",
      "Trained batch 4241 batch loss 5.8287878 epoch total loss 5.90192175\n",
      "Trained batch 4242 batch loss 6.51509094 epoch total loss 5.90206671\n",
      "Trained batch 4243 batch loss 6.33328247 epoch total loss 5.90216827\n",
      "Trained batch 4244 batch loss 6.23356056 epoch total loss 5.90224648\n",
      "Trained batch 4245 batch loss 6.26653624 epoch total loss 5.90233231\n",
      "Trained batch 4246 batch loss 6.01466179 epoch total loss 5.90235901\n",
      "Trained batch 4247 batch loss 6.05481815 epoch total loss 5.90239477\n",
      "Trained batch 4248 batch loss 6.38422 epoch total loss 5.90250826\n",
      "Trained batch 4249 batch loss 5.96392632 epoch total loss 5.90252304\n",
      "Trained batch 4250 batch loss 6.04606533 epoch total loss 5.9025569\n",
      "Trained batch 4251 batch loss 5.89827824 epoch total loss 5.90255594\n",
      "Trained batch 4252 batch loss 5.62931442 epoch total loss 5.90249157\n",
      "Trained batch 4253 batch loss 5.85937262 epoch total loss 5.90248156\n",
      "Trained batch 4254 batch loss 5.66153622 epoch total loss 5.90242481\n",
      "Trained batch 4255 batch loss 4.56422901 epoch total loss 5.90211058\n",
      "Trained batch 4256 batch loss 5.20957184 epoch total loss 5.9019475\n",
      "Trained batch 4257 batch loss 4.90235329 epoch total loss 5.90171289\n",
      "Trained batch 4258 batch loss 5.75678635 epoch total loss 5.90167856\n",
      "Trained batch 4259 batch loss 5.35626936 epoch total loss 5.90155029\n",
      "Trained batch 4260 batch loss 5.75321102 epoch total loss 5.90151596\n",
      "Trained batch 4261 batch loss 6.28886843 epoch total loss 5.90160656\n",
      "Trained batch 4262 batch loss 6.18876743 epoch total loss 5.90167427\n",
      "Trained batch 4263 batch loss 5.95899153 epoch total loss 5.90168762\n",
      "Trained batch 4264 batch loss 5.80763245 epoch total loss 5.90166569\n",
      "Trained batch 4265 batch loss 5.63066864 epoch total loss 5.90160227\n",
      "Trained batch 4266 batch loss 6.33121204 epoch total loss 5.90170336\n",
      "Trained batch 4267 batch loss 5.82730103 epoch total loss 5.90168619\n",
      "Trained batch 4268 batch loss 6.61642694 epoch total loss 5.90185356\n",
      "Trained batch 4269 batch loss 6.52306557 epoch total loss 5.90199947\n",
      "Trained batch 4270 batch loss 6.30145264 epoch total loss 5.90209246\n",
      "Trained batch 4271 batch loss 5.61835289 epoch total loss 5.90202618\n",
      "Trained batch 4272 batch loss 6.18806219 epoch total loss 5.90209341\n",
      "Trained batch 4273 batch loss 7.12519169 epoch total loss 5.90237951\n",
      "Trained batch 4274 batch loss 6.18476629 epoch total loss 5.90244579\n",
      "Trained batch 4275 batch loss 6.38592911 epoch total loss 5.9025588\n",
      "Trained batch 4276 batch loss 6.33749485 epoch total loss 5.90266085\n",
      "Trained batch 4277 batch loss 5.94519758 epoch total loss 5.90267086\n",
      "Trained batch 4278 batch loss 5.17414379 epoch total loss 5.9025\n",
      "Trained batch 4279 batch loss 4.92537498 epoch total loss 5.90227222\n",
      "Trained batch 4280 batch loss 6.38337612 epoch total loss 5.90238428\n",
      "Trained batch 4281 batch loss 6.00941896 epoch total loss 5.90240955\n",
      "Trained batch 4282 batch loss 4.85595465 epoch total loss 5.90216494\n",
      "Trained batch 4283 batch loss 5.26358366 epoch total loss 5.90201569\n",
      "Trained batch 4284 batch loss 5.89567184 epoch total loss 5.90201473\n",
      "Trained batch 4285 batch loss 5.53475142 epoch total loss 5.9019289\n",
      "Trained batch 4286 batch loss 5.45244694 epoch total loss 5.90182447\n",
      "Trained batch 4287 batch loss 5.45837879 epoch total loss 5.901721\n",
      "Trained batch 4288 batch loss 6.17254829 epoch total loss 5.90178394\n",
      "Trained batch 4289 batch loss 6.4649682 epoch total loss 5.90191507\n",
      "Trained batch 4290 batch loss 5.58590555 epoch total loss 5.90184164\n",
      "Trained batch 4291 batch loss 6.53624725 epoch total loss 5.90198946\n",
      "Trained batch 4292 batch loss 5.82908821 epoch total loss 5.90197229\n",
      "Trained batch 4293 batch loss 4.97606945 epoch total loss 5.90175676\n",
      "Trained batch 4294 batch loss 5.97515678 epoch total loss 5.90177393\n",
      "Trained batch 4295 batch loss 5.89062691 epoch total loss 5.90177107\n",
      "Trained batch 4296 batch loss 5.21315718 epoch total loss 5.90161085\n",
      "Trained batch 4297 batch loss 6.71656704 epoch total loss 5.90180063\n",
      "Trained batch 4298 batch loss 6.08550596 epoch total loss 5.90184355\n",
      "Trained batch 4299 batch loss 6.16388416 epoch total loss 5.90190458\n",
      "Trained batch 4300 batch loss 6.85399342 epoch total loss 5.90212584\n",
      "Trained batch 4301 batch loss 6.35630608 epoch total loss 5.90223122\n",
      "Trained batch 4302 batch loss 6.49846268 epoch total loss 5.9023695\n",
      "Trained batch 4303 batch loss 6.06992531 epoch total loss 5.9024086\n",
      "Trained batch 4304 batch loss 6.31390429 epoch total loss 5.90250444\n",
      "Trained batch 4305 batch loss 7.02969265 epoch total loss 5.90276623\n",
      "Trained batch 4306 batch loss 7.24545765 epoch total loss 5.90307808\n",
      "Trained batch 4307 batch loss 6.25574207 epoch total loss 5.90316\n",
      "Trained batch 4308 batch loss 6.39334583 epoch total loss 5.90327358\n",
      "Trained batch 4309 batch loss 6.52023411 epoch total loss 5.90341663\n",
      "Trained batch 4310 batch loss 6.09906 epoch total loss 5.90346193\n",
      "Trained batch 4311 batch loss 6.10566711 epoch total loss 5.90350914\n",
      "Trained batch 4312 batch loss 6.31658173 epoch total loss 5.90360498\n",
      "Trained batch 4313 batch loss 5.60120583 epoch total loss 5.90353489\n",
      "Trained batch 4314 batch loss 4.94365072 epoch total loss 5.90331221\n",
      "Trained batch 4315 batch loss 6.00746965 epoch total loss 5.90333652\n",
      "Trained batch 4316 batch loss 6.00375319 epoch total loss 5.90336\n",
      "Trained batch 4317 batch loss 5.68734741 epoch total loss 5.90331\n",
      "Trained batch 4318 batch loss 5.68411493 epoch total loss 5.9032588\n",
      "Trained batch 4319 batch loss 6.43642807 epoch total loss 5.90338182\n",
      "Trained batch 4320 batch loss 6.50790596 epoch total loss 5.90352201\n",
      "Trained batch 4321 batch loss 6.61337757 epoch total loss 5.90368605\n",
      "Trained batch 4322 batch loss 6.52306747 epoch total loss 5.90382957\n",
      "Trained batch 4323 batch loss 6.40035057 epoch total loss 5.90394449\n",
      "Trained batch 4324 batch loss 6.40617657 epoch total loss 5.90406084\n",
      "Trained batch 4325 batch loss 6.70842695 epoch total loss 5.90424681\n",
      "Trained batch 4326 batch loss 6.14668751 epoch total loss 5.9043026\n",
      "Trained batch 4327 batch loss 6.3007021 epoch total loss 5.90439415\n",
      "Trained batch 4328 batch loss 6.23470688 epoch total loss 5.90447044\n",
      "Trained batch 4329 batch loss 6.34165049 epoch total loss 5.90457153\n",
      "Trained batch 4330 batch loss 6.10890102 epoch total loss 5.90461874\n",
      "Trained batch 4331 batch loss 5.97775555 epoch total loss 5.90463591\n",
      "Trained batch 4332 batch loss 6.6968441 epoch total loss 5.90481901\n",
      "Trained batch 4333 batch loss 6.12574577 epoch total loss 5.90486956\n",
      "Trained batch 4334 batch loss 5.89772606 epoch total loss 5.90486813\n",
      "Trained batch 4335 batch loss 6.20691 epoch total loss 5.90493822\n",
      "Trained batch 4336 batch loss 5.78733921 epoch total loss 5.90491104\n",
      "Trained batch 4337 batch loss 6.80454445 epoch total loss 5.90511847\n",
      "Trained batch 4338 batch loss 6.11347389 epoch total loss 5.90516615\n",
      "Trained batch 4339 batch loss 5.25221348 epoch total loss 5.90501595\n",
      "Trained batch 4340 batch loss 5.5916748 epoch total loss 5.90494347\n",
      "Trained batch 4341 batch loss 5.95035124 epoch total loss 5.90495443\n",
      "Trained batch 4342 batch loss 5.74898481 epoch total loss 5.90491819\n",
      "Trained batch 4343 batch loss 5.59457493 epoch total loss 5.90484667\n",
      "Trained batch 4344 batch loss 5.7224412 epoch total loss 5.90480471\n",
      "Trained batch 4345 batch loss 5.40303564 epoch total loss 5.90468884\n",
      "Trained batch 4346 batch loss 5.91289806 epoch total loss 5.90469074\n",
      "Trained batch 4347 batch loss 5.66690922 epoch total loss 5.90463543\n",
      "Trained batch 4348 batch loss 5.36407185 epoch total loss 5.90451097\n",
      "Trained batch 4349 batch loss 5.1249218 epoch total loss 5.90433168\n",
      "Trained batch 4350 batch loss 5.74015617 epoch total loss 5.90429401\n",
      "Trained batch 4351 batch loss 5.43827677 epoch total loss 5.90418673\n",
      "Trained batch 4352 batch loss 5.9744544 epoch total loss 5.90420294\n",
      "Trained batch 4353 batch loss 5.6293211 epoch total loss 5.90414\n",
      "Trained batch 4354 batch loss 6.13651133 epoch total loss 5.9041934\n",
      "Trained batch 4355 batch loss 5.78298473 epoch total loss 5.90416527\n",
      "Trained batch 4356 batch loss 5.29905128 epoch total loss 5.90402651\n",
      "Trained batch 4357 batch loss 6.06202936 epoch total loss 5.90406275\n",
      "Trained batch 4358 batch loss 5.36424637 epoch total loss 5.90393877\n",
      "Trained batch 4359 batch loss 5.99419117 epoch total loss 5.90395927\n",
      "Trained batch 4360 batch loss 6.24003506 epoch total loss 5.90403652\n",
      "Trained batch 4361 batch loss 6.20968342 epoch total loss 5.90410662\n",
      "Trained batch 4362 batch loss 6.71089745 epoch total loss 5.90429163\n",
      "Trained batch 4363 batch loss 5.39815283 epoch total loss 5.90417576\n",
      "Trained batch 4364 batch loss 5.61777735 epoch total loss 5.90411\n",
      "Trained batch 4365 batch loss 4.91014528 epoch total loss 5.90388203\n",
      "Trained batch 4366 batch loss 5.48767376 epoch total loss 5.90378666\n",
      "Trained batch 4367 batch loss 5.20104218 epoch total loss 5.90362597\n",
      "Trained batch 4368 batch loss 5.98179436 epoch total loss 5.90364408\n",
      "Trained batch 4369 batch loss 5.45049763 epoch total loss 5.90354061\n",
      "Trained batch 4370 batch loss 6.26429653 epoch total loss 5.9036231\n",
      "Trained batch 4371 batch loss 6.30607605 epoch total loss 5.90371513\n",
      "Trained batch 4372 batch loss 6.47867775 epoch total loss 5.90384674\n",
      "Trained batch 4373 batch loss 6.4160347 epoch total loss 5.90396357\n",
      "Trained batch 4374 batch loss 5.97902298 epoch total loss 5.90398073\n",
      "Trained batch 4375 batch loss 6.85410166 epoch total loss 5.90419769\n",
      "Trained batch 4376 batch loss 5.95217419 epoch total loss 5.90420914\n",
      "Trained batch 4377 batch loss 6.62983 epoch total loss 5.9043746\n",
      "Trained batch 4378 batch loss 6.5931468 epoch total loss 5.90453196\n",
      "Trained batch 4379 batch loss 5.33360672 epoch total loss 5.90440178\n",
      "Trained batch 4380 batch loss 5.9548955 epoch total loss 5.90441322\n",
      "Trained batch 4381 batch loss 6.05170155 epoch total loss 5.9044466\n",
      "Trained batch 4382 batch loss 5.92722034 epoch total loss 5.90445185\n",
      "Trained batch 4383 batch loss 5.6019268 epoch total loss 5.90438271\n",
      "Trained batch 4384 batch loss 5.59262753 epoch total loss 5.90431166\n",
      "Trained batch 4385 batch loss 5.36506939 epoch total loss 5.90418863\n",
      "Trained batch 4386 batch loss 5.13564157 epoch total loss 5.90401316\n",
      "Trained batch 4387 batch loss 4.72862291 epoch total loss 5.90374517\n",
      "Trained batch 4388 batch loss 6.04350424 epoch total loss 5.90377712\n",
      "Trained batch 4389 batch loss 6.06092501 epoch total loss 5.90381289\n",
      "Trained batch 4390 batch loss 6.70265293 epoch total loss 5.90399456\n",
      "Trained batch 4391 batch loss 6.60096 epoch total loss 5.90415382\n",
      "Trained batch 4392 batch loss 6.49420547 epoch total loss 5.90428782\n",
      "Trained batch 4393 batch loss 6.25039625 epoch total loss 5.90436649\n",
      "Trained batch 4394 batch loss 5.52312708 epoch total loss 5.90428\n",
      "Trained batch 4395 batch loss 6.02955246 epoch total loss 5.90430832\n",
      "Trained batch 4396 batch loss 5.89090157 epoch total loss 5.90430546\n",
      "Trained batch 4397 batch loss 5.47872734 epoch total loss 5.90420866\n",
      "Trained batch 4398 batch loss 5.74437761 epoch total loss 5.90417194\n",
      "Trained batch 4399 batch loss 4.9345026 epoch total loss 5.90395164\n",
      "Trained batch 4400 batch loss 6.14960623 epoch total loss 5.90400743\n",
      "Trained batch 4401 batch loss 5.25533867 epoch total loss 5.90386\n",
      "Trained batch 4402 batch loss 6.49359608 epoch total loss 5.90399408\n",
      "Trained batch 4403 batch loss 4.81494236 epoch total loss 5.9037466\n",
      "Trained batch 4404 batch loss 7.12628651 epoch total loss 5.9040246\n",
      "Trained batch 4405 batch loss 6.45310926 epoch total loss 5.90414906\n",
      "Trained batch 4406 batch loss 7.41529369 epoch total loss 5.90449238\n",
      "Trained batch 4407 batch loss 5.34701681 epoch total loss 5.90436602\n",
      "Trained batch 4408 batch loss 5.18137455 epoch total loss 5.90420198\n",
      "Trained batch 4409 batch loss 5.40773869 epoch total loss 5.90408945\n",
      "Trained batch 4410 batch loss 5.35314083 epoch total loss 5.90396452\n",
      "Trained batch 4411 batch loss 5.25545454 epoch total loss 5.90381765\n",
      "Trained batch 4412 batch loss 6.19869947 epoch total loss 5.90388489\n",
      "Trained batch 4413 batch loss 6.61703968 epoch total loss 5.90404654\n",
      "Trained batch 4414 batch loss 6.76746321 epoch total loss 5.90424204\n",
      "Trained batch 4415 batch loss 6.39099216 epoch total loss 5.90435219\n",
      "Trained batch 4416 batch loss 6.73458576 epoch total loss 5.90454\n",
      "Trained batch 4417 batch loss 6.75153923 epoch total loss 5.90473175\n",
      "Trained batch 4418 batch loss 6.3559866 epoch total loss 5.90483379\n",
      "Trained batch 4419 batch loss 6.4226594 epoch total loss 5.9049511\n",
      "Trained batch 4420 batch loss 6.67024469 epoch total loss 5.90512419\n",
      "Trained batch 4421 batch loss 7.09327745 epoch total loss 5.90539312\n",
      "Trained batch 4422 batch loss 6.51719189 epoch total loss 5.90553141\n",
      "Trained batch 4423 batch loss 5.4785533 epoch total loss 5.90543461\n",
      "Trained batch 4424 batch loss 5.62563038 epoch total loss 5.90537167\n",
      "Trained batch 4425 batch loss 6.31263542 epoch total loss 5.90546322\n",
      "Trained batch 4426 batch loss 5.59089422 epoch total loss 5.90539265\n",
      "Trained batch 4427 batch loss 6.10472059 epoch total loss 5.90543795\n",
      "Trained batch 4428 batch loss 6.32472229 epoch total loss 5.90553236\n",
      "Trained batch 4429 batch loss 5.42774487 epoch total loss 5.90542459\n",
      "Trained batch 4430 batch loss 6.47487831 epoch total loss 5.90555286\n",
      "Trained batch 4431 batch loss 6.33560753 epoch total loss 5.90565\n",
      "Trained batch 4432 batch loss 5.74708939 epoch total loss 5.90561438\n",
      "Trained batch 4433 batch loss 5.79718208 epoch total loss 5.90559\n",
      "Trained batch 4434 batch loss 6.05609608 epoch total loss 5.90562391\n",
      "Trained batch 4435 batch loss 5.55800247 epoch total loss 5.90554571\n",
      "Trained batch 4436 batch loss 6.03082 epoch total loss 5.90557432\n",
      "Trained batch 4437 batch loss 5.81642246 epoch total loss 5.90555429\n",
      "Trained batch 4438 batch loss 5.55905151 epoch total loss 5.90547609\n",
      "Trained batch 4439 batch loss 6.09551334 epoch total loss 5.90551853\n",
      "Trained batch 4440 batch loss 6.24091911 epoch total loss 5.90559435\n",
      "Trained batch 4441 batch loss 6.24510193 epoch total loss 5.90567\n",
      "Trained batch 4442 batch loss 5.73277473 epoch total loss 5.90563154\n",
      "Trained batch 4443 batch loss 5.79978275 epoch total loss 5.90560722\n",
      "Trained batch 4444 batch loss 6.33804607 epoch total loss 5.9057045\n",
      "Trained batch 4445 batch loss 6.43838406 epoch total loss 5.90582418\n",
      "Trained batch 4446 batch loss 6.01759815 epoch total loss 5.90584946\n",
      "Trained batch 4447 batch loss 6.20769787 epoch total loss 5.90591717\n",
      "Trained batch 4448 batch loss 5.66475725 epoch total loss 5.90586281\n",
      "Trained batch 4449 batch loss 6.30827665 epoch total loss 5.90595341\n",
      "Trained batch 4450 batch loss 6.4796772 epoch total loss 5.90608215\n",
      "Trained batch 4451 batch loss 6.26817513 epoch total loss 5.90616369\n",
      "Trained batch 4452 batch loss 6.00700855 epoch total loss 5.90618658\n",
      "Trained batch 4453 batch loss 6.25336409 epoch total loss 5.90626431\n",
      "Trained batch 4454 batch loss 5.80729389 epoch total loss 5.90624189\n",
      "Trained batch 4455 batch loss 5.77184439 epoch total loss 5.90621185\n",
      "Trained batch 4456 batch loss 5.90743351 epoch total loss 5.90621233\n",
      "Trained batch 4457 batch loss 6.09285831 epoch total loss 5.90625429\n",
      "Trained batch 4458 batch loss 5.92597675 epoch total loss 5.90625858\n",
      "Trained batch 4459 batch loss 6.19985962 epoch total loss 5.90632439\n",
      "Trained batch 4460 batch loss 6.48372746 epoch total loss 5.90645409\n",
      "Trained batch 4461 batch loss 6.74905491 epoch total loss 5.90664339\n",
      "Trained batch 4462 batch loss 6.37036419 epoch total loss 5.90674734\n",
      "Trained batch 4463 batch loss 6.20680285 epoch total loss 5.90681458\n",
      "Trained batch 4464 batch loss 5.16546345 epoch total loss 5.90664864\n",
      "Trained batch 4465 batch loss 6.3686 epoch total loss 5.90675211\n",
      "Trained batch 4466 batch loss 6.11176443 epoch total loss 5.90679789\n",
      "Trained batch 4467 batch loss 5.42257595 epoch total loss 5.90668964\n",
      "Trained batch 4468 batch loss 5.44511795 epoch total loss 5.90658617\n",
      "Trained batch 4469 batch loss 5.12228298 epoch total loss 5.90641069\n",
      "Trained batch 4470 batch loss 6.03777313 epoch total loss 5.90644026\n",
      "Trained batch 4471 batch loss 6.19450665 epoch total loss 5.90650463\n",
      "Trained batch 4472 batch loss 5.87307882 epoch total loss 5.906497\n",
      "Trained batch 4473 batch loss 6.53229761 epoch total loss 5.90663719\n",
      "Trained batch 4474 batch loss 6.79257536 epoch total loss 5.90683556\n",
      "Trained batch 4475 batch loss 5.87251663 epoch total loss 5.90682793\n",
      "Trained batch 4476 batch loss 6.51008415 epoch total loss 5.90696239\n",
      "Trained batch 4477 batch loss 6.38465691 epoch total loss 5.90706921\n",
      "Trained batch 4478 batch loss 6.23042774 epoch total loss 5.90714169\n",
      "Trained batch 4479 batch loss 6.83983707 epoch total loss 5.90734959\n",
      "Trained batch 4480 batch loss 6.30670881 epoch total loss 5.90743876\n",
      "Trained batch 4481 batch loss 6.47538 epoch total loss 5.90756559\n",
      "Trained batch 4482 batch loss 6.79010296 epoch total loss 5.90776253\n",
      "Trained batch 4483 batch loss 6.22176743 epoch total loss 5.90783262\n",
      "Trained batch 4484 batch loss 6.59040642 epoch total loss 5.90798473\n",
      "Trained batch 4485 batch loss 6.64829445 epoch total loss 5.90815\n",
      "Trained batch 4486 batch loss 6.69570541 epoch total loss 5.90832567\n",
      "Trained batch 4487 batch loss 6.09706974 epoch total loss 5.90836763\n",
      "Trained batch 4488 batch loss 6.73817539 epoch total loss 5.90855265\n",
      "Trained batch 4489 batch loss 6.73664522 epoch total loss 5.90873718\n",
      "Trained batch 4490 batch loss 6.00819111 epoch total loss 5.90875912\n",
      "Trained batch 4491 batch loss 6.86092854 epoch total loss 5.90897131\n",
      "Trained batch 4492 batch loss 6.74521971 epoch total loss 5.90915728\n",
      "Trained batch 4493 batch loss 6.46131897 epoch total loss 5.9092803\n",
      "Trained batch 4494 batch loss 6.13140821 epoch total loss 5.90932941\n",
      "Trained batch 4495 batch loss 6.27395916 epoch total loss 5.90941048\n",
      "Trained batch 4496 batch loss 5.38135576 epoch total loss 5.90929317\n",
      "Trained batch 4497 batch loss 6.42068863 epoch total loss 5.90940666\n",
      "Trained batch 4498 batch loss 6.29957962 epoch total loss 5.90949297\n",
      "Trained batch 4499 batch loss 5.80110836 epoch total loss 5.90946913\n",
      "Trained batch 4500 batch loss 6.18882084 epoch total loss 5.90953112\n",
      "Trained batch 4501 batch loss 5.52399731 epoch total loss 5.90944529\n",
      "Trained batch 4502 batch loss 5.51976204 epoch total loss 5.90935898\n",
      "Trained batch 4503 batch loss 5.63422966 epoch total loss 5.90929794\n",
      "Trained batch 4504 batch loss 6.18685484 epoch total loss 5.90935946\n",
      "Trained batch 4505 batch loss 6.06315804 epoch total loss 5.90939379\n",
      "Trained batch 4506 batch loss 5.79770803 epoch total loss 5.90936852\n",
      "Trained batch 4507 batch loss 5.59201193 epoch total loss 5.90929842\n",
      "Trained batch 4508 batch loss 5.92167854 epoch total loss 5.9093008\n",
      "Trained batch 4509 batch loss 5.89212608 epoch total loss 5.90929747\n",
      "Trained batch 4510 batch loss 6.28750134 epoch total loss 5.90938091\n",
      "Trained batch 4511 batch loss 5.30939054 epoch total loss 5.90924788\n",
      "Trained batch 4512 batch loss 5.73302937 epoch total loss 5.90920877\n",
      "Trained batch 4513 batch loss 6.24886131 epoch total loss 5.90928364\n",
      "Trained batch 4514 batch loss 5.81087255 epoch total loss 5.9092617\n",
      "Trained batch 4515 batch loss 6.414608 epoch total loss 5.90937376\n",
      "Trained batch 4516 batch loss 6.2669878 epoch total loss 5.90945292\n",
      "Trained batch 4517 batch loss 6.49859142 epoch total loss 5.90958309\n",
      "Trained batch 4518 batch loss 5.78954 epoch total loss 5.90955687\n",
      "Trained batch 4519 batch loss 5.45636797 epoch total loss 5.90945673\n",
      "Trained batch 4520 batch loss 4.5458374 epoch total loss 5.90915442\n",
      "Trained batch 4521 batch loss 5.68058777 epoch total loss 5.90910387\n",
      "Trained batch 4522 batch loss 5.94265699 epoch total loss 5.9091115\n",
      "Trained batch 4523 batch loss 6.14095688 epoch total loss 5.90916252\n",
      "Trained batch 4524 batch loss 5.9913187 epoch total loss 5.90918112\n",
      "Trained batch 4525 batch loss 5.74517298 epoch total loss 5.90914488\n",
      "Trained batch 4526 batch loss 6.0895462 epoch total loss 5.90918493\n",
      "Trained batch 4527 batch loss 5.90884972 epoch total loss 5.90918446\n",
      "Trained batch 4528 batch loss 6.1665163 epoch total loss 5.9092412\n",
      "Trained batch 4529 batch loss 6.64116287 epoch total loss 5.90940285\n",
      "Trained batch 4530 batch loss 5.22709703 epoch total loss 5.90925217\n",
      "Trained batch 4531 batch loss 6.34693146 epoch total loss 5.90934896\n",
      "Trained batch 4532 batch loss 6.8439455 epoch total loss 5.90955496\n",
      "Trained batch 4533 batch loss 7.27938032 epoch total loss 5.90985727\n",
      "Trained batch 4534 batch loss 7.01141071 epoch total loss 5.91010046\n",
      "Trained batch 4535 batch loss 5.87188339 epoch total loss 5.91009188\n",
      "Trained batch 4536 batch loss 7.11477613 epoch total loss 5.91035748\n",
      "Trained batch 4537 batch loss 6.43838882 epoch total loss 5.91047335\n",
      "Trained batch 4538 batch loss 6.33967113 epoch total loss 5.91056824\n",
      "Trained batch 4539 batch loss 6.39957809 epoch total loss 5.910676\n",
      "Trained batch 4540 batch loss 4.16623926 epoch total loss 5.91029167\n",
      "Trained batch 4541 batch loss 5.90258646 epoch total loss 5.91029024\n",
      "Trained batch 4542 batch loss 5.96187687 epoch total loss 5.91030121\n",
      "Trained batch 4543 batch loss 5.95447254 epoch total loss 5.91031122\n",
      "Trained batch 4544 batch loss 5.90623522 epoch total loss 5.91031027\n",
      "Trained batch 4545 batch loss 6.1230154 epoch total loss 5.910357\n",
      "Trained batch 4546 batch loss 6.13433456 epoch total loss 5.91040611\n",
      "Trained batch 4547 batch loss 6.11476326 epoch total loss 5.91045141\n",
      "Trained batch 4548 batch loss 6.42061234 epoch total loss 5.91056347\n",
      "Trained batch 4549 batch loss 6.18044472 epoch total loss 5.9106226\n",
      "Trained batch 4550 batch loss 5.10061693 epoch total loss 5.91044474\n",
      "Trained batch 4551 batch loss 6.09839153 epoch total loss 5.91048574\n",
      "Trained batch 4552 batch loss 6.24999142 epoch total loss 5.91056061\n",
      "Trained batch 4553 batch loss 6.77044487 epoch total loss 5.91074896\n",
      "Trained batch 4554 batch loss 6.20966911 epoch total loss 5.91081476\n",
      "Trained batch 4555 batch loss 6.30938053 epoch total loss 5.91090202\n",
      "Trained batch 4556 batch loss 6.6364994 epoch total loss 5.91106129\n",
      "Trained batch 4557 batch loss 6.49179125 epoch total loss 5.9111886\n",
      "Trained batch 4558 batch loss 6.97776079 epoch total loss 5.91142273\n",
      "Trained batch 4559 batch loss 6.7666235 epoch total loss 5.9116106\n",
      "Trained batch 4560 batch loss 6.49598885 epoch total loss 5.91173887\n",
      "Trained batch 4561 batch loss 6.314363 epoch total loss 5.91182709\n",
      "Trained batch 4562 batch loss 6.53038263 epoch total loss 5.91196299\n",
      "Trained batch 4563 batch loss 6.133564 epoch total loss 5.91201115\n",
      "Trained batch 4564 batch loss 5.62948465 epoch total loss 5.91194916\n",
      "Trained batch 4565 batch loss 6.05428219 epoch total loss 5.91198063\n",
      "Trained batch 4566 batch loss 6.07551146 epoch total loss 5.91201639\n",
      "Trained batch 4567 batch loss 5.58978462 epoch total loss 5.91194582\n",
      "Trained batch 4568 batch loss 5.79356718 epoch total loss 5.91192\n",
      "Trained batch 4569 batch loss 6.38709831 epoch total loss 5.91202402\n",
      "Trained batch 4570 batch loss 6.01966 epoch total loss 5.91204739\n",
      "Trained batch 4571 batch loss 5.63365793 epoch total loss 5.91198635\n",
      "Trained batch 4572 batch loss 5.97429037 epoch total loss 5.912\n",
      "Trained batch 4573 batch loss 5.66751575 epoch total loss 5.91194677\n",
      "Trained batch 4574 batch loss 5.3551693 epoch total loss 5.91182518\n",
      "Trained batch 4575 batch loss 4.47321224 epoch total loss 5.91151047\n",
      "Trained batch 4576 batch loss 6.26418686 epoch total loss 5.91158724\n",
      "Trained batch 4577 batch loss 5.65935707 epoch total loss 5.9115324\n",
      "Trained batch 4578 batch loss 6.28907394 epoch total loss 5.91161489\n",
      "Trained batch 4579 batch loss 6.32154417 epoch total loss 5.91170454\n",
      "Trained batch 4580 batch loss 5.92210197 epoch total loss 5.91170692\n",
      "Trained batch 4581 batch loss 6.03209591 epoch total loss 5.91173267\n",
      "Trained batch 4582 batch loss 6.44266796 epoch total loss 5.91184902\n",
      "Trained batch 4583 batch loss 5.83773184 epoch total loss 5.91183281\n",
      "Trained batch 4584 batch loss 6.20606327 epoch total loss 5.91189718\n",
      "Trained batch 4585 batch loss 5.76557922 epoch total loss 5.91186523\n",
      "Trained batch 4586 batch loss 6.3243866 epoch total loss 5.91195536\n",
      "Trained batch 4587 batch loss 6.47516823 epoch total loss 5.9120779\n",
      "Trained batch 4588 batch loss 6.50985813 epoch total loss 5.91220808\n",
      "Trained batch 4589 batch loss 6.15146542 epoch total loss 5.91226053\n",
      "Trained batch 4590 batch loss 6.90321 epoch total loss 5.91247606\n",
      "Trained batch 4591 batch loss 6.72912312 epoch total loss 5.91265392\n",
      "Trained batch 4592 batch loss 6.49371815 epoch total loss 5.91278076\n",
      "Trained batch 4593 batch loss 6.53875589 epoch total loss 5.91291714\n",
      "Trained batch 4594 batch loss 6.56602716 epoch total loss 5.91305923\n",
      "Trained batch 4595 batch loss 6.61405277 epoch total loss 5.91321135\n",
      "Trained batch 4596 batch loss 6.33771896 epoch total loss 5.91330385\n",
      "Trained batch 4597 batch loss 6.400033 epoch total loss 5.91340971\n",
      "Trained batch 4598 batch loss 7.14754868 epoch total loss 5.91367865\n",
      "Trained batch 4599 batch loss 6.50324726 epoch total loss 5.91380692\n",
      "Trained batch 4600 batch loss 6.71696806 epoch total loss 5.91398144\n",
      "Trained batch 4601 batch loss 5.86170578 epoch total loss 5.91397\n",
      "Trained batch 4602 batch loss 6.58773136 epoch total loss 5.91411638\n",
      "Trained batch 4603 batch loss 6.03208542 epoch total loss 5.91414165\n",
      "Trained batch 4604 batch loss 6.2170372 epoch total loss 5.91420746\n",
      "Trained batch 4605 batch loss 5.36094809 epoch total loss 5.9140873\n",
      "Trained batch 4606 batch loss 5.82448864 epoch total loss 5.91406822\n",
      "Trained batch 4607 batch loss 5.34822607 epoch total loss 5.9139452\n",
      "Trained batch 4608 batch loss 6.67521191 epoch total loss 5.91411\n",
      "Trained batch 4609 batch loss 5.43069363 epoch total loss 5.91400576\n",
      "Trained batch 4610 batch loss 5.59820795 epoch total loss 5.91393709\n",
      "Trained batch 4611 batch loss 5.56222534 epoch total loss 5.9138608\n",
      "Trained batch 4612 batch loss 5.68771458 epoch total loss 5.91381168\n",
      "Trained batch 4613 batch loss 6.16885662 epoch total loss 5.913867\n",
      "Trained batch 4614 batch loss 6.248312 epoch total loss 5.91393948\n",
      "Trained batch 4615 batch loss 5.8182354 epoch total loss 5.9139185\n",
      "Trained batch 4616 batch loss 6.0524292 epoch total loss 5.91394854\n",
      "Trained batch 4617 batch loss 5.65339708 epoch total loss 5.91389227\n",
      "Trained batch 4618 batch loss 5.50700474 epoch total loss 5.91380453\n",
      "Trained batch 4619 batch loss 6.38251352 epoch total loss 5.9139061\n",
      "Trained batch 4620 batch loss 6.26422834 epoch total loss 5.91398191\n",
      "Trained batch 4621 batch loss 6.19417095 epoch total loss 5.914042\n",
      "Trained batch 4622 batch loss 6.49167871 epoch total loss 5.9141674\n",
      "Trained batch 4623 batch loss 6.45560837 epoch total loss 5.91428423\n",
      "Trained batch 4624 batch loss 6.56758 epoch total loss 5.91442585\n",
      "Trained batch 4625 batch loss 6.21480131 epoch total loss 5.9144907\n",
      "Trained batch 4626 batch loss 6.51446724 epoch total loss 5.9146204\n",
      "Trained batch 4627 batch loss 6.42023563 epoch total loss 5.9147296\n",
      "Trained batch 4628 batch loss 6.42766333 epoch total loss 5.91484\n",
      "Trained batch 4629 batch loss 6.6963892 epoch total loss 5.9150095\n",
      "Trained batch 4630 batch loss 6.29189491 epoch total loss 5.91509056\n",
      "Trained batch 4631 batch loss 6.22641516 epoch total loss 5.91515779\n",
      "Trained batch 4632 batch loss 5.67039299 epoch total loss 5.91510487\n",
      "Trained batch 4633 batch loss 6.11778259 epoch total loss 5.91514826\n",
      "Trained batch 4634 batch loss 5.10702133 epoch total loss 5.91497421\n",
      "Trained batch 4635 batch loss 4.91995239 epoch total loss 5.91475964\n",
      "Trained batch 4636 batch loss 4.15409231 epoch total loss 5.9143796\n",
      "Trained batch 4637 batch loss 6.1655364 epoch total loss 5.91443396\n",
      "Trained batch 4638 batch loss 5.74531937 epoch total loss 5.91439772\n",
      "Trained batch 4639 batch loss 6.15741062 epoch total loss 5.91445\n",
      "Trained batch 4640 batch loss 6.03535318 epoch total loss 5.91447639\n",
      "Trained batch 4641 batch loss 6.68839 epoch total loss 5.91464281\n",
      "Trained batch 4642 batch loss 6.23215914 epoch total loss 5.91471148\n",
      "Trained batch 4643 batch loss 6.05820465 epoch total loss 5.91474247\n",
      "Trained batch 4644 batch loss 6.58385944 epoch total loss 5.91488647\n",
      "Trained batch 4645 batch loss 6.18173027 epoch total loss 5.9149437\n",
      "Trained batch 4646 batch loss 6.04953527 epoch total loss 5.91497278\n",
      "Trained batch 4647 batch loss 6.51740932 epoch total loss 5.91510248\n",
      "Trained batch 4648 batch loss 6.25291681 epoch total loss 5.91517496\n",
      "Trained batch 4649 batch loss 5.60644054 epoch total loss 5.9151082\n",
      "Trained batch 4650 batch loss 5.5302372 epoch total loss 5.91502523\n",
      "Trained batch 4651 batch loss 5.91608572 epoch total loss 5.91502523\n",
      "Trained batch 4652 batch loss 5.68682384 epoch total loss 5.9149766\n",
      "Trained batch 4653 batch loss 5.43522644 epoch total loss 5.9148736\n",
      "Trained batch 4654 batch loss 6.11094 epoch total loss 5.91491556\n",
      "Trained batch 4655 batch loss 6.1577 epoch total loss 5.91496801\n",
      "Trained batch 4656 batch loss 6.59527397 epoch total loss 5.91511393\n",
      "Trained batch 4657 batch loss 6.93068027 epoch total loss 5.91533232\n",
      "Trained batch 4658 batch loss 6.92940903 epoch total loss 5.91555\n",
      "Trained batch 4659 batch loss 5.87892 epoch total loss 5.91554213\n",
      "Trained batch 4660 batch loss 5.81579065 epoch total loss 5.91552114\n",
      "Trained batch 4661 batch loss 5.9634943 epoch total loss 5.91553116\n",
      "Trained batch 4662 batch loss 6.12917 epoch total loss 5.91557693\n",
      "Trained batch 4663 batch loss 5.87536049 epoch total loss 5.91556835\n",
      "Trained batch 4664 batch loss 6.19442463 epoch total loss 5.91562843\n",
      "Trained batch 4665 batch loss 6.97206402 epoch total loss 5.91585493\n",
      "Trained batch 4666 batch loss 5.9139452 epoch total loss 5.91585445\n",
      "Trained batch 4667 batch loss 6.23267365 epoch total loss 5.91592216\n",
      "Trained batch 4668 batch loss 6.66293192 epoch total loss 5.91608238\n",
      "Trained batch 4669 batch loss 4.61858654 epoch total loss 5.91580439\n",
      "Trained batch 4670 batch loss 4.4890542 epoch total loss 5.91549873\n",
      "Trained batch 4671 batch loss 5.28442 epoch total loss 5.91536379\n",
      "Trained batch 4672 batch loss 4.80088615 epoch total loss 5.91512537\n",
      "Trained batch 4673 batch loss 4.93991756 epoch total loss 5.91491652\n",
      "Trained batch 4674 batch loss 4.66006184 epoch total loss 5.91464806\n",
      "Trained batch 4675 batch loss 4.80918121 epoch total loss 5.91441154\n",
      "Trained batch 4676 batch loss 4.7248106 epoch total loss 5.91415691\n",
      "Trained batch 4677 batch loss 4.58314133 epoch total loss 5.91387272\n",
      "Trained batch 4678 batch loss 5.42182922 epoch total loss 5.91376734\n",
      "Trained batch 4679 batch loss 5.32404709 epoch total loss 5.91364145\n",
      "Trained batch 4680 batch loss 5.1063776 epoch total loss 5.91346884\n",
      "Trained batch 4681 batch loss 5.39139318 epoch total loss 5.91335678\n",
      "Trained batch 4682 batch loss 4.61666775 epoch total loss 5.91308\n",
      "Trained batch 4683 batch loss 4.47641516 epoch total loss 5.91277313\n",
      "Trained batch 4684 batch loss 4.59979963 epoch total loss 5.91249323\n",
      "Trained batch 4685 batch loss 5.92874432 epoch total loss 5.91249657\n",
      "Trained batch 4686 batch loss 4.69337082 epoch total loss 5.91223669\n",
      "Trained batch 4687 batch loss 5.82054567 epoch total loss 5.91221666\n",
      "Trained batch 4688 batch loss 4.96698 epoch total loss 5.91201544\n",
      "Trained batch 4689 batch loss 4.36088371 epoch total loss 5.91168451\n",
      "Trained batch 4690 batch loss 5.61962223 epoch total loss 5.91162205\n",
      "Trained batch 4691 batch loss 6.70017958 epoch total loss 5.91179\n",
      "Trained batch 4692 batch loss 6.30501366 epoch total loss 5.91187382\n",
      "Trained batch 4693 batch loss 6.51093864 epoch total loss 5.91200161\n",
      "Trained batch 4694 batch loss 6.69629574 epoch total loss 5.91216898\n",
      "Trained batch 4695 batch loss 6.54898596 epoch total loss 5.9123044\n",
      "Trained batch 4696 batch loss 5.82009411 epoch total loss 5.91228485\n",
      "Trained batch 4697 batch loss 6.53332329 epoch total loss 5.91241693\n",
      "Trained batch 4698 batch loss 6.34531307 epoch total loss 5.91250944\n",
      "Trained batch 4699 batch loss 6.47401714 epoch total loss 5.91262913\n",
      "Trained batch 4700 batch loss 4.9127717 epoch total loss 5.91241598\n",
      "Trained batch 4701 batch loss 4.05397701 epoch total loss 5.91202068\n",
      "Trained batch 4702 batch loss 5.06792164 epoch total loss 5.91184139\n",
      "Trained batch 4703 batch loss 3.75009632 epoch total loss 5.91138172\n",
      "Trained batch 4704 batch loss 5.41131067 epoch total loss 5.91127586\n",
      "Trained batch 4705 batch loss 5.27233601 epoch total loss 5.91113949\n",
      "Trained batch 4706 batch loss 6.36372662 epoch total loss 5.91123581\n",
      "Trained batch 4707 batch loss 6.17704773 epoch total loss 5.91129255\n",
      "Trained batch 4708 batch loss 6.17016268 epoch total loss 5.91134739\n",
      "Trained batch 4709 batch loss 5.85128 epoch total loss 5.91133451\n",
      "Trained batch 4710 batch loss 6.0077424 epoch total loss 5.91135502\n",
      "Trained batch 4711 batch loss 6.144907 epoch total loss 5.91140461\n",
      "Trained batch 4712 batch loss 5.88807774 epoch total loss 5.9114\n",
      "Trained batch 4713 batch loss 6.03982735 epoch total loss 5.91142702\n",
      "Trained batch 4714 batch loss 5.9574275 epoch total loss 5.91143656\n",
      "Trained batch 4715 batch loss 5.84379244 epoch total loss 5.91142225\n",
      "Trained batch 4716 batch loss 5.90026617 epoch total loss 5.91142\n",
      "Trained batch 4717 batch loss 6.15604973 epoch total loss 5.91147184\n",
      "Trained batch 4718 batch loss 5.30637932 epoch total loss 5.91134357\n",
      "Trained batch 4719 batch loss 5.74066591 epoch total loss 5.91130733\n",
      "Trained batch 4720 batch loss 6.65097237 epoch total loss 5.91146374\n",
      "Trained batch 4721 batch loss 6.38895702 epoch total loss 5.91156483\n",
      "Trained batch 4722 batch loss 6.08952236 epoch total loss 5.9116025\n",
      "Trained batch 4723 batch loss 6.07576084 epoch total loss 5.91163731\n",
      "Trained batch 4724 batch loss 5.47937965 epoch total loss 5.91154575\n",
      "Trained batch 4725 batch loss 4.88337898 epoch total loss 5.91132832\n",
      "Trained batch 4726 batch loss 4.63500595 epoch total loss 5.91105795\n",
      "Trained batch 4727 batch loss 4.93458462 epoch total loss 5.91085148\n",
      "Trained batch 4728 batch loss 4.53165627 epoch total loss 5.91055965\n",
      "Trained batch 4729 batch loss 4.41300201 epoch total loss 5.91024303\n",
      "Trained batch 4730 batch loss 4.34263515 epoch total loss 5.90991116\n",
      "Trained batch 4731 batch loss 4.85519 epoch total loss 5.90968847\n",
      "Trained batch 4732 batch loss 4.16746521 epoch total loss 5.90932035\n",
      "Trained batch 4733 batch loss 4.73529243 epoch total loss 5.9090724\n",
      "Trained batch 4734 batch loss 5.45605135 epoch total loss 5.90897608\n",
      "Trained batch 4735 batch loss 5.96709681 epoch total loss 5.90898848\n",
      "Trained batch 4736 batch loss 6.88110733 epoch total loss 5.90919352\n",
      "Trained batch 4737 batch loss 7.07605934 epoch total loss 5.90944\n",
      "Trained batch 4738 batch loss 6.57956314 epoch total loss 5.90958166\n",
      "Trained batch 4739 batch loss 6.63258362 epoch total loss 5.90973425\n",
      "Trained batch 4740 batch loss 5.50738907 epoch total loss 5.90964937\n",
      "Trained batch 4741 batch loss 5.34282732 epoch total loss 5.90953\n",
      "Trained batch 4742 batch loss 6.24800682 epoch total loss 5.90960169\n",
      "Trained batch 4743 batch loss 6.63847733 epoch total loss 5.90975523\n",
      "Trained batch 4744 batch loss 6.0954237 epoch total loss 5.90979433\n",
      "Trained batch 4745 batch loss 6.4064579 epoch total loss 5.90989876\n",
      "Trained batch 4746 batch loss 6.37075186 epoch total loss 5.90999603\n",
      "Trained batch 4747 batch loss 5.84375095 epoch total loss 5.9099822\n",
      "Trained batch 4748 batch loss 6.47194386 epoch total loss 5.91010094\n",
      "Trained batch 4749 batch loss 5.52714825 epoch total loss 5.91002035\n",
      "Trained batch 4750 batch loss 5.79088688 epoch total loss 5.90999508\n",
      "Trained batch 4751 batch loss 5.7577343 epoch total loss 5.90996313\n",
      "Trained batch 4752 batch loss 5.9881506 epoch total loss 5.90997934\n",
      "Trained batch 4753 batch loss 6.00958729 epoch total loss 5.91000032\n",
      "Trained batch 4754 batch loss 5.85480976 epoch total loss 5.90998888\n",
      "Trained batch 4755 batch loss 6.77430868 epoch total loss 5.91017056\n",
      "Trained batch 4756 batch loss 6.08536959 epoch total loss 5.91020775\n",
      "Trained batch 4757 batch loss 5.63267088 epoch total loss 5.9101491\n",
      "Trained batch 4758 batch loss 5.40898705 epoch total loss 5.91004372\n",
      "Trained batch 4759 batch loss 5.82244396 epoch total loss 5.91002512\n",
      "Trained batch 4760 batch loss 5.42216778 epoch total loss 5.9099226\n",
      "Trained batch 4761 batch loss 5.75036716 epoch total loss 5.90988922\n",
      "Trained batch 4762 batch loss 6.07775402 epoch total loss 5.90992451\n",
      "Trained batch 4763 batch loss 5.68561 epoch total loss 5.9098773\n",
      "Trained batch 4764 batch loss 5.31285334 epoch total loss 5.90975189\n",
      "Trained batch 4765 batch loss 5.30792427 epoch total loss 5.90962601\n",
      "Trained batch 4766 batch loss 5.69114971 epoch total loss 5.90958\n",
      "Trained batch 4767 batch loss 5.24954081 epoch total loss 5.90944147\n",
      "Trained batch 4768 batch loss 5.37081099 epoch total loss 5.90932894\n",
      "Trained batch 4769 batch loss 5.68525028 epoch total loss 5.90928173\n",
      "Trained batch 4770 batch loss 5.59494591 epoch total loss 5.90921593\n",
      "Trained batch 4771 batch loss 5.59618378 epoch total loss 5.9091506\n",
      "Trained batch 4772 batch loss 5.18929386 epoch total loss 5.90899944\n",
      "Trained batch 4773 batch loss 5.25006914 epoch total loss 5.90886164\n",
      "Trained batch 4774 batch loss 5.96123695 epoch total loss 5.9088726\n",
      "Trained batch 4775 batch loss 5.31549 epoch total loss 5.90874815\n",
      "Trained batch 4776 batch loss 5.71529818 epoch total loss 5.90870762\n",
      "Trained batch 4777 batch loss 5.814991 epoch total loss 5.90868807\n",
      "Trained batch 4778 batch loss 5.67879057 epoch total loss 5.90864\n",
      "Trained batch 4779 batch loss 5.71867657 epoch total loss 5.90860033\n",
      "Trained batch 4780 batch loss 5.93172932 epoch total loss 5.9086051\n",
      "Trained batch 4781 batch loss 5.56815243 epoch total loss 5.90853405\n",
      "Trained batch 4782 batch loss 6.05536747 epoch total loss 5.90856457\n",
      "Trained batch 4783 batch loss 5.99819851 epoch total loss 5.90858316\n",
      "Trained batch 4784 batch loss 5.72087193 epoch total loss 5.90854406\n",
      "Trained batch 4785 batch loss 5.44034147 epoch total loss 5.90844584\n",
      "Trained batch 4786 batch loss 5.28055239 epoch total loss 5.9083147\n",
      "Trained batch 4787 batch loss 5.16228151 epoch total loss 5.90815926\n",
      "Trained batch 4788 batch loss 5.23469114 epoch total loss 5.90801811\n",
      "Trained batch 4789 batch loss 5.54066467 epoch total loss 5.90794182\n",
      "Trained batch 4790 batch loss 5.40459633 epoch total loss 5.90783644\n",
      "Trained batch 4791 batch loss 5.51087379 epoch total loss 5.90775394\n",
      "Trained batch 4792 batch loss 5.20288277 epoch total loss 5.9076066\n",
      "Trained batch 4793 batch loss 5.46024084 epoch total loss 5.90751362\n",
      "Trained batch 4794 batch loss 5.18770266 epoch total loss 5.90736341\n",
      "Trained batch 4795 batch loss 5.72014856 epoch total loss 5.90732431\n",
      "Trained batch 4796 batch loss 5.87584305 epoch total loss 5.90731764\n",
      "Trained batch 4797 batch loss 5.58282948 epoch total loss 5.90725\n",
      "Trained batch 4798 batch loss 5.46367598 epoch total loss 5.90715742\n",
      "Trained batch 4799 batch loss 5.06483078 epoch total loss 5.90698195\n",
      "Trained batch 4800 batch loss 5.90076923 epoch total loss 5.90698051\n",
      "Trained batch 4801 batch loss 5.88352108 epoch total loss 5.90697527\n",
      "Trained batch 4802 batch loss 5.48631144 epoch total loss 5.90688753\n",
      "Trained batch 4803 batch loss 4.22173595 epoch total loss 5.90653706\n",
      "Trained batch 4804 batch loss 4.69477034 epoch total loss 5.90628481\n",
      "Trained batch 4805 batch loss 4.42420673 epoch total loss 5.9059763\n",
      "Trained batch 4806 batch loss 6.05430698 epoch total loss 5.90600729\n",
      "Trained batch 4807 batch loss 4.98479366 epoch total loss 5.9058156\n",
      "Trained batch 4808 batch loss 6.34475422 epoch total loss 5.90590715\n",
      "Trained batch 4809 batch loss 5.93473768 epoch total loss 5.90591335\n",
      "Trained batch 4810 batch loss 5.54067039 epoch total loss 5.90583754\n",
      "Trained batch 4811 batch loss 5.62469673 epoch total loss 5.90577888\n",
      "Trained batch 4812 batch loss 5.2621851 epoch total loss 5.90564537\n",
      "Trained batch 4813 batch loss 5.75859976 epoch total loss 5.90561438\n",
      "Trained batch 4814 batch loss 5.43281174 epoch total loss 5.90551662\n",
      "Trained batch 4815 batch loss 5.41466379 epoch total loss 5.90541458\n",
      "Trained batch 4816 batch loss 5.92869091 epoch total loss 5.90541887\n",
      "Trained batch 4817 batch loss 5.32507277 epoch total loss 5.90529823\n",
      "Trained batch 4818 batch loss 5.65310764 epoch total loss 5.90524578\n",
      "Trained batch 4819 batch loss 5.64290571 epoch total loss 5.90519142\n",
      "Trained batch 4820 batch loss 6.56900597 epoch total loss 5.90532875\n",
      "Trained batch 4821 batch loss 5.75584698 epoch total loss 5.90529776\n",
      "Trained batch 4822 batch loss 5.15647268 epoch total loss 5.90514278\n",
      "Trained batch 4823 batch loss 5.09965515 epoch total loss 5.90497541\n",
      "Trained batch 4824 batch loss 6.11826754 epoch total loss 5.90501976\n",
      "Trained batch 4825 batch loss 5.83508682 epoch total loss 5.90500546\n",
      "Trained batch 4826 batch loss 5.70059824 epoch total loss 5.90496349\n",
      "Trained batch 4827 batch loss 5.78586435 epoch total loss 5.9049387\n",
      "Trained batch 4828 batch loss 6.04691601 epoch total loss 5.90496778\n",
      "Trained batch 4829 batch loss 5.52242279 epoch total loss 5.90488863\n",
      "Trained batch 4830 batch loss 6.60313368 epoch total loss 5.90503311\n",
      "Trained batch 4831 batch loss 6.33481693 epoch total loss 5.9051218\n",
      "Trained batch 4832 batch loss 5.90615463 epoch total loss 5.90512228\n",
      "Trained batch 4833 batch loss 5.19977665 epoch total loss 5.90497637\n",
      "Trained batch 4834 batch loss 5.15512848 epoch total loss 5.90482092\n",
      "Trained batch 4835 batch loss 6.64800739 epoch total loss 5.90497494\n",
      "Trained batch 4836 batch loss 6.06864166 epoch total loss 5.90500832\n",
      "Trained batch 4837 batch loss 6.29277706 epoch total loss 5.9050889\n",
      "Trained batch 4838 batch loss 6.09767246 epoch total loss 5.90512848\n",
      "Trained batch 4839 batch loss 5.80109358 epoch total loss 5.90510702\n",
      "Trained batch 4840 batch loss 6.22122717 epoch total loss 5.90517235\n",
      "Trained batch 4841 batch loss 5.76199961 epoch total loss 5.90514231\n",
      "Trained batch 4842 batch loss 5.36891842 epoch total loss 5.90503168\n",
      "Trained batch 4843 batch loss 5.23040676 epoch total loss 5.90489244\n",
      "Trained batch 4844 batch loss 6.48825407 epoch total loss 5.90501308\n",
      "Trained batch 4845 batch loss 5.58032608 epoch total loss 5.90494585\n",
      "Trained batch 4846 batch loss 5.95952368 epoch total loss 5.90495682\n",
      "Trained batch 4847 batch loss 5.08626318 epoch total loss 5.90478802\n",
      "Trained batch 4848 batch loss 5.442276 epoch total loss 5.90469265\n",
      "Trained batch 4849 batch loss 5.2796464 epoch total loss 5.90456343\n",
      "Trained batch 4850 batch loss 6.1443367 epoch total loss 5.90461302\n",
      "Trained batch 4851 batch loss 6.12408638 epoch total loss 5.90465832\n",
      "Trained batch 4852 batch loss 5.35173416 epoch total loss 5.90454435\n",
      "Trained batch 4853 batch loss 6.23317242 epoch total loss 5.90461206\n",
      "Trained batch 4854 batch loss 6.18531084 epoch total loss 5.90467\n",
      "Trained batch 4855 batch loss 6.17121792 epoch total loss 5.90472507\n",
      "Trained batch 4856 batch loss 5.26451826 epoch total loss 5.90459299\n",
      "Trained batch 4857 batch loss 4.51623678 epoch total loss 5.90430689\n",
      "Trained batch 4858 batch loss 4.9289732 epoch total loss 5.90410614\n",
      "Trained batch 4859 batch loss 5.71836472 epoch total loss 5.90406799\n",
      "Trained batch 4860 batch loss 5.83842421 epoch total loss 5.90405464\n",
      "Trained batch 4861 batch loss 6.06530428 epoch total loss 5.90408754\n",
      "Trained batch 4862 batch loss 5.05767441 epoch total loss 5.9039135\n",
      "Trained batch 4863 batch loss 6.55062 epoch total loss 5.90404654\n",
      "Trained batch 4864 batch loss 6.11962795 epoch total loss 5.90409088\n",
      "Trained batch 4865 batch loss 5.70519638 epoch total loss 5.90405\n",
      "Trained batch 4866 batch loss 6.22219467 epoch total loss 5.90411568\n",
      "Trained batch 4867 batch loss 6.06252718 epoch total loss 5.9041481\n",
      "Trained batch 4868 batch loss 6.34047413 epoch total loss 5.90423727\n",
      "Trained batch 4869 batch loss 6.90084648 epoch total loss 5.90444231\n",
      "Trained batch 4870 batch loss 6.48798466 epoch total loss 5.904562\n",
      "Trained batch 4871 batch loss 6.22730446 epoch total loss 5.90462828\n",
      "Trained batch 4872 batch loss 6.04266834 epoch total loss 5.90465641\n",
      "Trained batch 4873 batch loss 5.79122162 epoch total loss 5.90463305\n",
      "Trained batch 4874 batch loss 6.37344408 epoch total loss 5.90472937\n",
      "Trained batch 4875 batch loss 6.33794641 epoch total loss 5.90481806\n",
      "Trained batch 4876 batch loss 6.34448528 epoch total loss 5.90490818\n",
      "Trained batch 4877 batch loss 6.54362869 epoch total loss 5.90503883\n",
      "Trained batch 4878 batch loss 6.19768238 epoch total loss 5.90509892\n",
      "Trained batch 4879 batch loss 5.87158394 epoch total loss 5.90509176\n",
      "Trained batch 4880 batch loss 6.1389122 epoch total loss 5.90514\n",
      "Trained batch 4881 batch loss 6.63081551 epoch total loss 5.90528822\n",
      "Trained batch 4882 batch loss 6.0673008 epoch total loss 5.9053216\n",
      "Trained batch 4883 batch loss 5.90460396 epoch total loss 5.90532112\n",
      "Trained batch 4884 batch loss 5.98112488 epoch total loss 5.90533686\n",
      "Trained batch 4885 batch loss 4.89999866 epoch total loss 5.90513086\n",
      "Trained batch 4886 batch loss 6.06935024 epoch total loss 5.90516472\n",
      "Trained batch 4887 batch loss 6.34292889 epoch total loss 5.90525436\n",
      "Trained batch 4888 batch loss 6.25431919 epoch total loss 5.90532589\n",
      "Trained batch 4889 batch loss 5.73266 epoch total loss 5.9052906\n",
      "Trained batch 4890 batch loss 5.57184696 epoch total loss 5.90522242\n",
      "Trained batch 4891 batch loss 5.7932353 epoch total loss 5.90519953\n",
      "Trained batch 4892 batch loss 6.22669125 epoch total loss 5.90526485\n",
      "Trained batch 4893 batch loss 5.89415264 epoch total loss 5.90526295\n",
      "Trained batch 4894 batch loss 7.55521584 epoch total loss 5.9056\n",
      "Trained batch 4895 batch loss 7.17562819 epoch total loss 5.90585947\n",
      "Trained batch 4896 batch loss 7.43382072 epoch total loss 5.90617132\n",
      "Trained batch 4897 batch loss 7.5413866 epoch total loss 5.90650511\n",
      "Trained batch 4898 batch loss 7.22404909 epoch total loss 5.90677452\n",
      "Trained batch 4899 batch loss 7.16955423 epoch total loss 5.90703201\n",
      "Trained batch 4900 batch loss 7.01858377 epoch total loss 5.90725946\n",
      "Trained batch 4901 batch loss 6.7382946 epoch total loss 5.90742874\n",
      "Trained batch 4902 batch loss 6.21326828 epoch total loss 5.90749121\n",
      "Trained batch 4903 batch loss 6.53768349 epoch total loss 5.90761948\n",
      "Trained batch 4904 batch loss 5.87595558 epoch total loss 5.9076128\n",
      "Trained batch 4905 batch loss 5.7051487 epoch total loss 5.90757179\n",
      "Trained batch 4906 batch loss 6.70847178 epoch total loss 5.90773487\n",
      "Trained batch 4907 batch loss 6.30111027 epoch total loss 5.90781498\n",
      "Trained batch 4908 batch loss 6.91656542 epoch total loss 5.9080205\n",
      "Trained batch 4909 batch loss 5.82564163 epoch total loss 5.90800381\n",
      "Trained batch 4910 batch loss 6.04993725 epoch total loss 5.90803289\n",
      "Trained batch 4911 batch loss 5.93435192 epoch total loss 5.90803814\n",
      "Trained batch 4912 batch loss 6.12334299 epoch total loss 5.90808201\n",
      "Trained batch 4913 batch loss 3.74091935 epoch total loss 5.90764046\n",
      "Trained batch 4914 batch loss 5.35347 epoch total loss 5.90752792\n",
      "Trained batch 4915 batch loss 5.47809362 epoch total loss 5.90744066\n",
      "Trained batch 4916 batch loss 4.9909668 epoch total loss 5.90725374\n",
      "Trained batch 4917 batch loss 6.2868886 epoch total loss 5.90733147\n",
      "Trained batch 4918 batch loss 5.77164841 epoch total loss 5.90730381\n",
      "Trained batch 4919 batch loss 5.4892416 epoch total loss 5.90721846\n",
      "Trained batch 4920 batch loss 5.95404196 epoch total loss 5.90722752\n",
      "Trained batch 4921 batch loss 5.76036263 epoch total loss 5.90719795\n",
      "Trained batch 4922 batch loss 6.17088032 epoch total loss 5.90725136\n",
      "Trained batch 4923 batch loss 5.58441305 epoch total loss 5.90718555\n",
      "Trained batch 4924 batch loss 5.51555347 epoch total loss 5.90710592\n",
      "Trained batch 4925 batch loss 5.09417057 epoch total loss 5.90694094\n",
      "Trained batch 4926 batch loss 5.5515995 epoch total loss 5.90686846\n",
      "Trained batch 4927 batch loss 5.61591434 epoch total loss 5.90680933\n",
      "Trained batch 4928 batch loss 5.84228706 epoch total loss 5.90679598\n",
      "Trained batch 4929 batch loss 6.16242504 epoch total loss 5.90684795\n",
      "Trained batch 4930 batch loss 3.96709466 epoch total loss 5.90645456\n",
      "Trained batch 4931 batch loss 4.07211065 epoch total loss 5.90608263\n",
      "Trained batch 4932 batch loss 4.2284956 epoch total loss 5.90574217\n",
      "Trained batch 4933 batch loss 4.25331306 epoch total loss 5.90540743\n",
      "Trained batch 4934 batch loss 4.46257114 epoch total loss 5.90511513\n",
      "Trained batch 4935 batch loss 4.44125223 epoch total loss 5.90481853\n",
      "Trained batch 4936 batch loss 4.34324598 epoch total loss 5.90450239\n",
      "Trained batch 4937 batch loss 4.58849096 epoch total loss 5.90423536\n",
      "Trained batch 4938 batch loss 4.35681152 epoch total loss 5.90392208\n",
      "Trained batch 4939 batch loss 4.58448792 epoch total loss 5.90365505\n",
      "Trained batch 4940 batch loss 4.31576061 epoch total loss 5.90333366\n",
      "Trained batch 4941 batch loss 4.62371111 epoch total loss 5.90307474\n",
      "Trained batch 4942 batch loss 6.07175732 epoch total loss 5.90310907\n",
      "Trained batch 4943 batch loss 6.15972614 epoch total loss 5.90316105\n",
      "Trained batch 4944 batch loss 5.61562824 epoch total loss 5.90310287\n",
      "Trained batch 4945 batch loss 6.21552801 epoch total loss 5.90316582\n",
      "Trained batch 4946 batch loss 5.7020936 epoch total loss 5.90312481\n",
      "Trained batch 4947 batch loss 5.29348 epoch total loss 5.90300131\n",
      "Trained batch 4948 batch loss 5.56781197 epoch total loss 5.90293407\n",
      "Trained batch 4949 batch loss 5.46055698 epoch total loss 5.90284443\n",
      "Trained batch 4950 batch loss 6.1598773 epoch total loss 5.9028964\n",
      "Trained batch 4951 batch loss 6.12329 epoch total loss 5.90294123\n",
      "Trained batch 4952 batch loss 5.99266815 epoch total loss 5.90295887\n",
      "Trained batch 4953 batch loss 6.20965862 epoch total loss 5.90302086\n",
      "Trained batch 4954 batch loss 4.95914364 epoch total loss 5.90283\n",
      "Trained batch 4955 batch loss 5.29433107 epoch total loss 5.90270758\n",
      "Trained batch 4956 batch loss 4.8794651 epoch total loss 5.90250111\n",
      "Trained batch 4957 batch loss 4.91283512 epoch total loss 5.90230131\n",
      "Trained batch 4958 batch loss 3.58762503 epoch total loss 5.90183449\n",
      "Trained batch 4959 batch loss 3.70018721 epoch total loss 5.90139\n",
      "Trained batch 4960 batch loss 3.80891705 epoch total loss 5.90096807\n",
      "Trained batch 4961 batch loss 4.31873274 epoch total loss 5.90064907\n",
      "Trained batch 4962 batch loss 4.38862514 epoch total loss 5.90034437\n",
      "Trained batch 4963 batch loss 4.06769848 epoch total loss 5.8999753\n",
      "Trained batch 4964 batch loss 4.24546242 epoch total loss 5.89964247\n",
      "Trained batch 4965 batch loss 4.64362049 epoch total loss 5.89938927\n",
      "Trained batch 4966 batch loss 4.38500071 epoch total loss 5.89908457\n",
      "Trained batch 4967 batch loss 4.59504414 epoch total loss 5.89882231\n",
      "Trained batch 4968 batch loss 4.58462858 epoch total loss 5.89855719\n",
      "Trained batch 4969 batch loss 4.444592 epoch total loss 5.89826488\n",
      "Trained batch 4970 batch loss 4.52976131 epoch total loss 5.89798927\n",
      "Trained batch 4971 batch loss 4.57047462 epoch total loss 5.89772224\n",
      "Trained batch 4972 batch loss 4.39038372 epoch total loss 5.89741945\n",
      "Trained batch 4973 batch loss 4.43623066 epoch total loss 5.89712524\n",
      "Trained batch 4974 batch loss 4.34138393 epoch total loss 5.89681244\n",
      "Trained batch 4975 batch loss 4.4630394 epoch total loss 5.89652443\n",
      "Trained batch 4976 batch loss 4.62282181 epoch total loss 5.89626837\n",
      "Trained batch 4977 batch loss 4.43902636 epoch total loss 5.89597559\n",
      "Trained batch 4978 batch loss 4.79770899 epoch total loss 5.89575481\n",
      "Trained batch 4979 batch loss 4.73223495 epoch total loss 5.89552116\n",
      "Trained batch 4980 batch loss 5.06701946 epoch total loss 5.89535475\n",
      "Trained batch 4981 batch loss 4.42313147 epoch total loss 5.89505959\n",
      "Trained batch 4982 batch loss 5.28653049 epoch total loss 5.89493752\n",
      "Trained batch 4983 batch loss 6.1942606 epoch total loss 5.89499712\n",
      "Trained batch 4984 batch loss 4.56811047 epoch total loss 5.89473104\n",
      "Trained batch 4985 batch loss 5.66690636 epoch total loss 5.89468527\n",
      "Trained batch 4986 batch loss 6.34273481 epoch total loss 5.89477491\n",
      "Trained batch 4987 batch loss 5.14072323 epoch total loss 5.89462376\n",
      "Trained batch 4988 batch loss 4.52578354 epoch total loss 5.8943491\n",
      "Trained batch 4989 batch loss 4.84547901 epoch total loss 5.89413881\n",
      "Trained batch 4990 batch loss 5.99125 epoch total loss 5.89415884\n",
      "Trained batch 4991 batch loss 5.23616648 epoch total loss 5.89402676\n",
      "Trained batch 4992 batch loss 5.60504436 epoch total loss 5.89396906\n",
      "Trained batch 4993 batch loss 6.23825169 epoch total loss 5.8940382\n",
      "Trained batch 4994 batch loss 5.87662697 epoch total loss 5.89403439\n",
      "Trained batch 4995 batch loss 5.8466692 epoch total loss 5.89402485\n",
      "Trained batch 4996 batch loss 5.69488525 epoch total loss 5.89398527\n",
      "Trained batch 4997 batch loss 5.70746326 epoch total loss 5.8939476\n",
      "Trained batch 4998 batch loss 5.58419466 epoch total loss 5.89388561\n",
      "Trained batch 4999 batch loss 5.69732666 epoch total loss 5.89384651\n",
      "Trained batch 5000 batch loss 5.24228621 epoch total loss 5.89371586\n",
      "Trained batch 5001 batch loss 5.36139488 epoch total loss 5.89360952\n",
      "Trained batch 5002 batch loss 6.28890657 epoch total loss 5.89368868\n",
      "Trained batch 5003 batch loss 6.47352457 epoch total loss 5.89380455\n",
      "Trained batch 5004 batch loss 6.38157368 epoch total loss 5.89390182\n",
      "Trained batch 5005 batch loss 6.30603933 epoch total loss 5.89398432\n",
      "Trained batch 5006 batch loss 6.44040298 epoch total loss 5.89409304\n",
      "Trained batch 5007 batch loss 6.23329163 epoch total loss 5.89416075\n",
      "Trained batch 5008 batch loss 5.73479652 epoch total loss 5.8941288\n",
      "Trained batch 5009 batch loss 6.45585489 epoch total loss 5.89424086\n",
      "Trained batch 5010 batch loss 5.99741364 epoch total loss 5.89426136\n",
      "Trained batch 5011 batch loss 6.58661556 epoch total loss 5.89439964\n",
      "Trained batch 5012 batch loss 7.05456495 epoch total loss 5.89463091\n",
      "Trained batch 5013 batch loss 5.90363503 epoch total loss 5.89463282\n",
      "Trained batch 5014 batch loss 6.74122238 epoch total loss 5.89480209\n",
      "Trained batch 5015 batch loss 6.0646944 epoch total loss 5.89483595\n",
      "Trained batch 5016 batch loss 6.78928471 epoch total loss 5.89501429\n",
      "Trained batch 5017 batch loss 6.78508472 epoch total loss 5.89519167\n",
      "Trained batch 5018 batch loss 6.56246662 epoch total loss 5.89532471\n",
      "Trained batch 5019 batch loss 7.33002806 epoch total loss 5.89561033\n",
      "Trained batch 5020 batch loss 6.14385796 epoch total loss 5.89566\n",
      "Trained batch 5021 batch loss 6.52729559 epoch total loss 5.89578581\n",
      "Trained batch 5022 batch loss 6.25817 epoch total loss 5.89585781\n",
      "Trained batch 5023 batch loss 6.49212551 epoch total loss 5.89597654\n",
      "Trained batch 5024 batch loss 6.52392244 epoch total loss 5.89610147\n",
      "Trained batch 5025 batch loss 6.57719421 epoch total loss 5.89623737\n",
      "Trained batch 5026 batch loss 5.51574564 epoch total loss 5.89616156\n",
      "Trained batch 5027 batch loss 6.06599903 epoch total loss 5.89619541\n",
      "Trained batch 5028 batch loss 6.16756344 epoch total loss 5.89624929\n",
      "Trained batch 5029 batch loss 6.22472525 epoch total loss 5.89631462\n",
      "Trained batch 5030 batch loss 6.37196159 epoch total loss 5.89640903\n",
      "Trained batch 5031 batch loss 6.33251858 epoch total loss 5.89649582\n",
      "Trained batch 5032 batch loss 6.61624718 epoch total loss 5.89663887\n",
      "Trained batch 5033 batch loss 6.12347221 epoch total loss 5.89668369\n",
      "Trained batch 5034 batch loss 5.37610722 epoch total loss 5.8965807\n",
      "Trained batch 5035 batch loss 6.15813923 epoch total loss 5.89663267\n",
      "Trained batch 5036 batch loss 6.42119169 epoch total loss 5.89673662\n",
      "Trained batch 5037 batch loss 5.29613686 epoch total loss 5.89661789\n",
      "Trained batch 5038 batch loss 4.23274279 epoch total loss 5.89628744\n",
      "Trained batch 5039 batch loss 6.85072231 epoch total loss 5.89647722\n",
      "Trained batch 5040 batch loss 5.95266247 epoch total loss 5.89648819\n",
      "Trained batch 5041 batch loss 6.07777262 epoch total loss 5.89652443\n",
      "Trained batch 5042 batch loss 7.35539389 epoch total loss 5.89681387\n",
      "Trained batch 5043 batch loss 7.30942631 epoch total loss 5.89709377\n",
      "Trained batch 5044 batch loss 5.65816879 epoch total loss 5.89704609\n",
      "Trained batch 5045 batch loss 4.45560503 epoch total loss 5.89676046\n",
      "Trained batch 5046 batch loss 4.74834538 epoch total loss 5.89653254\n",
      "Trained batch 5047 batch loss 5.72643948 epoch total loss 5.89649916\n",
      "Trained batch 5048 batch loss 5.5903244 epoch total loss 5.89643812\n",
      "Trained batch 5049 batch loss 4.8324604 epoch total loss 5.89622736\n",
      "Trained batch 5050 batch loss 6.35517788 epoch total loss 5.89631844\n",
      "Trained batch 5051 batch loss 4.30031872 epoch total loss 5.89600277\n",
      "Trained batch 5052 batch loss 6.56495094 epoch total loss 5.89613485\n",
      "Trained batch 5053 batch loss 6.20046711 epoch total loss 5.89619541\n",
      "Trained batch 5054 batch loss 6.03437328 epoch total loss 5.89622259\n",
      "Trained batch 5055 batch loss 6.47915649 epoch total loss 5.89633799\n",
      "Trained batch 5056 batch loss 6.53060627 epoch total loss 5.89646339\n",
      "Trained batch 5057 batch loss 5.79778624 epoch total loss 5.89644384\n",
      "Trained batch 5058 batch loss 4.86750221 epoch total loss 5.89624\n",
      "Trained batch 5059 batch loss 5.78534031 epoch total loss 5.8962183\n",
      "Trained batch 5060 batch loss 4.57464027 epoch total loss 5.89595699\n",
      "Trained batch 5061 batch loss 4.82922268 epoch total loss 5.89574671\n",
      "Trained batch 5062 batch loss 4.68822575 epoch total loss 5.89550781\n",
      "Trained batch 5063 batch loss 5.37904739 epoch total loss 5.89540577\n",
      "Trained batch 5064 batch loss 5.1213088 epoch total loss 5.8952527\n",
      "Trained batch 5065 batch loss 5.23892975 epoch total loss 5.895123\n",
      "Trained batch 5066 batch loss 5.55177 epoch total loss 5.89505577\n",
      "Trained batch 5067 batch loss 4.83660364 epoch total loss 5.89484644\n",
      "Trained batch 5068 batch loss 4.88722134 epoch total loss 5.8946476\n",
      "Trained batch 5069 batch loss 4.57341862 epoch total loss 5.89438725\n",
      "Trained batch 5070 batch loss 5.62115288 epoch total loss 5.89433336\n",
      "Trained batch 5071 batch loss 4.38569355 epoch total loss 5.89403534\n",
      "Trained batch 5072 batch loss 5.05890179 epoch total loss 5.89387083\n",
      "Trained batch 5073 batch loss 4.94142723 epoch total loss 5.89368296\n",
      "Trained batch 5074 batch loss 5.09032726 epoch total loss 5.89352465\n",
      "Trained batch 5075 batch loss 5.71772718 epoch total loss 5.89349\n",
      "Trained batch 5076 batch loss 5.6838541 epoch total loss 5.89344835\n",
      "Trained batch 5077 batch loss 5.15280628 epoch total loss 5.89330244\n",
      "Trained batch 5078 batch loss 4.32251167 epoch total loss 5.89299297\n",
      "Trained batch 5079 batch loss 5.46275902 epoch total loss 5.89290857\n",
      "Trained batch 5080 batch loss 6.36706686 epoch total loss 5.89300203\n",
      "Trained batch 5081 batch loss 5.38006926 epoch total loss 5.89290094\n",
      "Trained batch 5082 batch loss 5.09627151 epoch total loss 5.89274406\n",
      "Trained batch 5083 batch loss 6.06392288 epoch total loss 5.89277792\n",
      "Trained batch 5084 batch loss 6.31638527 epoch total loss 5.89286137\n",
      "Trained batch 5085 batch loss 5.5116806 epoch total loss 5.8927865\n",
      "Trained batch 5086 batch loss 4.42058754 epoch total loss 5.89249659\n",
      "Trained batch 5087 batch loss 3.69541669 epoch total loss 5.89206457\n",
      "Trained batch 5088 batch loss 5.34184551 epoch total loss 5.89195681\n",
      "Trained batch 5089 batch loss 5.13847542 epoch total loss 5.89180851\n",
      "Trained batch 5090 batch loss 6.56811905 epoch total loss 5.89194155\n",
      "Trained batch 5091 batch loss 4.69325542 epoch total loss 5.89170599\n",
      "Trained batch 5092 batch loss 5.74319 epoch total loss 5.8916769\n",
      "Trained batch 5093 batch loss 6.00416231 epoch total loss 5.89169931\n",
      "Trained batch 5094 batch loss 6.09167576 epoch total loss 5.89173841\n",
      "Trained batch 5095 batch loss 6.09632444 epoch total loss 5.89177847\n",
      "Trained batch 5096 batch loss 5.95336437 epoch total loss 5.89179039\n",
      "Trained batch 5097 batch loss 5.51714706 epoch total loss 5.89171696\n",
      "Trained batch 5098 batch loss 6.3051343 epoch total loss 5.89179802\n",
      "Trained batch 5099 batch loss 6.06330585 epoch total loss 5.8918314\n",
      "Trained batch 5100 batch loss 6.00972 epoch total loss 5.89185476\n",
      "Trained batch 5101 batch loss 5.69000959 epoch total loss 5.89181519\n",
      "Trained batch 5102 batch loss 5.91087914 epoch total loss 5.89181852\n",
      "Trained batch 5103 batch loss 5.67378807 epoch total loss 5.89177608\n",
      "Trained batch 5104 batch loss 5.78796768 epoch total loss 5.89175558\n",
      "Trained batch 5105 batch loss 5.98460817 epoch total loss 5.8917737\n",
      "Trained batch 5106 batch loss 5.50609303 epoch total loss 5.89169788\n",
      "Trained batch 5107 batch loss 6.0401082 epoch total loss 5.89172697\n",
      "Trained batch 5108 batch loss 5.94291925 epoch total loss 5.89173746\n",
      "Trained batch 5109 batch loss 5.87958431 epoch total loss 5.8917346\n",
      "Trained batch 5110 batch loss 6.31283474 epoch total loss 5.89181709\n",
      "Trained batch 5111 batch loss 5.83504438 epoch total loss 5.89180613\n",
      "Trained batch 5112 batch loss 6.43312883 epoch total loss 5.89191198\n",
      "Trained batch 5113 batch loss 6.02789593 epoch total loss 5.89193869\n",
      "Trained batch 5114 batch loss 6.26169062 epoch total loss 5.89201117\n",
      "Trained batch 5115 batch loss 5.9793148 epoch total loss 5.89202785\n",
      "Trained batch 5116 batch loss 6.41992855 epoch total loss 5.89213085\n",
      "Trained batch 5117 batch loss 5.52048588 epoch total loss 5.89205837\n",
      "Trained batch 5118 batch loss 5.7641468 epoch total loss 5.8920331\n",
      "Trained batch 5119 batch loss 5.45783234 epoch total loss 5.89194822\n",
      "Trained batch 5120 batch loss 5.27839231 epoch total loss 5.89182854\n",
      "Trained batch 5121 batch loss 5.76705599 epoch total loss 5.89180422\n",
      "Trained batch 5122 batch loss 6.11043358 epoch total loss 5.89184713\n",
      "Trained batch 5123 batch loss 5.72023344 epoch total loss 5.89181376\n",
      "Trained batch 5124 batch loss 5.50147915 epoch total loss 5.89173746\n",
      "Trained batch 5125 batch loss 5.45233154 epoch total loss 5.89165211\n",
      "Trained batch 5126 batch loss 5.51915836 epoch total loss 5.89157963\n",
      "Trained batch 5127 batch loss 5.84838343 epoch total loss 5.89157104\n",
      "Trained batch 5128 batch loss 5.45755482 epoch total loss 5.89148617\n",
      "Trained batch 5129 batch loss 5.19670582 epoch total loss 5.89135075\n",
      "Trained batch 5130 batch loss 5.51293373 epoch total loss 5.89127731\n",
      "Trained batch 5131 batch loss 5.2076416 epoch total loss 5.8911438\n",
      "Trained batch 5132 batch loss 5.91446114 epoch total loss 5.89114809\n",
      "Trained batch 5133 batch loss 5.48145866 epoch total loss 5.89106846\n",
      "Trained batch 5134 batch loss 5.96822166 epoch total loss 5.89108372\n",
      "Trained batch 5135 batch loss 5.63988304 epoch total loss 5.89103508\n",
      "Trained batch 5136 batch loss 5.5920229 epoch total loss 5.89097691\n",
      "Trained batch 5137 batch loss 5.58443642 epoch total loss 5.89091682\n",
      "Trained batch 5138 batch loss 5.93152475 epoch total loss 5.89092493\n",
      "Trained batch 5139 batch loss 5.8713274 epoch total loss 5.89092112\n",
      "Trained batch 5140 batch loss 5.89749765 epoch total loss 5.89092255\n",
      "Trained batch 5141 batch loss 5.94040585 epoch total loss 5.89093208\n",
      "Trained batch 5142 batch loss 5.82230949 epoch total loss 5.89091873\n",
      "Trained batch 5143 batch loss 5.96339512 epoch total loss 5.89093256\n",
      "Trained batch 5144 batch loss 5.66084242 epoch total loss 5.89088774\n",
      "Trained batch 5145 batch loss 5.90963268 epoch total loss 5.89089155\n",
      "Trained batch 5146 batch loss 6.20467567 epoch total loss 5.89095259\n",
      "Trained batch 5147 batch loss 5.43420219 epoch total loss 5.8908639\n",
      "Trained batch 5148 batch loss 5.4915967 epoch total loss 5.89078617\n",
      "Trained batch 5149 batch loss 4.94517517 epoch total loss 5.89060259\n",
      "Trained batch 5150 batch loss 5.28620195 epoch total loss 5.89048529\n",
      "Trained batch 5151 batch loss 4.92431688 epoch total loss 5.89029789\n",
      "Trained batch 5152 batch loss 4.99011326 epoch total loss 5.89012289\n",
      "Trained batch 5153 batch loss 4.77694511 epoch total loss 5.88990736\n",
      "Trained batch 5154 batch loss 4.88105392 epoch total loss 5.88971138\n",
      "Trained batch 5155 batch loss 4.4300437 epoch total loss 5.88942814\n",
      "Trained batch 5156 batch loss 4.40871048 epoch total loss 5.88914061\n",
      "Trained batch 5157 batch loss 4.67358541 epoch total loss 5.88890505\n",
      "Trained batch 5158 batch loss 5.25820541 epoch total loss 5.88878298\n",
      "Trained batch 5159 batch loss 4.56781769 epoch total loss 5.88852692\n",
      "Trained batch 5160 batch loss 6.58436632 epoch total loss 5.88866186\n",
      "Trained batch 5161 batch loss 6.34643555 epoch total loss 5.88875\n",
      "Trained batch 5162 batch loss 6.16093302 epoch total loss 5.88880301\n",
      "Trained batch 5163 batch loss 6.30943251 epoch total loss 5.88888407\n",
      "Trained batch 5164 batch loss 6.84532547 epoch total loss 5.88906956\n",
      "Trained batch 5165 batch loss 6.24085045 epoch total loss 5.88913727\n",
      "Trained batch 5166 batch loss 6.35890102 epoch total loss 5.88922834\n",
      "Trained batch 5167 batch loss 5.64063263 epoch total loss 5.88918\n",
      "Trained batch 5168 batch loss 5.83011246 epoch total loss 5.88916874\n",
      "Trained batch 5169 batch loss 6.08459425 epoch total loss 5.88920641\n",
      "Trained batch 5170 batch loss 6.00172615 epoch total loss 5.88922834\n",
      "Trained batch 5171 batch loss 6.69311237 epoch total loss 5.88938379\n",
      "Trained batch 5172 batch loss 5.43042612 epoch total loss 5.8892951\n",
      "Trained batch 5173 batch loss 5.89287663 epoch total loss 5.88929558\n",
      "Trained batch 5174 batch loss 6.14798546 epoch total loss 5.88934565\n",
      "Trained batch 5175 batch loss 5.83448648 epoch total loss 5.88933516\n",
      "Trained batch 5176 batch loss 6.0208 epoch total loss 5.88936043\n",
      "Trained batch 5177 batch loss 6.66618443 epoch total loss 5.88951063\n",
      "Trained batch 5178 batch loss 6.03453827 epoch total loss 5.88953876\n",
      "Trained batch 5179 batch loss 5.55532312 epoch total loss 5.88947392\n",
      "Trained batch 5180 batch loss 5.71995401 epoch total loss 5.88944149\n",
      "Trained batch 5181 batch loss 6.45782 epoch total loss 5.88955116\n",
      "Trained batch 5182 batch loss 5.77037 epoch total loss 5.8895278\n",
      "Trained batch 5183 batch loss 5.59438372 epoch total loss 5.88947058\n",
      "Trained batch 5184 batch loss 5.91167164 epoch total loss 5.88947535\n",
      "Trained batch 5185 batch loss 5.85075665 epoch total loss 5.88946772\n",
      "Trained batch 5186 batch loss 5.64538479 epoch total loss 5.88942051\n",
      "Trained batch 5187 batch loss 6.27118397 epoch total loss 5.88949442\n",
      "Trained batch 5188 batch loss 5.81059933 epoch total loss 5.88947916\n",
      "Trained batch 5189 batch loss 5.66948509 epoch total loss 5.88943672\n",
      "Trained batch 5190 batch loss 5.62685394 epoch total loss 5.88938618\n",
      "Trained batch 5191 batch loss 5.35725 epoch total loss 5.88928366\n",
      "Trained batch 5192 batch loss 5.68471909 epoch total loss 5.88924456\n",
      "Trained batch 5193 batch loss 5.47754192 epoch total loss 5.8891654\n",
      "Trained batch 5194 batch loss 5.4513607 epoch total loss 5.889081\n",
      "Trained batch 5195 batch loss 5.31884861 epoch total loss 5.88897133\n",
      "Trained batch 5196 batch loss 6.22039413 epoch total loss 5.88903475\n",
      "Trained batch 5197 batch loss 5.55129719 epoch total loss 5.88897\n",
      "Trained batch 5198 batch loss 5.80414248 epoch total loss 5.88895369\n",
      "Trained batch 5199 batch loss 5.72193527 epoch total loss 5.88892174\n",
      "Trained batch 5200 batch loss 5.35089684 epoch total loss 5.88881826\n",
      "Trained batch 5201 batch loss 5.42936277 epoch total loss 5.88873\n",
      "Trained batch 5202 batch loss 5.3627634 epoch total loss 5.88862896\n",
      "Trained batch 5203 batch loss 5.74995327 epoch total loss 5.88860226\n",
      "Trained batch 5204 batch loss 5.57035303 epoch total loss 5.88854122\n",
      "Trained batch 5205 batch loss 5.43352604 epoch total loss 5.88845396\n",
      "Trained batch 5206 batch loss 4.04379129 epoch total loss 5.88809919\n",
      "Trained batch 5207 batch loss 5.46011448 epoch total loss 5.88801718\n",
      "Trained batch 5208 batch loss 5.13764763 epoch total loss 5.88787317\n",
      "Trained batch 5209 batch loss 5.52178383 epoch total loss 5.8878026\n",
      "Trained batch 5210 batch loss 5.55542564 epoch total loss 5.8877387\n",
      "Trained batch 5211 batch loss 6.57122 epoch total loss 5.88787\n",
      "Trained batch 5212 batch loss 5.73341274 epoch total loss 5.88784027\n",
      "Trained batch 5213 batch loss 5.59412527 epoch total loss 5.887784\n",
      "Trained batch 5214 batch loss 6.76071501 epoch total loss 5.88795137\n",
      "Trained batch 5215 batch loss 6.00399399 epoch total loss 5.88797331\n",
      "Trained batch 5216 batch loss 6.45816708 epoch total loss 5.88808298\n",
      "Trained batch 5217 batch loss 5.66765785 epoch total loss 5.88804054\n",
      "Trained batch 5218 batch loss 5.90923882 epoch total loss 5.88804483\n",
      "Trained batch 5219 batch loss 5.49058819 epoch total loss 5.88796854\n",
      "Trained batch 5220 batch loss 5.86706829 epoch total loss 5.88796473\n",
      "Trained batch 5221 batch loss 5.82635784 epoch total loss 5.8879528\n",
      "Trained batch 5222 batch loss 5.62634373 epoch total loss 5.88790274\n",
      "Trained batch 5223 batch loss 5.10464859 epoch total loss 5.88775301\n",
      "Trained batch 5224 batch loss 5.1649003 epoch total loss 5.88761473\n",
      "Trained batch 5225 batch loss 5.19460106 epoch total loss 5.88748217\n",
      "Trained batch 5226 batch loss 5.4998045 epoch total loss 5.88740778\n",
      "Trained batch 5227 batch loss 5.40606117 epoch total loss 5.88731575\n",
      "Trained batch 5228 batch loss 5.1667223 epoch total loss 5.88717794\n",
      "Trained batch 5229 batch loss 4.93040371 epoch total loss 5.88699484\n",
      "Trained batch 5230 batch loss 5.44886303 epoch total loss 5.88691092\n",
      "Trained batch 5231 batch loss 5.29074574 epoch total loss 5.88679695\n",
      "Trained batch 5232 batch loss 5.63689041 epoch total loss 5.88674927\n",
      "Trained batch 5233 batch loss 6.0060935 epoch total loss 5.88677216\n",
      "Trained batch 5234 batch loss 6.21762276 epoch total loss 5.8868351\n",
      "Trained batch 5235 batch loss 6.00738335 epoch total loss 5.88685846\n",
      "Trained batch 5236 batch loss 6.15418768 epoch total loss 5.88690948\n",
      "Trained batch 5237 batch loss 6.20612431 epoch total loss 5.88697052\n",
      "Trained batch 5238 batch loss 6.55431747 epoch total loss 5.88709784\n",
      "Trained batch 5239 batch loss 6.31456137 epoch total loss 5.88717937\n",
      "Trained batch 5240 batch loss 6.62033367 epoch total loss 5.88731956\n",
      "Trained batch 5241 batch loss 5.76513863 epoch total loss 5.8872962\n",
      "Trained batch 5242 batch loss 6.46737289 epoch total loss 5.88740683\n",
      "Trained batch 5243 batch loss 5.9660759 epoch total loss 5.88742208\n",
      "Trained batch 5244 batch loss 5.88449144 epoch total loss 5.88742161\n",
      "Trained batch 5245 batch loss 6.3282671 epoch total loss 5.88750553\n",
      "Trained batch 5246 batch loss 6.15406036 epoch total loss 5.88755655\n",
      "Trained batch 5247 batch loss 6.20113659 epoch total loss 5.88761616\n",
      "Trained batch 5248 batch loss 5.94479752 epoch total loss 5.88762712\n",
      "Trained batch 5249 batch loss 5.97823429 epoch total loss 5.88764429\n",
      "Trained batch 5250 batch loss 5.89187288 epoch total loss 5.88764524\n",
      "Trained batch 5251 batch loss 6.07532692 epoch total loss 5.88768148\n",
      "Trained batch 5252 batch loss 5.96420574 epoch total loss 5.88769627\n",
      "Trained batch 5253 batch loss 6.18582869 epoch total loss 5.88775253\n",
      "Trained batch 5254 batch loss 6.27231312 epoch total loss 5.88782597\n",
      "Trained batch 5255 batch loss 6.08016968 epoch total loss 5.88786221\n",
      "Trained batch 5256 batch loss 6.03521442 epoch total loss 5.88789034\n",
      "Trained batch 5257 batch loss 6.41503429 epoch total loss 5.88799047\n",
      "Trained batch 5258 batch loss 5.86978912 epoch total loss 5.88798714\n",
      "Trained batch 5259 batch loss 5.87265778 epoch total loss 5.88798428\n",
      "Trained batch 5260 batch loss 6.01685905 epoch total loss 5.88800859\n",
      "Trained batch 5261 batch loss 6.38744164 epoch total loss 5.88810349\n",
      "Trained batch 5262 batch loss 6.11143351 epoch total loss 5.88814592\n",
      "Trained batch 5263 batch loss 5.70823765 epoch total loss 5.88811207\n",
      "Trained batch 5264 batch loss 5.57953072 epoch total loss 5.88805342\n",
      "Trained batch 5265 batch loss 6.03653383 epoch total loss 5.88808155\n",
      "Trained batch 5266 batch loss 6.33553219 epoch total loss 5.8881669\n",
      "Trained batch 5267 batch loss 5.73034239 epoch total loss 5.88813686\n",
      "Trained batch 5268 batch loss 6.11340904 epoch total loss 5.8881793\n",
      "Trained batch 5269 batch loss 6.06345034 epoch total loss 5.88821268\n",
      "Trained batch 5270 batch loss 5.78847837 epoch total loss 5.88819361\n",
      "Trained batch 5271 batch loss 5.29821301 epoch total loss 5.88808203\n",
      "Trained batch 5272 batch loss 5.83359146 epoch total loss 5.88807154\n",
      "Trained batch 5273 batch loss 6.14845 epoch total loss 5.88812113\n",
      "Trained batch 5274 batch loss 5.7673645 epoch total loss 5.88809824\n",
      "Trained batch 5275 batch loss 5.46338749 epoch total loss 5.88801765\n",
      "Trained batch 5276 batch loss 5.87675095 epoch total loss 5.88801575\n",
      "Trained batch 5277 batch loss 6.31516266 epoch total loss 5.88809633\n",
      "Trained batch 5278 batch loss 5.5906086 epoch total loss 5.88804\n",
      "Trained batch 5279 batch loss 6.06619167 epoch total loss 5.88807344\n",
      "Trained batch 5280 batch loss 5.91465378 epoch total loss 5.88807869\n",
      "Trained batch 5281 batch loss 5.61502886 epoch total loss 5.88802671\n",
      "Trained batch 5282 batch loss 6.3824482 epoch total loss 5.88812065\n",
      "Trained batch 5283 batch loss 5.92184496 epoch total loss 5.88812685\n",
      "Trained batch 5284 batch loss 6.5787878 epoch total loss 5.8882575\n",
      "Trained batch 5285 batch loss 6.90310717 epoch total loss 5.88844919\n",
      "Trained batch 5286 batch loss 6.6063261 epoch total loss 5.88858509\n",
      "Trained batch 5287 batch loss 6.20434523 epoch total loss 5.8886447\n",
      "Trained batch 5288 batch loss 6.63088417 epoch total loss 5.88878536\n",
      "Trained batch 5289 batch loss 6.7706666 epoch total loss 5.88895226\n",
      "Trained batch 5290 batch loss 6.62334156 epoch total loss 5.88909101\n",
      "Trained batch 5291 batch loss 5.89712715 epoch total loss 5.88909245\n",
      "Trained batch 5292 batch loss 6.17536449 epoch total loss 5.88914633\n",
      "Trained batch 5293 batch loss 6.43346596 epoch total loss 5.88924932\n",
      "Trained batch 5294 batch loss 6.16340637 epoch total loss 5.8893013\n",
      "Trained batch 5295 batch loss 5.97479725 epoch total loss 5.88931751\n",
      "Trained batch 5296 batch loss 5.89334536 epoch total loss 5.88931799\n",
      "Trained batch 5297 batch loss 5.65374279 epoch total loss 5.88927364\n",
      "Trained batch 5298 batch loss 5.99742317 epoch total loss 5.88929415\n",
      "Trained batch 5299 batch loss 5.65809679 epoch total loss 5.88925076\n",
      "Trained batch 5300 batch loss 5.2991066 epoch total loss 5.88913918\n",
      "Trained batch 5301 batch loss 5.87643147 epoch total loss 5.88913679\n",
      "Trained batch 5302 batch loss 5.29489803 epoch total loss 5.88902473\n",
      "Trained batch 5303 batch loss 6.41016483 epoch total loss 5.88912296\n",
      "Trained batch 5304 batch loss 5.6890316 epoch total loss 5.88908529\n",
      "Trained batch 5305 batch loss 6.11387491 epoch total loss 5.88912773\n",
      "Trained batch 5306 batch loss 5.91305733 epoch total loss 5.88913202\n",
      "Trained batch 5307 batch loss 6.22719097 epoch total loss 5.88919544\n",
      "Trained batch 5308 batch loss 5.00991201 epoch total loss 5.88903\n",
      "Trained batch 5309 batch loss 5.68080616 epoch total loss 5.88899088\n",
      "Trained batch 5310 batch loss 5.88657904 epoch total loss 5.8889904\n",
      "Trained batch 5311 batch loss 6.12186956 epoch total loss 5.88903427\n",
      "Trained batch 5312 batch loss 6.36205959 epoch total loss 5.88912296\n",
      "Trained batch 5313 batch loss 6.42641068 epoch total loss 5.88922405\n",
      "Trained batch 5314 batch loss 5.73096848 epoch total loss 5.88919401\n",
      "Trained batch 5315 batch loss 5.33817101 epoch total loss 5.88909054\n",
      "Trained batch 5316 batch loss 5.54332066 epoch total loss 5.88902521\n",
      "Trained batch 5317 batch loss 5.69946861 epoch total loss 5.88898945\n",
      "Trained batch 5318 batch loss 6.20238876 epoch total loss 5.88904858\n",
      "Trained batch 5319 batch loss 5.65708399 epoch total loss 5.88900471\n",
      "Trained batch 5320 batch loss 5.4605751 epoch total loss 5.8889246\n",
      "Trained batch 5321 batch loss 5.6974721 epoch total loss 5.88888836\n",
      "Trained batch 5322 batch loss 6.06539297 epoch total loss 5.88892126\n",
      "Trained batch 5323 batch loss 5.85007429 epoch total loss 5.88891411\n",
      "Trained batch 5324 batch loss 4.995924 epoch total loss 5.88874626\n",
      "Trained batch 5325 batch loss 5.76730156 epoch total loss 5.88872337\n",
      "Trained batch 5326 batch loss 4.31207943 epoch total loss 5.88842773\n",
      "Trained batch 5327 batch loss 4.27788448 epoch total loss 5.88812542\n",
      "Trained batch 5328 batch loss 4.43258905 epoch total loss 5.88785172\n",
      "Trained batch 5329 batch loss 4.35856915 epoch total loss 5.88756514\n",
      "Trained batch 5330 batch loss 4.27961636 epoch total loss 5.8872633\n",
      "Trained batch 5331 batch loss 6.12285 epoch total loss 5.88730764\n",
      "Trained batch 5332 batch loss 6.00533676 epoch total loss 5.88732958\n",
      "Trained batch 5333 batch loss 5.00950336 epoch total loss 5.88716507\n",
      "Trained batch 5334 batch loss 5.46724796 epoch total loss 5.88708639\n",
      "Trained batch 5335 batch loss 3.72301674 epoch total loss 5.8866806\n",
      "Trained batch 5336 batch loss 5.97184086 epoch total loss 5.88669682\n",
      "Trained batch 5337 batch loss 6.21567345 epoch total loss 5.88675833\n",
      "Trained batch 5338 batch loss 6.23211908 epoch total loss 5.88682318\n",
      "Trained batch 5339 batch loss 6.24451637 epoch total loss 5.88689\n",
      "Trained batch 5340 batch loss 5.54473066 epoch total loss 5.88682604\n",
      "Trained batch 5341 batch loss 6.46467304 epoch total loss 5.88693428\n",
      "Trained batch 5342 batch loss 6.22092724 epoch total loss 5.88699675\n",
      "Trained batch 5343 batch loss 6.33886623 epoch total loss 5.88708115\n",
      "Trained batch 5344 batch loss 6.00973845 epoch total loss 5.88710403\n",
      "Trained batch 5345 batch loss 6.18789196 epoch total loss 5.8871603\n",
      "Trained batch 5346 batch loss 5.06011724 epoch total loss 5.88700533\n",
      "Trained batch 5347 batch loss 6.35076618 epoch total loss 5.88709259\n",
      "Trained batch 5348 batch loss 6.4719677 epoch total loss 5.88720179\n",
      "Trained batch 5349 batch loss 5.90143 epoch total loss 5.88720465\n",
      "Trained batch 5350 batch loss 5.47832584 epoch total loss 5.88712835\n",
      "Trained batch 5351 batch loss 5.82365131 epoch total loss 5.88711643\n",
      "Trained batch 5352 batch loss 5.84333944 epoch total loss 5.88710833\n",
      "Trained batch 5353 batch loss 5.82269573 epoch total loss 5.88709641\n",
      "Trained batch 5354 batch loss 5.84454441 epoch total loss 5.8870883\n",
      "Trained batch 5355 batch loss 5.89746666 epoch total loss 5.88709\n",
      "Trained batch 5356 batch loss 6.09570885 epoch total loss 5.88712931\n",
      "Trained batch 5357 batch loss 6.01185608 epoch total loss 5.88715267\n",
      "Trained batch 5358 batch loss 6.37574577 epoch total loss 5.88724375\n",
      "Trained batch 5359 batch loss 6.23499393 epoch total loss 5.8873086\n",
      "Trained batch 5360 batch loss 6.14259815 epoch total loss 5.88735628\n",
      "Trained batch 5361 batch loss 6.4452343 epoch total loss 5.88746\n",
      "Trained batch 5362 batch loss 5.40962601 epoch total loss 5.88737106\n",
      "Trained batch 5363 batch loss 5.15956 epoch total loss 5.88723564\n",
      "Trained batch 5364 batch loss 5.2993412 epoch total loss 5.88712597\n",
      "Trained batch 5365 batch loss 5.28317165 epoch total loss 5.88701344\n",
      "Trained batch 5366 batch loss 5.22222328 epoch total loss 5.88688946\n",
      "Trained batch 5367 batch loss 5.55257607 epoch total loss 5.88682699\n",
      "Trained batch 5368 batch loss 4.9320116 epoch total loss 5.88664913\n",
      "Trained batch 5369 batch loss 5.01712036 epoch total loss 5.88648748\n",
      "Trained batch 5370 batch loss 5.39610815 epoch total loss 5.88639593\n",
      "Trained batch 5371 batch loss 5.06193638 epoch total loss 5.88624287\n",
      "Trained batch 5372 batch loss 5.98809195 epoch total loss 5.88626194\n",
      "Trained batch 5373 batch loss 5.53676033 epoch total loss 5.88619661\n",
      "Trained batch 5374 batch loss 5.60428143 epoch total loss 5.88614416\n",
      "Trained batch 5375 batch loss 5.1031394 epoch total loss 5.88599873\n",
      "Trained batch 5376 batch loss 5.56481 epoch total loss 5.88593864\n",
      "Trained batch 5377 batch loss 5.51798 epoch total loss 5.88587046\n",
      "Trained batch 5378 batch loss 5.60390186 epoch total loss 5.88581753\n",
      "Trained batch 5379 batch loss 5.61156 epoch total loss 5.88576651\n",
      "Trained batch 5380 batch loss 4.82634926 epoch total loss 5.88556957\n",
      "Trained batch 5381 batch loss 5.33162498 epoch total loss 5.88546705\n",
      "Trained batch 5382 batch loss 7.33373308 epoch total loss 5.88573599\n",
      "Trained batch 5383 batch loss 5.72250938 epoch total loss 5.88570595\n",
      "Trained batch 5384 batch loss 6.61792088 epoch total loss 5.88584137\n",
      "Trained batch 5385 batch loss 6.50075 epoch total loss 5.88595581\n",
      "Trained batch 5386 batch loss 5.79324055 epoch total loss 5.88593817\n",
      "Trained batch 5387 batch loss 5.0012188 epoch total loss 5.88577414\n",
      "Trained batch 5388 batch loss 5.54030943 epoch total loss 5.88571024\n",
      "Trained batch 5389 batch loss 6.34974384 epoch total loss 5.88579655\n",
      "Trained batch 5390 batch loss 6.60448551 epoch total loss 5.88592958\n",
      "Trained batch 5391 batch loss 6.78182602 epoch total loss 5.88609552\n",
      "Trained batch 5392 batch loss 5.81301498 epoch total loss 5.88608217\n",
      "Trained batch 5393 batch loss 6.36676884 epoch total loss 5.88617134\n",
      "Trained batch 5394 batch loss 5.90214825 epoch total loss 5.8861742\n",
      "Trained batch 5395 batch loss 5.85558 epoch total loss 5.88616848\n",
      "Trained batch 5396 batch loss 5.68569374 epoch total loss 5.88613129\n",
      "Trained batch 5397 batch loss 5.91063213 epoch total loss 5.88613558\n",
      "Trained batch 5398 batch loss 6.10107279 epoch total loss 5.88617563\n",
      "Trained batch 5399 batch loss 6.03738737 epoch total loss 5.88620377\n",
      "Trained batch 5400 batch loss 5.78332853 epoch total loss 5.88618469\n",
      "Trained batch 5401 batch loss 6.08537483 epoch total loss 5.88622141\n",
      "Trained batch 5402 batch loss 5.76353073 epoch total loss 5.886199\n",
      "Trained batch 5403 batch loss 6.18067932 epoch total loss 5.88625336\n",
      "Trained batch 5404 batch loss 6.09641552 epoch total loss 5.88629246\n",
      "Trained batch 5405 batch loss 6.28986 epoch total loss 5.88636684\n",
      "Trained batch 5406 batch loss 6.14288235 epoch total loss 5.88641405\n",
      "Trained batch 5407 batch loss 6.13250399 epoch total loss 5.88646\n",
      "Trained batch 5408 batch loss 6.25614166 epoch total loss 5.88652802\n",
      "Trained batch 5409 batch loss 6.345119 epoch total loss 5.88661289\n",
      "Trained batch 5410 batch loss 6.28215027 epoch total loss 5.88668585\n",
      "Trained batch 5411 batch loss 6.04464149 epoch total loss 5.88671494\n",
      "Trained batch 5412 batch loss 5.7945013 epoch total loss 5.88669825\n",
      "Trained batch 5413 batch loss 5.80472088 epoch total loss 5.88668299\n",
      "Trained batch 5414 batch loss 6.13655424 epoch total loss 5.88672924\n",
      "Trained batch 5415 batch loss 5.57041121 epoch total loss 5.88667059\n",
      "Trained batch 5416 batch loss 6.54829836 epoch total loss 5.88679314\n",
      "Trained batch 5417 batch loss 6.29934502 epoch total loss 5.88686895\n",
      "Trained batch 5418 batch loss 6.34765911 epoch total loss 5.88695431\n",
      "Trained batch 5419 batch loss 5.67015266 epoch total loss 5.88691425\n",
      "Trained batch 5420 batch loss 5.89835739 epoch total loss 5.88691616\n",
      "Trained batch 5421 batch loss 6.10961819 epoch total loss 5.88695717\n",
      "Trained batch 5422 batch loss 5.95737839 epoch total loss 5.88697\n",
      "Trained batch 5423 batch loss 5.95381737 epoch total loss 5.88698244\n",
      "Trained batch 5424 batch loss 6.02214718 epoch total loss 5.88700724\n",
      "Trained batch 5425 batch loss 5.5235796 epoch total loss 5.88694\n",
      "Trained batch 5426 batch loss 5.82712 epoch total loss 5.88692904\n",
      "Trained batch 5427 batch loss 6.00599575 epoch total loss 5.88695097\n",
      "Trained batch 5428 batch loss 5.48571587 epoch total loss 5.88687706\n",
      "Trained batch 5429 batch loss 5.68376541 epoch total loss 5.88684\n",
      "Trained batch 5430 batch loss 6.19094753 epoch total loss 5.88689566\n",
      "Trained batch 5431 batch loss 6.41548252 epoch total loss 5.88699293\n",
      "Trained batch 5432 batch loss 5.9508028 epoch total loss 5.88700485\n",
      "Trained batch 5433 batch loss 6.03256178 epoch total loss 5.88703203\n",
      "Trained batch 5434 batch loss 6.19035912 epoch total loss 5.88708735\n",
      "Trained batch 5435 batch loss 5.99148273 epoch total loss 5.8871069\n",
      "Trained batch 5436 batch loss 6.19557142 epoch total loss 5.88716364\n",
      "Trained batch 5437 batch loss 6.08710289 epoch total loss 5.88720036\n",
      "Trained batch 5438 batch loss 6.32033825 epoch total loss 5.88728\n",
      "Trained batch 5439 batch loss 5.97859383 epoch total loss 5.88729668\n",
      "Trained batch 5440 batch loss 5.9089303 epoch total loss 5.88730097\n",
      "Trained batch 5441 batch loss 5.72750473 epoch total loss 5.8872714\n",
      "Trained batch 5442 batch loss 6.48166 epoch total loss 5.8873806\n",
      "Trained batch 5443 batch loss 6.16753387 epoch total loss 5.8874321\n",
      "Trained batch 5444 batch loss 6.04019547 epoch total loss 5.88746\n",
      "Trained batch 5445 batch loss 6.25407457 epoch total loss 5.88752747\n",
      "Trained batch 5446 batch loss 6.38212 epoch total loss 5.88761854\n",
      "Trained batch 5447 batch loss 7.16444874 epoch total loss 5.88785267\n",
      "Trained batch 5448 batch loss 6.11676359 epoch total loss 5.88789511\n",
      "Trained batch 5449 batch loss 5.6193161 epoch total loss 5.88784552\n",
      "Trained batch 5450 batch loss 5.55851 epoch total loss 5.88778543\n",
      "Trained batch 5451 batch loss 5.94057941 epoch total loss 5.88779497\n",
      "Trained batch 5452 batch loss 6.10517025 epoch total loss 5.88783503\n",
      "Trained batch 5453 batch loss 6.029809 epoch total loss 5.88786077\n",
      "Trained batch 5454 batch loss 5.96186638 epoch total loss 5.8878746\n",
      "Trained batch 5455 batch loss 5.60311127 epoch total loss 5.88782215\n",
      "Trained batch 5456 batch loss 5.73013639 epoch total loss 5.88779354\n",
      "Trained batch 5457 batch loss 5.86609554 epoch total loss 5.88778925\n",
      "Trained batch 5458 batch loss 6.14758968 epoch total loss 5.88783693\n",
      "Trained batch 5459 batch loss 6.11440659 epoch total loss 5.88787889\n",
      "Trained batch 5460 batch loss 5.61448956 epoch total loss 5.88782883\n",
      "Trained batch 5461 batch loss 5.49088097 epoch total loss 5.88775587\n",
      "Trained batch 5462 batch loss 5.81933498 epoch total loss 5.887743\n",
      "Trained batch 5463 batch loss 5.86271191 epoch total loss 5.8877387\n",
      "Trained batch 5464 batch loss 5.28962088 epoch total loss 5.88762903\n",
      "Trained batch 5465 batch loss 5.73598909 epoch total loss 5.88760138\n",
      "Trained batch 5466 batch loss 5.66979265 epoch total loss 5.8875618\n",
      "Trained batch 5467 batch loss 6.08450699 epoch total loss 5.88759756\n",
      "Trained batch 5468 batch loss 5.60471439 epoch total loss 5.88754606\n",
      "Trained batch 5469 batch loss 5.98088169 epoch total loss 5.88756323\n",
      "Trained batch 5470 batch loss 6.42209721 epoch total loss 5.8876605\n",
      "Trained batch 5471 batch loss 6.39329815 epoch total loss 5.88775301\n",
      "Trained batch 5472 batch loss 6.29914761 epoch total loss 5.88782787\n",
      "Trained batch 5473 batch loss 6.05633879 epoch total loss 5.88785887\n",
      "Trained batch 5474 batch loss 6.3798151 epoch total loss 5.88794851\n",
      "Trained batch 5475 batch loss 6.07281685 epoch total loss 5.88798237\n",
      "Trained batch 5476 batch loss 6.12475204 epoch total loss 5.88802576\n",
      "Trained batch 5477 batch loss 5.37723732 epoch total loss 5.8879323\n",
      "Trained batch 5478 batch loss 5.89402771 epoch total loss 5.88793325\n",
      "Trained batch 5479 batch loss 5.930058 epoch total loss 5.88794088\n",
      "Trained batch 5480 batch loss 5.76726151 epoch total loss 5.88791895\n",
      "Trained batch 5481 batch loss 5.96519661 epoch total loss 5.88793325\n",
      "Trained batch 5482 batch loss 5.52620411 epoch total loss 5.88786697\n",
      "Trained batch 5483 batch loss 5.90611172 epoch total loss 5.88787031\n",
      "Trained batch 5484 batch loss 5.69936848 epoch total loss 5.88783598\n",
      "Trained batch 5485 batch loss 5.58101463 epoch total loss 5.88777971\n",
      "Trained batch 5486 batch loss 5.81118059 epoch total loss 5.88776588\n",
      "Trained batch 5487 batch loss 5.50152111 epoch total loss 5.88769531\n",
      "Trained batch 5488 batch loss 5.99656 epoch total loss 5.88771534\n",
      "Trained batch 5489 batch loss 5.49652529 epoch total loss 5.88764381\n",
      "Trained batch 5490 batch loss 5.44655275 epoch total loss 5.88756371\n",
      "Trained batch 5491 batch loss 5.73122597 epoch total loss 5.8875351\n",
      "Trained batch 5492 batch loss 6.02182484 epoch total loss 5.88755941\n",
      "Trained batch 5493 batch loss 5.83849812 epoch total loss 5.88755035\n",
      "Trained batch 5494 batch loss 4.70136356 epoch total loss 5.88733435\n",
      "Trained batch 5495 batch loss 5.78224 epoch total loss 5.88731575\n",
      "Trained batch 5496 batch loss 5.93667507 epoch total loss 5.88732481\n",
      "Trained batch 5497 batch loss 5.80266666 epoch total loss 5.88730907\n",
      "Trained batch 5498 batch loss 6.26715851 epoch total loss 5.88737822\n",
      "Trained batch 5499 batch loss 5.40278816 epoch total loss 5.88729\n",
      "Trained batch 5500 batch loss 5.65411 epoch total loss 5.88724804\n",
      "Trained batch 5501 batch loss 5.71977711 epoch total loss 5.88721752\n",
      "Trained batch 5502 batch loss 5.87998629 epoch total loss 5.88721657\n",
      "Trained batch 5503 batch loss 5.60327148 epoch total loss 5.88716507\n",
      "Trained batch 5504 batch loss 5.50436497 epoch total loss 5.88709545\n",
      "Trained batch 5505 batch loss 5.99254131 epoch total loss 5.88711452\n",
      "Trained batch 5506 batch loss 5.11141396 epoch total loss 5.88697338\n",
      "Trained batch 5507 batch loss 5.22256374 epoch total loss 5.88685274\n",
      "Trained batch 5508 batch loss 4.78129387 epoch total loss 5.88665199\n",
      "Trained batch 5509 batch loss 5.8911705 epoch total loss 5.88665295\n",
      "Trained batch 5510 batch loss 6.25087452 epoch total loss 5.88671875\n",
      "Trained batch 5511 batch loss 5.6180439 epoch total loss 5.88666964\n",
      "Trained batch 5512 batch loss 6.03750944 epoch total loss 5.88669729\n",
      "Trained batch 5513 batch loss 5.51791477 epoch total loss 5.88663\n",
      "Trained batch 5514 batch loss 5.13216829 epoch total loss 5.88649368\n",
      "Trained batch 5515 batch loss 5.72980261 epoch total loss 5.88646507\n",
      "Trained batch 5516 batch loss 6.22045612 epoch total loss 5.88652563\n",
      "Trained batch 5517 batch loss 5.86449575 epoch total loss 5.88652182\n",
      "Trained batch 5518 batch loss 6.07212925 epoch total loss 5.88655567\n",
      "Trained batch 5519 batch loss 5.8351388 epoch total loss 5.88654661\n",
      "Trained batch 5520 batch loss 5.95934 epoch total loss 5.88655949\n",
      "Trained batch 5521 batch loss 5.88511372 epoch total loss 5.88655901\n",
      "Trained batch 5522 batch loss 5.98058319 epoch total loss 5.88657618\n",
      "Trained batch 5523 batch loss 6.38158512 epoch total loss 5.88666582\n",
      "Trained batch 5524 batch loss 5.98835754 epoch total loss 5.88668394\n",
      "Trained batch 5525 batch loss 5.55518913 epoch total loss 5.88662386\n",
      "Trained batch 5526 batch loss 5.49416637 epoch total loss 5.88655281\n",
      "Trained batch 5527 batch loss 5.83833504 epoch total loss 5.88654423\n",
      "Trained batch 5528 batch loss 5.98456049 epoch total loss 5.88656187\n",
      "Trained batch 5529 batch loss 6.18196 epoch total loss 5.88661528\n",
      "Trained batch 5530 batch loss 6.25356102 epoch total loss 5.88668156\n",
      "Trained batch 5531 batch loss 5.9780755 epoch total loss 5.88669825\n",
      "Trained batch 5532 batch loss 6.17618036 epoch total loss 5.8867507\n",
      "Trained batch 5533 batch loss 5.51502085 epoch total loss 5.88668346\n",
      "Trained batch 5534 batch loss 5.71885586 epoch total loss 5.88665295\n",
      "Trained batch 5535 batch loss 5.15766668 epoch total loss 5.88652134\n",
      "Trained batch 5536 batch loss 5.20136738 epoch total loss 5.88639784\n",
      "Trained batch 5537 batch loss 5.29443264 epoch total loss 5.88629103\n",
      "Trained batch 5538 batch loss 7.22796869 epoch total loss 5.88653326\n",
      "Trained batch 5539 batch loss 6.38296366 epoch total loss 5.88662291\n",
      "Trained batch 5540 batch loss 5.39621639 epoch total loss 5.88653421\n",
      "Trained batch 5541 batch loss 5.76155233 epoch total loss 5.8865118\n",
      "Trained batch 5542 batch loss 6.07731104 epoch total loss 5.88654661\n",
      "Trained batch 5543 batch loss 5.3496542 epoch total loss 5.88644934\n",
      "Trained batch 5544 batch loss 5.95864582 epoch total loss 5.88646269\n",
      "Trained batch 5545 batch loss 4.90672398 epoch total loss 5.88628578\n",
      "Trained batch 5546 batch loss 5.88914299 epoch total loss 5.88628626\n",
      "Trained batch 5547 batch loss 5.87304783 epoch total loss 5.88628387\n",
      "Trained batch 5548 batch loss 6.73862553 epoch total loss 5.88643742\n",
      "Trained batch 5549 batch loss 6.18752527 epoch total loss 5.88649178\n",
      "Trained batch 5550 batch loss 5.03086424 epoch total loss 5.88633776\n",
      "Trained batch 5551 batch loss 6.47958088 epoch total loss 5.88644457\n",
      "Trained batch 5552 batch loss 6.4982214 epoch total loss 5.88655472\n",
      "Trained batch 5553 batch loss 7.07807446 epoch total loss 5.88676929\n",
      "Trained batch 5554 batch loss 7.6503 epoch total loss 5.88708687\n",
      "Trained batch 5555 batch loss 7.34633446 epoch total loss 5.88734961\n",
      "Trained batch 5556 batch loss 6.62849045 epoch total loss 5.88748312\n",
      "Trained batch 5557 batch loss 7.15484524 epoch total loss 5.88771105\n",
      "Trained batch 5558 batch loss 7.1653614 epoch total loss 5.88794088\n",
      "Trained batch 5559 batch loss 6.53732347 epoch total loss 5.88805771\n",
      "Trained batch 5560 batch loss 7.03390503 epoch total loss 5.8882637\n",
      "Trained batch 5561 batch loss 7.03817 epoch total loss 5.88847065\n",
      "Trained batch 5562 batch loss 7.25037193 epoch total loss 5.88871527\n",
      "Trained batch 5563 batch loss 6.68698025 epoch total loss 5.8888588\n",
      "Trained batch 5564 batch loss 6.91883612 epoch total loss 5.88904381\n",
      "Trained batch 5565 batch loss 6.33433723 epoch total loss 5.88912439\n",
      "Trained batch 5566 batch loss 7.16267395 epoch total loss 5.88935328\n",
      "Trained batch 5567 batch loss 6.58464241 epoch total loss 5.88947868\n",
      "Trained batch 5568 batch loss 6.46365929 epoch total loss 5.88958168\n",
      "Trained batch 5569 batch loss 6.67151546 epoch total loss 5.88972235\n",
      "Trained batch 5570 batch loss 5.87339449 epoch total loss 5.88971949\n",
      "Trained batch 5571 batch loss 6.81304169 epoch total loss 5.88988543\n",
      "Trained batch 5572 batch loss 6.53039312 epoch total loss 5.89000034\n",
      "Trained batch 5573 batch loss 6.06936169 epoch total loss 5.89003277\n",
      "Trained batch 5574 batch loss 5.74222231 epoch total loss 5.89000607\n",
      "Trained batch 5575 batch loss 4.49823666 epoch total loss 5.88975668\n",
      "Trained batch 5576 batch loss 5.14218616 epoch total loss 5.88962269\n",
      "Trained batch 5577 batch loss 5.10558796 epoch total loss 5.88948202\n",
      "Trained batch 5578 batch loss 5.2072587 epoch total loss 5.88935947\n",
      "Trained batch 5579 batch loss 4.83775949 epoch total loss 5.88917065\n",
      "Trained batch 5580 batch loss 5.38689709 epoch total loss 5.88908052\n",
      "Trained batch 5581 batch loss 6.11245441 epoch total loss 5.88912106\n",
      "Trained batch 5582 batch loss 5.80819368 epoch total loss 5.88910627\n",
      "Trained batch 5583 batch loss 5.75601721 epoch total loss 5.88908291\n",
      "Trained batch 5584 batch loss 5.92473125 epoch total loss 5.88908958\n",
      "Trained batch 5585 batch loss 6.1472187 epoch total loss 5.88913584\n",
      "Trained batch 5586 batch loss 6.20128918 epoch total loss 5.8891921\n",
      "Trained batch 5587 batch loss 5.89376783 epoch total loss 5.88919306\n",
      "Trained batch 5588 batch loss 5.69590282 epoch total loss 5.88915825\n",
      "Trained batch 5589 batch loss 5.71075535 epoch total loss 5.8891263\n",
      "Trained batch 5590 batch loss 5.88360262 epoch total loss 5.88912535\n",
      "Trained batch 5591 batch loss 5.61193085 epoch total loss 5.88907623\n",
      "Trained batch 5592 batch loss 5.42628765 epoch total loss 5.88899326\n",
      "Trained batch 5593 batch loss 5.73628569 epoch total loss 5.88896561\n",
      "Trained batch 5594 batch loss 6.04335451 epoch total loss 5.88899326\n",
      "Trained batch 5595 batch loss 5.40041637 epoch total loss 5.888906\n",
      "Trained batch 5596 batch loss 5.9644146 epoch total loss 5.88892\n",
      "Trained batch 5597 batch loss 5.71879 epoch total loss 5.88888931\n",
      "Trained batch 5598 batch loss 6.04957151 epoch total loss 5.8889184\n",
      "Trained batch 5599 batch loss 5.72366047 epoch total loss 5.88888836\n",
      "Trained batch 5600 batch loss 7.00597286 epoch total loss 5.88908815\n",
      "Trained batch 5601 batch loss 5.64873791 epoch total loss 5.88904524\n",
      "Trained batch 5602 batch loss 5.94713688 epoch total loss 5.88905525\n",
      "Trained batch 5603 batch loss 5.47150803 epoch total loss 5.88898087\n",
      "Trained batch 5604 batch loss 5.76279259 epoch total loss 5.88895845\n",
      "Trained batch 5605 batch loss 5.50362682 epoch total loss 5.88889\n",
      "Trained batch 5606 batch loss 5.45398808 epoch total loss 5.88881207\n",
      "Trained batch 5607 batch loss 5.79865265 epoch total loss 5.88879538\n",
      "Trained batch 5608 batch loss 6.07095432 epoch total loss 5.8888278\n",
      "Trained batch 5609 batch loss 6.16444874 epoch total loss 5.88887691\n",
      "Trained batch 5610 batch loss 5.44914198 epoch total loss 5.88879871\n",
      "Trained batch 5611 batch loss 5.55768394 epoch total loss 5.88873959\n",
      "Trained batch 5612 batch loss 4.9876852 epoch total loss 5.88857937\n",
      "Trained batch 5613 batch loss 4.69199944 epoch total loss 5.88836622\n",
      "Trained batch 5614 batch loss 6.20337248 epoch total loss 5.88842201\n",
      "Trained batch 5615 batch loss 6.03588676 epoch total loss 5.88844824\n",
      "Trained batch 5616 batch loss 6.08478546 epoch total loss 5.88848352\n",
      "Trained batch 5617 batch loss 5.54978895 epoch total loss 5.88842344\n",
      "Trained batch 5618 batch loss 5.8717494 epoch total loss 5.88842\n",
      "Trained batch 5619 batch loss 5.48674583 epoch total loss 5.88834906\n",
      "Trained batch 5620 batch loss 6.00664139 epoch total loss 5.88837\n",
      "Trained batch 5621 batch loss 5.65035057 epoch total loss 5.8883276\n",
      "Trained batch 5622 batch loss 5.47163296 epoch total loss 5.88825369\n",
      "Trained batch 5623 batch loss 6.30497837 epoch total loss 5.8883276\n",
      "Trained batch 5624 batch loss 5.38725519 epoch total loss 5.88823843\n",
      "Trained batch 5625 batch loss 5.82031536 epoch total loss 5.88822651\n",
      "Trained batch 5626 batch loss 5.41897869 epoch total loss 5.88814259\n",
      "Trained batch 5627 batch loss 5.10844469 epoch total loss 5.8880043\n",
      "Trained batch 5628 batch loss 4.96098948 epoch total loss 5.88784\n",
      "Trained batch 5629 batch loss 6.55017519 epoch total loss 5.88795757\n",
      "Trained batch 5630 batch loss 6.24526691 epoch total loss 5.88802099\n",
      "Trained batch 5631 batch loss 6.84696341 epoch total loss 5.8881917\n",
      "Trained batch 5632 batch loss 6.51083946 epoch total loss 5.88830233\n",
      "Trained batch 5633 batch loss 6.965415 epoch total loss 5.88849354\n",
      "Trained batch 5634 batch loss 6.64778519 epoch total loss 5.88862801\n",
      "Trained batch 5635 batch loss 6.31227303 epoch total loss 5.88870335\n",
      "Trained batch 5636 batch loss 6.52613211 epoch total loss 5.88881683\n",
      "Trained batch 5637 batch loss 6.19245148 epoch total loss 5.88887024\n",
      "Trained batch 5638 batch loss 5.84532309 epoch total loss 5.88886261\n",
      "Trained batch 5639 batch loss 6.58858252 epoch total loss 5.88898659\n",
      "Trained batch 5640 batch loss 6.3357029 epoch total loss 5.88906574\n",
      "Trained batch 5641 batch loss 6.57918262 epoch total loss 5.88918829\n",
      "Trained batch 5642 batch loss 6.1716814 epoch total loss 5.88923836\n",
      "Trained batch 5643 batch loss 6.14698076 epoch total loss 5.88928413\n",
      "Trained batch 5644 batch loss 6.41183329 epoch total loss 5.88937664\n",
      "Trained batch 5645 batch loss 6.58585072 epoch total loss 5.88949966\n",
      "Trained batch 5646 batch loss 6.16357374 epoch total loss 5.8895483\n",
      "Trained batch 5647 batch loss 6.67499065 epoch total loss 5.88968754\n",
      "Trained batch 5648 batch loss 7.18575954 epoch total loss 5.88991737\n",
      "Trained batch 5649 batch loss 6.06977224 epoch total loss 5.88994932\n",
      "Trained batch 5650 batch loss 6.30458927 epoch total loss 5.89002275\n",
      "Trained batch 5651 batch loss 5.79430103 epoch total loss 5.89000559\n",
      "Trained batch 5652 batch loss 6.10014057 epoch total loss 5.89004326\n",
      "Trained batch 5653 batch loss 6.03897858 epoch total loss 5.89006948\n",
      "Trained batch 5654 batch loss 5.39450216 epoch total loss 5.88998175\n",
      "Trained batch 5655 batch loss 5.83055496 epoch total loss 5.88997173\n",
      "Trained batch 5656 batch loss 5.23454523 epoch total loss 5.88985586\n",
      "Trained batch 5657 batch loss 5.97830486 epoch total loss 5.88987112\n",
      "Trained batch 5658 batch loss 5.79193306 epoch total loss 5.88985395\n",
      "Trained batch 5659 batch loss 5.94146442 epoch total loss 5.88986301\n",
      "Trained batch 5660 batch loss 5.55187607 epoch total loss 5.88980293\n",
      "Trained batch 5661 batch loss 5.87779474 epoch total loss 5.88980103\n",
      "Trained batch 5662 batch loss 5.87693787 epoch total loss 5.88979864\n",
      "Trained batch 5663 batch loss 5.43283653 epoch total loss 5.88971806\n",
      "Trained batch 5664 batch loss 5.7510376 epoch total loss 5.88969326\n",
      "Trained batch 5665 batch loss 6.19883347 epoch total loss 5.8897481\n",
      "Trained batch 5666 batch loss 5.85107517 epoch total loss 5.88974094\n",
      "Trained batch 5667 batch loss 5.92803192 epoch total loss 5.8897481\n",
      "Trained batch 5668 batch loss 5.90305662 epoch total loss 5.88975048\n",
      "Trained batch 5669 batch loss 4.57724142 epoch total loss 5.88951921\n",
      "Trained batch 5670 batch loss 5.4157095 epoch total loss 5.88943529\n",
      "Trained batch 5671 batch loss 5.85375071 epoch total loss 5.88942909\n",
      "Trained batch 5672 batch loss 5.4620285 epoch total loss 5.88935375\n",
      "Trained batch 5673 batch loss 6.68533325 epoch total loss 5.88949347\n",
      "Trained batch 5674 batch loss 5.87469101 epoch total loss 5.88949108\n",
      "Trained batch 5675 batch loss 5.17336655 epoch total loss 5.88936472\n",
      "Trained batch 5676 batch loss 5.55844307 epoch total loss 5.88930655\n",
      "Trained batch 5677 batch loss 6.2709 epoch total loss 5.8893733\n",
      "Trained batch 5678 batch loss 5.50542116 epoch total loss 5.88930559\n",
      "Trained batch 5679 batch loss 6.39172363 epoch total loss 5.88939381\n",
      "Trained batch 5680 batch loss 6.1288147 epoch total loss 5.88943577\n",
      "Trained batch 5681 batch loss 6.55465698 epoch total loss 5.88955307\n",
      "Trained batch 5682 batch loss 5.73976898 epoch total loss 5.88952637\n",
      "Trained batch 5683 batch loss 6.05753 epoch total loss 5.88955593\n",
      "Trained batch 5684 batch loss 5.6408205 epoch total loss 5.88951254\n",
      "Trained batch 5685 batch loss 6.30921221 epoch total loss 5.88958597\n",
      "Trained batch 5686 batch loss 6.15750504 epoch total loss 5.88963318\n",
      "Trained batch 5687 batch loss 5.64276648 epoch total loss 5.88959\n",
      "Trained batch 5688 batch loss 5.77619648 epoch total loss 5.88957\n",
      "Trained batch 5689 batch loss 5.85292101 epoch total loss 5.88956356\n",
      "Trained batch 5690 batch loss 5.52822208 epoch total loss 5.88949966\n",
      "Trained batch 5691 batch loss 6.23542404 epoch total loss 5.88956\n",
      "Trained batch 5692 batch loss 4.72008324 epoch total loss 5.88935471\n",
      "Trained batch 5693 batch loss 6.2494936 epoch total loss 5.88941813\n",
      "Trained batch 5694 batch loss 6.24816036 epoch total loss 5.88948154\n",
      "Trained batch 5695 batch loss 5.20121479 epoch total loss 5.8893609\n",
      "Trained batch 5696 batch loss 5.63855791 epoch total loss 5.88931656\n",
      "Trained batch 5697 batch loss 5.15542316 epoch total loss 5.88918781\n",
      "Trained batch 5698 batch loss 5.77673435 epoch total loss 5.88916826\n",
      "Trained batch 5699 batch loss 5.2047863 epoch total loss 5.88904762\n",
      "Trained batch 5700 batch loss 5.80122709 epoch total loss 5.88903236\n",
      "Trained batch 5701 batch loss 5.97203207 epoch total loss 5.88904715\n",
      "Trained batch 5702 batch loss 5.71623707 epoch total loss 5.88901663\n",
      "Trained batch 5703 batch loss 5.10289526 epoch total loss 5.88887835\n",
      "Trained batch 5704 batch loss 5.67967081 epoch total loss 5.88884163\n",
      "Trained batch 5705 batch loss 5.6726594 epoch total loss 5.88880348\n",
      "Trained batch 5706 batch loss 5.89872217 epoch total loss 5.88880539\n",
      "Trained batch 5707 batch loss 5.56954527 epoch total loss 5.8887496\n",
      "Trained batch 5708 batch loss 5.83382416 epoch total loss 5.88873959\n",
      "Trained batch 5709 batch loss 5.87205267 epoch total loss 5.88873672\n",
      "Trained batch 5710 batch loss 5.26473713 epoch total loss 5.88862753\n",
      "Trained batch 5711 batch loss 6.21232033 epoch total loss 5.8886838\n",
      "Trained batch 5712 batch loss 5.66154099 epoch total loss 5.88864374\n",
      "Trained batch 5713 batch loss 5.34854698 epoch total loss 5.88854933\n",
      "Trained batch 5714 batch loss 5.90492201 epoch total loss 5.88855219\n",
      "Trained batch 5715 batch loss 5.60659695 epoch total loss 5.8885026\n",
      "Trained batch 5716 batch loss 5.1191473 epoch total loss 5.88836861\n",
      "Trained batch 5717 batch loss 5.67813778 epoch total loss 5.88833189\n",
      "Trained batch 5718 batch loss 5.68982315 epoch total loss 5.88829756\n",
      "Trained batch 5719 batch loss 4.08673191 epoch total loss 5.88798237\n",
      "Trained batch 5720 batch loss 6.3884511 epoch total loss 5.88806963\n",
      "Trained batch 5721 batch loss 6.17580318 epoch total loss 5.8881197\n",
      "Trained batch 5722 batch loss 5.05215311 epoch total loss 5.88797331\n",
      "Trained batch 5723 batch loss 6.2414813 epoch total loss 5.8880353\n",
      "Trained batch 5724 batch loss 4.6619463 epoch total loss 5.88782072\n",
      "Trained batch 5725 batch loss 4.87105 epoch total loss 5.88764334\n",
      "Trained batch 5726 batch loss 6.12769699 epoch total loss 5.8876853\n",
      "Trained batch 5727 batch loss 4.63322353 epoch total loss 5.88746643\n",
      "Trained batch 5728 batch loss 6.1704812 epoch total loss 5.88751602\n",
      "Trained batch 5729 batch loss 5.92716885 epoch total loss 5.8875227\n",
      "Trained batch 5730 batch loss 6.12429047 epoch total loss 5.88756418\n",
      "Trained batch 5731 batch loss 6.31968975 epoch total loss 5.88763952\n",
      "Trained batch 5732 batch loss 6.17409134 epoch total loss 5.88769\n",
      "Trained batch 5733 batch loss 6.41476059 epoch total loss 5.88778162\n",
      "Trained batch 5734 batch loss 5.38964319 epoch total loss 5.88769484\n",
      "Trained batch 5735 batch loss 4.95611668 epoch total loss 5.88753271\n",
      "Trained batch 5736 batch loss 5.29882717 epoch total loss 5.88742971\n",
      "Trained batch 5737 batch loss 4.13150215 epoch total loss 5.88712406\n",
      "Trained batch 5738 batch loss 4.5259409 epoch total loss 5.88688707\n",
      "Trained batch 5739 batch loss 6.13841248 epoch total loss 5.88693047\n",
      "Trained batch 5740 batch loss 5.21736097 epoch total loss 5.88681412\n",
      "Trained batch 5741 batch loss 5.75597 epoch total loss 5.88679171\n",
      "Trained batch 5742 batch loss 5.82328796 epoch total loss 5.88678074\n",
      "Trained batch 5743 batch loss 4.80314875 epoch total loss 5.88659239\n",
      "Trained batch 5744 batch loss 6.90201759 epoch total loss 5.88676929\n",
      "Trained batch 5745 batch loss 5.45266867 epoch total loss 5.88669348\n",
      "Trained batch 5746 batch loss 4.80123472 epoch total loss 5.88650465\n",
      "Trained batch 5747 batch loss 5.25276041 epoch total loss 5.8863945\n",
      "Trained batch 5748 batch loss 6.81324291 epoch total loss 5.88655567\n",
      "Trained batch 5749 batch loss 6.82471561 epoch total loss 5.88671875\n",
      "Trained batch 5750 batch loss 6.90754938 epoch total loss 5.88689613\n",
      "Trained batch 5751 batch loss 5.16903877 epoch total loss 5.8867712\n",
      "Trained batch 5752 batch loss 6.33438778 epoch total loss 5.88684893\n",
      "Trained batch 5753 batch loss 5.78577709 epoch total loss 5.88683128\n",
      "Trained batch 5754 batch loss 6.6996 epoch total loss 5.88697243\n",
      "Trained batch 5755 batch loss 6.30150175 epoch total loss 5.88704443\n",
      "Trained batch 5756 batch loss 5.96711159 epoch total loss 5.88705873\n",
      "Trained batch 5757 batch loss 5.65645695 epoch total loss 5.88701868\n",
      "Trained batch 5758 batch loss 4.69848967 epoch total loss 5.88681221\n",
      "Trained batch 5759 batch loss 5.53928566 epoch total loss 5.88675213\n",
      "Trained batch 5760 batch loss 4.69168711 epoch total loss 5.88654423\n",
      "Trained batch 5761 batch loss 4.17207623 epoch total loss 5.88624668\n",
      "Trained batch 5762 batch loss 5.96150351 epoch total loss 5.88625956\n",
      "Trained batch 5763 batch loss 6.19842148 epoch total loss 5.88631392\n",
      "Trained batch 5764 batch loss 5.54648733 epoch total loss 5.88625526\n",
      "Trained batch 5765 batch loss 5.69793558 epoch total loss 5.88622284\n",
      "Trained batch 5766 batch loss 4.37844324 epoch total loss 5.88596153\n",
      "Trained batch 5767 batch loss 6.0971 epoch total loss 5.88599825\n",
      "Trained batch 5768 batch loss 5.76415825 epoch total loss 5.88597727\n",
      "Trained batch 5769 batch loss 5.8866806 epoch total loss 5.88597727\n",
      "Trained batch 5770 batch loss 5.88837147 epoch total loss 5.88597727\n",
      "Trained batch 5771 batch loss 5.83039618 epoch total loss 5.88596821\n",
      "Trained batch 5772 batch loss 5.22304678 epoch total loss 5.88585329\n",
      "Trained batch 5773 batch loss 5.99657297 epoch total loss 5.88587236\n",
      "Trained batch 5774 batch loss 5.43614483 epoch total loss 5.88579464\n",
      "Trained batch 5775 batch loss 5.74302 epoch total loss 5.88577\n",
      "Trained batch 5776 batch loss 5.97496557 epoch total loss 5.88578558\n",
      "Trained batch 5777 batch loss 6.26876831 epoch total loss 5.88585186\n",
      "Trained batch 5778 batch loss 5.98231602 epoch total loss 5.88586807\n",
      "Trained batch 5779 batch loss 6.0419569 epoch total loss 5.88589525\n",
      "Trained batch 5780 batch loss 5.73388767 epoch total loss 5.88586903\n",
      "Trained batch 5781 batch loss 5.97711134 epoch total loss 5.88588476\n",
      "Trained batch 5782 batch loss 5.59452868 epoch total loss 5.88583422\n",
      "Trained batch 5783 batch loss 5.71303749 epoch total loss 5.88580465\n",
      "Trained batch 5784 batch loss 4.89890432 epoch total loss 5.88563395\n",
      "Trained batch 5785 batch loss 5.83812904 epoch total loss 5.88562632\n",
      "Trained batch 5786 batch loss 6.69354725 epoch total loss 5.88576603\n",
      "Trained batch 5787 batch loss 6.66228676 epoch total loss 5.8859005\n",
      "Trained batch 5788 batch loss 6.1362896 epoch total loss 5.88594389\n",
      "Trained batch 5789 batch loss 6.00275517 epoch total loss 5.88596439\n",
      "Trained batch 5790 batch loss 6.06537 epoch total loss 5.88599539\n",
      "Trained batch 5791 batch loss 5.99885893 epoch total loss 5.88601542\n",
      "Trained batch 5792 batch loss 6.30630875 epoch total loss 5.88608742\n",
      "Trained batch 5793 batch loss 6.61911106 epoch total loss 5.88621378\n",
      "Trained batch 5794 batch loss 6.58830786 epoch total loss 5.88633537\n",
      "Trained batch 5795 batch loss 6.04082584 epoch total loss 5.8863616\n",
      "Trained batch 5796 batch loss 6.16703129 epoch total loss 5.88641\n",
      "Trained batch 5797 batch loss 5.90104723 epoch total loss 5.88641262\n",
      "Trained batch 5798 batch loss 6.53941822 epoch total loss 5.88652515\n",
      "Trained batch 5799 batch loss 4.35478258 epoch total loss 5.88626146\n",
      "Trained batch 5800 batch loss 4.11751556 epoch total loss 5.88595629\n",
      "Trained batch 5801 batch loss 4.5009346 epoch total loss 5.88571739\n",
      "Trained batch 5802 batch loss 4.696661 epoch total loss 5.88551235\n",
      "Trained batch 5803 batch loss 4.7617321 epoch total loss 5.88531876\n",
      "Trained batch 5804 batch loss 4.66884708 epoch total loss 5.88510895\n",
      "Trained batch 5805 batch loss 4.61524773 epoch total loss 5.88489056\n",
      "Trained batch 5806 batch loss 4.05667686 epoch total loss 5.88457584\n",
      "Trained batch 5807 batch loss 4.34750271 epoch total loss 5.8843112\n",
      "Trained batch 5808 batch loss 4.98427439 epoch total loss 5.88415623\n",
      "Trained batch 5809 batch loss 4.59623528 epoch total loss 5.88393497\n",
      "Trained batch 5810 batch loss 5.42173 epoch total loss 5.88385534\n",
      "Trained batch 5811 batch loss 3.97986913 epoch total loss 5.88352776\n",
      "Trained batch 5812 batch loss 4.2554884 epoch total loss 5.88324738\n",
      "Trained batch 5813 batch loss 4.43320751 epoch total loss 5.88299799\n",
      "Trained batch 5814 batch loss 3.96166563 epoch total loss 5.88266754\n",
      "Trained batch 5815 batch loss 4.58572531 epoch total loss 5.88244438\n",
      "Trained batch 5816 batch loss 4.57215834 epoch total loss 5.88221884\n",
      "Trained batch 5817 batch loss 5.42639542 epoch total loss 5.88214\n",
      "Trained batch 5818 batch loss 6.82757473 epoch total loss 5.88230276\n",
      "Trained batch 5819 batch loss 6.13278151 epoch total loss 5.88234615\n",
      "Trained batch 5820 batch loss 5.75721884 epoch total loss 5.8823247\n",
      "Trained batch 5821 batch loss 5.89574051 epoch total loss 5.8823266\n",
      "Trained batch 5822 batch loss 4.57202816 epoch total loss 5.88210106\n",
      "Trained batch 5823 batch loss 4.44518375 epoch total loss 5.88185453\n",
      "Trained batch 5824 batch loss 5.75884342 epoch total loss 5.88183308\n",
      "Trained batch 5825 batch loss 6.11604357 epoch total loss 5.88187361\n",
      "Trained batch 5826 batch loss 5.34265184 epoch total loss 5.8817811\n",
      "Trained batch 5827 batch loss 6.56209373 epoch total loss 5.88189793\n",
      "Trained batch 5828 batch loss 5.77780104 epoch total loss 5.88188028\n",
      "Trained batch 5829 batch loss 5.89706278 epoch total loss 5.88188314\n",
      "Trained batch 5830 batch loss 5.52401066 epoch total loss 5.88182163\n",
      "Trained batch 5831 batch loss 5.29502106 epoch total loss 5.88172102\n",
      "Trained batch 5832 batch loss 5.81590557 epoch total loss 5.88171\n",
      "Trained batch 5833 batch loss 5.41867256 epoch total loss 5.88163042\n",
      "Trained batch 5834 batch loss 5.61384583 epoch total loss 5.88158464\n",
      "Trained batch 5835 batch loss 5.07548904 epoch total loss 5.88144636\n",
      "Trained batch 5836 batch loss 5.29737186 epoch total loss 5.88134575\n",
      "Trained batch 5837 batch loss 4.92396784 epoch total loss 5.88118219\n",
      "Trained batch 5838 batch loss 5.84340096 epoch total loss 5.88117599\n",
      "Trained batch 5839 batch loss 5.40212536 epoch total loss 5.88109398\n",
      "Trained batch 5840 batch loss 5.10124493 epoch total loss 5.88096046\n",
      "Trained batch 5841 batch loss 5.59126377 epoch total loss 5.8809104\n",
      "Trained batch 5842 batch loss 5.44716454 epoch total loss 5.88083601\n",
      "Trained batch 5843 batch loss 5.67648411 epoch total loss 5.88080072\n",
      "Trained batch 5844 batch loss 5.95890045 epoch total loss 5.88081408\n",
      "Trained batch 5845 batch loss 6.41540146 epoch total loss 5.88090515\n",
      "Trained batch 5846 batch loss 5.2102952 epoch total loss 5.88079071\n",
      "Trained batch 5847 batch loss 5.67709637 epoch total loss 5.88075542\n",
      "Trained batch 5848 batch loss 5.79215479 epoch total loss 5.88074064\n",
      "Trained batch 5849 batch loss 5.75874805 epoch total loss 5.88071966\n",
      "Trained batch 5850 batch loss 5.18792963 epoch total loss 5.88060093\n",
      "Trained batch 5851 batch loss 7.22540569 epoch total loss 5.88083076\n",
      "Trained batch 5852 batch loss 6.11684608 epoch total loss 5.8808713\n",
      "Trained batch 5853 batch loss 6.15786934 epoch total loss 5.8809185\n",
      "Trained batch 5854 batch loss 5.34476185 epoch total loss 5.88082647\n",
      "Trained batch 5855 batch loss 5.72594118 epoch total loss 5.88080025\n",
      "Trained batch 5856 batch loss 5.46326828 epoch total loss 5.8807292\n",
      "Trained batch 5857 batch loss 6.21028948 epoch total loss 5.88078547\n",
      "Trained batch 5858 batch loss 6.66804361 epoch total loss 5.88092\n",
      "Trained batch 5859 batch loss 6.10284424 epoch total loss 5.8809576\n",
      "Trained batch 5860 batch loss 5.37892914 epoch total loss 5.88087225\n",
      "Trained batch 5861 batch loss 4.35999 epoch total loss 5.88061237\n",
      "Trained batch 5862 batch loss 5.59112167 epoch total loss 5.88056278\n",
      "Trained batch 5863 batch loss 5.96688175 epoch total loss 5.88057804\n",
      "Trained batch 5864 batch loss 6.61972809 epoch total loss 5.88070393\n",
      "Trained batch 5865 batch loss 5.88046026 epoch total loss 5.88070393\n",
      "Trained batch 5866 batch loss 6.04644728 epoch total loss 5.88073206\n",
      "Trained batch 5867 batch loss 6.38957834 epoch total loss 5.88081932\n",
      "Trained batch 5868 batch loss 6.26482201 epoch total loss 5.88088465\n",
      "Trained batch 5869 batch loss 5.93193245 epoch total loss 5.88089371\n",
      "Trained batch 5870 batch loss 5.33386183 epoch total loss 5.88080025\n",
      "Trained batch 5871 batch loss 5.99981499 epoch total loss 5.88082027\n",
      "Trained batch 5872 batch loss 5.17179775 epoch total loss 5.88069963\n",
      "Trained batch 5873 batch loss 6.03254795 epoch total loss 5.88072538\n",
      "Trained batch 5874 batch loss 5.04178286 epoch total loss 5.88058281\n",
      "Trained batch 5875 batch loss 5.91902256 epoch total loss 5.88058901\n",
      "Trained batch 5876 batch loss 6.44472456 epoch total loss 5.88068533\n",
      "Trained batch 5877 batch loss 5.38927364 epoch total loss 5.88060188\n",
      "Trained batch 5878 batch loss 5.90383863 epoch total loss 5.8806057\n",
      "Trained batch 5879 batch loss 5.08916378 epoch total loss 5.88047123\n",
      "Trained batch 5880 batch loss 5.29915237 epoch total loss 5.88037252\n",
      "Trained batch 5881 batch loss 4.8599658 epoch total loss 5.88019896\n",
      "Trained batch 5882 batch loss 7.54169 epoch total loss 5.88048172\n",
      "Trained batch 5883 batch loss 7.8757019 epoch total loss 5.88082075\n",
      "Trained batch 5884 batch loss 6.79819059 epoch total loss 5.8809762\n",
      "Trained batch 5885 batch loss 6.89963245 epoch total loss 5.88114929\n",
      "Trained batch 5886 batch loss 7.0085516 epoch total loss 5.8813405\n",
      "Trained batch 5887 batch loss 7.12980223 epoch total loss 5.88155222\n",
      "Trained batch 5888 batch loss 6.92627573 epoch total loss 5.8817296\n",
      "Trained batch 5889 batch loss 6.75430965 epoch total loss 5.8818779\n",
      "Trained batch 5890 batch loss 6.73037148 epoch total loss 5.8820219\n",
      "Trained batch 5891 batch loss 6.60100174 epoch total loss 5.88214397\n",
      "Trained batch 5892 batch loss 6.40299129 epoch total loss 5.88223219\n",
      "Trained batch 5893 batch loss 6.12261 epoch total loss 5.88227272\n",
      "Trained batch 5894 batch loss 7.04700136 epoch total loss 5.88247061\n",
      "Trained batch 5895 batch loss 7.03640652 epoch total loss 5.88266611\n",
      "Trained batch 5896 batch loss 6.22012 epoch total loss 5.88272285\n",
      "Trained batch 5897 batch loss 6.78424358 epoch total loss 5.88287592\n",
      "Trained batch 5898 batch loss 6.88887024 epoch total loss 5.8830471\n",
      "Trained batch 5899 batch loss 6.70569181 epoch total loss 5.88318682\n",
      "Trained batch 5900 batch loss 7.05132484 epoch total loss 5.8833847\n",
      "Trained batch 5901 batch loss 5.823493 epoch total loss 5.88337469\n",
      "Trained batch 5902 batch loss 5.70221376 epoch total loss 5.88334417\n",
      "Trained batch 5903 batch loss 6.29705715 epoch total loss 5.88341379\n",
      "Trained batch 5904 batch loss 6.93963242 epoch total loss 5.88359308\n",
      "Trained batch 5905 batch loss 6.30420732 epoch total loss 5.88366461\n",
      "Trained batch 5906 batch loss 6.47622299 epoch total loss 5.88376474\n",
      "Trained batch 5907 batch loss 6.44288445 epoch total loss 5.88385916\n",
      "Trained batch 5908 batch loss 6.02913523 epoch total loss 5.88388348\n",
      "Trained batch 5909 batch loss 5.68887615 epoch total loss 5.88385057\n",
      "Trained batch 5910 batch loss 6.01883316 epoch total loss 5.88387346\n",
      "Trained batch 5911 batch loss 5.66926098 epoch total loss 5.88383675\n",
      "Trained batch 5912 batch loss 5.70325 epoch total loss 5.88380623\n",
      "Trained batch 5913 batch loss 5.69157696 epoch total loss 5.8837738\n",
      "Trained batch 5914 batch loss 5.18609524 epoch total loss 5.88365602\n",
      "Trained batch 5915 batch loss 4.59214783 epoch total loss 5.88343811\n",
      "Trained batch 5916 batch loss 4.99767 epoch total loss 5.88328791\n",
      "Trained batch 5917 batch loss 4.25859547 epoch total loss 5.88301325\n",
      "Trained batch 5918 batch loss 5.21139812 epoch total loss 5.88289976\n",
      "Trained batch 5919 batch loss 5.31194973 epoch total loss 5.88280344\n",
      "Trained batch 5920 batch loss 5.72065401 epoch total loss 5.88277578\n",
      "Trained batch 5921 batch loss 6.26768684 epoch total loss 5.88284063\n",
      "Trained batch 5922 batch loss 6.64075756 epoch total loss 5.8829689\n",
      "Trained batch 5923 batch loss 5.71459389 epoch total loss 5.88294029\n",
      "Trained batch 5924 batch loss 5.03325319 epoch total loss 5.88279724\n",
      "Trained batch 5925 batch loss 5.31483889 epoch total loss 5.88270187\n",
      "Trained batch 5926 batch loss 6.08828449 epoch total loss 5.88273668\n",
      "Trained batch 5927 batch loss 6.05561972 epoch total loss 5.88276577\n",
      "Trained batch 5928 batch loss 5.87067795 epoch total loss 5.88276386\n",
      "Trained batch 5929 batch loss 5.94572544 epoch total loss 5.88277435\n",
      "Trained batch 5930 batch loss 6.25208616 epoch total loss 5.88283682\n",
      "Trained batch 5931 batch loss 4.86064053 epoch total loss 5.8826642\n",
      "Trained batch 5932 batch loss 6.49076557 epoch total loss 5.8827672\n",
      "Trained batch 5933 batch loss 5.93043327 epoch total loss 5.88277483\n",
      "Trained batch 5934 batch loss 5.8257823 epoch total loss 5.88276529\n",
      "Trained batch 5935 batch loss 6.07883358 epoch total loss 5.88279819\n",
      "Trained batch 5936 batch loss 6.37236595 epoch total loss 5.88288\n",
      "Trained batch 5937 batch loss 6.18009377 epoch total loss 5.88293028\n",
      "Trained batch 5938 batch loss 4.99715519 epoch total loss 5.88278103\n",
      "Trained batch 5939 batch loss 6.03024721 epoch total loss 5.88280582\n",
      "Trained batch 5940 batch loss 5.66999054 epoch total loss 5.88277054\n",
      "Trained batch 5941 batch loss 5.66425514 epoch total loss 5.88273382\n",
      "Trained batch 5942 batch loss 5.9446249 epoch total loss 5.88274431\n",
      "Trained batch 5943 batch loss 4.9356966 epoch total loss 5.88258505\n",
      "Trained batch 5944 batch loss 5.57871 epoch total loss 5.88253403\n",
      "Trained batch 5945 batch loss 6.30455208 epoch total loss 5.88260508\n",
      "Trained batch 5946 batch loss 5.03048229 epoch total loss 5.88246155\n",
      "Trained batch 5947 batch loss 6.08356285 epoch total loss 5.8824954\n",
      "Trained batch 5948 batch loss 6.11341286 epoch total loss 5.88253403\n",
      "Trained batch 5949 batch loss 5.87979507 epoch total loss 5.88253355\n",
      "Trained batch 5950 batch loss 6.29115343 epoch total loss 5.88260221\n",
      "Trained batch 5951 batch loss 5.74827 epoch total loss 5.88258028\n",
      "Trained batch 5952 batch loss 5.95137691 epoch total loss 5.8825922\n",
      "Trained batch 5953 batch loss 5.17913437 epoch total loss 5.88247395\n",
      "Trained batch 5954 batch loss 5.33366489 epoch total loss 5.88238144\n",
      "Trained batch 5955 batch loss 5.69131517 epoch total loss 5.88234949\n",
      "Trained batch 5956 batch loss 6.17594194 epoch total loss 5.88239861\n",
      "Trained batch 5957 batch loss 5.92515182 epoch total loss 5.88240576\n",
      "Trained batch 5958 batch loss 6.26977539 epoch total loss 5.88247108\n",
      "Trained batch 5959 batch loss 5.74868917 epoch total loss 5.88244867\n",
      "Trained batch 5960 batch loss 5.1023941 epoch total loss 5.88231754\n",
      "Trained batch 5961 batch loss 5.55711937 epoch total loss 5.88226318\n",
      "Trained batch 5962 batch loss 5.64179802 epoch total loss 5.88222265\n",
      "Trained batch 5963 batch loss 5.51435089 epoch total loss 5.88216114\n",
      "Trained batch 5964 batch loss 6.80745602 epoch total loss 5.88231659\n",
      "Trained batch 5965 batch loss 6.5207777 epoch total loss 5.8824234\n",
      "Trained batch 5966 batch loss 6.16970444 epoch total loss 5.88247156\n",
      "Trained batch 5967 batch loss 6.99820185 epoch total loss 5.88265848\n",
      "Trained batch 5968 batch loss 6.74385118 epoch total loss 5.88280249\n",
      "Trained batch 5969 batch loss 5.54170227 epoch total loss 5.88274574\n",
      "Trained batch 5970 batch loss 6.74442959 epoch total loss 5.88289\n",
      "Trained batch 5971 batch loss 6.47013187 epoch total loss 5.88298845\n",
      "Trained batch 5972 batch loss 6.32063866 epoch total loss 5.88306189\n",
      "Trained batch 5973 batch loss 6.35202026 epoch total loss 5.88314\n",
      "Trained batch 5974 batch loss 6.54432774 epoch total loss 5.88325071\n",
      "Trained batch 5975 batch loss 5.76043558 epoch total loss 5.88323\n",
      "Trained batch 5976 batch loss 6.12352419 epoch total loss 5.88327074\n",
      "Trained batch 5977 batch loss 6.37015438 epoch total loss 5.88335228\n",
      "Trained batch 5978 batch loss 6.56835175 epoch total loss 5.88346672\n",
      "Trained batch 5979 batch loss 5.3953104 epoch total loss 5.8833847\n",
      "Trained batch 5980 batch loss 6.88760042 epoch total loss 5.88355255\n",
      "Trained batch 5981 batch loss 6.16312218 epoch total loss 5.88359928\n",
      "Trained batch 5982 batch loss 6.48472643 epoch total loss 5.8837\n",
      "Trained batch 5983 batch loss 6.37128735 epoch total loss 5.88378143\n",
      "Trained batch 5984 batch loss 7.15226 epoch total loss 5.88399315\n",
      "Trained batch 5985 batch loss 6.23690176 epoch total loss 5.88405275\n",
      "Trained batch 5986 batch loss 6.01736259 epoch total loss 5.88407469\n",
      "Trained batch 5987 batch loss 6.10069752 epoch total loss 5.88411093\n",
      "Trained batch 5988 batch loss 5.96811247 epoch total loss 5.88412523\n",
      "Trained batch 5989 batch loss 5.94974756 epoch total loss 5.88413572\n",
      "Trained batch 5990 batch loss 5.47191811 epoch total loss 5.88406706\n",
      "Trained batch 5991 batch loss 5.41854715 epoch total loss 5.88398933\n",
      "Trained batch 5992 batch loss 5.76094246 epoch total loss 5.88396883\n",
      "Trained batch 5993 batch loss 5.90365601 epoch total loss 5.88397217\n",
      "Trained batch 5994 batch loss 6.06192541 epoch total loss 5.88400173\n",
      "Trained batch 5995 batch loss 6.61179209 epoch total loss 5.88412333\n",
      "Trained batch 5996 batch loss 4.97407532 epoch total loss 5.88397169\n",
      "Trained batch 5997 batch loss 4.38818836 epoch total loss 5.88372183\n",
      "Trained batch 5998 batch loss 6.70516062 epoch total loss 5.88385916\n",
      "Trained batch 5999 batch loss 6.56142712 epoch total loss 5.88397217\n",
      "Trained batch 6000 batch loss 6.32725334 epoch total loss 5.88404608\n",
      "Trained batch 6001 batch loss 5.82297707 epoch total loss 5.88403606\n",
      "Trained batch 6002 batch loss 5.90128136 epoch total loss 5.8840394\n",
      "Trained batch 6003 batch loss 5.21753263 epoch total loss 5.8839283\n",
      "Trained batch 6004 batch loss 5.62851429 epoch total loss 5.88388586\n",
      "Trained batch 6005 batch loss 5.25142479 epoch total loss 5.88378048\n",
      "Trained batch 6006 batch loss 5.34684324 epoch total loss 5.88369131\n",
      "Trained batch 6007 batch loss 5.8449049 epoch total loss 5.88368464\n",
      "Trained batch 6008 batch loss 6.0769968 epoch total loss 5.88371706\n",
      "Trained batch 6009 batch loss 6.05367374 epoch total loss 5.88374519\n",
      "Trained batch 6010 batch loss 4.28185177 epoch total loss 5.88347864\n",
      "Trained batch 6011 batch loss 5.65476418 epoch total loss 5.88344097\n",
      "Trained batch 6012 batch loss 5.36137676 epoch total loss 5.88335419\n",
      "Trained batch 6013 batch loss 5.73883581 epoch total loss 5.88333035\n",
      "Trained batch 6014 batch loss 4.54775047 epoch total loss 5.88310814\n",
      "Trained batch 6015 batch loss 5.942204 epoch total loss 5.88311768\n",
      "Trained batch 6016 batch loss 6.30847549 epoch total loss 5.88318825\n",
      "Trained batch 6017 batch loss 5.31148624 epoch total loss 5.88309383\n",
      "Trained batch 6018 batch loss 6.62154388 epoch total loss 5.88321638\n",
      "Trained batch 6019 batch loss 6.6916728 epoch total loss 5.88335037\n",
      "Trained batch 6020 batch loss 6.10896397 epoch total loss 5.88338804\n",
      "Trained batch 6021 batch loss 5.33908 epoch total loss 5.88329792\n",
      "Trained batch 6022 batch loss 5.98938179 epoch total loss 5.88331509\n",
      "Trained batch 6023 batch loss 5.25335836 epoch total loss 5.88321066\n",
      "Trained batch 6024 batch loss 4.60733509 epoch total loss 5.88299847\n",
      "Trained batch 6025 batch loss 5.70568895 epoch total loss 5.88296938\n",
      "Trained batch 6026 batch loss 5.7674818 epoch total loss 5.88295\n",
      "Trained batch 6027 batch loss 5.9246254 epoch total loss 5.88295698\n",
      "Trained batch 6028 batch loss 5.93268108 epoch total loss 5.88296556\n",
      "Trained batch 6029 batch loss 5.54776859 epoch total loss 5.88291\n",
      "Trained batch 6030 batch loss 5.2483058 epoch total loss 5.88280487\n",
      "Trained batch 6031 batch loss 5.39115715 epoch total loss 5.88272333\n",
      "Trained batch 6032 batch loss 5.95157671 epoch total loss 5.88273478\n",
      "Trained batch 6033 batch loss 6.10889339 epoch total loss 5.88277245\n",
      "Trained batch 6034 batch loss 6.03685236 epoch total loss 5.88279772\n",
      "Trained batch 6035 batch loss 5.01651096 epoch total loss 5.88265371\n",
      "Trained batch 6036 batch loss 6.0074811 epoch total loss 5.88267469\n",
      "Trained batch 6037 batch loss 4.97839117 epoch total loss 5.88252449\n",
      "Trained batch 6038 batch loss 4.42617607 epoch total loss 5.88228321\n",
      "Trained batch 6039 batch loss 6.22218895 epoch total loss 5.88233948\n",
      "Trained batch 6040 batch loss 5.43437958 epoch total loss 5.88226557\n",
      "Trained batch 6041 batch loss 5.6542263 epoch total loss 5.88222742\n",
      "Trained batch 6042 batch loss 5.90854 epoch total loss 5.88223171\n",
      "Trained batch 6043 batch loss 6.24438667 epoch total loss 5.88229227\n",
      "Trained batch 6044 batch loss 5.56161642 epoch total loss 5.88223934\n",
      "Trained batch 6045 batch loss 5.19296551 epoch total loss 5.8821249\n",
      "Trained batch 6046 batch loss 5.66078568 epoch total loss 5.88208818\n",
      "Trained batch 6047 batch loss 5.80634165 epoch total loss 5.88207531\n",
      "Trained batch 6048 batch loss 5.82795954 epoch total loss 5.88206673\n",
      "Trained batch 6049 batch loss 6.09828138 epoch total loss 5.88210201\n",
      "Trained batch 6050 batch loss 6.18533421 epoch total loss 5.88215208\n",
      "Trained batch 6051 batch loss 6.23523092 epoch total loss 5.88221025\n",
      "Trained batch 6052 batch loss 5.75019169 epoch total loss 5.88218832\n",
      "Trained batch 6053 batch loss 5.69338131 epoch total loss 5.88215733\n",
      "Trained batch 6054 batch loss 4.93848658 epoch total loss 5.8820014\n",
      "Trained batch 6055 batch loss 5.31884336 epoch total loss 5.88190889\n",
      "Trained batch 6056 batch loss 5.26207161 epoch total loss 5.88180637\n",
      "Trained batch 6057 batch loss 5.72618484 epoch total loss 5.88178062\n",
      "Trained batch 6058 batch loss 6.2110219 epoch total loss 5.88183498\n",
      "Trained batch 6059 batch loss 5.80156183 epoch total loss 5.88182163\n",
      "Trained batch 6060 batch loss 5.70801401 epoch total loss 5.88179255\n",
      "Trained batch 6061 batch loss 5.64941788 epoch total loss 5.8817544\n",
      "Trained batch 6062 batch loss 5.67144966 epoch total loss 5.88171959\n",
      "Trained batch 6063 batch loss 5.42025471 epoch total loss 5.88164377\n",
      "Trained batch 6064 batch loss 5.73332 epoch total loss 5.88161945\n",
      "Trained batch 6065 batch loss 5.58059359 epoch total loss 5.88157\n",
      "Trained batch 6066 batch loss 5.50126076 epoch total loss 5.8815074\n",
      "Trained batch 6067 batch loss 5.55975771 epoch total loss 5.88145399\n",
      "Trained batch 6068 batch loss 5.28102446 epoch total loss 5.88135529\n",
      "Trained batch 6069 batch loss 4.90438843 epoch total loss 5.88119459\n",
      "Trained batch 6070 batch loss 6.28462839 epoch total loss 5.88126087\n",
      "Trained batch 6071 batch loss 6.22364473 epoch total loss 5.88131714\n",
      "Trained batch 6072 batch loss 6.17675209 epoch total loss 5.88136578\n",
      "Trained batch 6073 batch loss 6.95852 epoch total loss 5.88154268\n",
      "Trained batch 6074 batch loss 5.95581818 epoch total loss 5.88155508\n",
      "Trained batch 6075 batch loss 6.19338894 epoch total loss 5.88160706\n",
      "Trained batch 6076 batch loss 6.02695036 epoch total loss 5.8816309\n",
      "Trained batch 6077 batch loss 6.22186565 epoch total loss 5.88168716\n",
      "Trained batch 6078 batch loss 6.9489212 epoch total loss 5.88186264\n",
      "Trained batch 6079 batch loss 6.45908976 epoch total loss 5.88195801\n",
      "Trained batch 6080 batch loss 6.67241478 epoch total loss 5.88208771\n",
      "Trained batch 6081 batch loss 6.03193474 epoch total loss 5.8821125\n",
      "Trained batch 6082 batch loss 6.37575 epoch total loss 5.88219357\n",
      "Trained batch 6083 batch loss 6.23737764 epoch total loss 5.88225174\n",
      "Trained batch 6084 batch loss 6.12477684 epoch total loss 5.88229179\n",
      "Trained batch 6085 batch loss 5.43211746 epoch total loss 5.88221788\n",
      "Trained batch 6086 batch loss 5.35318184 epoch total loss 5.8821311\n",
      "Trained batch 6087 batch loss 5.80496311 epoch total loss 5.88211823\n",
      "Trained batch 6088 batch loss 5.83583736 epoch total loss 5.8821106\n",
      "Trained batch 6089 batch loss 4.76596737 epoch total loss 5.88192701\n",
      "Trained batch 6090 batch loss 5.80957556 epoch total loss 5.88191509\n",
      "Trained batch 6091 batch loss 5.28081131 epoch total loss 5.88181639\n",
      "Trained batch 6092 batch loss 5.99773026 epoch total loss 5.88183546\n",
      "Trained batch 6093 batch loss 4.7319 epoch total loss 5.88164616\n",
      "Trained batch 6094 batch loss 6.60087442 epoch total loss 5.88176441\n",
      "Trained batch 6095 batch loss 6.58869934 epoch total loss 5.88188076\n",
      "Trained batch 6096 batch loss 6.35340643 epoch total loss 5.88195753\n",
      "Trained batch 6097 batch loss 6.51606 epoch total loss 5.88206148\n",
      "Trained batch 6098 batch loss 5.59393167 epoch total loss 5.88201427\n",
      "Trained batch 6099 batch loss 5.91146946 epoch total loss 5.88201904\n",
      "Trained batch 6100 batch loss 5.24241924 epoch total loss 5.88191414\n",
      "Trained batch 6101 batch loss 4.54535 epoch total loss 5.88169527\n",
      "Trained batch 6102 batch loss 4.96102762 epoch total loss 5.88154411\n",
      "Trained batch 6103 batch loss 5.42911386 epoch total loss 5.88147\n",
      "Trained batch 6104 batch loss 4.01389647 epoch total loss 5.88116455\n",
      "Trained batch 6105 batch loss 4.76890898 epoch total loss 5.8809824\n",
      "Trained batch 6106 batch loss 5.52540922 epoch total loss 5.8809247\n",
      "Trained batch 6107 batch loss 5.85125446 epoch total loss 5.88092\n",
      "Trained batch 6108 batch loss 4.46071529 epoch total loss 5.88068724\n",
      "Trained batch 6109 batch loss 3.83886337 epoch total loss 5.88035345\n",
      "Trained batch 6110 batch loss 6.01181793 epoch total loss 5.88037491\n",
      "Trained batch 6111 batch loss 6.52006531 epoch total loss 5.88047934\n",
      "Trained batch 6112 batch loss 5.60704517 epoch total loss 5.88043451\n",
      "Trained batch 6113 batch loss 5.84299946 epoch total loss 5.88042831\n",
      "Trained batch 6114 batch loss 5.08190441 epoch total loss 5.88029766\n",
      "Trained batch 6115 batch loss 5.50265121 epoch total loss 5.88023615\n",
      "Trained batch 6116 batch loss 5.59332848 epoch total loss 5.88018942\n",
      "Trained batch 6117 batch loss 6.08554411 epoch total loss 5.8802228\n",
      "Trained batch 6118 batch loss 6.28551674 epoch total loss 5.88028908\n",
      "Trained batch 6119 batch loss 6.29031897 epoch total loss 5.88035583\n",
      "Trained batch 6120 batch loss 5.62075138 epoch total loss 5.88031387\n",
      "Trained batch 6121 batch loss 5.77497101 epoch total loss 5.88029623\n",
      "Trained batch 6122 batch loss 5.91926 epoch total loss 5.88030243\n",
      "Trained batch 6123 batch loss 5.27333641 epoch total loss 5.88020325\n",
      "Trained batch 6124 batch loss 5.80288601 epoch total loss 5.88019085\n",
      "Trained batch 6125 batch loss 5.3826642 epoch total loss 5.88011\n",
      "Trained batch 6126 batch loss 6.21051788 epoch total loss 5.88016367\n",
      "Trained batch 6127 batch loss 5.85610104 epoch total loss 5.88016\n",
      "Trained batch 6128 batch loss 5.15521383 epoch total loss 5.8800416\n",
      "Trained batch 6129 batch loss 4.36915493 epoch total loss 5.87979555\n",
      "Trained batch 6130 batch loss 5.34452105 epoch total loss 5.87970781\n",
      "Trained batch 6131 batch loss 4.1133604 epoch total loss 5.87942\n",
      "Trained batch 6132 batch loss 4.65715027 epoch total loss 5.87922049\n",
      "Trained batch 6133 batch loss 4.98853111 epoch total loss 5.87907505\n",
      "Trained batch 6134 batch loss 4.74466419 epoch total loss 5.87889051\n",
      "Trained batch 6135 batch loss 4.93199062 epoch total loss 5.87873602\n",
      "Trained batch 6136 batch loss 5.04784584 epoch total loss 5.8786006\n",
      "Trained batch 6137 batch loss 5.00195217 epoch total loss 5.87845755\n",
      "Trained batch 6138 batch loss 4.69005632 epoch total loss 5.87826395\n",
      "Trained batch 6139 batch loss 4.59873199 epoch total loss 5.87805557\n",
      "Trained batch 6140 batch loss 4.55626822 epoch total loss 5.87784\n",
      "Trained batch 6141 batch loss 4.6501565 epoch total loss 5.87764\n",
      "Trained batch 6142 batch loss 4.9519515 epoch total loss 5.87748909\n",
      "Trained batch 6143 batch loss 4.25607157 epoch total loss 5.8772254\n",
      "Trained batch 6144 batch loss 4.65962553 epoch total loss 5.87702751\n",
      "Trained batch 6145 batch loss 4.73756123 epoch total loss 5.87684202\n",
      "Trained batch 6146 batch loss 4.42731953 epoch total loss 5.87660599\n",
      "Trained batch 6147 batch loss 4.68531704 epoch total loss 5.87641191\n",
      "Trained batch 6148 batch loss 4.31981134 epoch total loss 5.87615871\n",
      "Trained batch 6149 batch loss 4.44795418 epoch total loss 5.87592697\n",
      "Trained batch 6150 batch loss 4.62961626 epoch total loss 5.87572432\n",
      "Trained batch 6151 batch loss 4.96104097 epoch total loss 5.87557554\n",
      "Trained batch 6152 batch loss 6.29805088 epoch total loss 5.87564373\n",
      "Trained batch 6153 batch loss 5.91007614 epoch total loss 5.87564945\n",
      "Trained batch 6154 batch loss 5.56681395 epoch total loss 5.87559938\n",
      "Trained batch 6155 batch loss 5.84828377 epoch total loss 5.87559462\n",
      "Trained batch 6156 batch loss 5.16527081 epoch total loss 5.87547922\n",
      "Trained batch 6157 batch loss 5.46326447 epoch total loss 5.87541246\n",
      "Trained batch 6158 batch loss 5.73908234 epoch total loss 5.87539\n",
      "Trained batch 6159 batch loss 4.97987509 epoch total loss 5.87524462\n",
      "Trained batch 6160 batch loss 6.22751427 epoch total loss 5.87530184\n",
      "Trained batch 6161 batch loss 5.20804405 epoch total loss 5.8751936\n",
      "Trained batch 6162 batch loss 5.65206194 epoch total loss 5.87515736\n",
      "Trained batch 6163 batch loss 6.12748384 epoch total loss 5.87519836\n",
      "Trained batch 6164 batch loss 5.2170043 epoch total loss 5.87509203\n",
      "Trained batch 6165 batch loss 5.96523142 epoch total loss 5.87510633\n",
      "Trained batch 6166 batch loss 4.74246693 epoch total loss 5.87492275\n",
      "Trained batch 6167 batch loss 4.4739356 epoch total loss 5.8746953\n",
      "Trained batch 6168 batch loss 4.72103 epoch total loss 5.87450838\n",
      "Trained batch 6169 batch loss 4.54289436 epoch total loss 5.87429285\n",
      "Trained batch 6170 batch loss 4.77720356 epoch total loss 5.87411499\n",
      "Trained batch 6171 batch loss 5.78721523 epoch total loss 5.87410116\n",
      "Trained batch 6172 batch loss 4.94347382 epoch total loss 5.87395048\n",
      "Trained batch 6173 batch loss 5.0510335 epoch total loss 5.87381744\n",
      "Trained batch 6174 batch loss 4.92367458 epoch total loss 5.87366295\n",
      "Trained batch 6175 batch loss 4.95095634 epoch total loss 5.87351322\n",
      "Trained batch 6176 batch loss 5.68970728 epoch total loss 5.87348413\n",
      "Trained batch 6177 batch loss 5.54644585 epoch total loss 5.87343121\n",
      "Trained batch 6178 batch loss 5.70892811 epoch total loss 5.87340403\n",
      "Trained batch 6179 batch loss 5.83525801 epoch total loss 5.87339783\n",
      "Trained batch 6180 batch loss 5.58406067 epoch total loss 5.87335157\n",
      "Trained batch 6181 batch loss 5.95456409 epoch total loss 5.87336445\n",
      "Trained batch 6182 batch loss 5.62411308 epoch total loss 5.87332439\n",
      "Trained batch 6183 batch loss 6.368577 epoch total loss 5.87340403\n",
      "Trained batch 6184 batch loss 5.9845829 epoch total loss 5.87342215\n",
      "Trained batch 6185 batch loss 5.14261913 epoch total loss 5.87330437\n",
      "Trained batch 6186 batch loss 5.15022 epoch total loss 5.87318707\n",
      "Trained batch 6187 batch loss 5.91523552 epoch total loss 5.87319374\n",
      "Trained batch 6188 batch loss 4.86219072 epoch total loss 5.87303066\n",
      "Trained batch 6189 batch loss 5.07421827 epoch total loss 5.87290144\n",
      "Trained batch 6190 batch loss 5.02013493 epoch total loss 5.87276363\n",
      "Trained batch 6191 batch loss 5.45867825 epoch total loss 5.8726964\n",
      "Trained batch 6192 batch loss 5.53427839 epoch total loss 5.87264204\n",
      "Trained batch 6193 batch loss 5.7295928 epoch total loss 5.87261868\n",
      "Trained batch 6194 batch loss 6.15470266 epoch total loss 5.87266493\n",
      "Trained batch 6195 batch loss 4.60750437 epoch total loss 5.87246084\n",
      "Trained batch 6196 batch loss 5.44902039 epoch total loss 5.87239265\n",
      "Trained batch 6197 batch loss 5.72724295 epoch total loss 5.87236881\n",
      "Trained batch 6198 batch loss 6.09215832 epoch total loss 5.87240458\n",
      "Trained batch 6199 batch loss 5.68431664 epoch total loss 5.87237406\n",
      "Trained batch 6200 batch loss 5.79548931 epoch total loss 5.87236214\n",
      "Trained batch 6201 batch loss 5.70323467 epoch total loss 5.87233496\n",
      "Trained batch 6202 batch loss 6.20886 epoch total loss 5.87238884\n",
      "Trained batch 6203 batch loss 4.68871498 epoch total loss 5.87219763\n",
      "Trained batch 6204 batch loss 4.52310896 epoch total loss 5.87198\n",
      "Trained batch 6205 batch loss 4.41732693 epoch total loss 5.87174606\n",
      "Trained batch 6206 batch loss 5.11955452 epoch total loss 5.87162495\n",
      "Trained batch 6207 batch loss 4.91424036 epoch total loss 5.87147093\n",
      "Trained batch 6208 batch loss 5.34949875 epoch total loss 5.87138653\n",
      "Trained batch 6209 batch loss 4.80754471 epoch total loss 5.87121534\n",
      "Trained batch 6210 batch loss 6.69858932 epoch total loss 5.87134838\n",
      "Trained batch 6211 batch loss 5.32681561 epoch total loss 5.87126112\n",
      "Trained batch 6212 batch loss 4.90537643 epoch total loss 5.87110567\n",
      "Trained batch 6213 batch loss 5.09442616 epoch total loss 5.87098074\n",
      "Trained batch 6214 batch loss 5.01315975 epoch total loss 5.87084246\n",
      "Trained batch 6215 batch loss 4.52805519 epoch total loss 5.87062597\n",
      "Trained batch 6216 batch loss 5.35242271 epoch total loss 5.87054253\n",
      "Trained batch 6217 batch loss 5.49918079 epoch total loss 5.87048292\n",
      "Trained batch 6218 batch loss 5.57110453 epoch total loss 5.87043476\n",
      "Trained batch 6219 batch loss 6.28724337 epoch total loss 5.870502\n",
      "Trained batch 6220 batch loss 6.4700613 epoch total loss 5.87059832\n",
      "Trained batch 6221 batch loss 5.83599281 epoch total loss 5.87059259\n",
      "Trained batch 6222 batch loss 6.1030612 epoch total loss 5.87063\n",
      "Trained batch 6223 batch loss 6.08599138 epoch total loss 5.8706646\n",
      "Trained batch 6224 batch loss 6.00059319 epoch total loss 5.8706851\n",
      "Trained batch 6225 batch loss 6.20593882 epoch total loss 5.87073898\n",
      "Trained batch 6226 batch loss 6.34897566 epoch total loss 5.87081575\n",
      "Trained batch 6227 batch loss 6.51464081 epoch total loss 5.87091923\n",
      "Trained batch 6228 batch loss 6.39021683 epoch total loss 5.87100267\n",
      "Trained batch 6229 batch loss 6.40195465 epoch total loss 5.87108803\n",
      "Trained batch 6230 batch loss 6.0676012 epoch total loss 5.8711195\n",
      "Trained batch 6231 batch loss 5.81463814 epoch total loss 5.87111044\n",
      "Trained batch 6232 batch loss 5.70987558 epoch total loss 5.87108517\n",
      "Trained batch 6233 batch loss 6.41209269 epoch total loss 5.87117147\n",
      "Trained batch 6234 batch loss 6.27545404 epoch total loss 5.8712368\n",
      "Trained batch 6235 batch loss 5.88269854 epoch total loss 5.87123871\n",
      "Trained batch 6236 batch loss 5.81761169 epoch total loss 5.87122965\n",
      "Trained batch 6237 batch loss 5.94833088 epoch total loss 5.87124205\n",
      "Trained batch 6238 batch loss 5.88682938 epoch total loss 5.87124443\n",
      "Trained batch 6239 batch loss 6.20654964 epoch total loss 5.87129831\n",
      "Trained batch 6240 batch loss 6.35751581 epoch total loss 5.87137651\n",
      "Trained batch 6241 batch loss 6.21594143 epoch total loss 5.87143183\n",
      "Trained batch 6242 batch loss 6.27006149 epoch total loss 5.87149572\n",
      "Trained batch 6243 batch loss 6.18136501 epoch total loss 5.87154484\n",
      "Trained batch 6244 batch loss 5.66549206 epoch total loss 5.87151146\n",
      "Trained batch 6245 batch loss 5.57939911 epoch total loss 5.87146473\n",
      "Trained batch 6246 batch loss 5.65929127 epoch total loss 5.87143087\n",
      "Trained batch 6247 batch loss 5.61687946 epoch total loss 5.87139034\n",
      "Trained batch 6248 batch loss 5.77902412 epoch total loss 5.87137508\n",
      "Trained batch 6249 batch loss 5.32630825 epoch total loss 5.8712883\n",
      "Trained batch 6250 batch loss 5.53643656 epoch total loss 5.87123442\n",
      "Trained batch 6251 batch loss 5.78391218 epoch total loss 5.87122059\n",
      "Trained batch 6252 batch loss 5.83504581 epoch total loss 5.87121487\n",
      "Trained batch 6253 batch loss 5.55616283 epoch total loss 5.87116432\n",
      "Trained batch 6254 batch loss 5.67585707 epoch total loss 5.87113333\n",
      "Trained batch 6255 batch loss 5.63814926 epoch total loss 5.87109566\n",
      "Trained batch 6256 batch loss 5.64986467 epoch total loss 5.87106\n",
      "Trained batch 6257 batch loss 5.75092602 epoch total loss 5.87104082\n",
      "Trained batch 6258 batch loss 5.62629 epoch total loss 5.87100124\n",
      "Trained batch 6259 batch loss 5.45206451 epoch total loss 5.87093449\n",
      "Trained batch 6260 batch loss 5.98635483 epoch total loss 5.87095356\n",
      "Trained batch 6261 batch loss 5.29503441 epoch total loss 5.87086153\n",
      "Trained batch 6262 batch loss 5.62588596 epoch total loss 5.87082243\n",
      "Trained batch 6263 batch loss 5.83360195 epoch total loss 5.87081623\n",
      "Trained batch 6264 batch loss 5.85737896 epoch total loss 5.87081385\n",
      "Trained batch 6265 batch loss 5.43486929 epoch total loss 5.87074375\n",
      "Trained batch 6266 batch loss 5.08717394 epoch total loss 5.87061882\n",
      "Trained batch 6267 batch loss 5.31134748 epoch total loss 5.87052965\n",
      "Trained batch 6268 batch loss 6.08865118 epoch total loss 5.87056446\n",
      "Trained batch 6269 batch loss 5.72072649 epoch total loss 5.8705411\n",
      "Trained batch 6270 batch loss 5.7983942 epoch total loss 5.87052917\n",
      "Trained batch 6271 batch loss 5.758461 epoch total loss 5.87051153\n",
      "Trained batch 6272 batch loss 5.59097815 epoch total loss 5.87046671\n",
      "Trained batch 6273 batch loss 5.44332933 epoch total loss 5.87039804\n",
      "Trained batch 6274 batch loss 4.52596569 epoch total loss 5.87018394\n",
      "Trained batch 6275 batch loss 5.1914072 epoch total loss 5.87007618\n",
      "Trained batch 6276 batch loss 5.64322567 epoch total loss 5.87004\n",
      "Trained batch 6277 batch loss 6.14792967 epoch total loss 5.87008429\n",
      "Trained batch 6278 batch loss 6.26360369 epoch total loss 5.87014675\n",
      "Trained batch 6279 batch loss 6.62015152 epoch total loss 5.87026644\n",
      "Trained batch 6280 batch loss 5.60682297 epoch total loss 5.870224\n",
      "Trained batch 6281 batch loss 6.15260553 epoch total loss 5.8702693\n",
      "Trained batch 6282 batch loss 6.51492786 epoch total loss 5.87037182\n",
      "Trained batch 6283 batch loss 6.51042747 epoch total loss 5.87047386\n",
      "Trained batch 6284 batch loss 5.91100597 epoch total loss 5.87048\n",
      "Trained batch 6285 batch loss 5.94270611 epoch total loss 5.8704915\n",
      "Trained batch 6286 batch loss 6.13891602 epoch total loss 5.87053442\n",
      "Trained batch 6287 batch loss 4.61327076 epoch total loss 5.87033463\n",
      "Trained batch 6288 batch loss 6.13183784 epoch total loss 5.87037611\n",
      "Trained batch 6289 batch loss 6.63396025 epoch total loss 5.8704977\n",
      "Trained batch 6290 batch loss 5.4955368 epoch total loss 5.8704381\n",
      "Trained batch 6291 batch loss 6.65630054 epoch total loss 5.87056303\n",
      "Trained batch 6292 batch loss 5.77659 epoch total loss 5.87054825\n",
      "Trained batch 6293 batch loss 5.99425793 epoch total loss 5.8705678\n",
      "Trained batch 6294 batch loss 6.2940917 epoch total loss 5.87063503\n",
      "Trained batch 6295 batch loss 6.2592 epoch total loss 5.87069654\n",
      "Trained batch 6296 batch loss 6.95911312 epoch total loss 5.87086964\n",
      "Trained batch 6297 batch loss 5.46509218 epoch total loss 5.87080526\n",
      "Trained batch 6298 batch loss 5.98688602 epoch total loss 5.87082386\n",
      "Trained batch 6299 batch loss 5.00931406 epoch total loss 5.87068701\n",
      "Trained batch 6300 batch loss 5.54712248 epoch total loss 5.87063551\n",
      "Trained batch 6301 batch loss 4.54583597 epoch total loss 5.87042522\n",
      "Trained batch 6302 batch loss 5.34575844 epoch total loss 5.87034273\n",
      "Trained batch 6303 batch loss 5.27089596 epoch total loss 5.87024736\n",
      "Trained batch 6304 batch loss 5.93363905 epoch total loss 5.87025738\n",
      "Trained batch 6305 batch loss 5.68359613 epoch total loss 5.87022781\n",
      "Trained batch 6306 batch loss 5.42115593 epoch total loss 5.87015629\n",
      "Trained batch 6307 batch loss 6.18959045 epoch total loss 5.87020731\n",
      "Trained batch 6308 batch loss 5.92352867 epoch total loss 5.87021542\n",
      "Trained batch 6309 batch loss 5.98704052 epoch total loss 5.87023449\n",
      "Trained batch 6310 batch loss 4.52072477 epoch total loss 5.87002039\n",
      "Trained batch 6311 batch loss 5.63501072 epoch total loss 5.8699832\n",
      "Trained batch 6312 batch loss 5.17040634 epoch total loss 5.86987257\n",
      "Trained batch 6313 batch loss 4.91068459 epoch total loss 5.86972094\n",
      "Trained batch 6314 batch loss 3.74559236 epoch total loss 5.86938429\n",
      "Trained batch 6315 batch loss 3.95165682 epoch total loss 5.86908102\n",
      "Trained batch 6316 batch loss 4.09616375 epoch total loss 5.86880064\n",
      "Trained batch 6317 batch loss 3.57833 epoch total loss 5.86843777\n",
      "Trained batch 6318 batch loss 5.72720242 epoch total loss 5.86841536\n",
      "Trained batch 6319 batch loss 5.97036171 epoch total loss 5.86843109\n",
      "Trained batch 6320 batch loss 6.07316732 epoch total loss 5.86846399\n",
      "Trained batch 6321 batch loss 5.40232277 epoch total loss 5.86839\n",
      "Trained batch 6322 batch loss 6.15677595 epoch total loss 5.86843586\n",
      "Trained batch 6323 batch loss 5.30198479 epoch total loss 5.86834574\n",
      "Trained batch 6324 batch loss 4.58119631 epoch total loss 5.8681426\n",
      "Trained batch 6325 batch loss 4.6475358 epoch total loss 5.86794949\n",
      "Trained batch 6326 batch loss 4.61651945 epoch total loss 5.86775208\n",
      "Trained batch 6327 batch loss 6.18768787 epoch total loss 5.86780262\n",
      "Trained batch 6328 batch loss 6.07626486 epoch total loss 5.86783552\n",
      "Trained batch 6329 batch loss 6.00317955 epoch total loss 5.86785698\n",
      "Trained batch 6330 batch loss 6.2186141 epoch total loss 5.86791277\n",
      "Trained batch 6331 batch loss 5.95611858 epoch total loss 5.8679266\n",
      "Trained batch 6332 batch loss 6.20473671 epoch total loss 5.86797953\n",
      "Trained batch 6333 batch loss 5.2816782 epoch total loss 5.86788702\n",
      "Trained batch 6334 batch loss 5.74846506 epoch total loss 5.86786842\n",
      "Trained batch 6335 batch loss 5.77446175 epoch total loss 5.86785364\n",
      "Trained batch 6336 batch loss 5.58333731 epoch total loss 5.86780834\n",
      "Trained batch 6337 batch loss 5.15059471 epoch total loss 5.86769533\n",
      "Trained batch 6338 batch loss 5.86040306 epoch total loss 5.8676939\n",
      "Trained batch 6339 batch loss 5.82791615 epoch total loss 5.8676877\n",
      "Trained batch 6340 batch loss 5.7191596 epoch total loss 5.86766434\n",
      "Trained batch 6341 batch loss 6.26847839 epoch total loss 5.86772776\n",
      "Trained batch 6342 batch loss 5.98370695 epoch total loss 5.86774635\n",
      "Trained batch 6343 batch loss 4.73560047 epoch total loss 5.86756754\n",
      "Trained batch 6344 batch loss 5.78275 epoch total loss 5.86755371\n",
      "Trained batch 6345 batch loss 6.69986677 epoch total loss 5.86768484\n",
      "Trained batch 6346 batch loss 6.30916071 epoch total loss 5.86775446\n",
      "Trained batch 6347 batch loss 4.6414938 epoch total loss 5.86756086\n",
      "Trained batch 6348 batch loss 6.35890388 epoch total loss 5.86763859\n",
      "Trained batch 6349 batch loss 6.0995574 epoch total loss 5.86767483\n",
      "Trained batch 6350 batch loss 6.3883605 epoch total loss 5.86775637\n",
      "Trained batch 6351 batch loss 6.700212 epoch total loss 5.8678875\n",
      "Trained batch 6352 batch loss 6.4920845 epoch total loss 5.86798573\n",
      "Trained batch 6353 batch loss 6.61852551 epoch total loss 5.8681035\n",
      "Trained batch 6354 batch loss 6.11496973 epoch total loss 5.86814213\n",
      "Trained batch 6355 batch loss 6.28285694 epoch total loss 5.86820745\n",
      "Trained batch 6356 batch loss 5.70541954 epoch total loss 5.86818171\n",
      "Trained batch 6357 batch loss 5.98311424 epoch total loss 5.8682003\n",
      "Trained batch 6358 batch loss 6.04753971 epoch total loss 5.86822844\n",
      "Trained batch 6359 batch loss 6.11666107 epoch total loss 5.86826754\n",
      "Trained batch 6360 batch loss 5.69639492 epoch total loss 5.86824036\n",
      "Trained batch 6361 batch loss 6.02919865 epoch total loss 5.86826515\n",
      "Trained batch 6362 batch loss 6.38067293 epoch total loss 5.86834526\n",
      "Trained batch 6363 batch loss 5.76847172 epoch total loss 5.86833\n",
      "Trained batch 6364 batch loss 6.36231756 epoch total loss 5.86840773\n",
      "Trained batch 6365 batch loss 6.34572 epoch total loss 5.86848307\n",
      "Trained batch 6366 batch loss 5.42817259 epoch total loss 5.86841393\n",
      "Trained batch 6367 batch loss 5.31973886 epoch total loss 5.86832809\n",
      "Trained batch 6368 batch loss 6.24185801 epoch total loss 5.86838675\n",
      "Trained batch 6369 batch loss 5.8873558 epoch total loss 5.86838961\n",
      "Trained batch 6370 batch loss 5.89487505 epoch total loss 5.8683939\n",
      "Trained batch 6371 batch loss 5.95550728 epoch total loss 5.86840773\n",
      "Trained batch 6372 batch loss 5.77351093 epoch total loss 5.86839294\n",
      "Trained batch 6373 batch loss 6.20784187 epoch total loss 5.86844587\n",
      "Trained batch 6374 batch loss 5.80931377 epoch total loss 5.86843634\n",
      "Trained batch 6375 batch loss 5.00090885 epoch total loss 5.86830044\n",
      "Trained batch 6376 batch loss 6.16506529 epoch total loss 5.86834669\n",
      "Trained batch 6377 batch loss 5.4339819 epoch total loss 5.8682785\n",
      "Trained batch 6378 batch loss 6.09761333 epoch total loss 5.86831427\n",
      "Trained batch 6379 batch loss 5.77066278 epoch total loss 5.86829901\n",
      "Trained batch 6380 batch loss 6.6393714 epoch total loss 5.86842\n",
      "Trained batch 6381 batch loss 6.57220745 epoch total loss 5.86853\n",
      "Trained batch 6382 batch loss 4.81017 epoch total loss 5.86836386\n",
      "Trained batch 6383 batch loss 5.82066774 epoch total loss 5.86835623\n",
      "Trained batch 6384 batch loss 6.2521944 epoch total loss 5.86841679\n",
      "Trained batch 6385 batch loss 5.6580286 epoch total loss 5.86838341\n",
      "Trained batch 6386 batch loss 5.91854572 epoch total loss 5.86839151\n",
      "Trained batch 6387 batch loss 6.43850136 epoch total loss 5.86848\n",
      "Trained batch 6388 batch loss 5.87124348 epoch total loss 5.86848068\n",
      "Trained batch 6389 batch loss 5.12060642 epoch total loss 5.86836386\n",
      "Trained batch 6390 batch loss 5.47412252 epoch total loss 5.86830187\n",
      "Trained batch 6391 batch loss 5.7679224 epoch total loss 5.86828661\n",
      "Trained batch 6392 batch loss 6.09357071 epoch total loss 5.8683219\n",
      "Trained batch 6393 batch loss 5.11446762 epoch total loss 5.86820364\n",
      "Trained batch 6394 batch loss 5.64128399 epoch total loss 5.86816788\n",
      "Trained batch 6395 batch loss 5.81666756 epoch total loss 5.86816\n",
      "Trained batch 6396 batch loss 6.3023262 epoch total loss 5.86822748\n",
      "Trained batch 6397 batch loss 6.04507923 epoch total loss 5.86825562\n",
      "Trained batch 6398 batch loss 5.21266079 epoch total loss 5.86815262\n",
      "Trained batch 6399 batch loss 5.64385796 epoch total loss 5.86811781\n",
      "Trained batch 6400 batch loss 5.80076742 epoch total loss 5.86810732\n",
      "Trained batch 6401 batch loss 5.38733196 epoch total loss 5.86803198\n",
      "Trained batch 6402 batch loss 4.81682062 epoch total loss 5.86786795\n",
      "Trained batch 6403 batch loss 5.27609348 epoch total loss 5.86777544\n",
      "Trained batch 6404 batch loss 4.90558147 epoch total loss 5.86762524\n",
      "Trained batch 6405 batch loss 6.23211288 epoch total loss 5.86768198\n",
      "Trained batch 6406 batch loss 6.66077757 epoch total loss 5.86780596\n",
      "Trained batch 6407 batch loss 5.79280281 epoch total loss 5.86779404\n",
      "Trained batch 6408 batch loss 5.85410786 epoch total loss 5.86779213\n",
      "Trained batch 6409 batch loss 6.64180279 epoch total loss 5.86791277\n",
      "Trained batch 6410 batch loss 5.41242933 epoch total loss 5.8678422\n",
      "Trained batch 6411 batch loss 5.3139267 epoch total loss 5.86775541\n",
      "Trained batch 6412 batch loss 5.69199133 epoch total loss 5.86772776\n",
      "Trained batch 6413 batch loss 5.77633047 epoch total loss 5.86771393\n",
      "Trained batch 6414 batch loss 7.09138823 epoch total loss 5.86790419\n",
      "Trained batch 6415 batch loss 7.05727768 epoch total loss 5.86809\n",
      "Trained batch 6416 batch loss 6.79902506 epoch total loss 5.86823511\n",
      "Trained batch 6417 batch loss 5.93020821 epoch total loss 5.86824465\n",
      "Trained batch 6418 batch loss 6.19855881 epoch total loss 5.86829662\n",
      "Trained batch 6419 batch loss 6.05342865 epoch total loss 5.86832571\n",
      "Trained batch 6420 batch loss 5.78455734 epoch total loss 5.86831236\n",
      "Trained batch 6421 batch loss 6.26279736 epoch total loss 5.86837387\n",
      "Trained batch 6422 batch loss 5.43138218 epoch total loss 5.86830568\n",
      "Trained batch 6423 batch loss 5.49575377 epoch total loss 5.86824751\n",
      "Trained batch 6424 batch loss 5.90700197 epoch total loss 5.86825323\n",
      "Trained batch 6425 batch loss 5.7946167 epoch total loss 5.86824179\n",
      "Trained batch 6426 batch loss 6.4805994 epoch total loss 5.86833715\n",
      "Trained batch 6427 batch loss 6.83774757 epoch total loss 5.86848736\n",
      "Trained batch 6428 batch loss 5.03550386 epoch total loss 5.86835814\n",
      "Trained batch 6429 batch loss 6.55782223 epoch total loss 5.86846542\n",
      "Trained batch 6430 batch loss 6.06187725 epoch total loss 5.86849546\n",
      "Trained batch 6431 batch loss 6.2917943 epoch total loss 5.86856127\n",
      "Trained batch 6432 batch loss 6.61042881 epoch total loss 5.86867666\n",
      "Trained batch 6433 batch loss 5.96389771 epoch total loss 5.86869144\n",
      "Trained batch 6434 batch loss 5.44401455 epoch total loss 5.86862564\n",
      "Trained batch 6435 batch loss 5.42815304 epoch total loss 5.86855745\n",
      "Trained batch 6436 batch loss 5.90931511 epoch total loss 5.86856413\n",
      "Trained batch 6437 batch loss 4.72016287 epoch total loss 5.86838531\n",
      "Trained batch 6438 batch loss 3.9481647 epoch total loss 5.86808729\n",
      "Trained batch 6439 batch loss 5.50668335 epoch total loss 5.8680315\n",
      "Trained batch 6440 batch loss 5.53304338 epoch total loss 5.86797905\n",
      "Trained batch 6441 batch loss 5.69832659 epoch total loss 5.86795282\n",
      "Trained batch 6442 batch loss 5.89134502 epoch total loss 5.86795616\n",
      "Trained batch 6443 batch loss 6.62371778 epoch total loss 5.86807394\n",
      "Trained batch 6444 batch loss 6.14564371 epoch total loss 5.86811686\n",
      "Trained batch 6445 batch loss 6.64501715 epoch total loss 5.86823702\n",
      "Trained batch 6446 batch loss 6.25432444 epoch total loss 5.8682971\n",
      "Trained batch 6447 batch loss 5.79272604 epoch total loss 5.86828518\n",
      "Trained batch 6448 batch loss 6.18993664 epoch total loss 5.86833572\n",
      "Trained batch 6449 batch loss 6.41836452 epoch total loss 5.8684206\n",
      "Trained batch 6450 batch loss 4.6336174 epoch total loss 5.86822939\n",
      "Trained batch 6451 batch loss 6.24554205 epoch total loss 5.86828756\n",
      "Trained batch 6452 batch loss 6.27952194 epoch total loss 5.86835194\n",
      "Trained batch 6453 batch loss 5.80787039 epoch total loss 5.8683424\n",
      "Trained batch 6454 batch loss 5.82719707 epoch total loss 5.8683362\n",
      "Trained batch 6455 batch loss 5.99991512 epoch total loss 5.8683567\n",
      "Trained batch 6456 batch loss 5.80587101 epoch total loss 5.86834669\n",
      "Trained batch 6457 batch loss 5.68455029 epoch total loss 5.86831808\n",
      "Trained batch 6458 batch loss 5.95325279 epoch total loss 5.86833143\n",
      "Trained batch 6459 batch loss 5.62511539 epoch total loss 5.86829376\n",
      "Trained batch 6460 batch loss 6.34032106 epoch total loss 5.86836672\n",
      "Trained batch 6461 batch loss 6.39750767 epoch total loss 5.86844873\n",
      "Trained batch 6462 batch loss 6.10727119 epoch total loss 5.86848545\n",
      "Trained batch 6463 batch loss 6.27030134 epoch total loss 5.86854744\n",
      "Trained batch 6464 batch loss 6.08446312 epoch total loss 5.86858082\n",
      "Trained batch 6465 batch loss 6.52701855 epoch total loss 5.86868286\n",
      "Trained batch 6466 batch loss 6.35222054 epoch total loss 5.86875772\n",
      "Trained batch 6467 batch loss 6.58841133 epoch total loss 5.8688693\n",
      "Trained batch 6468 batch loss 5.67363548 epoch total loss 5.86883879\n",
      "Trained batch 6469 batch loss 6.10403633 epoch total loss 5.86887503\n",
      "Trained batch 6470 batch loss 5.62053251 epoch total loss 5.86883688\n",
      "Trained batch 6471 batch loss 6.07258129 epoch total loss 5.86886883\n",
      "Trained batch 6472 batch loss 5.91884899 epoch total loss 5.86887646\n",
      "Trained batch 6473 batch loss 5.78034878 epoch total loss 5.86886263\n",
      "Trained batch 6474 batch loss 5.51386 epoch total loss 5.86880827\n",
      "Trained batch 6475 batch loss 5.65311527 epoch total loss 5.86877489\n",
      "Trained batch 6476 batch loss 5.13692 epoch total loss 5.86866188\n",
      "Trained batch 6477 batch loss 5.43429852 epoch total loss 5.86859465\n",
      "Trained batch 6478 batch loss 5.91799927 epoch total loss 5.86860228\n",
      "Trained batch 6479 batch loss 6.00409031 epoch total loss 5.86862326\n",
      "Trained batch 6480 batch loss 5.66544628 epoch total loss 5.86859131\n",
      "Trained batch 6481 batch loss 5.52697754 epoch total loss 5.86853886\n",
      "Trained batch 6482 batch loss 5.79083443 epoch total loss 5.86852646\n",
      "Trained batch 6483 batch loss 5.59873295 epoch total loss 5.86848497\n",
      "Trained batch 6484 batch loss 6.00032949 epoch total loss 5.868505\n",
      "Trained batch 6485 batch loss 4.98892832 epoch total loss 5.8683691\n",
      "Trained batch 6486 batch loss 5.86586 epoch total loss 5.8683691\n",
      "Trained batch 6487 batch loss 5.87312794 epoch total loss 5.86837\n",
      "Trained batch 6488 batch loss 5.47563362 epoch total loss 5.86831\n",
      "Trained batch 6489 batch loss 5.41730165 epoch total loss 5.86824036\n",
      "Trained batch 6490 batch loss 5.27192974 epoch total loss 5.8681488\n",
      "Trained batch 6491 batch loss 5.83577251 epoch total loss 5.86814356\n",
      "Trained batch 6492 batch loss 5.1756115 epoch total loss 5.86803722\n",
      "Trained batch 6493 batch loss 5.71782732 epoch total loss 5.86801434\n",
      "Trained batch 6494 batch loss 5.65857506 epoch total loss 5.86798191\n",
      "Trained batch 6495 batch loss 5.61285734 epoch total loss 5.86794281\n",
      "Trained batch 6496 batch loss 4.11720467 epoch total loss 5.8676734\n",
      "Trained batch 6497 batch loss 5.3089447 epoch total loss 5.86758709\n",
      "Trained batch 6498 batch loss 5.13959169 epoch total loss 5.86747551\n",
      "Trained batch 6499 batch loss 5.8111763 epoch total loss 5.86746693\n",
      "Trained batch 6500 batch loss 5.74403667 epoch total loss 5.86744785\n",
      "Trained batch 6501 batch loss 5.73322392 epoch total loss 5.86742735\n",
      "Trained batch 6502 batch loss 5.85231 epoch total loss 5.86742496\n",
      "Trained batch 6503 batch loss 5.78957 epoch total loss 5.86741257\n",
      "Trained batch 6504 batch loss 6.9449687 epoch total loss 5.86757851\n",
      "Trained batch 6505 batch loss 5.89452505 epoch total loss 5.8675828\n",
      "Trained batch 6506 batch loss 6.31095219 epoch total loss 5.86765099\n",
      "Trained batch 6507 batch loss 5.608325 epoch total loss 5.86761141\n",
      "Trained batch 6508 batch loss 5.54371548 epoch total loss 5.86756134\n",
      "Trained batch 6509 batch loss 6.06242 epoch total loss 5.86759138\n",
      "Trained batch 6510 batch loss 6.17286968 epoch total loss 5.86763811\n",
      "Trained batch 6511 batch loss 5.27764511 epoch total loss 5.86754751\n",
      "Trained batch 6512 batch loss 6.48212147 epoch total loss 5.86764145\n",
      "Trained batch 6513 batch loss 6.46144199 epoch total loss 5.86773252\n",
      "Trained batch 6514 batch loss 6.58184719 epoch total loss 5.8678422\n",
      "Trained batch 6515 batch loss 6.59234142 epoch total loss 5.86795378\n",
      "Trained batch 6516 batch loss 6.68396664 epoch total loss 5.86807871\n",
      "Trained batch 6517 batch loss 6.56785965 epoch total loss 5.868186\n",
      "Trained batch 6518 batch loss 6.17748451 epoch total loss 5.8682332\n",
      "Trained batch 6519 batch loss 5.41535044 epoch total loss 5.86816359\n",
      "Trained batch 6520 batch loss 4.61375952 epoch total loss 5.86797094\n",
      "Trained batch 6521 batch loss 5.32559109 epoch total loss 5.86788797\n",
      "Trained batch 6522 batch loss 5.55442715 epoch total loss 5.86784\n",
      "Trained batch 6523 batch loss 5.01160812 epoch total loss 5.86770868\n",
      "Trained batch 6524 batch loss 5.36588287 epoch total loss 5.86763191\n",
      "Trained batch 6525 batch loss 6.05025482 epoch total loss 5.86766\n",
      "Trained batch 6526 batch loss 6.57373857 epoch total loss 5.86776829\n",
      "Trained batch 6527 batch loss 5.7711153 epoch total loss 5.86775303\n",
      "Trained batch 6528 batch loss 6.19342136 epoch total loss 5.8678031\n",
      "Trained batch 6529 batch loss 6.00289583 epoch total loss 5.86782408\n",
      "Trained batch 6530 batch loss 6.42896271 epoch total loss 5.86791\n",
      "Trained batch 6531 batch loss 5.99682522 epoch total loss 5.86793\n",
      "Trained batch 6532 batch loss 6.55475283 epoch total loss 5.86803484\n",
      "Trained batch 6533 batch loss 5.47213 epoch total loss 5.86797428\n",
      "Trained batch 6534 batch loss 6.18703747 epoch total loss 5.8680234\n",
      "Trained batch 6535 batch loss 6.17164946 epoch total loss 5.86806965\n",
      "Trained batch 6536 batch loss 6.65936 epoch total loss 5.86819077\n",
      "Trained batch 6537 batch loss 5.62766361 epoch total loss 5.86815453\n",
      "Trained batch 6538 batch loss 5.74184132 epoch total loss 5.86813498\n",
      "Trained batch 6539 batch loss 5.90902662 epoch total loss 5.86814165\n",
      "Trained batch 6540 batch loss 6.6617384 epoch total loss 5.86826277\n",
      "Trained batch 6541 batch loss 6.80073452 epoch total loss 5.86840534\n",
      "Trained batch 6542 batch loss 5.29863644 epoch total loss 5.8683176\n",
      "Trained batch 6543 batch loss 5.58985043 epoch total loss 5.86827517\n",
      "Trained batch 6544 batch loss 6.33132553 epoch total loss 5.86834621\n",
      "Trained batch 6545 batch loss 5.21256304 epoch total loss 5.8682456\n",
      "Trained batch 6546 batch loss 5.8752346 epoch total loss 5.86824656\n",
      "Trained batch 6547 batch loss 5.74897957 epoch total loss 5.86822844\n",
      "Trained batch 6548 batch loss 5.85226536 epoch total loss 5.86822605\n",
      "Trained batch 6549 batch loss 7.07592583 epoch total loss 5.86841\n",
      "Trained batch 6550 batch loss 6.86464691 epoch total loss 5.86856222\n",
      "Trained batch 6551 batch loss 6.6283493 epoch total loss 5.86867809\n",
      "Trained batch 6552 batch loss 6.81375074 epoch total loss 5.8688221\n",
      "Trained batch 6553 batch loss 5.0068922 epoch total loss 5.86869097\n",
      "Trained batch 6554 batch loss 4.94088745 epoch total loss 5.86854935\n",
      "Trained batch 6555 batch loss 4.93287659 epoch total loss 5.86840677\n",
      "Trained batch 6556 batch loss 4.88745832 epoch total loss 5.86825705\n",
      "Trained batch 6557 batch loss 5.04808378 epoch total loss 5.86813164\n",
      "Trained batch 6558 batch loss 4.43573475 epoch total loss 5.86791372\n",
      "Trained batch 6559 batch loss 4.76192188 epoch total loss 5.86774492\n",
      "Trained batch 6560 batch loss 4.78104162 epoch total loss 5.86757946\n",
      "Trained batch 6561 batch loss 4.59649849 epoch total loss 5.86738586\n",
      "Trained batch 6562 batch loss 4.14165401 epoch total loss 5.86712265\n",
      "Trained batch 6563 batch loss 5.0596652 epoch total loss 5.86699963\n",
      "Trained batch 6564 batch loss 5.77911234 epoch total loss 5.8669858\n",
      "Trained batch 6565 batch loss 6.15377855 epoch total loss 5.86702919\n",
      "Trained batch 6566 batch loss 5.58942366 epoch total loss 5.86698723\n",
      "Trained batch 6567 batch loss 5.68113852 epoch total loss 5.86695862\n",
      "Trained batch 6568 batch loss 6.11832142 epoch total loss 5.86699677\n",
      "Trained batch 6569 batch loss 6.00330639 epoch total loss 5.86701727\n",
      "Trained batch 6570 batch loss 6.05953884 epoch total loss 5.86704636\n",
      "Trained batch 6571 batch loss 5.94615173 epoch total loss 5.86705828\n",
      "Trained batch 6572 batch loss 6.11952686 epoch total loss 5.86709738\n",
      "Trained batch 6573 batch loss 6.16840076 epoch total loss 5.86714315\n",
      "Trained batch 6574 batch loss 6.31606531 epoch total loss 5.86721134\n",
      "Trained batch 6575 batch loss 5.66805649 epoch total loss 5.86718082\n",
      "Trained batch 6576 batch loss 6.33348894 epoch total loss 5.86725187\n",
      "Trained batch 6577 batch loss 6.27363825 epoch total loss 5.86731339\n",
      "Trained batch 6578 batch loss 5.81347322 epoch total loss 5.86730528\n",
      "Trained batch 6579 batch loss 6.44560957 epoch total loss 5.86739302\n",
      "Trained batch 6580 batch loss 6.40860462 epoch total loss 5.86747551\n",
      "Trained batch 6581 batch loss 5.87010384 epoch total loss 5.86747599\n",
      "Trained batch 6582 batch loss 5.84668064 epoch total loss 5.86747313\n",
      "Trained batch 6583 batch loss 5.79974508 epoch total loss 5.86746264\n",
      "Trained batch 6584 batch loss 5.99177551 epoch total loss 5.86748171\n",
      "Trained batch 6585 batch loss 6.14297962 epoch total loss 5.86752367\n",
      "Trained batch 6586 batch loss 5.82945728 epoch total loss 5.86751795\n",
      "Trained batch 6587 batch loss 5.75019884 epoch total loss 5.8675\n",
      "Trained batch 6588 batch loss 5.71218681 epoch total loss 5.86747646\n",
      "Trained batch 6589 batch loss 5.87446833 epoch total loss 5.86747742\n",
      "Trained batch 6590 batch loss 5.68872309 epoch total loss 5.86745\n",
      "Trained batch 6591 batch loss 5.86572075 epoch total loss 5.86745\n",
      "Trained batch 6592 batch loss 6.03263569 epoch total loss 5.86747503\n",
      "Trained batch 6593 batch loss 5.6438303 epoch total loss 5.86744118\n",
      "Trained batch 6594 batch loss 4.8487339 epoch total loss 5.86728621\n",
      "Trained batch 6595 batch loss 6.4005537 epoch total loss 5.86736774\n",
      "Trained batch 6596 batch loss 5.46773434 epoch total loss 5.86730719\n",
      "Trained batch 6597 batch loss 4.43463 epoch total loss 5.86708975\n",
      "Trained batch 6598 batch loss 5.32518959 epoch total loss 5.86700773\n",
      "Trained batch 6599 batch loss 6.02998829 epoch total loss 5.86703253\n",
      "Trained batch 6600 batch loss 5.84899902 epoch total loss 5.86702967\n",
      "Trained batch 6601 batch loss 5.58101749 epoch total loss 5.86698627\n",
      "Trained batch 6602 batch loss 6.0790863 epoch total loss 5.86701822\n",
      "Trained batch 6603 batch loss 5.66701746 epoch total loss 5.86698818\n",
      "Trained batch 6604 batch loss 5.96604586 epoch total loss 5.86700296\n",
      "Trained batch 6605 batch loss 5.54408884 epoch total loss 5.86695385\n",
      "Trained batch 6606 batch loss 5.54581261 epoch total loss 5.86690521\n",
      "Trained batch 6607 batch loss 6.50823402 epoch total loss 5.86700249\n",
      "Trained batch 6608 batch loss 5.0429163 epoch total loss 5.86687756\n",
      "Trained batch 6609 batch loss 5.7293644 epoch total loss 5.86685705\n",
      "Trained batch 6610 batch loss 5.84186363 epoch total loss 5.86685371\n",
      "Trained batch 6611 batch loss 6.01059818 epoch total loss 5.86687565\n",
      "Trained batch 6612 batch loss 5.79577827 epoch total loss 5.86686516\n",
      "Trained batch 6613 batch loss 6.1218338 epoch total loss 5.86690331\n",
      "Trained batch 6614 batch loss 6.54317284 epoch total loss 5.86700583\n",
      "Trained batch 6615 batch loss 6.07305813 epoch total loss 5.86703682\n",
      "Trained batch 6616 batch loss 6.40639 epoch total loss 5.86711836\n",
      "Trained batch 6617 batch loss 6.49542904 epoch total loss 5.86721325\n",
      "Trained batch 6618 batch loss 6.53287888 epoch total loss 5.86731386\n",
      "Trained batch 6619 batch loss 6.00294209 epoch total loss 5.86733437\n",
      "Trained batch 6620 batch loss 6.54745054 epoch total loss 5.86743689\n",
      "Trained batch 6621 batch loss 5.14363861 epoch total loss 5.86732769\n",
      "Trained batch 6622 batch loss 5.87640285 epoch total loss 5.86732912\n",
      "Trained batch 6623 batch loss 5.65379667 epoch total loss 5.8672967\n",
      "Trained batch 6624 batch loss 6.46228647 epoch total loss 5.86738634\n",
      "Trained batch 6625 batch loss 5.80538368 epoch total loss 5.8673768\n",
      "Trained batch 6626 batch loss 5.22084808 epoch total loss 5.86727953\n",
      "Trained batch 6627 batch loss 6.44284487 epoch total loss 5.86736631\n",
      "Trained batch 6628 batch loss 5.40070534 epoch total loss 5.86729574\n",
      "Trained batch 6629 batch loss 5.53564882 epoch total loss 5.86724567\n",
      "Trained batch 6630 batch loss 5.98269844 epoch total loss 5.86726332\n",
      "Trained batch 6631 batch loss 6.22590399 epoch total loss 5.86731768\n",
      "Trained batch 6632 batch loss 5.79621553 epoch total loss 5.86730719\n",
      "Trained batch 6633 batch loss 6.3160038 epoch total loss 5.8673749\n",
      "Trained batch 6634 batch loss 6.19488287 epoch total loss 5.86742401\n",
      "Trained batch 6635 batch loss 5.21206951 epoch total loss 5.86732531\n",
      "Trained batch 6636 batch loss 5.34625435 epoch total loss 5.8672471\n",
      "Trained batch 6637 batch loss 5.42693663 epoch total loss 5.86718035\n",
      "Trained batch 6638 batch loss 5.66078138 epoch total loss 5.86714935\n",
      "Trained batch 6639 batch loss 6.21172142 epoch total loss 5.86720085\n",
      "Trained batch 6640 batch loss 6.17393732 epoch total loss 5.86724758\n",
      "Trained batch 6641 batch loss 5.42561769 epoch total loss 5.86718082\n",
      "Trained batch 6642 batch loss 5.43659639 epoch total loss 5.86711645\n",
      "Trained batch 6643 batch loss 5.31026077 epoch total loss 5.86703205\n",
      "Trained batch 6644 batch loss 5.62825394 epoch total loss 5.86699629\n",
      "Trained batch 6645 batch loss 5.23630714 epoch total loss 5.8669014\n",
      "Trained batch 6646 batch loss 5.98686552 epoch total loss 5.86691952\n",
      "Trained batch 6647 batch loss 5.54983044 epoch total loss 5.86687183\n",
      "Trained batch 6648 batch loss 5.88370228 epoch total loss 5.86687422\n",
      "Trained batch 6649 batch loss 4.36473 epoch total loss 5.8666482\n",
      "Trained batch 6650 batch loss 6.82128716 epoch total loss 5.86679173\n",
      "Trained batch 6651 batch loss 5.98274326 epoch total loss 5.86680937\n",
      "Trained batch 6652 batch loss 6.15017843 epoch total loss 5.86685181\n",
      "Trained batch 6653 batch loss 6.13225746 epoch total loss 5.86689138\n",
      "Trained batch 6654 batch loss 5.65549469 epoch total loss 5.86686\n",
      "Trained batch 6655 batch loss 6.0852561 epoch total loss 5.86689281\n",
      "Trained batch 6656 batch loss 5.79236507 epoch total loss 5.86688185\n",
      "Trained batch 6657 batch loss 5.46600294 epoch total loss 5.86682129\n",
      "Trained batch 6658 batch loss 5.76731968 epoch total loss 5.86680603\n",
      "Trained batch 6659 batch loss 5.57958126 epoch total loss 5.86676264\n",
      "Trained batch 6660 batch loss 6.50082397 epoch total loss 5.86685801\n",
      "Trained batch 6661 batch loss 6.16871643 epoch total loss 5.86690331\n",
      "Trained batch 6662 batch loss 6.50941467 epoch total loss 5.86699915\n",
      "Trained batch 6663 batch loss 5.83082294 epoch total loss 5.8669939\n",
      "Trained batch 6664 batch loss 6.34794235 epoch total loss 5.86706638\n",
      "Trained batch 6665 batch loss 5.86926651 epoch total loss 5.86706686\n",
      "Trained batch 6666 batch loss 5.86049175 epoch total loss 5.86706543\n",
      "Trained batch 6667 batch loss 6.21932507 epoch total loss 5.86711836\n",
      "Trained batch 6668 batch loss 6.22563267 epoch total loss 5.86717224\n",
      "Trained batch 6669 batch loss 5.67004585 epoch total loss 5.86714315\n",
      "Trained batch 6670 batch loss 5.43470716 epoch total loss 5.86707783\n",
      "Trained batch 6671 batch loss 6.14536762 epoch total loss 5.86712\n",
      "Trained batch 6672 batch loss 5.88340759 epoch total loss 5.8671217\n",
      "Trained batch 6673 batch loss 6.11897135 epoch total loss 5.86715937\n",
      "Trained batch 6674 batch loss 6.53648806 epoch total loss 5.8672595\n",
      "Trained batch 6675 batch loss 5.99197435 epoch total loss 5.8672781\n",
      "Trained batch 6676 batch loss 7.4132266 epoch total loss 5.86751\n",
      "Trained batch 6677 batch loss 6.26231384 epoch total loss 5.86756897\n",
      "Trained batch 6678 batch loss 6.55819416 epoch total loss 5.86767244\n",
      "Trained batch 6679 batch loss 5.67647028 epoch total loss 5.86764383\n",
      "Trained batch 6680 batch loss 6.29786396 epoch total loss 5.86770773\n",
      "Trained batch 6681 batch loss 5.09641552 epoch total loss 5.86759281\n",
      "Trained batch 6682 batch loss 5.35199642 epoch total loss 5.86751556\n",
      "Trained batch 6683 batch loss 6.52642441 epoch total loss 5.86761427\n",
      "Trained batch 6684 batch loss 5.55143642 epoch total loss 5.86756659\n",
      "Trained batch 6685 batch loss 5.39336777 epoch total loss 5.86749601\n",
      "Trained batch 6686 batch loss 5.89541054 epoch total loss 5.86750031\n",
      "Trained batch 6687 batch loss 5.38932276 epoch total loss 5.86742878\n",
      "Trained batch 6688 batch loss 5.10695457 epoch total loss 5.86731482\n",
      "Trained batch 6689 batch loss 4.62658691 epoch total loss 5.86712933\n",
      "Trained batch 6690 batch loss 4.99941635 epoch total loss 5.86699963\n",
      "Trained batch 6691 batch loss 5.56672668 epoch total loss 5.86695433\n",
      "Trained batch 6692 batch loss 5.09325647 epoch total loss 5.86683893\n",
      "Trained batch 6693 batch loss 5.09769106 epoch total loss 5.86672401\n",
      "Trained batch 6694 batch loss 5.27054119 epoch total loss 5.86663485\n",
      "Trained batch 6695 batch loss 5.7503581 epoch total loss 5.86661768\n",
      "Trained batch 6696 batch loss 6.33135319 epoch total loss 5.86668682\n",
      "Trained batch 6697 batch loss 5.60003138 epoch total loss 5.86664724\n",
      "Trained batch 6698 batch loss 4.94861126 epoch total loss 5.86651039\n",
      "Trained batch 6699 batch loss 5.91062784 epoch total loss 5.86651707\n",
      "Trained batch 6700 batch loss 5.33193731 epoch total loss 5.86643696\n",
      "Trained batch 6701 batch loss 6.13050127 epoch total loss 5.86647654\n",
      "Trained batch 6702 batch loss 6.01371479 epoch total loss 5.86649847\n",
      "Trained batch 6703 batch loss 5.98401976 epoch total loss 5.86651611\n",
      "Trained batch 6704 batch loss 6.58960438 epoch total loss 5.86662388\n",
      "Trained batch 6705 batch loss 6.39803886 epoch total loss 5.86670351\n",
      "Trained batch 6706 batch loss 6.29548 epoch total loss 5.86676741\n",
      "Trained batch 6707 batch loss 6.2758069 epoch total loss 5.86682892\n",
      "Trained batch 6708 batch loss 6.23214912 epoch total loss 5.8668828\n",
      "Trained batch 6709 batch loss 6.35054255 epoch total loss 5.86695528\n",
      "Trained batch 6710 batch loss 6.26621723 epoch total loss 5.86701441\n",
      "Trained batch 6711 batch loss 6.36005688 epoch total loss 5.86708784\n",
      "Trained batch 6712 batch loss 5.75675392 epoch total loss 5.86707163\n",
      "Trained batch 6713 batch loss 5.66961098 epoch total loss 5.86704206\n",
      "Trained batch 6714 batch loss 6.30363512 epoch total loss 5.86710739\n",
      "Trained batch 6715 batch loss 6.31481647 epoch total loss 5.86717415\n",
      "Trained batch 6716 batch loss 6.11027622 epoch total loss 5.86721039\n",
      "Trained batch 6717 batch loss 5.48731422 epoch total loss 5.86715364\n",
      "Trained batch 6718 batch loss 5.98896313 epoch total loss 5.86717176\n",
      "Trained batch 6719 batch loss 6.12009382 epoch total loss 5.86720943\n",
      "Trained batch 6720 batch loss 5.43858099 epoch total loss 5.86714554\n",
      "Trained batch 6721 batch loss 5.66117668 epoch total loss 5.86711502\n",
      "Trained batch 6722 batch loss 5.94222832 epoch total loss 5.86712599\n",
      "Trained batch 6723 batch loss 6.25758457 epoch total loss 5.86718416\n",
      "Trained batch 6724 batch loss 4.70654678 epoch total loss 5.86701155\n",
      "Trained batch 6725 batch loss 5.28942776 epoch total loss 5.86692572\n",
      "Trained batch 6726 batch loss 5.40928173 epoch total loss 5.86685753\n",
      "Trained batch 6727 batch loss 6.23952055 epoch total loss 5.86691284\n",
      "Trained batch 6728 batch loss 6.55652 epoch total loss 5.86701488\n",
      "Trained batch 6729 batch loss 6.07073545 epoch total loss 5.8670454\n",
      "Trained batch 6730 batch loss 6.17334175 epoch total loss 5.8670907\n",
      "Trained batch 6731 batch loss 6.50583839 epoch total loss 5.86718512\n",
      "Trained batch 6732 batch loss 5.91170216 epoch total loss 5.86719179\n",
      "Trained batch 6733 batch loss 6.17760563 epoch total loss 5.86723757\n",
      "Trained batch 6734 batch loss 6.24683714 epoch total loss 5.86729383\n",
      "Trained batch 6735 batch loss 6.02813148 epoch total loss 5.8673172\n",
      "Trained batch 6736 batch loss 6.32293606 epoch total loss 5.86738539\n",
      "Trained batch 6737 batch loss 6.17935 epoch total loss 5.86743164\n",
      "Trained batch 6738 batch loss 6.10004711 epoch total loss 5.86746645\n",
      "Trained batch 6739 batch loss 6.39638615 epoch total loss 5.86754465\n",
      "Trained batch 6740 batch loss 6.18248224 epoch total loss 5.86759138\n",
      "Trained batch 6741 batch loss 6.34852123 epoch total loss 5.86766291\n",
      "Trained batch 6742 batch loss 5.33411407 epoch total loss 5.86758375\n",
      "Trained batch 6743 batch loss 6.37400293 epoch total loss 5.86765909\n",
      "Trained batch 6744 batch loss 6.03673029 epoch total loss 5.86768389\n",
      "Trained batch 6745 batch loss 5.93710947 epoch total loss 5.86769438\n",
      "Trained batch 6746 batch loss 6.08464289 epoch total loss 5.8677268\n",
      "Trained batch 6747 batch loss 5.85240507 epoch total loss 5.86772442\n",
      "Trained batch 6748 batch loss 5.66765499 epoch total loss 5.86769438\n",
      "Trained batch 6749 batch loss 6.14272118 epoch total loss 5.86773539\n",
      "Trained batch 6750 batch loss 6.43688965 epoch total loss 5.86782\n",
      "Trained batch 6751 batch loss 5.96623755 epoch total loss 5.86783457\n",
      "Trained batch 6752 batch loss 5.40074635 epoch total loss 5.86776543\n",
      "Trained batch 6753 batch loss 6.92116547 epoch total loss 5.86792135\n",
      "Trained batch 6754 batch loss 5.4054594 epoch total loss 5.86785316\n",
      "Trained batch 6755 batch loss 5.48028326 epoch total loss 5.86779594\n",
      "Trained batch 6756 batch loss 6.53604889 epoch total loss 5.86789465\n",
      "Trained batch 6757 batch loss 5.78281641 epoch total loss 5.86788177\n",
      "Trained batch 6758 batch loss 5.89069033 epoch total loss 5.86788511\n",
      "Trained batch 6759 batch loss 5.80394077 epoch total loss 5.86787605\n",
      "Trained batch 6760 batch loss 5.78569031 epoch total loss 5.86786366\n",
      "Trained batch 6761 batch loss 5.67482948 epoch total loss 5.86783504\n",
      "Trained batch 6762 batch loss 4.96129227 epoch total loss 5.86770105\n",
      "Trained batch 6763 batch loss 5.77739906 epoch total loss 5.8676877\n",
      "Trained batch 6764 batch loss 5.6936264 epoch total loss 5.86766243\n",
      "Trained batch 6765 batch loss 6.26096773 epoch total loss 5.8677206\n",
      "Trained batch 6766 batch loss 5.42002678 epoch total loss 5.8676548\n",
      "Trained batch 6767 batch loss 5.6401186 epoch total loss 5.86762094\n",
      "Trained batch 6768 batch loss 5.75216389 epoch total loss 5.86760426\n",
      "Trained batch 6769 batch loss 6.21436596 epoch total loss 5.86765528\n",
      "Trained batch 6770 batch loss 5.61887169 epoch total loss 5.86761856\n",
      "Trained batch 6771 batch loss 5.13297653 epoch total loss 5.86751\n",
      "Trained batch 6772 batch loss 5.98100901 epoch total loss 5.86752653\n",
      "Trained batch 6773 batch loss 6.17841434 epoch total loss 5.86757278\n",
      "Trained batch 6774 batch loss 5.42856121 epoch total loss 5.86750793\n",
      "Trained batch 6775 batch loss 6.16073418 epoch total loss 5.86755133\n",
      "Trained batch 6776 batch loss 5.65951729 epoch total loss 5.86752081\n",
      "Trained batch 6777 batch loss 5.94706488 epoch total loss 5.86753225\n",
      "Trained batch 6778 batch loss 5.53254414 epoch total loss 5.86748266\n",
      "Trained batch 6779 batch loss 5.33183622 epoch total loss 5.86740351\n",
      "Trained batch 6780 batch loss 5.96119356 epoch total loss 5.86741734\n",
      "Trained batch 6781 batch loss 5.77937031 epoch total loss 5.86740446\n",
      "Trained batch 6782 batch loss 5.7827158 epoch total loss 5.86739206\n",
      "Trained batch 6783 batch loss 5.79954433 epoch total loss 5.86738205\n",
      "Trained batch 6784 batch loss 6.26750088 epoch total loss 5.8674407\n",
      "Trained batch 6785 batch loss 5.76108 epoch total loss 5.86742544\n",
      "Trained batch 6786 batch loss 5.51271439 epoch total loss 5.86737299\n",
      "Trained batch 6787 batch loss 4.5563364 epoch total loss 5.86717939\n",
      "Trained batch 6788 batch loss 4.41962433 epoch total loss 5.86696577\n",
      "Trained batch 6789 batch loss 4.8654356 epoch total loss 5.8668189\n",
      "Trained batch 6790 batch loss 4.35462666 epoch total loss 5.86659622\n",
      "Trained batch 6791 batch loss 4.61601591 epoch total loss 5.86641216\n",
      "Trained batch 6792 batch loss 4.74366426 epoch total loss 5.8662467\n",
      "Trained batch 6793 batch loss 3.9293251 epoch total loss 5.86596155\n",
      "Trained batch 6794 batch loss 4.47264576 epoch total loss 5.86575651\n",
      "Trained batch 6795 batch loss 5.66938734 epoch total loss 5.86572742\n",
      "Trained batch 6796 batch loss 4.50970697 epoch total loss 5.86552763\n",
      "Trained batch 6797 batch loss 4.57692766 epoch total loss 5.86533833\n",
      "Trained batch 6798 batch loss 4.87741041 epoch total loss 5.86519289\n",
      "Trained batch 6799 batch loss 4.69234657 epoch total loss 5.86502028\n",
      "Trained batch 6800 batch loss 4.03563166 epoch total loss 5.86475134\n",
      "Trained batch 6801 batch loss 5.52137804 epoch total loss 5.86470032\n",
      "Trained batch 6802 batch loss 6.30634975 epoch total loss 5.86476517\n",
      "Trained batch 6803 batch loss 6.16289711 epoch total loss 5.86480904\n",
      "Trained batch 6804 batch loss 5.67560196 epoch total loss 5.86478138\n",
      "Trained batch 6805 batch loss 5.20065117 epoch total loss 5.86468363\n",
      "Trained batch 6806 batch loss 5.56171131 epoch total loss 5.86463928\n",
      "Trained batch 6807 batch loss 5.84460831 epoch total loss 5.86463594\n",
      "Trained batch 6808 batch loss 6.49362516 epoch total loss 5.86472845\n",
      "Trained batch 6809 batch loss 5.23651409 epoch total loss 5.86463642\n",
      "Trained batch 6810 batch loss 6.50647783 epoch total loss 5.86473083\n",
      "Trained batch 6811 batch loss 5.30528259 epoch total loss 5.86464834\n",
      "Trained batch 6812 batch loss 5.55592775 epoch total loss 5.86460304\n",
      "Trained batch 6813 batch loss 6.51193905 epoch total loss 5.86469793\n",
      "Trained batch 6814 batch loss 5.58456898 epoch total loss 5.86465693\n",
      "Trained batch 6815 batch loss 4.81790352 epoch total loss 5.86450338\n",
      "Trained batch 6816 batch loss 4.2827158 epoch total loss 5.86427116\n",
      "Trained batch 6817 batch loss 5.49758101 epoch total loss 5.8642168\n",
      "Trained batch 6818 batch loss 5.40651178 epoch total loss 5.86415\n",
      "Trained batch 6819 batch loss 6.82124472 epoch total loss 5.86429\n",
      "Trained batch 6820 batch loss 5.91971493 epoch total loss 5.86429787\n",
      "Trained batch 6821 batch loss 4.22591782 epoch total loss 5.86405802\n",
      "Trained batch 6822 batch loss 4.70405293 epoch total loss 5.86388779\n",
      "Trained batch 6823 batch loss 5.85171223 epoch total loss 5.86388588\n",
      "Trained batch 6824 batch loss 5.83830786 epoch total loss 5.86388206\n",
      "Trained batch 6825 batch loss 6.33448839 epoch total loss 5.86395168\n",
      "Trained batch 6826 batch loss 5.59919739 epoch total loss 5.86391258\n",
      "Trained batch 6827 batch loss 5.87377691 epoch total loss 5.86391401\n",
      "Trained batch 6828 batch loss 6.18548822 epoch total loss 5.86396074\n",
      "Trained batch 6829 batch loss 6.16091204 epoch total loss 5.86400414\n",
      "Trained batch 6830 batch loss 4.9933672 epoch total loss 5.86387682\n",
      "Trained batch 6831 batch loss 4.41994715 epoch total loss 5.86366558\n",
      "Trained batch 6832 batch loss 6.11686325 epoch total loss 5.86370277\n",
      "Trained batch 6833 batch loss 6.03466129 epoch total loss 5.86372757\n",
      "Trained batch 6834 batch loss 5.47861481 epoch total loss 5.86367178\n",
      "Trained batch 6835 batch loss 5.65586185 epoch total loss 5.86364126\n",
      "Trained batch 6836 batch loss 6.01353264 epoch total loss 5.8636632\n",
      "Trained batch 6837 batch loss 6.2890358 epoch total loss 5.86372519\n",
      "Trained batch 6838 batch loss 6.11039543 epoch total loss 5.86376095\n",
      "Trained batch 6839 batch loss 6.03696203 epoch total loss 5.86378622\n",
      "Trained batch 6840 batch loss 5.68371296 epoch total loss 5.86376\n",
      "Trained batch 6841 batch loss 6.08633804 epoch total loss 5.86379242\n",
      "Trained batch 6842 batch loss 6.34500551 epoch total loss 5.86386251\n",
      "Trained batch 6843 batch loss 6.23210812 epoch total loss 5.86391592\n",
      "Trained batch 6844 batch loss 4.48895311 epoch total loss 5.86371517\n",
      "Trained batch 6845 batch loss 4.75745821 epoch total loss 5.86355352\n",
      "Trained batch 6846 batch loss 5.02611732 epoch total loss 5.86343145\n",
      "Trained batch 6847 batch loss 6.46756554 epoch total loss 5.86351967\n",
      "Trained batch 6848 batch loss 5.70354509 epoch total loss 5.8634963\n",
      "Trained batch 6849 batch loss 5.75286674 epoch total loss 5.86348\n",
      "Trained batch 6850 batch loss 5.11839676 epoch total loss 5.86337137\n",
      "Trained batch 6851 batch loss 5.743505 epoch total loss 5.86335373\n",
      "Trained batch 6852 batch loss 5.67848778 epoch total loss 5.86332703\n",
      "Trained batch 6853 batch loss 5.78110695 epoch total loss 5.86331511\n",
      "Trained batch 6854 batch loss 6.08666945 epoch total loss 5.86334753\n",
      "Trained batch 6855 batch loss 5.98212433 epoch total loss 5.86336422\n",
      "Trained batch 6856 batch loss 5.90517187 epoch total loss 5.8633709\n",
      "Trained batch 6857 batch loss 6.09623194 epoch total loss 5.86340475\n",
      "Trained batch 6858 batch loss 5.74272633 epoch total loss 5.86338711\n",
      "Trained batch 6859 batch loss 4.578475 epoch total loss 5.86319971\n",
      "Trained batch 6860 batch loss 5.33431959 epoch total loss 5.86312294\n",
      "Trained batch 6861 batch loss 5.69154 epoch total loss 5.86309814\n",
      "Trained batch 6862 batch loss 5.94324446 epoch total loss 5.86310911\n",
      "Trained batch 6863 batch loss 5.6308465 epoch total loss 5.86307526\n",
      "Trained batch 6864 batch loss 6.0405736 epoch total loss 5.86310101\n",
      "Trained batch 6865 batch loss 5.90331554 epoch total loss 5.86310673\n",
      "Trained batch 6866 batch loss 5.803689 epoch total loss 5.86309814\n",
      "Trained batch 6867 batch loss 6.1395483 epoch total loss 5.86313868\n",
      "Trained batch 6868 batch loss 5.29172564 epoch total loss 5.86305523\n",
      "Trained batch 6869 batch loss 5.38892937 epoch total loss 5.86298656\n",
      "Trained batch 6870 batch loss 5.77203941 epoch total loss 5.86297369\n",
      "Trained batch 6871 batch loss 5.78104973 epoch total loss 5.86296177\n",
      "Trained batch 6872 batch loss 6.02464294 epoch total loss 5.86298513\n",
      "Trained batch 6873 batch loss 5.65145874 epoch total loss 5.86295462\n",
      "Trained batch 6874 batch loss 5.95310831 epoch total loss 5.86296749\n",
      "Trained batch 6875 batch loss 5.88039 epoch total loss 5.86297\n",
      "Trained batch 6876 batch loss 6.68637943 epoch total loss 5.86309\n",
      "Trained batch 6877 batch loss 6.43025541 epoch total loss 5.86317205\n",
      "Trained batch 6878 batch loss 6.08590031 epoch total loss 5.86320448\n",
      "Trained batch 6879 batch loss 6.98964691 epoch total loss 5.86336803\n",
      "Trained batch 6880 batch loss 6.81523132 epoch total loss 5.86350679\n",
      "Trained batch 6881 batch loss 5.80112267 epoch total loss 5.86349773\n",
      "Trained batch 6882 batch loss 6.22339821 epoch total loss 5.86354971\n",
      "Trained batch 6883 batch loss 5.41397381 epoch total loss 5.86348438\n",
      "Trained batch 6884 batch loss 5.94845104 epoch total loss 5.86349678\n",
      "Trained batch 6885 batch loss 3.6867342 epoch total loss 5.86318064\n",
      "Trained batch 6886 batch loss 3.62320757 epoch total loss 5.86285591\n",
      "Trained batch 6887 batch loss 3.99194574 epoch total loss 5.86258411\n",
      "Trained batch 6888 batch loss 3.75561953 epoch total loss 5.86227798\n",
      "Trained batch 6889 batch loss 4.89147472 epoch total loss 5.86213684\n",
      "Trained batch 6890 batch loss 3.86432028 epoch total loss 5.86184692\n",
      "Trained batch 6891 batch loss 4.53122044 epoch total loss 5.8616538\n",
      "Trained batch 6892 batch loss 4.25058794 epoch total loss 5.86142\n",
      "Trained batch 6893 batch loss 3.95935297 epoch total loss 5.86114407\n",
      "Trained batch 6894 batch loss 4.4313 epoch total loss 5.86093664\n",
      "Trained batch 6895 batch loss 3.55727911 epoch total loss 5.86060286\n",
      "Trained batch 6896 batch loss 3.75521111 epoch total loss 5.8602972\n",
      "Trained batch 6897 batch loss 4.15261602 epoch total loss 5.86004972\n",
      "Trained batch 6898 batch loss 4.21734667 epoch total loss 5.85981178\n",
      "Trained batch 6899 batch loss 3.49225426 epoch total loss 5.85946846\n",
      "Trained batch 6900 batch loss 3.52639151 epoch total loss 5.85913038\n",
      "Trained batch 6901 batch loss 3.56740427 epoch total loss 5.85879803\n",
      "Trained batch 6902 batch loss 4.17768 epoch total loss 5.85855436\n",
      "Trained batch 6903 batch loss 4.77897072 epoch total loss 5.85839796\n",
      "Trained batch 6904 batch loss 5.75714254 epoch total loss 5.85838318\n",
      "Trained batch 6905 batch loss 5.64549685 epoch total loss 5.85835218\n",
      "Trained batch 6906 batch loss 6.15976238 epoch total loss 5.85839605\n",
      "Trained batch 6907 batch loss 6.07132816 epoch total loss 5.85842657\n",
      "Trained batch 6908 batch loss 6.58854485 epoch total loss 5.85853243\n",
      "Trained batch 6909 batch loss 6.78962231 epoch total loss 5.85866737\n",
      "Trained batch 6910 batch loss 6.48267937 epoch total loss 5.8587575\n",
      "Trained batch 6911 batch loss 5.87747192 epoch total loss 5.85876083\n",
      "Trained batch 6912 batch loss 6.6689887 epoch total loss 5.85887766\n",
      "Trained batch 6913 batch loss 6.28916454 epoch total loss 5.85894\n",
      "Trained batch 6914 batch loss 6.84239578 epoch total loss 5.85908222\n",
      "Trained batch 6915 batch loss 6.6443 epoch total loss 5.85919571\n",
      "Trained batch 6916 batch loss 6.68448496 epoch total loss 5.85931492\n",
      "Trained batch 6917 batch loss 6.86546135 epoch total loss 5.85946083\n",
      "Trained batch 6918 batch loss 6.58260298 epoch total loss 5.85956526\n",
      "Trained batch 6919 batch loss 6.89106655 epoch total loss 5.85971451\n",
      "Trained batch 6920 batch loss 6.80424738 epoch total loss 5.85985088\n",
      "Trained batch 6921 batch loss 6.47947025 epoch total loss 5.85994053\n",
      "Trained batch 6922 batch loss 6.64475918 epoch total loss 5.86005402\n",
      "Trained batch 6923 batch loss 6.82051468 epoch total loss 5.86019278\n",
      "Trained batch 6924 batch loss 6.71544266 epoch total loss 5.8603158\n",
      "Trained batch 6925 batch loss 6.58061218 epoch total loss 5.86042\n",
      "Trained batch 6926 batch loss 6.84225512 epoch total loss 5.86056232\n",
      "Trained batch 6927 batch loss 6.53111553 epoch total loss 5.86065912\n",
      "Trained batch 6928 batch loss 6.46107435 epoch total loss 5.86074591\n",
      "Trained batch 6929 batch loss 6.4514389 epoch total loss 5.86083126\n",
      "Trained batch 6930 batch loss 6.46173716 epoch total loss 5.86091757\n",
      "Trained batch 6931 batch loss 6.41865635 epoch total loss 5.86099815\n",
      "Trained batch 6932 batch loss 6.3195858 epoch total loss 5.86106443\n",
      "Trained batch 6933 batch loss 5.90475559 epoch total loss 5.86107111\n",
      "Trained batch 6934 batch loss 6.41961098 epoch total loss 5.86115122\n",
      "Trained batch 6935 batch loss 6.15863037 epoch total loss 5.86119413\n",
      "Trained batch 6936 batch loss 6.20696259 epoch total loss 5.8612442\n",
      "Trained batch 6937 batch loss 6.25646877 epoch total loss 5.86130142\n",
      "Trained batch 6938 batch loss 6.08886147 epoch total loss 5.86133432\n",
      "Trained batch 6939 batch loss 5.84478951 epoch total loss 5.86133194\n",
      "Trained batch 6940 batch loss 6.02884245 epoch total loss 5.86135578\n",
      "Trained batch 6941 batch loss 6.07553291 epoch total loss 5.8613863\n",
      "Trained batch 6942 batch loss 5.51156 epoch total loss 5.86133623\n",
      "Trained batch 6943 batch loss 7.03127 epoch total loss 5.86150455\n",
      "Trained batch 6944 batch loss 5.81104326 epoch total loss 5.8614974\n",
      "Trained batch 6945 batch loss 5.6407547 epoch total loss 5.86146545\n",
      "Trained batch 6946 batch loss 5.77332926 epoch total loss 5.86145306\n",
      "Trained batch 6947 batch loss 6.65189457 epoch total loss 5.86156702\n",
      "Trained batch 6948 batch loss 6.34183788 epoch total loss 5.86163616\n",
      "Trained batch 6949 batch loss 6.33379841 epoch total loss 5.86170387\n",
      "Trained batch 6950 batch loss 6.57955551 epoch total loss 5.86180687\n",
      "Trained batch 6951 batch loss 5.11245632 epoch total loss 5.8616991\n",
      "Trained batch 6952 batch loss 5.43486 epoch total loss 5.86163759\n",
      "Trained batch 6953 batch loss 6.00918293 epoch total loss 5.86165857\n",
      "Trained batch 6954 batch loss 5.59788 epoch total loss 5.8616209\n",
      "Trained batch 6955 batch loss 6.18121052 epoch total loss 5.86166668\n",
      "Trained batch 6956 batch loss 6.46853924 epoch total loss 5.86175394\n",
      "Trained batch 6957 batch loss 5.54125595 epoch total loss 5.86170816\n",
      "Trained batch 6958 batch loss 5.32265186 epoch total loss 5.86163092\n",
      "Trained batch 6959 batch loss 5.76207256 epoch total loss 5.86161613\n",
      "Trained batch 6960 batch loss 5.88747168 epoch total loss 5.86162\n",
      "Trained batch 6961 batch loss 4.42847109 epoch total loss 5.86141443\n",
      "Trained batch 6962 batch loss 5.99133348 epoch total loss 5.86143303\n",
      "Trained batch 6963 batch loss 5.65287161 epoch total loss 5.86140299\n",
      "Trained batch 6964 batch loss 6.34832859 epoch total loss 5.86147261\n",
      "Trained batch 6965 batch loss 5.51427937 epoch total loss 5.86142302\n",
      "Trained batch 6966 batch loss 6.13163471 epoch total loss 5.86146212\n",
      "Trained batch 6967 batch loss 6.04282761 epoch total loss 5.86148834\n",
      "Trained batch 6968 batch loss 5.37058544 epoch total loss 5.86141777\n",
      "Trained batch 6969 batch loss 6.31827211 epoch total loss 5.8614831\n",
      "Trained batch 6970 batch loss 5.39495516 epoch total loss 5.86141634\n",
      "Trained batch 6971 batch loss 6.18915367 epoch total loss 5.86146307\n",
      "Trained batch 6972 batch loss 5.98569 epoch total loss 5.86148071\n",
      "Trained batch 6973 batch loss 5.42231321 epoch total loss 5.86141729\n",
      "Trained batch 6974 batch loss 5.69657 epoch total loss 5.86139345\n",
      "Trained batch 6975 batch loss 5.3018446 epoch total loss 5.86131334\n",
      "Trained batch 6976 batch loss 5.92170238 epoch total loss 5.86132193\n",
      "Trained batch 6977 batch loss 6.07155609 epoch total loss 5.86135197\n",
      "Trained batch 6978 batch loss 6.1743679 epoch total loss 5.86139679\n",
      "Trained batch 6979 batch loss 5.41870308 epoch total loss 5.86133337\n",
      "Trained batch 6980 batch loss 5.86681843 epoch total loss 5.86133432\n",
      "Trained batch 6981 batch loss 5.1385541 epoch total loss 5.86123037\n",
      "Trained batch 6982 batch loss 5.31979179 epoch total loss 5.86115313\n",
      "Trained batch 6983 batch loss 5.54732418 epoch total loss 5.86110783\n",
      "Trained batch 6984 batch loss 6.41250563 epoch total loss 5.86118698\n",
      "Trained batch 6985 batch loss 6.24422407 epoch total loss 5.86124229\n",
      "Trained batch 6986 batch loss 5.73352051 epoch total loss 5.86122417\n",
      "Trained batch 6987 batch loss 5.97756672 epoch total loss 5.86124086\n",
      "Trained batch 6988 batch loss 5.64320755 epoch total loss 5.86120939\n",
      "Trained batch 6989 batch loss 6.27873087 epoch total loss 5.861269\n",
      "Trained batch 6990 batch loss 6.02513504 epoch total loss 5.86129236\n",
      "Trained batch 6991 batch loss 5.33878899 epoch total loss 5.86121798\n",
      "Trained batch 6992 batch loss 5.75300789 epoch total loss 5.86120224\n",
      "Trained batch 6993 batch loss 6.26072025 epoch total loss 5.86125946\n",
      "Trained batch 6994 batch loss 5.93456745 epoch total loss 5.86127\n",
      "Trained batch 6995 batch loss 5.77794361 epoch total loss 5.86125803\n",
      "Trained batch 6996 batch loss 6.06387711 epoch total loss 5.86128664\n",
      "Trained batch 6997 batch loss 6.0557785 epoch total loss 5.8613143\n",
      "Trained batch 6998 batch loss 5.60077763 epoch total loss 5.8612771\n",
      "Trained batch 6999 batch loss 5.43192768 epoch total loss 5.86121607\n",
      "Trained batch 7000 batch loss 5.57821846 epoch total loss 5.86117601\n",
      "Trained batch 7001 batch loss 5.24764681 epoch total loss 5.8610878\n",
      "Trained batch 7002 batch loss 5.90731573 epoch total loss 5.86109447\n",
      "Trained batch 7003 batch loss 6.52573872 epoch total loss 5.86118937\n",
      "Trained batch 7004 batch loss 6.25201225 epoch total loss 5.86124563\n",
      "Trained batch 7005 batch loss 6.0610323 epoch total loss 5.86127424\n",
      "Trained batch 7006 batch loss 6.21035671 epoch total loss 5.86132431\n",
      "Trained batch 7007 batch loss 5.08726358 epoch total loss 5.86121368\n",
      "Trained batch 7008 batch loss 4.61684465 epoch total loss 5.86103582\n",
      "Trained batch 7009 batch loss 4.95945835 epoch total loss 5.86090755\n",
      "Trained batch 7010 batch loss 4.85238552 epoch total loss 5.86076355\n",
      "Trained batch 7011 batch loss 6.36164284 epoch total loss 5.86083555\n",
      "Trained batch 7012 batch loss 4.81792355 epoch total loss 5.8606863\n",
      "Trained batch 7013 batch loss 5.94509315 epoch total loss 5.86069822\n",
      "Trained batch 7014 batch loss 5.82112169 epoch total loss 5.8606925\n",
      "Trained batch 7015 batch loss 5.82227802 epoch total loss 5.86068726\n",
      "Trained batch 7016 batch loss 5.67716455 epoch total loss 5.86066103\n",
      "Trained batch 7017 batch loss 5.82445145 epoch total loss 5.86065578\n",
      "Trained batch 7018 batch loss 5.54109144 epoch total loss 5.86061049\n",
      "Trained batch 7019 batch loss 5.54132462 epoch total loss 5.86056519\n",
      "Trained batch 7020 batch loss 5.96256924 epoch total loss 5.86057949\n",
      "Trained batch 7021 batch loss 5.98315763 epoch total loss 5.86059713\n",
      "Trained batch 7022 batch loss 7.08362961 epoch total loss 5.86077118\n",
      "Trained batch 7023 batch loss 6.07868814 epoch total loss 5.86080217\n",
      "Trained batch 7024 batch loss 5.70372677 epoch total loss 5.86078\n",
      "Trained batch 7025 batch loss 6.29239655 epoch total loss 5.86084127\n",
      "Trained batch 7026 batch loss 6.26657677 epoch total loss 5.86089897\n",
      "Trained batch 7027 batch loss 5.7652874 epoch total loss 5.86088514\n",
      "Trained batch 7028 batch loss 6.22709274 epoch total loss 5.8609376\n",
      "Trained batch 7029 batch loss 6.62946415 epoch total loss 5.86104679\n",
      "Trained batch 7030 batch loss 5.73207617 epoch total loss 5.86102819\n",
      "Trained batch 7031 batch loss 6.58058548 epoch total loss 5.86113071\n",
      "Trained batch 7032 batch loss 6.64346218 epoch total loss 5.86124182\n",
      "Trained batch 7033 batch loss 6.61535406 epoch total loss 5.86134958\n",
      "Trained batch 7034 batch loss 5.97627783 epoch total loss 5.8613658\n",
      "Trained batch 7035 batch loss 6.67043495 epoch total loss 5.86148119\n",
      "Trained batch 7036 batch loss 5.57134581 epoch total loss 5.8614397\n",
      "Trained batch 7037 batch loss 6.21750927 epoch total loss 5.86149073\n",
      "Trained batch 7038 batch loss 5.23299646 epoch total loss 5.86140156\n",
      "Trained batch 7039 batch loss 6.2279706 epoch total loss 5.86145306\n",
      "Trained batch 7040 batch loss 6.13906288 epoch total loss 5.86149311\n",
      "Trained batch 7041 batch loss 5.90975523 epoch total loss 5.8615\n",
      "Trained batch 7042 batch loss 5.24061584 epoch total loss 5.86141205\n",
      "Trained batch 7043 batch loss 6.1220088 epoch total loss 5.86144876\n",
      "Trained batch 7044 batch loss 7.02349854 epoch total loss 5.86161375\n",
      "Trained batch 7045 batch loss 6.57803345 epoch total loss 5.86171532\n",
      "Trained batch 7046 batch loss 6.44954491 epoch total loss 5.86179876\n",
      "Trained batch 7047 batch loss 5.3602519 epoch total loss 5.86172771\n",
      "Trained batch 7048 batch loss 6.83623171 epoch total loss 5.86186552\n",
      "Trained batch 7049 batch loss 6.2777276 epoch total loss 5.86192465\n",
      "Trained batch 7050 batch loss 6.6111331 epoch total loss 5.86203051\n",
      "Trained batch 7051 batch loss 5.29600811 epoch total loss 5.8619504\n",
      "Trained batch 7052 batch loss 6.39367867 epoch total loss 5.86202621\n",
      "Trained batch 7053 batch loss 6.6106739 epoch total loss 5.86213207\n",
      "Trained batch 7054 batch loss 5.73059464 epoch total loss 5.86211348\n",
      "Trained batch 7055 batch loss 7.24666929 epoch total loss 5.86230946\n",
      "Trained batch 7056 batch loss 6.46686268 epoch total loss 5.86239529\n",
      "Trained batch 7057 batch loss 6.9331193 epoch total loss 5.8625474\n",
      "Trained batch 7058 batch loss 6.47826099 epoch total loss 5.86263418\n",
      "Trained batch 7059 batch loss 6.08433628 epoch total loss 5.86266613\n",
      "Trained batch 7060 batch loss 5.28793812 epoch total loss 5.86258459\n",
      "Trained batch 7061 batch loss 3.60039115 epoch total loss 5.86226463\n",
      "Trained batch 7062 batch loss 5.56934547 epoch total loss 5.86222315\n",
      "Trained batch 7063 batch loss 5.09139442 epoch total loss 5.86211395\n",
      "Trained batch 7064 batch loss 4.17286062 epoch total loss 5.86187458\n",
      "Trained batch 7065 batch loss 5.72619295 epoch total loss 5.86185551\n",
      "Trained batch 7066 batch loss 5.72261524 epoch total loss 5.86183548\n",
      "Trained batch 7067 batch loss 6.1499815 epoch total loss 5.86187601\n",
      "Trained batch 7068 batch loss 5.80874681 epoch total loss 5.86186886\n",
      "Trained batch 7069 batch loss 5.49382925 epoch total loss 5.86181641\n",
      "Trained batch 7070 batch loss 5.66132069 epoch total loss 5.8617878\n",
      "Trained batch 7071 batch loss 5.82385349 epoch total loss 5.86178255\n",
      "Trained batch 7072 batch loss 5.35087585 epoch total loss 5.86171055\n",
      "Trained batch 7073 batch loss 5.55154419 epoch total loss 5.8616662\n",
      "Trained batch 7074 batch loss 5.78397512 epoch total loss 5.86165571\n",
      "Trained batch 7075 batch loss 5.71180487 epoch total loss 5.86163425\n",
      "Trained batch 7076 batch loss 5.61057377 epoch total loss 5.86159849\n",
      "Trained batch 7077 batch loss 6.19409752 epoch total loss 5.8616457\n",
      "Trained batch 7078 batch loss 5.48710537 epoch total loss 5.86159325\n",
      "Trained batch 7079 batch loss 5.49564 epoch total loss 5.86154127\n",
      "Trained batch 7080 batch loss 5.61880684 epoch total loss 5.86150694\n",
      "Trained batch 7081 batch loss 5.8620038 epoch total loss 5.86150694\n",
      "Trained batch 7082 batch loss 5.08223343 epoch total loss 5.86139727\n",
      "Trained batch 7083 batch loss 5.70771217 epoch total loss 5.86137533\n",
      "Trained batch 7084 batch loss 5.7445488 epoch total loss 5.86135912\n",
      "Trained batch 7085 batch loss 5.11047363 epoch total loss 5.86125278\n",
      "Trained batch 7086 batch loss 5.67381191 epoch total loss 5.86122608\n",
      "Trained batch 7087 batch loss 5.58650923 epoch total loss 5.86118746\n",
      "Trained batch 7088 batch loss 5.62733269 epoch total loss 5.86115456\n",
      "Trained batch 7089 batch loss 5.56197405 epoch total loss 5.86111259\n",
      "Trained batch 7090 batch loss 4.86997461 epoch total loss 5.86097288\n",
      "Trained batch 7091 batch loss 5.055 epoch total loss 5.86085892\n",
      "Trained batch 7092 batch loss 5.38160896 epoch total loss 5.86079168\n",
      "Trained batch 7093 batch loss 6.41723061 epoch total loss 5.86087036\n",
      "Trained batch 7094 batch loss 4.70321655 epoch total loss 5.86070681\n",
      "Trained batch 7095 batch loss 5.64975834 epoch total loss 5.86067724\n",
      "Trained batch 7096 batch loss 4.84829187 epoch total loss 5.86053419\n",
      "Trained batch 7097 batch loss 5.14785814 epoch total loss 5.86043406\n",
      "Trained batch 7098 batch loss 5.01263332 epoch total loss 5.86031437\n",
      "Trained batch 7099 batch loss 5.69986773 epoch total loss 5.86029196\n",
      "Trained batch 7100 batch loss 5.58164883 epoch total loss 5.86025238\n",
      "Trained batch 7101 batch loss 4.4136076 epoch total loss 5.86004877\n",
      "Trained batch 7102 batch loss 5.50719738 epoch total loss 5.85999918\n",
      "Trained batch 7103 batch loss 5.76767063 epoch total loss 5.85998631\n",
      "Trained batch 7104 batch loss 6.33829689 epoch total loss 5.86005402\n",
      "Trained batch 7105 batch loss 5.64585 epoch total loss 5.86002398\n",
      "Trained batch 7106 batch loss 6.25496864 epoch total loss 5.86007929\n",
      "Trained batch 7107 batch loss 6.14729404 epoch total loss 5.86012\n",
      "Trained batch 7108 batch loss 5.45113659 epoch total loss 5.86006212\n",
      "Trained batch 7109 batch loss 6.05137253 epoch total loss 5.86008883\n",
      "Trained batch 7110 batch loss 4.72699738 epoch total loss 5.85992956\n",
      "Trained batch 7111 batch loss 6.06871653 epoch total loss 5.85995913\n",
      "Trained batch 7112 batch loss 5.89124203 epoch total loss 5.85996342\n",
      "Trained batch 7113 batch loss 5.81003666 epoch total loss 5.85995579\n",
      "Trained batch 7114 batch loss 5.49564266 epoch total loss 5.85990477\n",
      "Trained batch 7115 batch loss 5.10333443 epoch total loss 5.85979843\n",
      "Trained batch 7116 batch loss 4.35745907 epoch total loss 5.85958767\n",
      "Trained batch 7117 batch loss 4.58536625 epoch total loss 5.85940838\n",
      "Trained batch 7118 batch loss 4.62355947 epoch total loss 5.85923529\n",
      "Trained batch 7119 batch loss 4.51278305 epoch total loss 5.85904598\n",
      "Trained batch 7120 batch loss 5.39743423 epoch total loss 5.85898113\n",
      "Trained batch 7121 batch loss 3.82863331 epoch total loss 5.85869598\n",
      "Trained batch 7122 batch loss 4.4913125 epoch total loss 5.85850382\n",
      "Trained batch 7123 batch loss 4.4915309 epoch total loss 5.85831213\n",
      "Trained batch 7124 batch loss 4.20487881 epoch total loss 5.85808\n",
      "Trained batch 7125 batch loss 4.40317392 epoch total loss 5.85787535\n",
      "Trained batch 7126 batch loss 4.40825033 epoch total loss 5.85767221\n",
      "Trained batch 7127 batch loss 4.49174118 epoch total loss 5.857481\n",
      "Trained batch 7128 batch loss 4.62596035 epoch total loss 5.85730791\n",
      "Trained batch 7129 batch loss 5.56409502 epoch total loss 5.85726643\n",
      "Trained batch 7130 batch loss 6.2689333 epoch total loss 5.85732412\n",
      "Trained batch 7131 batch loss 6.29571819 epoch total loss 5.85738611\n",
      "Trained batch 7132 batch loss 6.57098484 epoch total loss 5.85748577\n",
      "Trained batch 7133 batch loss 5.76143646 epoch total loss 5.85747242\n",
      "Trained batch 7134 batch loss 5.66505051 epoch total loss 5.85744524\n",
      "Trained batch 7135 batch loss 6.01636887 epoch total loss 5.85746765\n",
      "Trained batch 7136 batch loss 6.14694214 epoch total loss 5.85750818\n",
      "Trained batch 7137 batch loss 6.27684879 epoch total loss 5.85756731\n",
      "Trained batch 7138 batch loss 6.58010435 epoch total loss 5.85766888\n",
      "Trained batch 7139 batch loss 6.17891598 epoch total loss 5.8577137\n",
      "Trained batch 7140 batch loss 5.73402786 epoch total loss 5.85769653\n",
      "Trained batch 7141 batch loss 6.21108627 epoch total loss 5.85774612\n",
      "Trained batch 7142 batch loss 5.85087299 epoch total loss 5.85774517\n",
      "Trained batch 7143 batch loss 6.39552 epoch total loss 5.85782\n",
      "Trained batch 7144 batch loss 6.61243439 epoch total loss 5.85792589\n",
      "Trained batch 7145 batch loss 6.40943432 epoch total loss 5.85800314\n",
      "Trained batch 7146 batch loss 5.38748169 epoch total loss 5.85793734\n",
      "Trained batch 7147 batch loss 5.31666851 epoch total loss 5.85786152\n",
      "Trained batch 7148 batch loss 6.05718517 epoch total loss 5.85788965\n",
      "Trained batch 7149 batch loss 5.77145481 epoch total loss 5.85787725\n",
      "Trained batch 7150 batch loss 5.84469461 epoch total loss 5.85787535\n",
      "Trained batch 7151 batch loss 6.40086651 epoch total loss 5.85795164\n",
      "Trained batch 7152 batch loss 6.238873 epoch total loss 5.85800457\n",
      "Trained batch 7153 batch loss 5.44284534 epoch total loss 5.8579464\n",
      "Trained batch 7154 batch loss 5.94668341 epoch total loss 5.85795879\n",
      "Trained batch 7155 batch loss 6.64565849 epoch total loss 5.85806847\n",
      "Trained batch 7156 batch loss 6.6123209 epoch total loss 5.85817385\n",
      "Trained batch 7157 batch loss 5.88142872 epoch total loss 5.85817766\n",
      "Trained batch 7158 batch loss 6.02905655 epoch total loss 5.85820103\n",
      "Trained batch 7159 batch loss 6.14301157 epoch total loss 5.85824108\n",
      "Trained batch 7160 batch loss 5.26003408 epoch total loss 5.85815763\n",
      "Trained batch 7161 batch loss 5.85209179 epoch total loss 5.85815716\n",
      "Trained batch 7162 batch loss 6.06794691 epoch total loss 5.85818577\n",
      "Trained batch 7163 batch loss 5.56181955 epoch total loss 5.85814476\n",
      "Trained batch 7164 batch loss 6.07559299 epoch total loss 5.8581748\n",
      "Trained batch 7165 batch loss 5.68759632 epoch total loss 5.85815096\n",
      "Trained batch 7166 batch loss 6.46608829 epoch total loss 5.85823584\n",
      "Trained batch 7167 batch loss 6.24377584 epoch total loss 5.85828924\n",
      "Trained batch 7168 batch loss 6.90480614 epoch total loss 5.85843563\n",
      "Trained batch 7169 batch loss 6.93443 epoch total loss 5.85858536\n",
      "Trained batch 7170 batch loss 7.01236057 epoch total loss 5.85874653\n",
      "Trained batch 7171 batch loss 6.80270433 epoch total loss 5.85887766\n",
      "Trained batch 7172 batch loss 6.92031288 epoch total loss 5.85902596\n",
      "Trained batch 7173 batch loss 6.69000626 epoch total loss 5.85914183\n",
      "Trained batch 7174 batch loss 5.69339228 epoch total loss 5.85911894\n",
      "Trained batch 7175 batch loss 5.99507904 epoch total loss 5.85913801\n",
      "Trained batch 7176 batch loss 5.71271372 epoch total loss 5.85911751\n",
      "Trained batch 7177 batch loss 6.46827126 epoch total loss 5.85920238\n",
      "Trained batch 7178 batch loss 5.76068974 epoch total loss 5.85918903\n",
      "Trained batch 7179 batch loss 6.30233097 epoch total loss 5.85925055\n",
      "Trained batch 7180 batch loss 6.43004227 epoch total loss 5.8593297\n",
      "Trained batch 7181 batch loss 6.23349953 epoch total loss 5.85938215\n",
      "Trained batch 7182 batch loss 6.34436512 epoch total loss 5.85944939\n",
      "Trained batch 7183 batch loss 6.31046724 epoch total loss 5.85951185\n",
      "Trained batch 7184 batch loss 5.37485504 epoch total loss 5.85944462\n",
      "Trained batch 7185 batch loss 5.10351706 epoch total loss 5.85933971\n",
      "Trained batch 7186 batch loss 5.83490181 epoch total loss 5.85933638\n",
      "Trained batch 7187 batch loss 5.97765493 epoch total loss 5.85935259\n",
      "Trained batch 7188 batch loss 6.8574338 epoch total loss 5.85949183\n",
      "Trained batch 7189 batch loss 6.46224833 epoch total loss 5.85957527\n",
      "Trained batch 7190 batch loss 5.98216057 epoch total loss 5.85959244\n",
      "Trained batch 7191 batch loss 6.23070049 epoch total loss 5.85964394\n",
      "Trained batch 7192 batch loss 6.65824795 epoch total loss 5.85975504\n",
      "Trained batch 7193 batch loss 6.77508068 epoch total loss 5.85988235\n",
      "Trained batch 7194 batch loss 6.58005095 epoch total loss 5.85998201\n",
      "Trained batch 7195 batch loss 6.06053638 epoch total loss 5.86000967\n",
      "Trained batch 7196 batch loss 5.97924185 epoch total loss 5.86002636\n",
      "Trained batch 7197 batch loss 6.53456974 epoch total loss 5.8601203\n",
      "Trained batch 7198 batch loss 7.02686453 epoch total loss 5.86028242\n",
      "Trained batch 7199 batch loss 6.88910818 epoch total loss 5.86042547\n",
      "Trained batch 7200 batch loss 6.95545769 epoch total loss 5.86057758\n",
      "Trained batch 7201 batch loss 5.8934803 epoch total loss 5.86058235\n",
      "Trained batch 7202 batch loss 6.5219965 epoch total loss 5.86067438\n",
      "Trained batch 7203 batch loss 6.09269381 epoch total loss 5.86070681\n",
      "Trained batch 7204 batch loss 6.43300343 epoch total loss 5.86078644\n",
      "Trained batch 7205 batch loss 5.26473 epoch total loss 5.86070395\n",
      "Trained batch 7206 batch loss 5.93134403 epoch total loss 5.86071348\n",
      "Trained batch 7207 batch loss 7.01043653 epoch total loss 5.86087322\n",
      "Trained batch 7208 batch loss 5.77891684 epoch total loss 5.8608613\n",
      "Trained batch 7209 batch loss 6.21710873 epoch total loss 5.86091137\n",
      "Trained batch 7210 batch loss 5.78726196 epoch total loss 5.86090136\n",
      "Trained batch 7211 batch loss 6.75314093 epoch total loss 5.86102486\n",
      "Trained batch 7212 batch loss 5.72600651 epoch total loss 5.86100626\n",
      "Trained batch 7213 batch loss 4.86692667 epoch total loss 5.86086845\n",
      "Trained batch 7214 batch loss 5.89049816 epoch total loss 5.86087275\n",
      "Trained batch 7215 batch loss 5.43215466 epoch total loss 5.86081362\n",
      "Trained batch 7216 batch loss 5.95130444 epoch total loss 5.86082649\n",
      "Trained batch 7217 batch loss 5.19560909 epoch total loss 5.86073399\n",
      "Trained batch 7218 batch loss 5.18238544 epoch total loss 5.86064\n",
      "Trained batch 7219 batch loss 5.78793669 epoch total loss 5.86063051\n",
      "Trained batch 7220 batch loss 5.68843937 epoch total loss 5.86060619\n",
      "Trained batch 7221 batch loss 5.5203414 epoch total loss 5.86055899\n",
      "Trained batch 7222 batch loss 5.79160404 epoch total loss 5.86055\n",
      "Trained batch 7223 batch loss 6.11034203 epoch total loss 5.86058426\n",
      "Trained batch 7224 batch loss 5.90165901 epoch total loss 5.86059\n",
      "Trained batch 7225 batch loss 6.20550442 epoch total loss 5.86063814\n",
      "Trained batch 7226 batch loss 4.62889767 epoch total loss 5.86046743\n",
      "Trained batch 7227 batch loss 4.97211 epoch total loss 5.86034489\n",
      "Trained batch 7228 batch loss 6.37094975 epoch total loss 5.86041546\n",
      "Trained batch 7229 batch loss 4.24316549 epoch total loss 5.86019135\n",
      "Trained batch 7230 batch loss 4.07398224 epoch total loss 5.85994434\n",
      "Trained batch 7231 batch loss 4.25753689 epoch total loss 5.85972309\n",
      "Trained batch 7232 batch loss 4.21429825 epoch total loss 5.85949564\n",
      "Trained batch 7233 batch loss 4.57517242 epoch total loss 5.85931778\n",
      "Trained batch 7234 batch loss 4.18602085 epoch total loss 5.85908651\n",
      "Trained batch 7235 batch loss 4.37871361 epoch total loss 5.85888195\n",
      "Trained batch 7236 batch loss 4.47325516 epoch total loss 5.85869026\n",
      "Trained batch 7237 batch loss 4.49870872 epoch total loss 5.85850286\n",
      "Trained batch 7238 batch loss 5.66151905 epoch total loss 5.85847521\n",
      "Trained batch 7239 batch loss 6.20296478 epoch total loss 5.85852289\n",
      "Trained batch 7240 batch loss 6.13013077 epoch total loss 5.85856\n",
      "Trained batch 7241 batch loss 5.75682068 epoch total loss 5.85854626\n",
      "Trained batch 7242 batch loss 5.08570385 epoch total loss 5.85844\n",
      "Trained batch 7243 batch loss 5.25376701 epoch total loss 5.858356\n",
      "Trained batch 7244 batch loss 5.72784519 epoch total loss 5.85833788\n",
      "Trained batch 7245 batch loss 6.42834806 epoch total loss 5.85841703\n",
      "Trained batch 7246 batch loss 6.13654804 epoch total loss 5.85845518\n",
      "Trained batch 7247 batch loss 5.31801891 epoch total loss 5.85838032\n",
      "Trained batch 7248 batch loss 6.40114594 epoch total loss 5.85845566\n",
      "Trained batch 7249 batch loss 7.31486511 epoch total loss 5.85865688\n",
      "Trained batch 7250 batch loss 5.86370659 epoch total loss 5.85865736\n",
      "Trained batch 7251 batch loss 6.84573841 epoch total loss 5.85879374\n",
      "Trained batch 7252 batch loss 5.74265051 epoch total loss 5.85877752\n",
      "Trained batch 7253 batch loss 6.00216198 epoch total loss 5.85879755\n",
      "Trained batch 7254 batch loss 6.34631824 epoch total loss 5.85886526\n",
      "Trained batch 7255 batch loss 5.39216042 epoch total loss 5.85880041\n",
      "Trained batch 7256 batch loss 6.03802299 epoch total loss 5.85882521\n",
      "Trained batch 7257 batch loss 5.80517435 epoch total loss 5.85881805\n",
      "Trained batch 7258 batch loss 6.13768101 epoch total loss 5.8588562\n",
      "Trained batch 7259 batch loss 5.61247206 epoch total loss 5.85882235\n",
      "Trained batch 7260 batch loss 6.49516344 epoch total loss 5.85891\n",
      "Trained batch 7261 batch loss 6.14468431 epoch total loss 5.85894966\n",
      "Trained batch 7262 batch loss 6.64227676 epoch total loss 5.85905695\n",
      "Trained batch 7263 batch loss 4.41296244 epoch total loss 5.85885811\n",
      "Trained batch 7264 batch loss 5.96625566 epoch total loss 5.85887289\n",
      "Trained batch 7265 batch loss 6.00585747 epoch total loss 5.85889292\n",
      "Trained batch 7266 batch loss 5.49897099 epoch total loss 5.85884333\n",
      "Trained batch 7267 batch loss 6.40987062 epoch total loss 5.85891914\n",
      "Trained batch 7268 batch loss 4.97154808 epoch total loss 5.85879707\n",
      "Trained batch 7269 batch loss 6.42844868 epoch total loss 5.85887575\n",
      "Trained batch 7270 batch loss 5.38017273 epoch total loss 5.85881\n",
      "Trained batch 7271 batch loss 5.96172428 epoch total loss 5.85882378\n",
      "Trained batch 7272 batch loss 6.39403534 epoch total loss 5.85889769\n",
      "Trained batch 7273 batch loss 6.8303256 epoch total loss 5.8590312\n",
      "Trained batch 7274 batch loss 6.60047817 epoch total loss 5.85913324\n",
      "Trained batch 7275 batch loss 6.19115591 epoch total loss 5.85917902\n",
      "Trained batch 7276 batch loss 6.42578888 epoch total loss 5.85925674\n",
      "Trained batch 7277 batch loss 5.71900177 epoch total loss 5.85923767\n",
      "Trained batch 7278 batch loss 6.30612946 epoch total loss 5.85929871\n",
      "Trained batch 7279 batch loss 4.88503551 epoch total loss 5.85916519\n",
      "Trained batch 7280 batch loss 6.01357698 epoch total loss 5.85918617\n",
      "Trained batch 7281 batch loss 6.13370562 epoch total loss 5.85922384\n",
      "Trained batch 7282 batch loss 6.45809746 epoch total loss 5.85930586\n",
      "Trained batch 7283 batch loss 5.89827728 epoch total loss 5.8593111\n",
      "Trained batch 7284 batch loss 5.60610485 epoch total loss 5.85927629\n",
      "Trained batch 7285 batch loss 5.67508316 epoch total loss 5.85925102\n",
      "Trained batch 7286 batch loss 5.23852348 epoch total loss 5.85916615\n",
      "Trained batch 7287 batch loss 5.25860596 epoch total loss 5.85908318\n",
      "Trained batch 7288 batch loss 6.07230759 epoch total loss 5.85911274\n",
      "Trained batch 7289 batch loss 6.07744598 epoch total loss 5.85914278\n",
      "Trained batch 7290 batch loss 5.70352793 epoch total loss 5.85912132\n",
      "Trained batch 7291 batch loss 6.21615744 epoch total loss 5.85917044\n",
      "Trained batch 7292 batch loss 6.22045422 epoch total loss 5.85921955\n",
      "Trained batch 7293 batch loss 5.9044714 epoch total loss 5.85922623\n",
      "Trained batch 7294 batch loss 6.10235548 epoch total loss 5.85925913\n",
      "Trained batch 7295 batch loss 5.7093811 epoch total loss 5.8592391\n",
      "Trained batch 7296 batch loss 5.97876406 epoch total loss 5.85925579\n",
      "Trained batch 7297 batch loss 5.3767724 epoch total loss 5.85918903\n",
      "Trained batch 7298 batch loss 5.5535059 epoch total loss 5.85914755\n",
      "Trained batch 7299 batch loss 6.035851 epoch total loss 5.85917187\n",
      "Trained batch 7300 batch loss 6.83945131 epoch total loss 5.85930586\n",
      "Trained batch 7301 batch loss 6.1344924 epoch total loss 5.85934353\n",
      "Trained batch 7302 batch loss 6.61352158 epoch total loss 5.85944653\n",
      "Trained batch 7303 batch loss 5.79642725 epoch total loss 5.85943794\n",
      "Trained batch 7304 batch loss 6.30198908 epoch total loss 5.8594985\n",
      "Trained batch 7305 batch loss 6.43630505 epoch total loss 5.85957766\n",
      "Trained batch 7306 batch loss 7.20089293 epoch total loss 5.85976124\n",
      "Trained batch 7307 batch loss 7.08223438 epoch total loss 5.85992813\n",
      "Trained batch 7308 batch loss 6.62808037 epoch total loss 5.86003351\n",
      "Trained batch 7309 batch loss 6.7911787 epoch total loss 5.8601613\n",
      "Trained batch 7310 batch loss 5.81813908 epoch total loss 5.86015511\n",
      "Trained batch 7311 batch loss 6.64760685 epoch total loss 5.86026287\n",
      "Trained batch 7312 batch loss 7.0642643 epoch total loss 5.86042738\n",
      "Trained batch 7313 batch loss 7.92950535 epoch total loss 5.86071\n",
      "Trained batch 7314 batch loss 7.1808095 epoch total loss 5.86089087\n",
      "Trained batch 7315 batch loss 6.5257225 epoch total loss 5.86098194\n",
      "Trained batch 7316 batch loss 6.2906332 epoch total loss 5.86104\n",
      "Trained batch 7317 batch loss 6.18919468 epoch total loss 5.86108494\n",
      "Trained batch 7318 batch loss 6.76919651 epoch total loss 5.86120892\n",
      "Trained batch 7319 batch loss 6.8040309 epoch total loss 5.86133814\n",
      "Trained batch 7320 batch loss 5.4673748 epoch total loss 5.86128426\n",
      "Trained batch 7321 batch loss 6.37687826 epoch total loss 5.86135435\n",
      "Trained batch 7322 batch loss 6.35980034 epoch total loss 5.86142254\n",
      "Trained batch 7323 batch loss 6.68534756 epoch total loss 5.8615346\n",
      "Trained batch 7324 batch loss 6.29409 epoch total loss 5.86159372\n",
      "Trained batch 7325 batch loss 6.51496124 epoch total loss 5.86168289\n",
      "Trained batch 7326 batch loss 6.50717735 epoch total loss 5.86177111\n",
      "Trained batch 7327 batch loss 6.8152 epoch total loss 5.86190128\n",
      "Trained batch 7328 batch loss 6.37874651 epoch total loss 5.86197186\n",
      "Trained batch 7329 batch loss 6.31798267 epoch total loss 5.86203384\n",
      "Trained batch 7330 batch loss 5.65175152 epoch total loss 5.86200523\n",
      "Trained batch 7331 batch loss 5.7344408 epoch total loss 5.86198807\n",
      "Trained batch 7332 batch loss 6.72293186 epoch total loss 5.86210537\n",
      "Trained batch 7333 batch loss 5.98017025 epoch total loss 5.86212158\n",
      "Trained batch 7334 batch loss 6.46871948 epoch total loss 5.86220407\n",
      "Trained batch 7335 batch loss 5.95597649 epoch total loss 5.86221743\n",
      "Trained batch 7336 batch loss 5.67252207 epoch total loss 5.8621912\n",
      "Trained batch 7337 batch loss 5.63011503 epoch total loss 5.86215925\n",
      "Trained batch 7338 batch loss 5.28758144 epoch total loss 5.86208153\n",
      "Trained batch 7339 batch loss 5.38217258 epoch total loss 5.8620162\n",
      "Trained batch 7340 batch loss 5.29757786 epoch total loss 5.86193895\n",
      "Trained batch 7341 batch loss 5.16663 epoch total loss 5.86184454\n",
      "Trained batch 7342 batch loss 6.24313831 epoch total loss 5.86189651\n",
      "Trained batch 7343 batch loss 6.49849129 epoch total loss 5.8619833\n",
      "Trained batch 7344 batch loss 5.78584385 epoch total loss 5.86197281\n",
      "Trained batch 7345 batch loss 5.16982555 epoch total loss 5.8618784\n",
      "Trained batch 7346 batch loss 6.28248119 epoch total loss 5.86193562\n",
      "Trained batch 7347 batch loss 5.87908459 epoch total loss 5.86193752\n",
      "Trained batch 7348 batch loss 5.77292776 epoch total loss 5.8619256\n",
      "Trained batch 7349 batch loss 5.99881315 epoch total loss 5.86194468\n",
      "Trained batch 7350 batch loss 6.26992178 epoch total loss 5.862\n",
      "Trained batch 7351 batch loss 4.55370617 epoch total loss 5.86182213\n",
      "Trained batch 7352 batch loss 4.86618423 epoch total loss 5.86168671\n",
      "Trained batch 7353 batch loss 5.56532383 epoch total loss 5.86164665\n",
      "Trained batch 7354 batch loss 5.21046638 epoch total loss 5.86155796\n",
      "Trained batch 7355 batch loss 5.28587818 epoch total loss 5.86147976\n",
      "Trained batch 7356 batch loss 6.07219791 epoch total loss 5.86150837\n",
      "Trained batch 7357 batch loss 5.47877598 epoch total loss 5.86145639\n",
      "Trained batch 7358 batch loss 6.14819145 epoch total loss 5.86149549\n",
      "Trained batch 7359 batch loss 6.70018101 epoch total loss 5.86160898\n",
      "Trained batch 7360 batch loss 6.39827776 epoch total loss 5.86168194\n",
      "Trained batch 7361 batch loss 6.20033646 epoch total loss 5.86172819\n",
      "Trained batch 7362 batch loss 6.7679863 epoch total loss 5.86185122\n",
      "Trained batch 7363 batch loss 7.08986759 epoch total loss 5.86201811\n",
      "Trained batch 7364 batch loss 6.6881361 epoch total loss 5.86213\n",
      "Trained batch 7365 batch loss 6.24497318 epoch total loss 5.86218214\n",
      "Trained batch 7366 batch loss 6.25022459 epoch total loss 5.86223507\n",
      "Trained batch 7367 batch loss 6.62910843 epoch total loss 5.86233902\n",
      "Trained batch 7368 batch loss 5.31770945 epoch total loss 5.86226511\n",
      "Trained batch 7369 batch loss 5.42750025 epoch total loss 5.86220551\n",
      "Trained batch 7370 batch loss 5.96794891 epoch total loss 5.86222029\n",
      "Trained batch 7371 batch loss 6.65909481 epoch total loss 5.86232853\n",
      "Trained batch 7372 batch loss 6.62871265 epoch total loss 5.86243248\n",
      "Trained batch 7373 batch loss 6.54984093 epoch total loss 5.86252594\n",
      "Trained batch 7374 batch loss 7.10025787 epoch total loss 5.86269379\n",
      "Trained batch 7375 batch loss 5.61394167 epoch total loss 5.86266\n",
      "Trained batch 7376 batch loss 5.63411331 epoch total loss 5.86262894\n",
      "Trained batch 7377 batch loss 6.39462948 epoch total loss 5.86270094\n",
      "Trained batch 7378 batch loss 5.6208849 epoch total loss 5.86266804\n",
      "Trained batch 7379 batch loss 5.42165709 epoch total loss 5.86260843\n",
      "Trained batch 7380 batch loss 5.92521763 epoch total loss 5.86261702\n",
      "Trained batch 7381 batch loss 4.81708384 epoch total loss 5.8624754\n",
      "Trained batch 7382 batch loss 4.8320322 epoch total loss 5.86233568\n",
      "Trained batch 7383 batch loss 5.70463133 epoch total loss 5.86231422\n",
      "Trained batch 7384 batch loss 5.44717121 epoch total loss 5.86225748\n",
      "Trained batch 7385 batch loss 6.25883675 epoch total loss 5.86231136\n",
      "Trained batch 7386 batch loss 5.92558384 epoch total loss 5.86232\n",
      "Trained batch 7387 batch loss 5.37376785 epoch total loss 5.86225367\n",
      "Trained batch 7388 batch loss 4.9189949 epoch total loss 5.86212587\n",
      "Trained batch 7389 batch loss 5.44745541 epoch total loss 5.86207\n",
      "Trained batch 7390 batch loss 5.38715649 epoch total loss 5.86200571\n",
      "Trained batch 7391 batch loss 5.14662933 epoch total loss 5.86190939\n",
      "Trained batch 7392 batch loss 5.52075863 epoch total loss 5.86186314\n",
      "Trained batch 7393 batch loss 5.52836561 epoch total loss 5.86181784\n",
      "Trained batch 7394 batch loss 5.7782 epoch total loss 5.86180639\n",
      "Trained batch 7395 batch loss 5.51527262 epoch total loss 5.86175919\n",
      "Trained batch 7396 batch loss 6.35932636 epoch total loss 5.8618269\n",
      "Trained batch 7397 batch loss 5.87171459 epoch total loss 5.86182785\n",
      "Trained batch 7398 batch loss 5.96198034 epoch total loss 5.8618412\n",
      "Trained batch 7399 batch loss 5.59952974 epoch total loss 5.86180544\n",
      "Trained batch 7400 batch loss 5.24216604 epoch total loss 5.86172199\n",
      "Trained batch 7401 batch loss 5.67218781 epoch total loss 5.86169624\n",
      "Trained batch 7402 batch loss 5.2519865 epoch total loss 5.86161423\n",
      "Trained batch 7403 batch loss 5.21051598 epoch total loss 5.86152649\n",
      "Trained batch 7404 batch loss 5.2209878 epoch total loss 5.86144\n",
      "Trained batch 7405 batch loss 5.06954193 epoch total loss 5.86133337\n",
      "Trained batch 7406 batch loss 5.01995277 epoch total loss 5.86121941\n",
      "Trained batch 7407 batch loss 5.66557693 epoch total loss 5.8611927\n",
      "Trained batch 7408 batch loss 6.01707 epoch total loss 5.86121368\n",
      "Trained batch 7409 batch loss 5.53021622 epoch total loss 5.86116934\n",
      "Trained batch 7410 batch loss 6.35380936 epoch total loss 5.8612361\n",
      "Trained batch 7411 batch loss 6.00454 epoch total loss 5.86125517\n",
      "Trained batch 7412 batch loss 6.24562311 epoch total loss 5.86130714\n",
      "Trained batch 7413 batch loss 7.05345964 epoch total loss 5.86146784\n",
      "Trained batch 7414 batch loss 5.99397087 epoch total loss 5.86148548\n",
      "Trained batch 7415 batch loss 6.68223143 epoch total loss 5.86159658\n",
      "Trained batch 7416 batch loss 6.59958124 epoch total loss 5.86169577\n",
      "Trained batch 7417 batch loss 6.73121262 epoch total loss 5.86181307\n",
      "Trained batch 7418 batch loss 6.95786524 epoch total loss 5.86196041\n",
      "Trained batch 7419 batch loss 6.60259342 epoch total loss 5.86206\n",
      "Trained batch 7420 batch loss 6.59401178 epoch total loss 5.86215878\n",
      "Trained batch 7421 batch loss 7.27811813 epoch total loss 5.86234951\n",
      "Trained batch 7422 batch loss 7.17333508 epoch total loss 5.86252594\n",
      "Trained batch 7423 batch loss 6.69767189 epoch total loss 5.86263895\n",
      "Trained batch 7424 batch loss 6.38877487 epoch total loss 5.86271\n",
      "Trained batch 7425 batch loss 7.13926601 epoch total loss 5.86288214\n",
      "Trained batch 7426 batch loss 6.01130438 epoch total loss 5.86290216\n",
      "Trained batch 7427 batch loss 6.72369385 epoch total loss 5.86301756\n",
      "Trained batch 7428 batch loss 6.37163687 epoch total loss 5.86308622\n",
      "Trained batch 7429 batch loss 6.92603493 epoch total loss 5.86322927\n",
      "Trained batch 7430 batch loss 6.07661486 epoch total loss 5.86325788\n",
      "Trained batch 7431 batch loss 6.84847641 epoch total loss 5.86339045\n",
      "Trained batch 7432 batch loss 6.54945469 epoch total loss 5.86348295\n",
      "Trained batch 7433 batch loss 6.48142862 epoch total loss 5.86356592\n",
      "Trained batch 7434 batch loss 6.91178799 epoch total loss 5.86370707\n",
      "Trained batch 7435 batch loss 6.40242863 epoch total loss 5.86377954\n",
      "Trained batch 7436 batch loss 5.56362915 epoch total loss 5.86373901\n",
      "Trained batch 7437 batch loss 6.33650827 epoch total loss 5.86380243\n",
      "Trained batch 7438 batch loss 6.1431942 epoch total loss 5.86384\n",
      "Trained batch 7439 batch loss 5.85506773 epoch total loss 5.86383867\n",
      "Trained batch 7440 batch loss 5.19766235 epoch total loss 5.8637495\n",
      "Trained batch 7441 batch loss 6.13300037 epoch total loss 5.86378574\n",
      "Trained batch 7442 batch loss 5.45252 epoch total loss 5.86373043\n",
      "Trained batch 7443 batch loss 5.31079292 epoch total loss 5.86365652\n",
      "Trained batch 7444 batch loss 5.45176792 epoch total loss 5.86360121\n",
      "Trained batch 7445 batch loss 5.96081448 epoch total loss 5.86361456\n",
      "Trained batch 7446 batch loss 6.9707489 epoch total loss 5.86376333\n",
      "Trained batch 7447 batch loss 6.35463333 epoch total loss 5.86382961\n",
      "Trained batch 7448 batch loss 6.13969326 epoch total loss 5.86386633\n",
      "Trained batch 7449 batch loss 6.37297249 epoch total loss 5.86393452\n",
      "Trained batch 7450 batch loss 6.80938768 epoch total loss 5.86406136\n",
      "Trained batch 7451 batch loss 6.26653814 epoch total loss 5.86411524\n",
      "Trained batch 7452 batch loss 6.45621824 epoch total loss 5.86419487\n",
      "Trained batch 7453 batch loss 6.53522158 epoch total loss 5.86428499\n",
      "Trained batch 7454 batch loss 6.49928951 epoch total loss 5.86437035\n",
      "Trained batch 7455 batch loss 6.43586683 epoch total loss 5.86444712\n",
      "Trained batch 7456 batch loss 6.61198616 epoch total loss 5.86454773\n",
      "Trained batch 7457 batch loss 6.94022846 epoch total loss 5.86469173\n",
      "Trained batch 7458 batch loss 6.46902657 epoch total loss 5.8647728\n",
      "Trained batch 7459 batch loss 6.66464758 epoch total loss 5.86488\n",
      "Trained batch 7460 batch loss 6.63252831 epoch total loss 5.86498308\n",
      "Trained batch 7461 batch loss 6.66304731 epoch total loss 5.86509037\n",
      "Trained batch 7462 batch loss 6.5855608 epoch total loss 5.86518669\n",
      "Trained batch 7463 batch loss 6.28159618 epoch total loss 5.86524248\n",
      "Trained batch 7464 batch loss 5.81811 epoch total loss 5.86523581\n",
      "Trained batch 7465 batch loss 5.70657206 epoch total loss 5.86521482\n",
      "Trained batch 7466 batch loss 5.47096729 epoch total loss 5.86516237\n",
      "Trained batch 7467 batch loss 5.87428951 epoch total loss 5.86516333\n",
      "Trained batch 7468 batch loss 5.30996609 epoch total loss 5.86508894\n",
      "Trained batch 7469 batch loss 5.29683256 epoch total loss 5.86501312\n",
      "Trained batch 7470 batch loss 6.24912071 epoch total loss 5.86506462\n",
      "Trained batch 7471 batch loss 4.76517391 epoch total loss 5.86491728\n",
      "Trained batch 7472 batch loss 6.01919556 epoch total loss 5.86493778\n",
      "Trained batch 7473 batch loss 6.12087107 epoch total loss 5.86497211\n",
      "Trained batch 7474 batch loss 5.651 epoch total loss 5.86494398\n",
      "Trained batch 7475 batch loss 5.70476055 epoch total loss 5.86492205\n",
      "Trained batch 7476 batch loss 6.05285072 epoch total loss 5.86494732\n",
      "Trained batch 7477 batch loss 6.06770754 epoch total loss 5.8649745\n",
      "Trained batch 7478 batch loss 6.29241419 epoch total loss 5.86503172\n",
      "Trained batch 7479 batch loss 5.8993144 epoch total loss 5.86503601\n",
      "Trained batch 7480 batch loss 5.57762861 epoch total loss 5.86499786\n",
      "Trained batch 7481 batch loss 6.55915928 epoch total loss 5.86509037\n",
      "Trained batch 7482 batch loss 5.85362148 epoch total loss 5.86508942\n",
      "Trained batch 7483 batch loss 6.86514378 epoch total loss 5.86522245\n",
      "Trained batch 7484 batch loss 5.09336948 epoch total loss 5.86511946\n",
      "Trained batch 7485 batch loss 5.91797543 epoch total loss 5.86512661\n",
      "Trained batch 7486 batch loss 5.2575717 epoch total loss 5.86504555\n",
      "Trained batch 7487 batch loss 6.13537312 epoch total loss 5.86508179\n",
      "Trained batch 7488 batch loss 5.6869545 epoch total loss 5.86505795\n",
      "Trained batch 7489 batch loss 5.44415188 epoch total loss 5.86500216\n",
      "Trained batch 7490 batch loss 6.08986759 epoch total loss 5.8650322\n",
      "Trained batch 7491 batch loss 6.34852695 epoch total loss 5.86509657\n",
      "Trained batch 7492 batch loss 5.71195126 epoch total loss 5.86507607\n",
      "Trained batch 7493 batch loss 6.32603884 epoch total loss 5.8651371\n",
      "Trained batch 7494 batch loss 6.28284 epoch total loss 5.86519289\n",
      "Trained batch 7495 batch loss 6.69024754 epoch total loss 5.86530304\n",
      "Trained batch 7496 batch loss 6.65371323 epoch total loss 5.86540794\n",
      "Trained batch 7497 batch loss 6.48268461 epoch total loss 5.86549044\n",
      "Trained batch 7498 batch loss 6.05612183 epoch total loss 5.86551571\n",
      "Trained batch 7499 batch loss 6.84434 epoch total loss 5.86564636\n",
      "Trained batch 7500 batch loss 6.36155319 epoch total loss 5.86571264\n",
      "Trained batch 7501 batch loss 6.27867 epoch total loss 5.86576748\n",
      "Trained batch 7502 batch loss 6.63318062 epoch total loss 5.86586952\n",
      "Trained batch 7503 batch loss 6.32022476 epoch total loss 5.86593\n",
      "Trained batch 7504 batch loss 6.26058197 epoch total loss 5.86598301\n",
      "Trained batch 7505 batch loss 6.25095654 epoch total loss 5.86603403\n",
      "Trained batch 7506 batch loss 6.20642 epoch total loss 5.86607933\n",
      "Trained batch 7507 batch loss 6.16110373 epoch total loss 5.86611891\n",
      "Trained batch 7508 batch loss 6.05337191 epoch total loss 5.8661437\n",
      "Trained batch 7509 batch loss 6.52222824 epoch total loss 5.86623144\n",
      "Trained batch 7510 batch loss 6.5487318 epoch total loss 5.86632204\n",
      "Trained batch 7511 batch loss 6.42906 epoch total loss 5.8663969\n",
      "Trained batch 7512 batch loss 6.88738775 epoch total loss 5.8665328\n",
      "Trained batch 7513 batch loss 6.53267336 epoch total loss 5.86662149\n",
      "Trained batch 7514 batch loss 7.09659481 epoch total loss 5.86678505\n",
      "Trained batch 7515 batch loss 6.50451946 epoch total loss 5.86687\n",
      "Trained batch 7516 batch loss 6.36337328 epoch total loss 5.86693573\n",
      "Trained batch 7517 batch loss 6.81416702 epoch total loss 5.86706161\n",
      "Trained batch 7518 batch loss 6.39351654 epoch total loss 5.86713171\n",
      "Trained batch 7519 batch loss 6.13085651 epoch total loss 5.86716652\n",
      "Trained batch 7520 batch loss 6.48540211 epoch total loss 5.86724901\n",
      "Trained batch 7521 batch loss 5.71498299 epoch total loss 5.86722851\n",
      "Trained batch 7522 batch loss 5.85853481 epoch total loss 5.86722755\n",
      "Trained batch 7523 batch loss 5.38678741 epoch total loss 5.86716366\n",
      "Trained batch 7524 batch loss 5.28374672 epoch total loss 5.86708641\n",
      "Trained batch 7525 batch loss 4.03359509 epoch total loss 5.86684275\n",
      "Trained batch 7526 batch loss 4.64941311 epoch total loss 5.8666811\n",
      "Trained batch 7527 batch loss 4.69635201 epoch total loss 5.86652517\n",
      "Trained batch 7528 batch loss 5.27689838 epoch total loss 5.86644697\n",
      "Trained batch 7529 batch loss 4.92717791 epoch total loss 5.86632204\n",
      "Trained batch 7530 batch loss 5.03316402 epoch total loss 5.86621141\n",
      "Trained batch 7531 batch loss 4.7250371 epoch total loss 5.86606\n",
      "Trained batch 7532 batch loss 4.98960781 epoch total loss 5.86594343\n",
      "Trained batch 7533 batch loss 5.09002399 epoch total loss 5.86584044\n",
      "Trained batch 7534 batch loss 5.00488 epoch total loss 5.86572599\n",
      "Trained batch 7535 batch loss 5.54142094 epoch total loss 5.86568308\n",
      "Trained batch 7536 batch loss 6.24746609 epoch total loss 5.86573362\n",
      "Trained batch 7537 batch loss 5.6149044 epoch total loss 5.86570024\n",
      "Trained batch 7538 batch loss 6.89822769 epoch total loss 5.8658371\n",
      "Trained batch 7539 batch loss 5.53536 epoch total loss 5.86579323\n",
      "Trained batch 7540 batch loss 6.1580739 epoch total loss 5.86583185\n",
      "Trained batch 7541 batch loss 5.67765045 epoch total loss 5.86580658\n",
      "Trained batch 7542 batch loss 6.71113205 epoch total loss 5.86591864\n",
      "Trained batch 7543 batch loss 6.20614719 epoch total loss 5.86596394\n",
      "Trained batch 7544 batch loss 5.23391199 epoch total loss 5.86588\n",
      "Trained batch 7545 batch loss 6.30944872 epoch total loss 5.86593866\n",
      "Trained batch 7546 batch loss 5.49157238 epoch total loss 5.86588907\n",
      "Trained batch 7547 batch loss 5.82019615 epoch total loss 5.86588335\n",
      "Trained batch 7548 batch loss 5.92834187 epoch total loss 5.86589146\n",
      "Trained batch 7549 batch loss 5.92943525 epoch total loss 5.8659\n",
      "Trained batch 7550 batch loss 5.39788485 epoch total loss 5.86583805\n",
      "Trained batch 7551 batch loss 5.70648432 epoch total loss 5.86581707\n",
      "Trained batch 7552 batch loss 5.2622118 epoch total loss 5.86573696\n",
      "Trained batch 7553 batch loss 3.93987656 epoch total loss 5.86548233\n",
      "Trained batch 7554 batch loss 6.03862762 epoch total loss 5.86550522\n",
      "Trained batch 7555 batch loss 5.49345207 epoch total loss 5.8654561\n",
      "Trained batch 7556 batch loss 5.39619303 epoch total loss 5.86539364\n",
      "Trained batch 7557 batch loss 5.65832806 epoch total loss 5.86536646\n",
      "Trained batch 7558 batch loss 5.82440758 epoch total loss 5.86536121\n",
      "Trained batch 7559 batch loss 5.50966501 epoch total loss 5.86531353\n",
      "Trained batch 7560 batch loss 5.39420176 epoch total loss 5.86525154\n",
      "Trained batch 7561 batch loss 5.63554764 epoch total loss 5.86522102\n",
      "Trained batch 7562 batch loss 5.28023481 epoch total loss 5.86514378\n",
      "Trained batch 7563 batch loss 5.16101265 epoch total loss 5.86505079\n",
      "Trained batch 7564 batch loss 5.82168102 epoch total loss 5.86504507\n",
      "Trained batch 7565 batch loss 5.61914682 epoch total loss 5.86501265\n",
      "Trained batch 7566 batch loss 6.15561771 epoch total loss 5.86505127\n",
      "Trained batch 7567 batch loss 5.40289116 epoch total loss 5.86498976\n",
      "Trained batch 7568 batch loss 5.78185 epoch total loss 5.86497879\n",
      "Trained batch 7569 batch loss 5.5264616 epoch total loss 5.86493444\n",
      "Trained batch 7570 batch loss 5.30053473 epoch total loss 5.86485958\n",
      "Trained batch 7571 batch loss 5.68985415 epoch total loss 5.86483669\n",
      "Trained batch 7572 batch loss 5.5844 epoch total loss 5.8648\n",
      "Trained batch 7573 batch loss 5.41451359 epoch total loss 5.86474037\n",
      "Trained batch 7574 batch loss 5.5857935 epoch total loss 5.86470366\n",
      "Trained batch 7575 batch loss 5.35919476 epoch total loss 5.8646369\n",
      "Trained batch 7576 batch loss 5.98717356 epoch total loss 5.86465311\n",
      "Trained batch 7577 batch loss 5.16959524 epoch total loss 5.86456156\n",
      "Trained batch 7578 batch loss 6.16715574 epoch total loss 5.86460161\n",
      "Trained batch 7579 batch loss 5.42281818 epoch total loss 5.86454296\n",
      "Trained batch 7580 batch loss 5.08467245 epoch total loss 5.86444044\n",
      "Trained batch 7581 batch loss 6.63851547 epoch total loss 5.86454201\n",
      "Trained batch 7582 batch loss 5.5575037 epoch total loss 5.86450195\n",
      "Trained batch 7583 batch loss 5.25898075 epoch total loss 5.86442184\n",
      "Trained batch 7584 batch loss 5.65249968 epoch total loss 5.86439371\n",
      "Trained batch 7585 batch loss 5.70030212 epoch total loss 5.86437178\n",
      "Trained batch 7586 batch loss 5.35314894 epoch total loss 5.86430454\n",
      "Trained batch 7587 batch loss 5.04642582 epoch total loss 5.86419678\n",
      "Trained batch 7588 batch loss 5.62909794 epoch total loss 5.86416578\n",
      "Trained batch 7589 batch loss 5.62239 epoch total loss 5.86413383\n",
      "Trained batch 7590 batch loss 5.26926708 epoch total loss 5.86405516\n",
      "Trained batch 7591 batch loss 5.53224707 epoch total loss 5.86401129\n",
      "Trained batch 7592 batch loss 5.20101833 epoch total loss 5.86392403\n",
      "Trained batch 7593 batch loss 5.3747921 epoch total loss 5.86385965\n",
      "Trained batch 7594 batch loss 5.31414509 epoch total loss 5.8637867\n",
      "Trained batch 7595 batch loss 5.00908279 epoch total loss 5.86367416\n",
      "Trained batch 7596 batch loss 5.23681402 epoch total loss 5.86359167\n",
      "Trained batch 7597 batch loss 6.07481575 epoch total loss 5.86362\n",
      "Trained batch 7598 batch loss 5.35725546 epoch total loss 5.86355257\n",
      "Trained batch 7599 batch loss 5.5370121 epoch total loss 5.86350965\n",
      "Trained batch 7600 batch loss 6.00211048 epoch total loss 5.86352777\n",
      "Trained batch 7601 batch loss 5.31662416 epoch total loss 5.86345577\n",
      "Trained batch 7602 batch loss 5.25256634 epoch total loss 5.86337566\n",
      "Trained batch 7603 batch loss 5.37170553 epoch total loss 5.86331081\n",
      "Trained batch 7604 batch loss 5.73440647 epoch total loss 5.86329412\n",
      "Trained batch 7605 batch loss 6.31266212 epoch total loss 5.86335325\n",
      "Trained batch 7606 batch loss 5.72775555 epoch total loss 5.86333513\n",
      "Trained batch 7607 batch loss 5.88631153 epoch total loss 5.86333847\n",
      "Trained batch 7608 batch loss 6.25306463 epoch total loss 5.86338949\n",
      "Trained batch 7609 batch loss 5.72532511 epoch total loss 5.86337137\n",
      "Trained batch 7610 batch loss 5.89169931 epoch total loss 5.86337519\n",
      "Trained batch 7611 batch loss 5.80606461 epoch total loss 5.86336756\n",
      "Trained batch 7612 batch loss 6.04875422 epoch total loss 5.8633914\n",
      "Trained batch 7613 batch loss 5.99440384 epoch total loss 5.86340904\n",
      "Trained batch 7614 batch loss 6.40533495 epoch total loss 5.86348\n",
      "Trained batch 7615 batch loss 5.72699165 epoch total loss 5.86346245\n",
      "Trained batch 7616 batch loss 5.08503771 epoch total loss 5.8633604\n",
      "Trained batch 7617 batch loss 5.0558567 epoch total loss 5.86325407\n",
      "Trained batch 7618 batch loss 5.57001781 epoch total loss 5.86321545\n",
      "Trained batch 7619 batch loss 5.67395639 epoch total loss 5.86319113\n",
      "Trained batch 7620 batch loss 5.41484642 epoch total loss 5.863132\n",
      "Trained batch 7621 batch loss 4.83493519 epoch total loss 5.86299706\n",
      "Trained batch 7622 batch loss 4.46086073 epoch total loss 5.86281347\n",
      "Trained batch 7623 batch loss 5.27665377 epoch total loss 5.8627367\n",
      "Trained batch 7624 batch loss 5.77778149 epoch total loss 5.86272526\n",
      "Trained batch 7625 batch loss 5.62764835 epoch total loss 5.86269474\n",
      "Trained batch 7626 batch loss 5.34527588 epoch total loss 5.86262655\n",
      "Trained batch 7627 batch loss 5.59272 epoch total loss 5.86259127\n",
      "Trained batch 7628 batch loss 5.88177 epoch total loss 5.86259413\n",
      "Trained batch 7629 batch loss 5.66934347 epoch total loss 5.86256838\n",
      "Trained batch 7630 batch loss 5.99699593 epoch total loss 5.86258602\n",
      "Trained batch 7631 batch loss 4.69700146 epoch total loss 5.86243296\n",
      "Trained batch 7632 batch loss 6.06154776 epoch total loss 5.86245918\n",
      "Trained batch 7633 batch loss 5.72315788 epoch total loss 5.86244106\n",
      "Trained batch 7634 batch loss 4.82270479 epoch total loss 5.86230516\n",
      "Trained batch 7635 batch loss 5.13920879 epoch total loss 5.86221027\n",
      "Trained batch 7636 batch loss 5.59817791 epoch total loss 5.86217594\n",
      "Trained batch 7637 batch loss 5.43539286 epoch total loss 5.86211967\n",
      "Trained batch 7638 batch loss 5.13993216 epoch total loss 5.86202526\n",
      "Trained batch 7639 batch loss 5.85077 epoch total loss 5.86202383\n",
      "Trained batch 7640 batch loss 4.96798038 epoch total loss 5.86190701\n",
      "Trained batch 7641 batch loss 5.16203833 epoch total loss 5.86181498\n",
      "Trained batch 7642 batch loss 6.26693726 epoch total loss 5.8618679\n",
      "Trained batch 7643 batch loss 5.85953569 epoch total loss 5.86186743\n",
      "Trained batch 7644 batch loss 6.84723234 epoch total loss 5.86199665\n",
      "Trained batch 7645 batch loss 6.45128489 epoch total loss 5.8620739\n",
      "Trained batch 7646 batch loss 5.58705902 epoch total loss 5.86203766\n",
      "Trained batch 7647 batch loss 5.38573885 epoch total loss 5.86197567\n",
      "Trained batch 7648 batch loss 6.20259857 epoch total loss 5.86202\n",
      "Trained batch 7649 batch loss 5.79600334 epoch total loss 5.86201191\n",
      "Trained batch 7650 batch loss 5.87336826 epoch total loss 5.86201334\n",
      "Trained batch 7651 batch loss 5.59144354 epoch total loss 5.86197758\n",
      "Trained batch 7652 batch loss 4.38451195 epoch total loss 5.86178446\n",
      "Trained batch 7653 batch loss 6.4898634 epoch total loss 5.86186647\n",
      "Trained batch 7654 batch loss 6.07260561 epoch total loss 5.86189413\n",
      "Trained batch 7655 batch loss 5.68724632 epoch total loss 5.86187124\n",
      "Trained batch 7656 batch loss 6.57101345 epoch total loss 5.86196375\n",
      "Trained batch 7657 batch loss 5.7693491 epoch total loss 5.86195183\n",
      "Trained batch 7658 batch loss 5.18040371 epoch total loss 5.86186266\n",
      "Trained batch 7659 batch loss 4.81677437 epoch total loss 5.86172628\n",
      "Trained batch 7660 batch loss 5.6368084 epoch total loss 5.86169672\n",
      "Trained batch 7661 batch loss 5.77440357 epoch total loss 5.86168528\n",
      "Trained batch 7662 batch loss 5.63457489 epoch total loss 5.86165524\n",
      "Trained batch 7663 batch loss 5.80290365 epoch total loss 5.86164808\n",
      "Trained batch 7664 batch loss 6.03226 epoch total loss 5.86167\n",
      "Trained batch 7665 batch loss 6.13356113 epoch total loss 5.8617053\n",
      "Trained batch 7666 batch loss 5.87654877 epoch total loss 5.86170721\n",
      "Trained batch 7667 batch loss 5.07249451 epoch total loss 5.86160469\n",
      "Trained batch 7668 batch loss 5.73133659 epoch total loss 5.86158752\n",
      "Trained batch 7669 batch loss 5.97921371 epoch total loss 5.86160278\n",
      "Trained batch 7670 batch loss 5.20560169 epoch total loss 5.86151743\n",
      "Trained batch 7671 batch loss 5.83792114 epoch total loss 5.86151457\n",
      "Trained batch 7672 batch loss 5.26961327 epoch total loss 5.8614378\n",
      "Trained batch 7673 batch loss 5.5342207 epoch total loss 5.86139488\n",
      "Trained batch 7674 batch loss 5.77881 epoch total loss 5.86138391\n",
      "Trained batch 7675 batch loss 5.71272945 epoch total loss 5.86136436\n",
      "Trained batch 7676 batch loss 5.86925936 epoch total loss 5.8613658\n",
      "Trained batch 7677 batch loss 5.72132301 epoch total loss 5.86134768\n",
      "Trained batch 7678 batch loss 5.08215141 epoch total loss 5.86124611\n",
      "Trained batch 7679 batch loss 5.97919464 epoch total loss 5.86126184\n",
      "Trained batch 7680 batch loss 6.14339399 epoch total loss 5.86129856\n",
      "Trained batch 7681 batch loss 4.16823864 epoch total loss 5.86107826\n",
      "Trained batch 7682 batch loss 6.44355488 epoch total loss 5.86115408\n",
      "Trained batch 7683 batch loss 6.26927567 epoch total loss 5.86120749\n",
      "Trained batch 7684 batch loss 5.79449654 epoch total loss 5.86119843\n",
      "Trained batch 7685 batch loss 6.24918652 epoch total loss 5.86124897\n",
      "Trained batch 7686 batch loss 5.85203934 epoch total loss 5.86124802\n",
      "Trained batch 7687 batch loss 6.18086147 epoch total loss 5.86128902\n",
      "Trained batch 7688 batch loss 6.54483128 epoch total loss 5.86137772\n",
      "Trained batch 7689 batch loss 5.29623032 epoch total loss 5.86130428\n",
      "Trained batch 7690 batch loss 5.83926153 epoch total loss 5.8613019\n",
      "Trained batch 7691 batch loss 5.64480734 epoch total loss 5.86127329\n",
      "Trained batch 7692 batch loss 5.76385927 epoch total loss 5.86126089\n",
      "Trained batch 7693 batch loss 5.53102398 epoch total loss 5.86121798\n",
      "Trained batch 7694 batch loss 4.89483166 epoch total loss 5.86109257\n",
      "Trained batch 7695 batch loss 4.71198511 epoch total loss 5.86094332\n",
      "Trained batch 7696 batch loss 4.42638683 epoch total loss 5.8607564\n",
      "Trained batch 7697 batch loss 4.66221905 epoch total loss 5.86060095\n",
      "Trained batch 7698 batch loss 4.92124462 epoch total loss 5.86047935\n",
      "Trained batch 7699 batch loss 4.70914459 epoch total loss 5.86033\n",
      "Trained batch 7700 batch loss 5.09192085 epoch total loss 5.86023045\n",
      "Trained batch 7701 batch loss 6.50616646 epoch total loss 5.86031437\n",
      "Trained batch 7702 batch loss 4.92510509 epoch total loss 5.86019325\n",
      "Trained batch 7703 batch loss 5.04791498 epoch total loss 5.86008739\n",
      "Trained batch 7704 batch loss 5.14517498 epoch total loss 5.85999441\n",
      "Trained batch 7705 batch loss 5.04797602 epoch total loss 5.85988903\n",
      "Trained batch 7706 batch loss 4.77828884 epoch total loss 5.85974836\n",
      "Trained batch 7707 batch loss 4.91243076 epoch total loss 5.85962582\n",
      "Trained batch 7708 batch loss 4.6538372 epoch total loss 5.85946941\n",
      "Trained batch 7709 batch loss 4.59070587 epoch total loss 5.85930443\n",
      "Trained batch 7710 batch loss 3.97712779 epoch total loss 5.85906029\n",
      "Trained batch 7711 batch loss 3.92212796 epoch total loss 5.85880899\n",
      "Trained batch 7712 batch loss 4.5414238 epoch total loss 5.85863876\n",
      "Trained batch 7713 batch loss 3.94282675 epoch total loss 5.85839\n",
      "Trained batch 7714 batch loss 4.18241835 epoch total loss 5.85817289\n",
      "Trained batch 7715 batch loss 4.18448877 epoch total loss 5.85795593\n",
      "Trained batch 7716 batch loss 4.19860935 epoch total loss 5.85774088\n",
      "Trained batch 7717 batch loss 4.75146866 epoch total loss 5.85759735\n",
      "Trained batch 7718 batch loss 4.4707756 epoch total loss 5.85741806\n",
      "Trained batch 7719 batch loss 3.91588831 epoch total loss 5.85716629\n",
      "Trained batch 7720 batch loss 6.31015682 epoch total loss 5.85722446\n",
      "Trained batch 7721 batch loss 4.96709585 epoch total loss 5.85710955\n",
      "Trained batch 7722 batch loss 5.51409578 epoch total loss 5.8570652\n",
      "Trained batch 7723 batch loss 5.76040554 epoch total loss 5.8570528\n",
      "Trained batch 7724 batch loss 6.44951916 epoch total loss 5.85712957\n",
      "Trained batch 7725 batch loss 6.21284819 epoch total loss 5.85717535\n",
      "Trained batch 7726 batch loss 6.45494938 epoch total loss 5.8572526\n",
      "Trained batch 7727 batch loss 6.10667801 epoch total loss 5.85728455\n",
      "Trained batch 7728 batch loss 6.47787189 epoch total loss 5.85736465\n",
      "Trained batch 7729 batch loss 6.08436584 epoch total loss 5.85739422\n",
      "Trained batch 7730 batch loss 7.04833317 epoch total loss 5.85754824\n",
      "Trained batch 7731 batch loss 6.80966854 epoch total loss 5.85767126\n",
      "Trained batch 7732 batch loss 6.50685692 epoch total loss 5.85775518\n",
      "Trained batch 7733 batch loss 6.45673609 epoch total loss 5.85783291\n",
      "Trained batch 7734 batch loss 6.11331701 epoch total loss 5.85786581\n",
      "Trained batch 7735 batch loss 6.0715065 epoch total loss 5.85789347\n",
      "Trained batch 7736 batch loss 5.72210503 epoch total loss 5.85787582\n",
      "Trained batch 7737 batch loss 5.79628 epoch total loss 5.85786772\n",
      "Trained batch 7738 batch loss 5.37338495 epoch total loss 5.85780573\n",
      "Trained batch 7739 batch loss 5.90858936 epoch total loss 5.8578124\n",
      "Trained batch 7740 batch loss 6.14648581 epoch total loss 5.85785\n",
      "Trained batch 7741 batch loss 5.76222229 epoch total loss 5.8578372\n",
      "Trained batch 7742 batch loss 5.74913454 epoch total loss 5.85782337\n",
      "Trained batch 7743 batch loss 6.19611359 epoch total loss 5.85786724\n",
      "Trained batch 7744 batch loss 5.24522781 epoch total loss 5.85778809\n",
      "Trained batch 7745 batch loss 6.17708826 epoch total loss 5.85782909\n",
      "Trained batch 7746 batch loss 6.29471159 epoch total loss 5.85788536\n",
      "Trained batch 7747 batch loss 5.23772335 epoch total loss 5.85780525\n",
      "Trained batch 7748 batch loss 4.26173782 epoch total loss 5.85759926\n",
      "Trained batch 7749 batch loss 4.62434101 epoch total loss 5.85744047\n",
      "Trained batch 7750 batch loss 5.76287031 epoch total loss 5.85742807\n",
      "Trained batch 7751 batch loss 6.10692501 epoch total loss 5.85746\n",
      "Trained batch 7752 batch loss 6.04032707 epoch total loss 5.85748339\n",
      "Trained batch 7753 batch loss 4.80139828 epoch total loss 5.85734701\n",
      "Trained batch 7754 batch loss 5.56232882 epoch total loss 5.85730886\n",
      "Trained batch 7755 batch loss 5.76392078 epoch total loss 5.85729742\n",
      "Trained batch 7756 batch loss 5.71472073 epoch total loss 5.85727882\n",
      "Trained batch 7757 batch loss 5.86931705 epoch total loss 5.85728073\n",
      "Trained batch 7758 batch loss 6.33529377 epoch total loss 5.85734224\n",
      "Trained batch 7759 batch loss 5.34458828 epoch total loss 5.85727596\n",
      "Trained batch 7760 batch loss 5.8903017 epoch total loss 5.85728025\n",
      "Trained batch 7761 batch loss 5.65210629 epoch total loss 5.85725403\n",
      "Trained batch 7762 batch loss 6.13628483 epoch total loss 5.85729\n",
      "Trained batch 7763 batch loss 5.49494267 epoch total loss 5.85724354\n",
      "Trained batch 7764 batch loss 5.95095348 epoch total loss 5.85725546\n",
      "Trained batch 7765 batch loss 5.94184589 epoch total loss 5.85726595\n",
      "Trained batch 7766 batch loss 5.49521637 epoch total loss 5.8572197\n",
      "Trained batch 7767 batch loss 5.32766628 epoch total loss 5.85715151\n",
      "Trained batch 7768 batch loss 4.87888241 epoch total loss 5.85702562\n",
      "Trained batch 7769 batch loss 5.44769049 epoch total loss 5.85697317\n",
      "Trained batch 7770 batch loss 4.44609928 epoch total loss 5.8567915\n",
      "Trained batch 7771 batch loss 5.35771656 epoch total loss 5.8567276\n",
      "Trained batch 7772 batch loss 4.98814774 epoch total loss 5.85661554\n",
      "Trained batch 7773 batch loss 5.01890373 epoch total loss 5.85650778\n",
      "Trained batch 7774 batch loss 5.17999554 epoch total loss 5.85642099\n",
      "Trained batch 7775 batch loss 4.91248894 epoch total loss 5.8563\n",
      "Trained batch 7776 batch loss 5.0914669 epoch total loss 5.85620117\n",
      "Trained batch 7777 batch loss 4.43006897 epoch total loss 5.85601759\n",
      "Trained batch 7778 batch loss 5.20828342 epoch total loss 5.85593414\n",
      "Trained batch 7779 batch loss 5.8869276 epoch total loss 5.85593843\n",
      "Trained batch 7780 batch loss 5.29792595 epoch total loss 5.85586643\n",
      "Trained batch 7781 batch loss 5.82874584 epoch total loss 5.85586262\n",
      "Trained batch 7782 batch loss 4.89456224 epoch total loss 5.85573912\n",
      "Trained batch 7783 batch loss 6.2908206 epoch total loss 5.85579491\n",
      "Trained batch 7784 batch loss 5.06991482 epoch total loss 5.85569429\n",
      "Trained batch 7785 batch loss 5.806777 epoch total loss 5.8556881\n",
      "Trained batch 7786 batch loss 5.74030399 epoch total loss 5.85567331\n",
      "Trained batch 7787 batch loss 5.89665365 epoch total loss 5.85567904\n",
      "Trained batch 7788 batch loss 5.91522694 epoch total loss 5.85568666\n",
      "Trained batch 7789 batch loss 5.87282848 epoch total loss 5.85568857\n",
      "Trained batch 7790 batch loss 5.0144124 epoch total loss 5.85558081\n",
      "Trained batch 7791 batch loss 4.97773647 epoch total loss 5.8554678\n",
      "Trained batch 7792 batch loss 4.91118622 epoch total loss 5.8553462\n",
      "Trained batch 7793 batch loss 6.32425642 epoch total loss 5.85540676\n",
      "Trained batch 7794 batch loss 7.19351768 epoch total loss 5.85557842\n",
      "Trained batch 7795 batch loss 4.62954903 epoch total loss 5.85542107\n",
      "Trained batch 7796 batch loss 5.92519617 epoch total loss 5.85543\n",
      "Trained batch 7797 batch loss 5.86271667 epoch total loss 5.85543108\n",
      "Trained batch 7798 batch loss 6.21411705 epoch total loss 5.85547733\n",
      "Trained batch 7799 batch loss 5.52814865 epoch total loss 5.85543537\n",
      "Trained batch 7800 batch loss 5.39313793 epoch total loss 5.85537624\n",
      "Trained batch 7801 batch loss 4.07774258 epoch total loss 5.85514832\n",
      "Trained batch 7802 batch loss 6.12608385 epoch total loss 5.85518265\n",
      "Trained batch 7803 batch loss 4.63669586 epoch total loss 5.85502672\n",
      "Trained batch 7804 batch loss 5.85994577 epoch total loss 5.8550272\n",
      "Trained batch 7805 batch loss 5.98318529 epoch total loss 5.85504389\n",
      "Trained batch 7806 batch loss 5.69518852 epoch total loss 5.85502338\n",
      "Trained batch 7807 batch loss 5.77746296 epoch total loss 5.85501337\n",
      "Trained batch 7808 batch loss 6.7491703 epoch total loss 5.85512829\n",
      "Trained batch 7809 batch loss 4.61278 epoch total loss 5.85496902\n",
      "Trained batch 7810 batch loss 4.06137705 epoch total loss 5.85473967\n",
      "Trained batch 7811 batch loss 4.77197647 epoch total loss 5.85460091\n",
      "Trained batch 7812 batch loss 5.54481459 epoch total loss 5.85456133\n",
      "Trained batch 7813 batch loss 5.02808762 epoch total loss 5.85445547\n",
      "Trained batch 7814 batch loss 4.61912251 epoch total loss 5.85429716\n",
      "Trained batch 7815 batch loss 5.11689711 epoch total loss 5.85420275\n",
      "Trained batch 7816 batch loss 4.4553647 epoch total loss 5.85402393\n",
      "Trained batch 7817 batch loss 4.09936476 epoch total loss 5.85379934\n",
      "Trained batch 7818 batch loss 6.38803244 epoch total loss 5.85386753\n",
      "Trained batch 7819 batch loss 6.51436806 epoch total loss 5.85395193\n",
      "Trained batch 7820 batch loss 6.40342236 epoch total loss 5.85402203\n",
      "Trained batch 7821 batch loss 4.74539661 epoch total loss 5.85388041\n",
      "Trained batch 7822 batch loss 5.81769657 epoch total loss 5.85387564\n",
      "Trained batch 7823 batch loss 6.79792 epoch total loss 5.85399628\n",
      "Trained batch 7824 batch loss 4.99198961 epoch total loss 5.85388613\n",
      "Trained batch 7825 batch loss 5.22087288 epoch total loss 5.85380554\n",
      "Trained batch 7826 batch loss 7.52867699 epoch total loss 5.85401917\n",
      "Trained batch 7827 batch loss 6.48279476 epoch total loss 5.85409975\n",
      "Trained batch 7828 batch loss 6.00191212 epoch total loss 5.85411835\n",
      "Trained batch 7829 batch loss 6.12293625 epoch total loss 5.85415268\n",
      "Trained batch 7830 batch loss 6.66256046 epoch total loss 5.85425615\n",
      "Trained batch 7831 batch loss 6.00725651 epoch total loss 5.8542757\n",
      "Trained batch 7832 batch loss 6.40930891 epoch total loss 5.85434675\n",
      "Trained batch 7833 batch loss 7.37742138 epoch total loss 5.8545413\n",
      "Trained batch 7834 batch loss 6.10013437 epoch total loss 5.85457277\n",
      "Trained batch 7835 batch loss 5.84677029 epoch total loss 5.85457182\n",
      "Trained batch 7836 batch loss 5.67127419 epoch total loss 5.85454845\n",
      "Trained batch 7837 batch loss 5.21862888 epoch total loss 5.85446739\n",
      "Trained batch 7838 batch loss 5.52132702 epoch total loss 5.85442448\n",
      "Trained batch 7839 batch loss 5.44886541 epoch total loss 5.85437298\n",
      "Trained batch 7840 batch loss 5.36626244 epoch total loss 5.85431099\n",
      "Trained batch 7841 batch loss 5.83523464 epoch total loss 5.85430861\n",
      "Trained batch 7842 batch loss 5.93690395 epoch total loss 5.8543191\n",
      "Trained batch 7843 batch loss 5.56760406 epoch total loss 5.85428238\n",
      "Trained batch 7844 batch loss 5.43240738 epoch total loss 5.85422897\n",
      "Trained batch 7845 batch loss 5.72467899 epoch total loss 5.85421228\n",
      "Trained batch 7846 batch loss 5.48952198 epoch total loss 5.85416603\n",
      "Trained batch 7847 batch loss 5.11918211 epoch total loss 5.85407257\n",
      "Trained batch 7848 batch loss 5.23435116 epoch total loss 5.85399342\n",
      "Trained batch 7849 batch loss 5.57854795 epoch total loss 5.85395813\n",
      "Trained batch 7850 batch loss 5.5379777 epoch total loss 5.85391808\n",
      "Trained batch 7851 batch loss 4.91910648 epoch total loss 5.85379887\n",
      "Trained batch 7852 batch loss 5.10499287 epoch total loss 5.8537035\n",
      "Trained batch 7853 batch loss 5.15648842 epoch total loss 5.85361481\n",
      "Trained batch 7854 batch loss 5.34668636 epoch total loss 5.85355043\n",
      "Trained batch 7855 batch loss 5.12457085 epoch total loss 5.85345793\n",
      "Trained batch 7856 batch loss 5.93716478 epoch total loss 5.85346842\n",
      "Trained batch 7857 batch loss 5.73791504 epoch total loss 5.85345364\n",
      "Trained batch 7858 batch loss 5.5172205 epoch total loss 5.85341072\n",
      "Trained batch 7859 batch loss 5.91461134 epoch total loss 5.85341835\n",
      "Trained batch 7860 batch loss 5.36478519 epoch total loss 5.85335588\n",
      "Trained batch 7861 batch loss 5.59131241 epoch total loss 5.85332251\n",
      "Trained batch 7862 batch loss 5.80679703 epoch total loss 5.85331678\n",
      "Trained batch 7863 batch loss 5.87461281 epoch total loss 5.85331964\n",
      "Trained batch 7864 batch loss 5.95027542 epoch total loss 5.85333204\n",
      "Trained batch 7865 batch loss 6.1449728 epoch total loss 5.85336876\n",
      "Trained batch 7866 batch loss 6.14762783 epoch total loss 5.85340643\n",
      "Trained batch 7867 batch loss 5.67951 epoch total loss 5.85338449\n",
      "Trained batch 7868 batch loss 5.61257458 epoch total loss 5.85335398\n",
      "Trained batch 7869 batch loss 5.46042824 epoch total loss 5.85330391\n",
      "Trained batch 7870 batch loss 5.54653072 epoch total loss 5.85326481\n",
      "Trained batch 7871 batch loss 6.11977625 epoch total loss 5.85329914\n",
      "Trained batch 7872 batch loss 5.89483643 epoch total loss 5.85330439\n",
      "Trained batch 7873 batch loss 5.61702824 epoch total loss 5.85327435\n",
      "Trained batch 7874 batch loss 6.1322937 epoch total loss 5.85330963\n",
      "Trained batch 7875 batch loss 6.83787823 epoch total loss 5.85343456\n",
      "Trained batch 7876 batch loss 7.15367413 epoch total loss 5.85359955\n",
      "Trained batch 7877 batch loss 7.27666759 epoch total loss 5.85378027\n",
      "Trained batch 7878 batch loss 7.36110115 epoch total loss 5.85397148\n",
      "Trained batch 7879 batch loss 6.76344204 epoch total loss 5.8540864\n",
      "Trained batch 7880 batch loss 6.1251955 epoch total loss 5.85412073\n",
      "Trained batch 7881 batch loss 6.12604 epoch total loss 5.85415506\n",
      "Trained batch 7882 batch loss 5.7428236 epoch total loss 5.85414124\n",
      "Trained batch 7883 batch loss 5.78705406 epoch total loss 5.85413218\n",
      "Trained batch 7884 batch loss 5.56509781 epoch total loss 5.85409594\n",
      "Trained batch 7885 batch loss 5.05473614 epoch total loss 5.85399437\n",
      "Trained batch 7886 batch loss 6.87059736 epoch total loss 5.85412359\n",
      "Trained batch 7887 batch loss 6.44135475 epoch total loss 5.85419798\n",
      "Trained batch 7888 batch loss 5.99131775 epoch total loss 5.85421515\n",
      "Trained batch 7889 batch loss 6.08732128 epoch total loss 5.85424471\n",
      "Trained batch 7890 batch loss 6.16880035 epoch total loss 5.85428429\n",
      "Trained batch 7891 batch loss 5.87695694 epoch total loss 5.85428762\n",
      "Trained batch 7892 batch loss 5.09345293 epoch total loss 5.8541913\n",
      "Trained batch 7893 batch loss 5.3390255 epoch total loss 5.85412598\n",
      "Trained batch 7894 batch loss 5.74378777 epoch total loss 5.85411167\n",
      "Trained batch 7895 batch loss 5.82424402 epoch total loss 5.85410833\n",
      "Trained batch 7896 batch loss 6.00539494 epoch total loss 5.85412693\n",
      "Trained batch 7897 batch loss 5.92496586 epoch total loss 5.85413599\n",
      "Trained batch 7898 batch loss 5.94263458 epoch total loss 5.85414743\n",
      "Trained batch 7899 batch loss 6.29819965 epoch total loss 5.85420322\n",
      "Trained batch 7900 batch loss 6.22060776 epoch total loss 5.85424948\n",
      "Trained batch 7901 batch loss 5.45661068 epoch total loss 5.85419893\n",
      "Trained batch 7902 batch loss 5.618433 epoch total loss 5.85416937\n",
      "Trained batch 7903 batch loss 5.29686356 epoch total loss 5.8540988\n",
      "Trained batch 7904 batch loss 6.3360076 epoch total loss 5.85415936\n",
      "Trained batch 7905 batch loss 5.62168074 epoch total loss 5.85413027\n",
      "Trained batch 7906 batch loss 5.66389179 epoch total loss 5.85410595\n",
      "Trained batch 7907 batch loss 5.58471489 epoch total loss 5.85407209\n",
      "Trained batch 7908 batch loss 5.7051878 epoch total loss 5.8540535\n",
      "Trained batch 7909 batch loss 6.11942768 epoch total loss 5.85408735\n",
      "Trained batch 7910 batch loss 6.03967381 epoch total loss 5.85411072\n",
      "Trained batch 7911 batch loss 6.32314873 epoch total loss 5.85417032\n",
      "Trained batch 7912 batch loss 5.81538725 epoch total loss 5.85416555\n",
      "Trained batch 7913 batch loss 5.78668594 epoch total loss 5.85415649\n",
      "Trained batch 7914 batch loss 5.98768711 epoch total loss 5.85417366\n",
      "Trained batch 7915 batch loss 4.20003319 epoch total loss 5.85396433\n",
      "Trained batch 7916 batch loss 4.76390886 epoch total loss 5.853827\n",
      "Trained batch 7917 batch loss 5.3740654 epoch total loss 5.85376644\n",
      "Trained batch 7918 batch loss 5.72212696 epoch total loss 5.85374975\n",
      "Trained batch 7919 batch loss 5.75038815 epoch total loss 5.85373688\n",
      "Trained batch 7920 batch loss 4.98298168 epoch total loss 5.8536272\n",
      "Trained batch 7921 batch loss 6.67263 epoch total loss 5.85373\n",
      "Trained batch 7922 batch loss 6.68877935 epoch total loss 5.85383558\n",
      "Trained batch 7923 batch loss 5.92555237 epoch total loss 5.85384464\n",
      "Trained batch 7924 batch loss 6.29128838 epoch total loss 5.8539\n",
      "Trained batch 7925 batch loss 6.04381275 epoch total loss 5.8539238\n",
      "Trained batch 7926 batch loss 6.43853569 epoch total loss 5.85399771\n",
      "Trained batch 7927 batch loss 4.80931807 epoch total loss 5.85386562\n",
      "Trained batch 7928 batch loss 6.17160892 epoch total loss 5.85390568\n",
      "Trained batch 7929 batch loss 6.69797325 epoch total loss 5.85401249\n",
      "Trained batch 7930 batch loss 5.70436335 epoch total loss 5.85399342\n",
      "Trained batch 7931 batch loss 6.39231777 epoch total loss 5.85406113\n",
      "Trained batch 7932 batch loss 6.4853 epoch total loss 5.85414076\n",
      "Trained batch 7933 batch loss 6.39588 epoch total loss 5.85420847\n",
      "Trained batch 7934 batch loss 5.57849026 epoch total loss 5.85417366\n",
      "Trained batch 7935 batch loss 5.96676731 epoch total loss 5.85418797\n",
      "Trained batch 7936 batch loss 6.25077629 epoch total loss 5.85423756\n",
      "Trained batch 7937 batch loss 6.9406662 epoch total loss 5.85437489\n",
      "Trained batch 7938 batch loss 6.87928772 epoch total loss 5.85450363\n",
      "Trained batch 7939 batch loss 6.17457485 epoch total loss 5.85454416\n",
      "Trained batch 7940 batch loss 7.41243362 epoch total loss 5.85474062\n",
      "Trained batch 7941 batch loss 6.18117714 epoch total loss 5.85478163\n",
      "Trained batch 7942 batch loss 6.56162214 epoch total loss 5.8548708\n",
      "Trained batch 7943 batch loss 5.91697311 epoch total loss 5.85487843\n",
      "Trained batch 7944 batch loss 6.24279976 epoch total loss 5.85492754\n",
      "Trained batch 7945 batch loss 6.41734409 epoch total loss 5.85499811\n",
      "Trained batch 7946 batch loss 5.93233681 epoch total loss 5.85500813\n",
      "Trained batch 7947 batch loss 5.89848518 epoch total loss 5.85501337\n",
      "Trained batch 7948 batch loss 5.51035595 epoch total loss 5.85497046\n",
      "Trained batch 7949 batch loss 6.68888664 epoch total loss 5.85507536\n",
      "Trained batch 7950 batch loss 6.1588707 epoch total loss 5.85511351\n",
      "Trained batch 7951 batch loss 5.87064362 epoch total loss 5.85511541\n",
      "Trained batch 7952 batch loss 5.55516052 epoch total loss 5.85507774\n",
      "Trained batch 7953 batch loss 5.23403168 epoch total loss 5.85499954\n",
      "Trained batch 7954 batch loss 3.66769552 epoch total loss 5.85472488\n",
      "Trained batch 7955 batch loss 4.85339737 epoch total loss 5.85459852\n",
      "Trained batch 7956 batch loss 4.83312035 epoch total loss 5.85447025\n",
      "Trained batch 7957 batch loss 5.09161 epoch total loss 5.85437393\n",
      "Trained batch 7958 batch loss 5.45302677 epoch total loss 5.85432339\n",
      "Trained batch 7959 batch loss 4.58029461 epoch total loss 5.85416365\n",
      "Trained batch 7960 batch loss 6.16852379 epoch total loss 5.85420322\n",
      "Trained batch 7961 batch loss 6.0010252 epoch total loss 5.85422134\n",
      "Trained batch 7962 batch loss 5.93819141 epoch total loss 5.85423183\n",
      "Trained batch 7963 batch loss 6.08260059 epoch total loss 5.85426044\n",
      "Trained batch 7964 batch loss 5.34525442 epoch total loss 5.85419655\n",
      "Trained batch 7965 batch loss 4.12572289 epoch total loss 5.85397911\n",
      "Trained batch 7966 batch loss 5.78421164 epoch total loss 5.85397053\n",
      "Trained batch 7967 batch loss 5.40969086 epoch total loss 5.85391521\n",
      "Trained batch 7968 batch loss 5.63179111 epoch total loss 5.85388708\n",
      "Trained batch 7969 batch loss 5.39367294 epoch total loss 5.85382938\n",
      "Trained batch 7970 batch loss 4.96240473 epoch total loss 5.85371733\n",
      "Trained batch 7971 batch loss 4.7182951 epoch total loss 5.85357523\n",
      "Trained batch 7972 batch loss 5.02467442 epoch total loss 5.8534708\n",
      "Trained batch 7973 batch loss 5.36544275 epoch total loss 5.85341024\n",
      "Trained batch 7974 batch loss 5.44681263 epoch total loss 5.85335875\n",
      "Trained batch 7975 batch loss 5.487988 epoch total loss 5.85331297\n",
      "Trained batch 7976 batch loss 5.76269245 epoch total loss 5.85330153\n",
      "Trained batch 7977 batch loss 6.45687628 epoch total loss 5.85337734\n",
      "Trained batch 7978 batch loss 6.75140762 epoch total loss 5.85349\n",
      "Trained batch 7979 batch loss 6.27372646 epoch total loss 5.85354233\n",
      "Trained batch 7980 batch loss 7.12649107 epoch total loss 5.85370159\n",
      "Trained batch 7981 batch loss 6.11715651 epoch total loss 5.85373449\n",
      "Trained batch 7982 batch loss 7.03016043 epoch total loss 5.85388231\n",
      "Trained batch 7983 batch loss 5.82173347 epoch total loss 5.85387802\n",
      "Trained batch 7984 batch loss 6.1015358 epoch total loss 5.85390902\n",
      "Trained batch 7985 batch loss 5.76365852 epoch total loss 5.85389757\n",
      "Trained batch 7986 batch loss 6.29907513 epoch total loss 5.85395336\n",
      "Trained batch 7987 batch loss 6.26929855 epoch total loss 5.85400534\n",
      "Trained batch 7988 batch loss 6.54137135 epoch total loss 5.85409164\n",
      "Trained batch 7989 batch loss 6.20210028 epoch total loss 5.85413551\n",
      "Trained batch 7990 batch loss 6.34844208 epoch total loss 5.85419703\n",
      "Trained batch 7991 batch loss 5.947649 epoch total loss 5.85420895\n",
      "Trained batch 7992 batch loss 5.86582279 epoch total loss 5.85421085\n",
      "Trained batch 7993 batch loss 6.45492172 epoch total loss 5.85428572\n",
      "Trained batch 7994 batch loss 6.05981588 epoch total loss 5.85431099\n",
      "Trained batch 7995 batch loss 6.24486256 epoch total loss 5.85436\n",
      "Trained batch 7996 batch loss 5.97871399 epoch total loss 5.85437584\n",
      "Trained batch 7997 batch loss 6.08904552 epoch total loss 5.8544054\n",
      "Trained batch 7998 batch loss 6.15385914 epoch total loss 5.8544426\n",
      "Trained batch 7999 batch loss 6.06193829 epoch total loss 5.85446882\n",
      "Trained batch 8000 batch loss 6.01291 epoch total loss 5.85448837\n",
      "Trained batch 8001 batch loss 6.1105957 epoch total loss 5.85452032\n",
      "Trained batch 8002 batch loss 6.10593891 epoch total loss 5.85455132\n",
      "Trained batch 8003 batch loss 5.71553 epoch total loss 5.85453415\n",
      "Trained batch 8004 batch loss 6.13821936 epoch total loss 5.85456944\n",
      "Trained batch 8005 batch loss 6.41464043 epoch total loss 5.85463905\n",
      "Trained batch 8006 batch loss 5.35247803 epoch total loss 5.85457659\n",
      "Trained batch 8007 batch loss 5.70331955 epoch total loss 5.85455751\n",
      "Trained batch 8008 batch loss 5.42082119 epoch total loss 5.85450363\n",
      "Trained batch 8009 batch loss 5.47717 epoch total loss 5.85445642\n",
      "Trained batch 8010 batch loss 5.45581913 epoch total loss 5.85440683\n",
      "Trained batch 8011 batch loss 5.18288708 epoch total loss 5.85432291\n",
      "Trained batch 8012 batch loss 5.54238224 epoch total loss 5.85428381\n",
      "Trained batch 8013 batch loss 5.91823769 epoch total loss 5.85429192\n",
      "Trained batch 8014 batch loss 5.37160254 epoch total loss 5.85423183\n",
      "Trained batch 8015 batch loss 5.81889486 epoch total loss 5.85422754\n",
      "Trained batch 8016 batch loss 5.49945307 epoch total loss 5.8541832\n",
      "Trained batch 8017 batch loss 4.42992973 epoch total loss 5.85400534\n",
      "Trained batch 8018 batch loss 5.54205608 epoch total loss 5.85396671\n",
      "Trained batch 8019 batch loss 5.79184484 epoch total loss 5.85395908\n",
      "Trained batch 8020 batch loss 5.65069151 epoch total loss 5.85393381\n",
      "Trained batch 8021 batch loss 5.5385313 epoch total loss 5.85389471\n",
      "Trained batch 8022 batch loss 5.21021 epoch total loss 5.8538146\n",
      "Trained batch 8023 batch loss 5.27929354 epoch total loss 5.8537426\n",
      "Trained batch 8024 batch loss 5.8172164 epoch total loss 5.85373831\n",
      "Trained batch 8025 batch loss 5.68296 epoch total loss 5.85371685\n",
      "Trained batch 8026 batch loss 5.71679211 epoch total loss 5.85369968\n",
      "Trained batch 8027 batch loss 5.77799511 epoch total loss 5.85369\n",
      "Trained batch 8028 batch loss 5.22140408 epoch total loss 5.85361147\n",
      "Trained batch 8029 batch loss 5.5398941 epoch total loss 5.85357237\n",
      "Trained batch 8030 batch loss 4.72907639 epoch total loss 5.85343266\n",
      "Trained batch 8031 batch loss 5.21224737 epoch total loss 5.85335255\n",
      "Trained batch 8032 batch loss 6.59759331 epoch total loss 5.85344505\n",
      "Trained batch 8033 batch loss 6.15768 epoch total loss 5.85348272\n",
      "Trained batch 8034 batch loss 6.71880484 epoch total loss 5.85359049\n",
      "Trained batch 8035 batch loss 6.16349411 epoch total loss 5.85362911\n",
      "Trained batch 8036 batch loss 5.91688871 epoch total loss 5.85363722\n",
      "Trained batch 8037 batch loss 6.22399044 epoch total loss 5.85368299\n",
      "Trained batch 8038 batch loss 6.08618546 epoch total loss 5.85371208\n",
      "Trained batch 8039 batch loss 5.77834 epoch total loss 5.85370255\n",
      "Trained batch 8040 batch loss 5.98682261 epoch total loss 5.85371923\n",
      "Trained batch 8041 batch loss 5.89984369 epoch total loss 5.85372496\n",
      "Trained batch 8042 batch loss 6.05219221 epoch total loss 5.85374928\n",
      "Trained batch 8043 batch loss 6.06813288 epoch total loss 5.8537755\n",
      "Trained batch 8044 batch loss 6.18808317 epoch total loss 5.85381699\n",
      "Trained batch 8045 batch loss 6.15715694 epoch total loss 5.85385466\n",
      "Trained batch 8046 batch loss 5.53406429 epoch total loss 5.85381508\n",
      "Trained batch 8047 batch loss 6.23315334 epoch total loss 5.85386229\n",
      "Trained batch 8048 batch loss 6.34235477 epoch total loss 5.85392332\n",
      "Trained batch 8049 batch loss 6.27453327 epoch total loss 5.8539753\n",
      "Trained batch 8050 batch loss 6.52635384 epoch total loss 5.85405922\n",
      "Trained batch 8051 batch loss 6.98737955 epoch total loss 5.8542\n",
      "Trained batch 8052 batch loss 6.95479488 epoch total loss 5.85433626\n",
      "Trained batch 8053 batch loss 6.24916315 epoch total loss 5.85438538\n",
      "Trained batch 8054 batch loss 6.73974609 epoch total loss 5.85449553\n",
      "Trained batch 8055 batch loss 6.97391367 epoch total loss 5.85463428\n",
      "Trained batch 8056 batch loss 7.3622036 epoch total loss 5.85482121\n",
      "Trained batch 8057 batch loss 5.45574951 epoch total loss 5.85477209\n",
      "Trained batch 8058 batch loss 7.46566248 epoch total loss 5.85497189\n",
      "Trained batch 8059 batch loss 6.90612602 epoch total loss 5.85510254\n",
      "Trained batch 8060 batch loss 6.61255074 epoch total loss 5.85519648\n",
      "Trained batch 8061 batch loss 6.10831118 epoch total loss 5.85522795\n",
      "Trained batch 8062 batch loss 6.31906223 epoch total loss 5.85528564\n",
      "Trained batch 8063 batch loss 6.65258598 epoch total loss 5.85538435\n",
      "Trained batch 8064 batch loss 6.30173969 epoch total loss 5.85543966\n",
      "Trained batch 8065 batch loss 6.99226141 epoch total loss 5.85558081\n",
      "Trained batch 8066 batch loss 6.21651 epoch total loss 5.85562515\n",
      "Trained batch 8067 batch loss 6.13635921 epoch total loss 5.85566\n",
      "Trained batch 8068 batch loss 5.51752377 epoch total loss 5.855618\n",
      "Trained batch 8069 batch loss 5.83798456 epoch total loss 5.85561609\n",
      "Trained batch 8070 batch loss 5.91276693 epoch total loss 5.85562325\n",
      "Trained batch 8071 batch loss 6.11173964 epoch total loss 5.85565519\n",
      "Trained batch 8072 batch loss 5.75241661 epoch total loss 5.85564232\n",
      "Trained batch 8073 batch loss 6.61165714 epoch total loss 5.85573626\n",
      "Trained batch 8074 batch loss 5.47761583 epoch total loss 5.85568953\n",
      "Trained batch 8075 batch loss 6.04173279 epoch total loss 5.85571241\n",
      "Trained batch 8076 batch loss 5.72297382 epoch total loss 5.8556962\n",
      "Trained batch 8077 batch loss 6.04658508 epoch total loss 5.85571957\n",
      "Trained batch 8078 batch loss 6.21863747 epoch total loss 5.85576487\n",
      "Trained batch 8079 batch loss 5.78282785 epoch total loss 5.85575533\n",
      "Trained batch 8080 batch loss 5.91477871 epoch total loss 5.85576248\n",
      "Trained batch 8081 batch loss 5.85046482 epoch total loss 5.855762\n",
      "Trained batch 8082 batch loss 5.98900557 epoch total loss 5.85577869\n",
      "Trained batch 8083 batch loss 6.83346081 epoch total loss 5.85589933\n",
      "Trained batch 8084 batch loss 6.68098259 epoch total loss 5.85600138\n",
      "Trained batch 8085 batch loss 6.23559475 epoch total loss 5.85604811\n",
      "Trained batch 8086 batch loss 5.61036968 epoch total loss 5.85601759\n",
      "Trained batch 8087 batch loss 5.48165369 epoch total loss 5.85597134\n",
      "Trained batch 8088 batch loss 5.57131195 epoch total loss 5.85593557\n",
      "Trained batch 8089 batch loss 5.74972439 epoch total loss 5.8559227\n",
      "Trained batch 8090 batch loss 6.05381 epoch total loss 5.85594702\n",
      "Trained batch 8091 batch loss 5.59866333 epoch total loss 5.85591555\n",
      "Trained batch 8092 batch loss 5.094769 epoch total loss 5.85582113\n",
      "Trained batch 8093 batch loss 5.44103861 epoch total loss 5.85577\n",
      "Trained batch 8094 batch loss 5.63186502 epoch total loss 5.85574245\n",
      "Trained batch 8095 batch loss 5.92043114 epoch total loss 5.85575056\n",
      "Trained batch 8096 batch loss 5.46609879 epoch total loss 5.8557024\n",
      "Trained batch 8097 batch loss 5.49248123 epoch total loss 5.85565758\n",
      "Trained batch 8098 batch loss 5.4521265 epoch total loss 5.85560751\n",
      "Trained batch 8099 batch loss 5.41894245 epoch total loss 5.85555363\n",
      "Trained batch 8100 batch loss 5.22810745 epoch total loss 5.8554759\n",
      "Trained batch 8101 batch loss 6.01372623 epoch total loss 5.85549593\n",
      "Trained batch 8102 batch loss 5.93628407 epoch total loss 5.85550594\n",
      "Trained batch 8103 batch loss 6.87842941 epoch total loss 5.85563231\n",
      "Trained batch 8104 batch loss 6.05702686 epoch total loss 5.8556571\n",
      "Trained batch 8105 batch loss 5.00887394 epoch total loss 5.85555267\n",
      "Trained batch 8106 batch loss 4.70019054 epoch total loss 5.85541\n",
      "Trained batch 8107 batch loss 5.65947533 epoch total loss 5.85538578\n",
      "Trained batch 8108 batch loss 6.34159708 epoch total loss 5.85544586\n",
      "Trained batch 8109 batch loss 6.11631393 epoch total loss 5.85547781\n",
      "Trained batch 8110 batch loss 6.44691753 epoch total loss 5.85555077\n",
      "Trained batch 8111 batch loss 6.65329742 epoch total loss 5.85564899\n",
      "Trained batch 8112 batch loss 6.91843796 epoch total loss 5.85577965\n",
      "Trained batch 8113 batch loss 6.09219646 epoch total loss 5.85580921\n",
      "Trained batch 8114 batch loss 5.83738 epoch total loss 5.85580683\n",
      "Trained batch 8115 batch loss 6.52015638 epoch total loss 5.85588837\n",
      "Trained batch 8116 batch loss 6.45615435 epoch total loss 5.85596275\n",
      "Trained batch 8117 batch loss 5.64121723 epoch total loss 5.85593605\n",
      "Trained batch 8118 batch loss 5.74828196 epoch total loss 5.85592318\n",
      "Trained batch 8119 batch loss 5.29544926 epoch total loss 5.85585403\n",
      "Trained batch 8120 batch loss 6.63639259 epoch total loss 5.85595036\n",
      "Trained batch 8121 batch loss 6.15316963 epoch total loss 5.8559866\n",
      "Trained batch 8122 batch loss 6.52572727 epoch total loss 5.85606956\n",
      "Trained batch 8123 batch loss 6.27422142 epoch total loss 5.85612106\n",
      "Trained batch 8124 batch loss 5.60881233 epoch total loss 5.85609055\n",
      "Trained batch 8125 batch loss 6.00324869 epoch total loss 5.85610867\n",
      "Trained batch 8126 batch loss 6.15947151 epoch total loss 5.85614586\n",
      "Trained batch 8127 batch loss 5.88101339 epoch total loss 5.8561492\n",
      "Trained batch 8128 batch loss 4.91353703 epoch total loss 5.85603333\n",
      "Trained batch 8129 batch loss 5.91410255 epoch total loss 5.85604048\n",
      "Trained batch 8130 batch loss 5.9188714 epoch total loss 5.85604811\n",
      "Trained batch 8131 batch loss 5.97650337 epoch total loss 5.85606289\n",
      "Trained batch 8132 batch loss 6.12638664 epoch total loss 5.85609627\n",
      "Trained batch 8133 batch loss 5.94017696 epoch total loss 5.85610676\n",
      "Trained batch 8134 batch loss 6.13445 epoch total loss 5.85614061\n",
      "Trained batch 8135 batch loss 5.630548 epoch total loss 5.85611248\n",
      "Trained batch 8136 batch loss 6.35574913 epoch total loss 5.85617399\n",
      "Trained batch 8137 batch loss 5.99602699 epoch total loss 5.85619116\n",
      "Trained batch 8138 batch loss 5.7643652 epoch total loss 5.85618\n",
      "Trained batch 8139 batch loss 5.75586414 epoch total loss 5.85616779\n",
      "Trained batch 8140 batch loss 5.8079114 epoch total loss 5.85616207\n",
      "Trained batch 8141 batch loss 5.90489578 epoch total loss 5.85616827\n",
      "Trained batch 8142 batch loss 5.87061501 epoch total loss 5.85617\n",
      "Trained batch 8143 batch loss 4.94393349 epoch total loss 5.85605812\n",
      "Trained batch 8144 batch loss 6.31147718 epoch total loss 5.85611439\n",
      "Trained batch 8145 batch loss 5.55185318 epoch total loss 5.85607672\n",
      "Trained batch 8146 batch loss 5.83073 epoch total loss 5.85607386\n",
      "Trained batch 8147 batch loss 5.81839228 epoch total loss 5.85606956\n",
      "Trained batch 8148 batch loss 5.64106846 epoch total loss 5.85604286\n",
      "Trained batch 8149 batch loss 5.29994297 epoch total loss 5.85597515\n",
      "Trained batch 8150 batch loss 5.48480654 epoch total loss 5.85592937\n",
      "Trained batch 8151 batch loss 6.18645239 epoch total loss 5.85597\n",
      "Trained batch 8152 batch loss 5.12965202 epoch total loss 5.85588074\n",
      "Trained batch 8153 batch loss 5.9652729 epoch total loss 5.85589409\n",
      "Trained batch 8154 batch loss 6.58265877 epoch total loss 5.85598326\n",
      "Trained batch 8155 batch loss 5.56316 epoch total loss 5.85594749\n",
      "Trained batch 8156 batch loss 4.06391287 epoch total loss 5.8557272\n",
      "Trained batch 8157 batch loss 4.5442 epoch total loss 5.8555665\n",
      "Trained batch 8158 batch loss 4.8945961 epoch total loss 5.85544872\n",
      "Trained batch 8159 batch loss 3.52809191 epoch total loss 5.8551631\n",
      "Trained batch 8160 batch loss 3.32780027 epoch total loss 5.85485363\n",
      "Trained batch 8161 batch loss 4.26468754 epoch total loss 5.85465908\n",
      "Trained batch 8162 batch loss 4.68085432 epoch total loss 5.85451508\n",
      "Trained batch 8163 batch loss 5.30893612 epoch total loss 5.85444784\n",
      "Trained batch 8164 batch loss 4.77295256 epoch total loss 5.85431576\n",
      "Trained batch 8165 batch loss 4.91160154 epoch total loss 5.8542\n",
      "Trained batch 8166 batch loss 3.45478153 epoch total loss 5.85390615\n",
      "Trained batch 8167 batch loss 5.57348728 epoch total loss 5.85387182\n",
      "Trained batch 8168 batch loss 4.62357521 epoch total loss 5.85372114\n",
      "Trained batch 8169 batch loss 4.36582565 epoch total loss 5.85353947\n",
      "Trained batch 8170 batch loss 4.10156059 epoch total loss 5.85332489\n",
      "Trained batch 8171 batch loss 5.30392456 epoch total loss 5.85325766\n",
      "Trained batch 8172 batch loss 4.77645588 epoch total loss 5.85312605\n",
      "Trained batch 8173 batch loss 6.60361624 epoch total loss 5.85321808\n",
      "Trained batch 8174 batch loss 6.06144714 epoch total loss 5.85324383\n",
      "Trained batch 8175 batch loss 5.45262241 epoch total loss 5.85319471\n",
      "Trained batch 8176 batch loss 5.59351158 epoch total loss 5.85316324\n",
      "Trained batch 8177 batch loss 5.43309927 epoch total loss 5.85311174\n",
      "Trained batch 8178 batch loss 6.13106394 epoch total loss 5.85314608\n",
      "Trained batch 8179 batch loss 6.20946455 epoch total loss 5.85318947\n",
      "Trained batch 8180 batch loss 6.15826511 epoch total loss 5.85322714\n",
      "Trained batch 8181 batch loss 5.85126877 epoch total loss 5.85322714\n",
      "Trained batch 8182 batch loss 5.77762 epoch total loss 5.8532176\n",
      "Trained batch 8183 batch loss 5.45674324 epoch total loss 5.85316944\n",
      "Trained batch 8184 batch loss 4.9341011 epoch total loss 5.85305691\n",
      "Trained batch 8185 batch loss 4.56083584 epoch total loss 5.85289907\n",
      "Trained batch 8186 batch loss 4.53775072 epoch total loss 5.85273886\n",
      "Trained batch 8187 batch loss 5.06305027 epoch total loss 5.85264206\n",
      "Trained batch 8188 batch loss 5.61837864 epoch total loss 5.85261345\n",
      "Trained batch 8189 batch loss 5.34721279 epoch total loss 5.85255194\n",
      "Trained batch 8190 batch loss 5.87847376 epoch total loss 5.85255527\n",
      "Trained batch 8191 batch loss 6.00025082 epoch total loss 5.85257292\n",
      "Trained batch 8192 batch loss 5.46728182 epoch total loss 5.85252619\n",
      "Trained batch 8193 batch loss 5.54574585 epoch total loss 5.85248899\n",
      "Trained batch 8194 batch loss 5.82904911 epoch total loss 5.85248613\n",
      "Trained batch 8195 batch loss 5.83978176 epoch total loss 5.85248423\n",
      "Trained batch 8196 batch loss 5.92445183 epoch total loss 5.85249329\n",
      "Trained batch 8197 batch loss 5.83356094 epoch total loss 5.8524909\n",
      "Trained batch 8198 batch loss 6.00622892 epoch total loss 5.85251\n",
      "Trained batch 8199 batch loss 5.18432283 epoch total loss 5.85242796\n",
      "Trained batch 8200 batch loss 6.06919193 epoch total loss 5.85245466\n",
      "Trained batch 8201 batch loss 5.51271629 epoch total loss 5.85241318\n",
      "Trained batch 8202 batch loss 5.75754642 epoch total loss 5.85240173\n",
      "Trained batch 8203 batch loss 5.73262 epoch total loss 5.85238743\n",
      "Trained batch 8204 batch loss 5.58153343 epoch total loss 5.85235453\n",
      "Trained batch 8205 batch loss 4.17467213 epoch total loss 5.85215\n",
      "Trained batch 8206 batch loss 5.4904995 epoch total loss 5.85210609\n",
      "Trained batch 8207 batch loss 5.13052082 epoch total loss 5.85201788\n",
      "Trained batch 8208 batch loss 5.68053341 epoch total loss 5.8519969\n",
      "Trained batch 8209 batch loss 5.38737392 epoch total loss 5.85194\n",
      "Trained batch 8210 batch loss 5.32422638 epoch total loss 5.85187626\n",
      "Trained batch 8211 batch loss 5.71308327 epoch total loss 5.85185957\n",
      "Trained batch 8212 batch loss 6.03881884 epoch total loss 5.85188198\n",
      "Trained batch 8213 batch loss 6.99753094 epoch total loss 5.85202169\n",
      "Trained batch 8214 batch loss 6.07578802 epoch total loss 5.8520484\n",
      "Trained batch 8215 batch loss 5.75946045 epoch total loss 5.85203695\n",
      "Trained batch 8216 batch loss 4.63580751 epoch total loss 5.85188913\n",
      "Trained batch 8217 batch loss 5.28705883 epoch total loss 5.85182\n",
      "Trained batch 8218 batch loss 5.55305958 epoch total loss 5.85178423\n",
      "Trained batch 8219 batch loss 6.44167042 epoch total loss 5.85185575\n",
      "Trained batch 8220 batch loss 6.22767782 epoch total loss 5.85190153\n",
      "Trained batch 8221 batch loss 6.1488204 epoch total loss 5.85193729\n",
      "Trained batch 8222 batch loss 6.45328665 epoch total loss 5.85201073\n",
      "Trained batch 8223 batch loss 5.99695683 epoch total loss 5.85202789\n",
      "Trained batch 8224 batch loss 6.00550127 epoch total loss 5.85204649\n",
      "Trained batch 8225 batch loss 5.912714 epoch total loss 5.85205412\n",
      "Trained batch 8226 batch loss 5.70932245 epoch total loss 5.85203695\n",
      "Trained batch 8227 batch loss 5.43063688 epoch total loss 5.85198545\n",
      "Trained batch 8228 batch loss 5.45554113 epoch total loss 5.85193777\n",
      "Trained batch 8229 batch loss 5.37773609 epoch total loss 5.85188\n",
      "Trained batch 8230 batch loss 6.05973244 epoch total loss 5.85190535\n",
      "Trained batch 8231 batch loss 5.77023506 epoch total loss 5.85189533\n",
      "Trained batch 8232 batch loss 4.9096508 epoch total loss 5.85178089\n",
      "Trained batch 8233 batch loss 5.87881422 epoch total loss 5.85178423\n",
      "Trained batch 8234 batch loss 5.65256119 epoch total loss 5.85176\n",
      "Trained batch 8235 batch loss 4.85599422 epoch total loss 5.85163879\n",
      "Trained batch 8236 batch loss 6.6429739 epoch total loss 5.85173512\n",
      "Trained batch 8237 batch loss 7.18781948 epoch total loss 5.85189724\n",
      "Trained batch 8238 batch loss 6.04247093 epoch total loss 5.8519206\n",
      "Trained batch 8239 batch loss 5.15096 epoch total loss 5.85183573\n",
      "Trained batch 8240 batch loss 6.9892416 epoch total loss 5.85197353\n",
      "Trained batch 8241 batch loss 5.04093552 epoch total loss 5.85187483\n",
      "Trained batch 8242 batch loss 6.09368753 epoch total loss 5.85190439\n",
      "Trained batch 8243 batch loss 5.41964769 epoch total loss 5.85185146\n",
      "Trained batch 8244 batch loss 4.82290888 epoch total loss 5.85172701\n",
      "Trained batch 8245 batch loss 5.44633 epoch total loss 5.85167742\n",
      "Trained batch 8246 batch loss 5.3736577 epoch total loss 5.85161972\n",
      "Trained batch 8247 batch loss 5.11752176 epoch total loss 5.85153055\n",
      "Trained batch 8248 batch loss 5.44679356 epoch total loss 5.85148144\n",
      "Trained batch 8249 batch loss 5.57758045 epoch total loss 5.85144854\n",
      "Trained batch 8250 batch loss 6.30749512 epoch total loss 5.85150385\n",
      "Trained batch 8251 batch loss 4.56508255 epoch total loss 5.85134792\n",
      "Trained batch 8252 batch loss 4.47573042 epoch total loss 5.85118151\n",
      "Trained batch 8253 batch loss 4.33895206 epoch total loss 5.8509984\n",
      "Trained batch 8254 batch loss 4.86054134 epoch total loss 5.85087824\n",
      "Trained batch 8255 batch loss 6.14175224 epoch total loss 5.85091305\n",
      "Trained batch 8256 batch loss 5.52695894 epoch total loss 5.85087395\n",
      "Trained batch 8257 batch loss 5.90441132 epoch total loss 5.85088062\n",
      "Trained batch 8258 batch loss 5.45966816 epoch total loss 5.85083342\n",
      "Trained batch 8259 batch loss 5.68154907 epoch total loss 5.85081291\n",
      "Trained batch 8260 batch loss 5.71289968 epoch total loss 5.85079622\n",
      "Trained batch 8261 batch loss 5.38392973 epoch total loss 5.85074\n",
      "Trained batch 8262 batch loss 5.5019083 epoch total loss 5.85069752\n",
      "Trained batch 8263 batch loss 5.95207834 epoch total loss 5.85071\n",
      "Trained batch 8264 batch loss 5.62763 epoch total loss 5.85068274\n",
      "Trained batch 8265 batch loss 6.33459 epoch total loss 5.85074139\n",
      "Trained batch 8266 batch loss 5.51559305 epoch total loss 5.85070086\n",
      "Trained batch 8267 batch loss 6.0030551 epoch total loss 5.85071945\n",
      "Trained batch 8268 batch loss 4.49634075 epoch total loss 5.8505559\n",
      "Trained batch 8269 batch loss 6.60844612 epoch total loss 5.85064745\n",
      "Trained batch 8270 batch loss 6.08723879 epoch total loss 5.85067606\n",
      "Trained batch 8271 batch loss 5.6459465 epoch total loss 5.85065079\n",
      "Trained batch 8272 batch loss 4.92965841 epoch total loss 5.85053968\n",
      "Trained batch 8273 batch loss 6.16536617 epoch total loss 5.85057735\n",
      "Trained batch 8274 batch loss 5.57763672 epoch total loss 5.85054445\n",
      "Trained batch 8275 batch loss 6.69329309 epoch total loss 5.85064602\n",
      "Trained batch 8276 batch loss 5.61408186 epoch total loss 5.85061741\n",
      "Trained batch 8277 batch loss 6.32397175 epoch total loss 5.85067463\n",
      "Trained batch 8278 batch loss 6.39648 epoch total loss 5.85074043\n",
      "Trained batch 8279 batch loss 5.75923061 epoch total loss 5.85072947\n",
      "Trained batch 8280 batch loss 4.83442879 epoch total loss 5.85060692\n",
      "Trained batch 8281 batch loss 6.73586893 epoch total loss 5.85071325\n",
      "Trained batch 8282 batch loss 6.36179209 epoch total loss 5.85077524\n",
      "Trained batch 8283 batch loss 4.67948 epoch total loss 5.8506341\n",
      "Trained batch 8284 batch loss 6.47446632 epoch total loss 5.85070896\n",
      "Trained batch 8285 batch loss 6.09980488 epoch total loss 5.85073948\n",
      "Trained batch 8286 batch loss 6.90566063 epoch total loss 5.85086679\n",
      "Trained batch 8287 batch loss 6.25864029 epoch total loss 5.85091591\n",
      "Trained batch 8288 batch loss 6.35586357 epoch total loss 5.85097647\n",
      "Trained batch 8289 batch loss 6.83546543 epoch total loss 5.85109568\n",
      "Trained batch 8290 batch loss 7.3610487 epoch total loss 5.85127735\n",
      "Trained batch 8291 batch loss 6.15884495 epoch total loss 5.85131454\n",
      "Trained batch 8292 batch loss 6.4233427 epoch total loss 5.85138369\n",
      "Trained batch 8293 batch loss 7.01542759 epoch total loss 5.85152388\n",
      "Trained batch 8294 batch loss 5.55122089 epoch total loss 5.85148764\n",
      "Trained batch 8295 batch loss 7.07080936 epoch total loss 5.8516345\n",
      "Trained batch 8296 batch loss 6.79070377 epoch total loss 5.85174751\n",
      "Trained batch 8297 batch loss 7.01573944 epoch total loss 5.8518877\n",
      "Trained batch 8298 batch loss 6.19508648 epoch total loss 5.85192919\n",
      "Trained batch 8299 batch loss 5.95542622 epoch total loss 5.85194206\n",
      "Trained batch 8300 batch loss 6.58507 epoch total loss 5.85203028\n",
      "Trained batch 8301 batch loss 5.84505558 epoch total loss 5.85202932\n",
      "Trained batch 8302 batch loss 5.65338802 epoch total loss 5.85200548\n",
      "Trained batch 8303 batch loss 5.69269228 epoch total loss 5.85198593\n",
      "Trained batch 8304 batch loss 6.3985548 epoch total loss 5.85205173\n",
      "Trained batch 8305 batch loss 4.72920179 epoch total loss 5.85191679\n",
      "Trained batch 8306 batch loss 6.25417519 epoch total loss 5.85196495\n",
      "Trained batch 8307 batch loss 5.77026749 epoch total loss 5.85195494\n",
      "Trained batch 8308 batch loss 4.76446676 epoch total loss 5.85182428\n",
      "Trained batch 8309 batch loss 4.79671192 epoch total loss 5.85169744\n",
      "Trained batch 8310 batch loss 5.20400429 epoch total loss 5.85161924\n",
      "Trained batch 8311 batch loss 4.82789707 epoch total loss 5.85149622\n",
      "Trained batch 8312 batch loss 5.36004066 epoch total loss 5.85143709\n",
      "Trained batch 8313 batch loss 5.51962948 epoch total loss 5.85139704\n",
      "Trained batch 8314 batch loss 5.48738766 epoch total loss 5.85135365\n",
      "Trained batch 8315 batch loss 5.34964514 epoch total loss 5.85129309\n",
      "Trained batch 8316 batch loss 5.73689127 epoch total loss 5.85127974\n",
      "Trained batch 8317 batch loss 4.89372826 epoch total loss 5.85116482\n",
      "Trained batch 8318 batch loss 6.8181982 epoch total loss 5.85128069\n",
      "Trained batch 8319 batch loss 4.83911133 epoch total loss 5.8511591\n",
      "Trained batch 8320 batch loss 4.89958334 epoch total loss 5.85104465\n",
      "Trained batch 8321 batch loss 4.79268169 epoch total loss 5.85091734\n",
      "Trained batch 8322 batch loss 5.45379 epoch total loss 5.85086966\n",
      "Trained batch 8323 batch loss 3.81257677 epoch total loss 5.85062456\n",
      "Trained batch 8324 batch loss 5.6363 epoch total loss 5.85059929\n",
      "Trained batch 8325 batch loss 5.36778831 epoch total loss 5.85054111\n",
      "Trained batch 8326 batch loss 5.34589 epoch total loss 5.85048056\n",
      "Trained batch 8327 batch loss 5.25581074 epoch total loss 5.85040903\n",
      "Trained batch 8328 batch loss 6.51682568 epoch total loss 5.85048866\n",
      "Trained batch 8329 batch loss 5.67686081 epoch total loss 5.85046768\n",
      "Trained batch 8330 batch loss 5.65281 epoch total loss 5.85044432\n",
      "Trained batch 8331 batch loss 5.82749939 epoch total loss 5.85044146\n",
      "Trained batch 8332 batch loss 5.68224239 epoch total loss 5.85042143\n",
      "Trained batch 8333 batch loss 5.45957279 epoch total loss 5.8503747\n",
      "Trained batch 8334 batch loss 5.68560505 epoch total loss 5.85035515\n",
      "Trained batch 8335 batch loss 5.89314747 epoch total loss 5.85036039\n",
      "Trained batch 8336 batch loss 5.63362169 epoch total loss 5.85033417\n",
      "Trained batch 8337 batch loss 6.05029392 epoch total loss 5.85035849\n",
      "Trained batch 8338 batch loss 5.77119923 epoch total loss 5.85034847\n",
      "Trained batch 8339 batch loss 6.28691 epoch total loss 5.85040092\n",
      "Trained batch 8340 batch loss 5.9032321 epoch total loss 5.85040712\n",
      "Trained batch 8341 batch loss 5.26795578 epoch total loss 5.85033751\n",
      "Trained batch 8342 batch loss 5.92107725 epoch total loss 5.85034609\n",
      "Trained batch 8343 batch loss 6.15589905 epoch total loss 5.8503828\n",
      "Trained batch 8344 batch loss 6.35710049 epoch total loss 5.85044336\n",
      "Trained batch 8345 batch loss 5.3554616 epoch total loss 5.85038376\n",
      "Trained batch 8346 batch loss 5.88146353 epoch total loss 5.85038757\n",
      "Trained batch 8347 batch loss 6.55612373 epoch total loss 5.85047197\n",
      "Trained batch 8348 batch loss 5.05495 epoch total loss 5.85037661\n",
      "Trained batch 8349 batch loss 5.71588802 epoch total loss 5.85036039\n",
      "Trained batch 8350 batch loss 6.07830429 epoch total loss 5.85038805\n",
      "Trained batch 8351 batch loss 5.09810925 epoch total loss 5.85029745\n",
      "Trained batch 8352 batch loss 5.45185423 epoch total loss 5.85025024\n",
      "Trained batch 8353 batch loss 5.19461155 epoch total loss 5.85017157\n",
      "Trained batch 8354 batch loss 5.28563213 epoch total loss 5.85010386\n",
      "Trained batch 8355 batch loss 6.17535496 epoch total loss 5.85014296\n",
      "Trained batch 8356 batch loss 5.40085793 epoch total loss 5.85008955\n",
      "Trained batch 8357 batch loss 5.56752157 epoch total loss 5.85005569\n",
      "Trained batch 8358 batch loss 5.611269 epoch total loss 5.85002661\n",
      "Trained batch 8359 batch loss 6.23515081 epoch total loss 5.85007286\n",
      "Trained batch 8360 batch loss 6.2785511 epoch total loss 5.85012388\n",
      "Trained batch 8361 batch loss 5.70035362 epoch total loss 5.85010576\n",
      "Trained batch 8362 batch loss 6.00785589 epoch total loss 5.85012484\n",
      "Trained batch 8363 batch loss 3.35647917 epoch total loss 5.84982634\n",
      "Trained batch 8364 batch loss 6.11497355 epoch total loss 5.84985781\n",
      "Trained batch 8365 batch loss 6.24850178 epoch total loss 5.84990549\n",
      "Trained batch 8366 batch loss 5.90567684 epoch total loss 5.84991264\n",
      "Trained batch 8367 batch loss 5.40107536 epoch total loss 5.84985876\n",
      "Trained batch 8368 batch loss 5.13784313 epoch total loss 5.84977388\n",
      "Trained batch 8369 batch loss 5.38789177 epoch total loss 5.84971857\n",
      "Trained batch 8370 batch loss 5.1117382 epoch total loss 5.84963036\n",
      "Trained batch 8371 batch loss 5.9488163 epoch total loss 5.84964228\n",
      "Trained batch 8372 batch loss 4.35061455 epoch total loss 5.84946346\n",
      "Trained batch 8373 batch loss 6.03864861 epoch total loss 5.84948587\n",
      "Trained batch 8374 batch loss 5.68788195 epoch total loss 5.8494668\n",
      "Trained batch 8375 batch loss 5.45613766 epoch total loss 5.84941959\n",
      "Trained batch 8376 batch loss 5.96606255 epoch total loss 5.84943342\n",
      "Trained batch 8377 batch loss 5.77711201 epoch total loss 5.84942484\n",
      "Trained batch 8378 batch loss 5.89264774 epoch total loss 5.84943056\n",
      "Trained batch 8379 batch loss 5.74440861 epoch total loss 5.84941816\n",
      "Trained batch 8380 batch loss 5.58613682 epoch total loss 5.84938669\n",
      "Trained batch 8381 batch loss 6.12456799 epoch total loss 5.84941959\n",
      "Trained batch 8382 batch loss 5.55851078 epoch total loss 5.84938478\n",
      "Trained batch 8383 batch loss 5.75926876 epoch total loss 5.84937382\n",
      "Trained batch 8384 batch loss 5.86262417 epoch total loss 5.84937525\n",
      "Trained batch 8385 batch loss 5.34331417 epoch total loss 5.84931517\n",
      "Trained batch 8386 batch loss 5.53392553 epoch total loss 5.8492775\n",
      "Trained batch 8387 batch loss 4.90352201 epoch total loss 5.84916496\n",
      "Trained batch 8388 batch loss 4.52774334 epoch total loss 5.84900713\n",
      "Trained batch 8389 batch loss 5.01027822 epoch total loss 5.84890747\n",
      "Trained batch 8390 batch loss 5.87435341 epoch total loss 5.84891033\n",
      "Trained batch 8391 batch loss 6.08651543 epoch total loss 5.84893894\n",
      "Trained batch 8392 batch loss 5.97416496 epoch total loss 5.84895372\n",
      "Trained batch 8393 batch loss 5.8908782 epoch total loss 5.84895849\n",
      "Trained batch 8394 batch loss 6.15917587 epoch total loss 5.84899569\n",
      "Trained batch 8395 batch loss 4.58645773 epoch total loss 5.84884501\n",
      "Trained batch 8396 batch loss 5.84973717 epoch total loss 5.84884548\n",
      "Trained batch 8397 batch loss 6.53413582 epoch total loss 5.84892702\n",
      "Trained batch 8398 batch loss 5.27668667 epoch total loss 5.84885931\n",
      "Trained batch 8399 batch loss 6.72632217 epoch total loss 5.84896374\n",
      "Trained batch 8400 batch loss 6.2983532 epoch total loss 5.84901714\n",
      "Trained batch 8401 batch loss 6.41482067 epoch total loss 5.84908438\n",
      "Trained batch 8402 batch loss 4.44729137 epoch total loss 5.84891748\n",
      "Trained batch 8403 batch loss 5.51560783 epoch total loss 5.84887791\n",
      "Trained batch 8404 batch loss 6.04674435 epoch total loss 5.84890127\n",
      "Trained batch 8405 batch loss 5.65248585 epoch total loss 5.84887791\n",
      "Trained batch 8406 batch loss 5.45148563 epoch total loss 5.84883118\n",
      "Trained batch 8407 batch loss 5.76147079 epoch total loss 5.84882069\n",
      "Trained batch 8408 batch loss 6.49038124 epoch total loss 5.84889698\n",
      "Trained batch 8409 batch loss 5.70303059 epoch total loss 5.84888\n",
      "Trained batch 8410 batch loss 6.00983858 epoch total loss 5.84889936\n",
      "Trained batch 8411 batch loss 5.83597231 epoch total loss 5.84889746\n",
      "Trained batch 8412 batch loss 5.64196968 epoch total loss 5.84887266\n",
      "Trained batch 8413 batch loss 6.38931179 epoch total loss 5.84893751\n",
      "Trained batch 8414 batch loss 5.81417274 epoch total loss 5.84893274\n",
      "Trained batch 8415 batch loss 6.12505198 epoch total loss 5.84896564\n",
      "Trained batch 8416 batch loss 5.19996738 epoch total loss 5.8488884\n",
      "Trained batch 8417 batch loss 5.25476456 epoch total loss 5.84881783\n",
      "Trained batch 8418 batch loss 4.84132528 epoch total loss 5.84869814\n",
      "Trained batch 8419 batch loss 6.04190588 epoch total loss 5.84872103\n",
      "Trained batch 8420 batch loss 6.22580338 epoch total loss 5.84876585\n",
      "Trained batch 8421 batch loss 5.5316 epoch total loss 5.84872818\n",
      "Trained batch 8422 batch loss 5.62327194 epoch total loss 5.84870148\n",
      "Trained batch 8423 batch loss 5.26754189 epoch total loss 5.84863234\n",
      "Trained batch 8424 batch loss 4.67164421 epoch total loss 5.84849262\n",
      "Trained batch 8425 batch loss 5.38115311 epoch total loss 5.84843731\n",
      "Trained batch 8426 batch loss 4.92884541 epoch total loss 5.84832859\n",
      "Trained batch 8427 batch loss 5.47795439 epoch total loss 5.84828424\n",
      "Trained batch 8428 batch loss 5.66775894 epoch total loss 5.84826279\n",
      "Trained batch 8429 batch loss 6.49997139 epoch total loss 5.84834\n",
      "Trained batch 8430 batch loss 5.6774292 epoch total loss 5.84832\n",
      "Trained batch 8431 batch loss 5.5818367 epoch total loss 5.84828806\n",
      "Trained batch 8432 batch loss 6.1622 epoch total loss 5.84832573\n",
      "Trained batch 8433 batch loss 5.86081696 epoch total loss 5.84832716\n",
      "Trained batch 8434 batch loss 5.89693451 epoch total loss 5.84833288\n",
      "Trained batch 8435 batch loss 6.50387764 epoch total loss 5.84841061\n",
      "Trained batch 8436 batch loss 5.57152939 epoch total loss 5.8483777\n",
      "Trained batch 8437 batch loss 5.72734165 epoch total loss 5.8483634\n",
      "Trained batch 8438 batch loss 5.84194326 epoch total loss 5.84836292\n",
      "Trained batch 8439 batch loss 6.12490606 epoch total loss 5.84839535\n",
      "Trained batch 8440 batch loss 5.66027498 epoch total loss 5.84837294\n",
      "Trained batch 8441 batch loss 6.63765812 epoch total loss 5.8484664\n",
      "Trained batch 8442 batch loss 5.93281841 epoch total loss 5.84847641\n",
      "Trained batch 8443 batch loss 6.51595 epoch total loss 5.84855556\n",
      "Trained batch 8444 batch loss 6.46699572 epoch total loss 5.848629\n",
      "Trained batch 8445 batch loss 5.4673624 epoch total loss 5.84858418\n",
      "Trained batch 8446 batch loss 6.36856842 epoch total loss 5.84864569\n",
      "Trained batch 8447 batch loss 6.25748539 epoch total loss 5.84869385\n",
      "Trained batch 8448 batch loss 6.34379673 epoch total loss 5.8487525\n",
      "Trained batch 8449 batch loss 5.79812336 epoch total loss 5.8487463\n",
      "Trained batch 8450 batch loss 5.1249094 epoch total loss 5.84866095\n",
      "Trained batch 8451 batch loss 5.68833637 epoch total loss 5.84864187\n",
      "Trained batch 8452 batch loss 6.63231373 epoch total loss 5.84873438\n",
      "Trained batch 8453 batch loss 6.39400673 epoch total loss 5.84879923\n",
      "Trained batch 8454 batch loss 5.75571632 epoch total loss 5.84878778\n",
      "Trained batch 8455 batch loss 6.55239582 epoch total loss 5.84887075\n",
      "Trained batch 8456 batch loss 6.16745949 epoch total loss 5.84890842\n",
      "Trained batch 8457 batch loss 6.54824543 epoch total loss 5.84899092\n",
      "Trained batch 8458 batch loss 6.09612274 epoch total loss 5.84902048\n",
      "Trained batch 8459 batch loss 7.03866386 epoch total loss 5.84916115\n",
      "Trained batch 8460 batch loss 6.46918 epoch total loss 5.84923458\n",
      "Trained batch 8461 batch loss 6.14427567 epoch total loss 5.84926939\n",
      "Trained batch 8462 batch loss 6.26512623 epoch total loss 5.8493185\n",
      "Trained batch 8463 batch loss 6.10728264 epoch total loss 5.84934902\n",
      "Trained batch 8464 batch loss 5.84034061 epoch total loss 5.84934759\n",
      "Trained batch 8465 batch loss 5.89667 epoch total loss 5.84935331\n",
      "Trained batch 8466 batch loss 5.67588 epoch total loss 5.84933281\n",
      "Trained batch 8467 batch loss 5.29031 epoch total loss 5.84926701\n",
      "Trained batch 8468 batch loss 5.84518719 epoch total loss 5.84926605\n",
      "Trained batch 8469 batch loss 5.31373453 epoch total loss 5.84920263\n",
      "Trained batch 8470 batch loss 5.44096422 epoch total loss 5.84915447\n",
      "Trained batch 8471 batch loss 6.26785898 epoch total loss 5.84920406\n",
      "Trained batch 8472 batch loss 5.88563728 epoch total loss 5.84920883\n",
      "Trained batch 8473 batch loss 5.87896442 epoch total loss 5.84921217\n",
      "Trained batch 8474 batch loss 5.73396492 epoch total loss 5.84919882\n",
      "Trained batch 8475 batch loss 6.29080105 epoch total loss 5.84925032\n",
      "Trained batch 8476 batch loss 6.21802187 epoch total loss 5.84929419\n",
      "Trained batch 8477 batch loss 5.99299812 epoch total loss 5.84931087\n",
      "Trained batch 8478 batch loss 5.13731956 epoch total loss 5.84922695\n",
      "Trained batch 8479 batch loss 6.17289686 epoch total loss 5.8492651\n",
      "Trained batch 8480 batch loss 6.00060844 epoch total loss 5.84928274\n",
      "Trained batch 8481 batch loss 5.40551758 epoch total loss 5.84923077\n",
      "Trained batch 8482 batch loss 5.10651398 epoch total loss 5.84914303\n",
      "Trained batch 8483 batch loss 5.89996815 epoch total loss 5.84914875\n",
      "Trained batch 8484 batch loss 6.15420675 epoch total loss 5.84918451\n",
      "Trained batch 8485 batch loss 5.81429 epoch total loss 5.84918\n",
      "Trained batch 8486 batch loss 6.11611176 epoch total loss 5.84921169\n",
      "Trained batch 8487 batch loss 6.09815693 epoch total loss 5.84924078\n",
      "Trained batch 8488 batch loss 6.41729641 epoch total loss 5.84930801\n",
      "Trained batch 8489 batch loss 5.8702879 epoch total loss 5.8493104\n",
      "Trained batch 8490 batch loss 6.60068703 epoch total loss 5.84939909\n",
      "Trained batch 8491 batch loss 6.35392666 epoch total loss 5.84945869\n",
      "Trained batch 8492 batch loss 5.67286396 epoch total loss 5.84943771\n",
      "Trained batch 8493 batch loss 5.75029373 epoch total loss 5.84942627\n",
      "Trained batch 8494 batch loss 6.07309151 epoch total loss 5.8494525\n",
      "Trained batch 8495 batch loss 5.37090874 epoch total loss 5.84939623\n",
      "Trained batch 8496 batch loss 6.12787628 epoch total loss 5.84942913\n",
      "Trained batch 8497 batch loss 6.39697504 epoch total loss 5.84949398\n",
      "Trained batch 8498 batch loss 6.35001659 epoch total loss 5.84955263\n",
      "Trained batch 8499 batch loss 5.62839222 epoch total loss 5.84952688\n",
      "Trained batch 8500 batch loss 5.48594952 epoch total loss 5.84948397\n",
      "Trained batch 8501 batch loss 5.32749939 epoch total loss 5.84942245\n",
      "Trained batch 8502 batch loss 5.90705872 epoch total loss 5.84942913\n",
      "Trained batch 8503 batch loss 5.76580238 epoch total loss 5.84941959\n",
      "Trained batch 8504 batch loss 5.93114567 epoch total loss 5.84942865\n",
      "Trained batch 8505 batch loss 5.67498398 epoch total loss 5.84940863\n",
      "Trained batch 8506 batch loss 6.10330343 epoch total loss 5.84943819\n",
      "Trained batch 8507 batch loss 5.87182283 epoch total loss 5.84944057\n",
      "Trained batch 8508 batch loss 5.4580183 epoch total loss 5.84939432\n",
      "Trained batch 8509 batch loss 5.85953426 epoch total loss 5.84939575\n",
      "Trained batch 8510 batch loss 6.15201092 epoch total loss 5.84943104\n",
      "Trained batch 8511 batch loss 5.64535236 epoch total loss 5.8494072\n",
      "Trained batch 8512 batch loss 6.9929924 epoch total loss 5.84954166\n",
      "Trained batch 8513 batch loss 6.21820545 epoch total loss 5.84958506\n",
      "Trained batch 8514 batch loss 5.79269314 epoch total loss 5.84957838\n",
      "Trained batch 8515 batch loss 5.869277 epoch total loss 5.84958076\n",
      "Trained batch 8516 batch loss 3.70274305 epoch total loss 5.84932852\n",
      "Trained batch 8517 batch loss 5.81409025 epoch total loss 5.84932423\n",
      "Trained batch 8518 batch loss 5.7775507 epoch total loss 5.84931564\n",
      "Trained batch 8519 batch loss 5.7338748 epoch total loss 5.84930229\n",
      "Trained batch 8520 batch loss 5.40208626 epoch total loss 5.84925\n",
      "Trained batch 8521 batch loss 5.04861784 epoch total loss 5.8491559\n",
      "Trained batch 8522 batch loss 5.19167757 epoch total loss 5.84907866\n",
      "Trained batch 8523 batch loss 4.46375179 epoch total loss 5.84891605\n",
      "Trained batch 8524 batch loss 5.24003506 epoch total loss 5.84884453\n",
      "Trained batch 8525 batch loss 5.72152615 epoch total loss 5.84882975\n",
      "Trained batch 8526 batch loss 5.53860569 epoch total loss 5.84879351\n",
      "Trained batch 8527 batch loss 6.22403049 epoch total loss 5.84883738\n",
      "Trained batch 8528 batch loss 4.50810719 epoch total loss 5.84868\n",
      "Trained batch 8529 batch loss 4.25767231 epoch total loss 5.84849358\n",
      "Trained batch 8530 batch loss 5.22120619 epoch total loss 5.84842\n",
      "Trained batch 8531 batch loss 5.05175781 epoch total loss 5.84832668\n",
      "Trained batch 8532 batch loss 5.28455544 epoch total loss 5.8482604\n",
      "Trained batch 8533 batch loss 4.61502695 epoch total loss 5.84811592\n",
      "Trained batch 8534 batch loss 5.83219624 epoch total loss 5.84811401\n",
      "Trained batch 8535 batch loss 4.74412155 epoch total loss 5.84798431\n",
      "Trained batch 8536 batch loss 4.95321703 epoch total loss 5.84787941\n",
      "Trained batch 8537 batch loss 4.60891867 epoch total loss 5.84773445\n",
      "Trained batch 8538 batch loss 4.069242 epoch total loss 5.84752655\n",
      "Trained batch 8539 batch loss 5.37048817 epoch total loss 5.84747028\n",
      "Trained batch 8540 batch loss 5.65072393 epoch total loss 5.84744787\n",
      "Trained batch 8541 batch loss 5.25665855 epoch total loss 5.84737873\n",
      "Trained batch 8542 batch loss 4.35329437 epoch total loss 5.84720373\n",
      "Trained batch 8543 batch loss 5.46449471 epoch total loss 5.84715891\n",
      "Trained batch 8544 batch loss 5.42987394 epoch total loss 5.84711\n",
      "Trained batch 8545 batch loss 5.5754261 epoch total loss 5.84707785\n",
      "Trained batch 8546 batch loss 4.95526743 epoch total loss 5.8469739\n",
      "Trained batch 8547 batch loss 4.97819614 epoch total loss 5.84687185\n",
      "Trained batch 8548 batch loss 5.76909447 epoch total loss 5.84686279\n",
      "Trained batch 8549 batch loss 7.39527512 epoch total loss 5.84704399\n",
      "Trained batch 8550 batch loss 7.55658627 epoch total loss 5.84724379\n",
      "Trained batch 8551 batch loss 7.35375 epoch total loss 5.84742\n",
      "Trained batch 8552 batch loss 7.2842207 epoch total loss 5.84758806\n",
      "Trained batch 8553 batch loss 7.64513111 epoch total loss 5.84779835\n",
      "Trained batch 8554 batch loss 6.94306612 epoch total loss 5.84792614\n",
      "Trained batch 8555 batch loss 7.04361296 epoch total loss 5.84806585\n",
      "Trained batch 8556 batch loss 7.54065323 epoch total loss 5.84826326\n",
      "Trained batch 8557 batch loss 6.61283207 epoch total loss 5.84835291\n",
      "Trained batch 8558 batch loss 6.82261658 epoch total loss 5.84846687\n",
      "Trained batch 8559 batch loss 5.30893755 epoch total loss 5.84840393\n",
      "Trained batch 8560 batch loss 6.32573605 epoch total loss 5.84845924\n",
      "Trained batch 8561 batch loss 5.96960735 epoch total loss 5.84847355\n",
      "Trained batch 8562 batch loss 5.76983547 epoch total loss 5.84846401\n",
      "Trained batch 8563 batch loss 6.71544552 epoch total loss 5.84856558\n",
      "Trained batch 8564 batch loss 5.80747223 epoch total loss 5.84856081\n",
      "Trained batch 8565 batch loss 6.09774685 epoch total loss 5.84859\n",
      "Trained batch 8566 batch loss 6.55995226 epoch total loss 5.84867287\n",
      "Trained batch 8567 batch loss 5.55419445 epoch total loss 5.84863853\n",
      "Trained batch 8568 batch loss 5.60823345 epoch total loss 5.8486104\n",
      "Trained batch 8569 batch loss 6.27017975 epoch total loss 5.84865952\n",
      "Trained batch 8570 batch loss 5.87148285 epoch total loss 5.84866238\n",
      "Trained batch 8571 batch loss 6.21865177 epoch total loss 5.84870529\n",
      "Trained batch 8572 batch loss 6.20439053 epoch total loss 5.84874678\n",
      "Trained batch 8573 batch loss 5.79414606 epoch total loss 5.84874\n",
      "Trained batch 8574 batch loss 5.91967392 epoch total loss 5.84874821\n",
      "Trained batch 8575 batch loss 5.94726849 epoch total loss 5.84876\n",
      "Trained batch 8576 batch loss 6.29743385 epoch total loss 5.8488121\n",
      "Trained batch 8577 batch loss 6.45460129 epoch total loss 5.84888268\n",
      "Trained batch 8578 batch loss 5.47695875 epoch total loss 5.84883928\n",
      "Trained batch 8579 batch loss 5.62009621 epoch total loss 5.84881258\n",
      "Trained batch 8580 batch loss 5.95263958 epoch total loss 5.84882498\n",
      "Trained batch 8581 batch loss 6.37855053 epoch total loss 5.84888649\n",
      "Trained batch 8582 batch loss 6.33246803 epoch total loss 5.84894323\n",
      "Trained batch 8583 batch loss 5.50576878 epoch total loss 5.8489027\n",
      "Trained batch 8584 batch loss 6.28953505 epoch total loss 5.8489542\n",
      "Trained batch 8585 batch loss 6.9916954 epoch total loss 5.84908724\n",
      "Trained batch 8586 batch loss 6.98062897 epoch total loss 5.84921885\n",
      "Trained batch 8587 batch loss 6.70686 epoch total loss 5.84931898\n",
      "Trained batch 8588 batch loss 4.95950365 epoch total loss 5.84921551\n",
      "Trained batch 8589 batch loss 6.33487797 epoch total loss 5.84927225\n",
      "Trained batch 8590 batch loss 6.33805943 epoch total loss 5.84932947\n",
      "Trained batch 8591 batch loss 5.99707794 epoch total loss 5.84934616\n",
      "Trained batch 8592 batch loss 5.28038025 epoch total loss 5.84928036\n",
      "Trained batch 8593 batch loss 5.57864 epoch total loss 5.84924889\n",
      "Trained batch 8594 batch loss 4.85594177 epoch total loss 5.84913301\n",
      "Trained batch 8595 batch loss 5.25536156 epoch total loss 5.84906387\n",
      "Trained batch 8596 batch loss 5.53938484 epoch total loss 5.84902763\n",
      "Trained batch 8597 batch loss 5.60431337 epoch total loss 5.8489995\n",
      "Trained batch 8598 batch loss 6.10586357 epoch total loss 5.84902906\n",
      "Trained batch 8599 batch loss 6.116 epoch total loss 5.84906054\n",
      "Trained batch 8600 batch loss 6.30714369 epoch total loss 5.84911394\n",
      "Trained batch 8601 batch loss 6.01719666 epoch total loss 5.84913301\n",
      "Trained batch 8602 batch loss 4.61750031 epoch total loss 5.84899\n",
      "Trained batch 8603 batch loss 6.15803528 epoch total loss 5.84902573\n",
      "Trained batch 8604 batch loss 4.88375044 epoch total loss 5.84891319\n",
      "Trained batch 8605 batch loss 6.41467857 epoch total loss 5.848979\n",
      "Trained batch 8606 batch loss 5.7635603 epoch total loss 5.84896898\n",
      "Trained batch 8607 batch loss 6.03431511 epoch total loss 5.84899044\n",
      "Trained batch 8608 batch loss 5.84343 epoch total loss 5.84899\n",
      "Trained batch 8609 batch loss 5.57063437 epoch total loss 5.84895754\n",
      "Trained batch 8610 batch loss 5.88054895 epoch total loss 5.84896088\n",
      "Trained batch 8611 batch loss 6.5355854 epoch total loss 5.84904051\n",
      "Trained batch 8612 batch loss 5.74502277 epoch total loss 5.84902859\n",
      "Trained batch 8613 batch loss 5.86993504 epoch total loss 5.84903145\n",
      "Trained batch 8614 batch loss 6.44378138 epoch total loss 5.84910059\n",
      "Trained batch 8615 batch loss 6.00239801 epoch total loss 5.84911871\n",
      "Trained batch 8616 batch loss 6.09718895 epoch total loss 5.84914732\n",
      "Trained batch 8617 batch loss 5.83459377 epoch total loss 5.84914589\n",
      "Trained batch 8618 batch loss 5.99996471 epoch total loss 5.84916353\n",
      "Trained batch 8619 batch loss 5.8161993 epoch total loss 5.84915972\n",
      "Trained batch 8620 batch loss 5.17108297 epoch total loss 5.84908104\n",
      "Trained batch 8621 batch loss 6.55836773 epoch total loss 5.84916306\n",
      "Trained batch 8622 batch loss 5.88101959 epoch total loss 5.84916735\n",
      "Trained batch 8623 batch loss 5.6940937 epoch total loss 5.84914923\n",
      "Trained batch 8624 batch loss 5.34499931 epoch total loss 5.84909058\n",
      "Trained batch 8625 batch loss 5.77407 epoch total loss 5.84908199\n",
      "Trained batch 8626 batch loss 5.83584 epoch total loss 5.84908056\n",
      "Trained batch 8627 batch loss 5.11792564 epoch total loss 5.84899569\n",
      "Trained batch 8628 batch loss 5.67936182 epoch total loss 5.84897614\n",
      "Trained batch 8629 batch loss 5.44420481 epoch total loss 5.84892941\n",
      "Trained batch 8630 batch loss 6.2762723 epoch total loss 5.848979\n",
      "Trained batch 8631 batch loss 6.65547466 epoch total loss 5.84907246\n",
      "Trained batch 8632 batch loss 6.47044086 epoch total loss 5.84914398\n",
      "Trained batch 8633 batch loss 5.7683959 epoch total loss 5.84913492\n",
      "Trained batch 8634 batch loss 5.32288074 epoch total loss 5.84907436\n",
      "Trained batch 8635 batch loss 5.74657869 epoch total loss 5.84906244\n",
      "Trained batch 8636 batch loss 6.23235798 epoch total loss 5.84910631\n",
      "Trained batch 8637 batch loss 6.28813219 epoch total loss 5.84915733\n",
      "Trained batch 8638 batch loss 6.08070135 epoch total loss 5.84918451\n",
      "Trained batch 8639 batch loss 5.80828953 epoch total loss 5.84917974\n",
      "Trained batch 8640 batch loss 5.61587143 epoch total loss 5.84915257\n",
      "Trained batch 8641 batch loss 5.56467962 epoch total loss 5.84912\n",
      "Trained batch 8642 batch loss 5.94587231 epoch total loss 5.84913111\n",
      "Trained batch 8643 batch loss 6.21901369 epoch total loss 5.84917402\n",
      "Trained batch 8644 batch loss 6.31744671 epoch total loss 5.84922791\n",
      "Trained batch 8645 batch loss 6.2594471 epoch total loss 5.84927511\n",
      "Trained batch 8646 batch loss 5.56184721 epoch total loss 5.84924221\n",
      "Trained batch 8647 batch loss 5.62683535 epoch total loss 5.84921598\n",
      "Trained batch 8648 batch loss 6.80533791 epoch total loss 5.84932661\n",
      "Trained batch 8649 batch loss 5.84195328 epoch total loss 5.84932613\n",
      "Trained batch 8650 batch loss 5.14098692 epoch total loss 5.84924412\n",
      "Trained batch 8651 batch loss 5.1168313 epoch total loss 5.84915924\n",
      "Trained batch 8652 batch loss 5.75858974 epoch total loss 5.84914875\n",
      "Trained batch 8653 batch loss 5.82889748 epoch total loss 5.84914637\n",
      "Trained batch 8654 batch loss 5.82824898 epoch total loss 5.84914398\n",
      "Trained batch 8655 batch loss 4.86811829 epoch total loss 5.84903049\n",
      "Trained batch 8656 batch loss 5.3535161 epoch total loss 5.84897375\n",
      "Trained batch 8657 batch loss 5.66499853 epoch total loss 5.84895229\n",
      "Trained batch 8658 batch loss 5.79591274 epoch total loss 5.84894609\n",
      "Trained batch 8659 batch loss 5.66212463 epoch total loss 5.84892464\n",
      "Trained batch 8660 batch loss 5.40400505 epoch total loss 5.84887314\n",
      "Trained batch 8661 batch loss 6.01089191 epoch total loss 5.84889221\n",
      "Trained batch 8662 batch loss 6.13407087 epoch total loss 5.84892464\n",
      "Trained batch 8663 batch loss 6.60314274 epoch total loss 5.8490119\n",
      "Trained batch 8664 batch loss 5.68562937 epoch total loss 5.84899282\n",
      "Trained batch 8665 batch loss 5.36463785 epoch total loss 5.84893703\n",
      "Trained batch 8666 batch loss 5.84191322 epoch total loss 5.84893656\n",
      "Trained batch 8667 batch loss 5.10700846 epoch total loss 5.84885073\n",
      "Trained batch 8668 batch loss 5.5225153 epoch total loss 5.84881306\n",
      "Trained batch 8669 batch loss 5.97791672 epoch total loss 5.84882784\n",
      "Trained batch 8670 batch loss 6.304564 epoch total loss 5.84888029\n",
      "Trained batch 8671 batch loss 5.71613026 epoch total loss 5.84886503\n",
      "Trained batch 8672 batch loss 5.8022275 epoch total loss 5.84885931\n",
      "Trained batch 8673 batch loss 5.51219463 epoch total loss 5.84882069\n",
      "Trained batch 8674 batch loss 5.47591496 epoch total loss 5.84877777\n",
      "Trained batch 8675 batch loss 5.17901516 epoch total loss 5.84870052\n",
      "Trained batch 8676 batch loss 5.9377985 epoch total loss 5.84871054\n",
      "Trained batch 8677 batch loss 5.43583536 epoch total loss 5.84866333\n",
      "Trained batch 8678 batch loss 5.73389101 epoch total loss 5.84865\n",
      "Trained batch 8679 batch loss 4.33556 epoch total loss 5.84847593\n",
      "Trained batch 8680 batch loss 4.22283 epoch total loss 5.84828854\n",
      "Trained batch 8681 batch loss 4.72511292 epoch total loss 5.84815931\n",
      "Trained batch 8682 batch loss 4.73338175 epoch total loss 5.84803104\n",
      "Trained batch 8683 batch loss 5.72270393 epoch total loss 5.84801674\n",
      "Trained batch 8684 batch loss 5.09742069 epoch total loss 5.84793\n",
      "Trained batch 8685 batch loss 6.42786789 epoch total loss 5.84799719\n",
      "Trained batch 8686 batch loss 6.24692488 epoch total loss 5.84804296\n",
      "Trained batch 8687 batch loss 5.71619415 epoch total loss 5.84802771\n",
      "Trained batch 8688 batch loss 5.93772411 epoch total loss 5.8480382\n",
      "Trained batch 8689 batch loss 6.01622295 epoch total loss 5.84805727\n",
      "Trained batch 8690 batch loss 7.19050407 epoch total loss 5.84821177\n",
      "Trained batch 8691 batch loss 6.48162174 epoch total loss 5.84828472\n",
      "Trained batch 8692 batch loss 6.23716402 epoch total loss 5.84832954\n",
      "Trained batch 8693 batch loss 6.53989267 epoch total loss 5.8484087\n",
      "Trained batch 8694 batch loss 6.1945467 epoch total loss 5.84844875\n",
      "Trained batch 8695 batch loss 4.79244089 epoch total loss 5.84832764\n",
      "Trained batch 8696 batch loss 4.97752285 epoch total loss 5.84822702\n",
      "Trained batch 8697 batch loss 6.033885 epoch total loss 5.84824848\n",
      "Trained batch 8698 batch loss 6.45108843 epoch total loss 5.84831762\n",
      "Trained batch 8699 batch loss 5.68522453 epoch total loss 5.84829903\n",
      "Trained batch 8700 batch loss 6.95072603 epoch total loss 5.84842539\n",
      "Trained batch 8701 batch loss 6.3086071 epoch total loss 5.84847832\n",
      "Trained batch 8702 batch loss 5.79289 epoch total loss 5.84847212\n",
      "Trained batch 8703 batch loss 6.01673222 epoch total loss 5.84849119\n",
      "Trained batch 8704 batch loss 5.61032295 epoch total loss 5.84846354\n",
      "Trained batch 8705 batch loss 5.43343 epoch total loss 5.84841585\n",
      "Trained batch 8706 batch loss 5.54728651 epoch total loss 5.84838152\n",
      "Trained batch 8707 batch loss 5.96977139 epoch total loss 5.84839535\n",
      "Trained batch 8708 batch loss 6.17438602 epoch total loss 5.84843254\n",
      "Trained batch 8709 batch loss 6.89570713 epoch total loss 5.8485527\n",
      "Trained batch 8710 batch loss 6.26784372 epoch total loss 5.84860134\n",
      "Trained batch 8711 batch loss 6.01801872 epoch total loss 5.84862089\n",
      "Trained batch 8712 batch loss 6.21276951 epoch total loss 5.84866238\n",
      "Trained batch 8713 batch loss 6.83332443 epoch total loss 5.84877539\n",
      "Trained batch 8714 batch loss 5.02343941 epoch total loss 5.8486805\n",
      "Trained batch 8715 batch loss 5.82890701 epoch total loss 5.84867811\n",
      "Trained batch 8716 batch loss 5.9464612 epoch total loss 5.84868908\n",
      "Trained batch 8717 batch loss 5.97290134 epoch total loss 5.84870338\n",
      "Trained batch 8718 batch loss 6.24004269 epoch total loss 5.84874821\n",
      "Trained batch 8719 batch loss 6.33404446 epoch total loss 5.848804\n",
      "Trained batch 8720 batch loss 6.25063038 epoch total loss 5.84885025\n",
      "Trained batch 8721 batch loss 5.52589226 epoch total loss 5.84881306\n",
      "Trained batch 8722 batch loss 5.47375154 epoch total loss 5.84877\n",
      "Trained batch 8723 batch loss 6.08489037 epoch total loss 5.84879732\n",
      "Trained batch 8724 batch loss 5.99163103 epoch total loss 5.84881353\n",
      "Trained batch 8725 batch loss 5.91365814 epoch total loss 5.84882116\n",
      "Trained batch 8726 batch loss 5.88300085 epoch total loss 5.84882498\n",
      "Trained batch 8727 batch loss 5.10196781 epoch total loss 5.84873962\n",
      "Trained batch 8728 batch loss 5.77009392 epoch total loss 5.84873056\n",
      "Trained batch 8729 batch loss 5.828372 epoch total loss 5.84872818\n",
      "Trained batch 8730 batch loss 5.85713768 epoch total loss 5.84872866\n",
      "Trained batch 8731 batch loss 4.96964836 epoch total loss 5.84862804\n",
      "Trained batch 8732 batch loss 5.83731222 epoch total loss 5.84862661\n",
      "Trained batch 8733 batch loss 6.41853094 epoch total loss 5.84869194\n",
      "Trained batch 8734 batch loss 5.9588 epoch total loss 5.84870434\n",
      "Trained batch 8735 batch loss 6.39384842 epoch total loss 5.8487668\n",
      "Trained batch 8736 batch loss 5.68769407 epoch total loss 5.84874821\n",
      "Trained batch 8737 batch loss 4.68242502 epoch total loss 5.84861469\n",
      "Trained batch 8738 batch loss 4.45880127 epoch total loss 5.84845543\n",
      "Trained batch 8739 batch loss 4.77144718 epoch total loss 5.84833193\n",
      "Trained batch 8740 batch loss 4.75813437 epoch total loss 5.84820747\n",
      "Trained batch 8741 batch loss 6.73592377 epoch total loss 5.84830856\n",
      "Trained batch 8742 batch loss 6.37159061 epoch total loss 5.84836864\n",
      "Trained batch 8743 batch loss 5.41086578 epoch total loss 5.84831858\n",
      "Trained batch 8744 batch loss 6.35377502 epoch total loss 5.84837627\n",
      "Trained batch 8745 batch loss 6.21490431 epoch total loss 5.84841824\n",
      "Trained batch 8746 batch loss 5.51837635 epoch total loss 5.84838057\n",
      "Trained batch 8747 batch loss 5.56046867 epoch total loss 5.84834766\n",
      "Trained batch 8748 batch loss 6.54811954 epoch total loss 5.8484273\n",
      "Trained batch 8749 batch loss 6.44315624 epoch total loss 5.84849501\n",
      "Trained batch 8750 batch loss 7.14608 epoch total loss 5.8486433\n",
      "Trained batch 8751 batch loss 5.48918724 epoch total loss 5.84860229\n",
      "Trained batch 8752 batch loss 6.09716415 epoch total loss 5.84863043\n",
      "Trained batch 8753 batch loss 6.126194 epoch total loss 5.84866238\n",
      "Trained batch 8754 batch loss 5.58622217 epoch total loss 5.84863234\n",
      "Trained batch 8755 batch loss 5.99912214 epoch total loss 5.8486495\n",
      "Trained batch 8756 batch loss 5.62418175 epoch total loss 5.84862375\n",
      "Trained batch 8757 batch loss 4.86728 epoch total loss 5.8485117\n",
      "Trained batch 8758 batch loss 5.83969545 epoch total loss 5.84851074\n",
      "Trained batch 8759 batch loss 5.49008751 epoch total loss 5.84846973\n",
      "Trained batch 8760 batch loss 5.87510586 epoch total loss 5.8484726\n",
      "Trained batch 8761 batch loss 5.84311247 epoch total loss 5.84847212\n",
      "Trained batch 8762 batch loss 5.46170807 epoch total loss 5.84842777\n",
      "Trained batch 8763 batch loss 5.0985589 epoch total loss 5.84834242\n",
      "Trained batch 8764 batch loss 5.90583801 epoch total loss 5.84834909\n",
      "Trained batch 8765 batch loss 6.49841 epoch total loss 5.848423\n",
      "Trained batch 8766 batch loss 6.0979991 epoch total loss 5.84845161\n",
      "Trained batch 8767 batch loss 5.55293941 epoch total loss 5.84841824\n",
      "Trained batch 8768 batch loss 5.79506207 epoch total loss 5.84841251\n",
      "Trained batch 8769 batch loss 6.22347355 epoch total loss 5.84845495\n",
      "Trained batch 8770 batch loss 6.18215275 epoch total loss 5.8484931\n",
      "Trained batch 8771 batch loss 5.64399433 epoch total loss 5.84846973\n",
      "Trained batch 8772 batch loss 5.91778851 epoch total loss 5.84847784\n",
      "Trained batch 8773 batch loss 5.70063 epoch total loss 5.84846067\n",
      "Trained batch 8774 batch loss 5.68979788 epoch total loss 5.84844303\n",
      "Trained batch 8775 batch loss 5.46274376 epoch total loss 5.84839869\n",
      "Trained batch 8776 batch loss 5.90147305 epoch total loss 5.84840488\n",
      "Trained batch 8777 batch loss 5.60662174 epoch total loss 5.84837723\n",
      "Trained batch 8778 batch loss 5.88049793 epoch total loss 5.84838057\n",
      "Trained batch 8779 batch loss 6.02277279 epoch total loss 5.84840059\n",
      "Trained batch 8780 batch loss 6.28353882 epoch total loss 5.84845\n",
      "Trained batch 8781 batch loss 5.06450844 epoch total loss 5.84836149\n",
      "Trained batch 8782 batch loss 5.15039158 epoch total loss 5.84828186\n",
      "Trained batch 8783 batch loss 5.81486082 epoch total loss 5.84827852\n",
      "Trained batch 8784 batch loss 4.57316494 epoch total loss 5.84813356\n",
      "Trained batch 8785 batch loss 4.19976902 epoch total loss 5.84794569\n",
      "Trained batch 8786 batch loss 4.69326544 epoch total loss 5.84781408\n",
      "Trained batch 8787 batch loss 5.9407835 epoch total loss 5.84782457\n",
      "Trained batch 8788 batch loss 4.84223652 epoch total loss 5.84771061\n",
      "Trained batch 8789 batch loss 4.60611582 epoch total loss 5.84756899\n",
      "Trained batch 8790 batch loss 6.38854027 epoch total loss 5.8476305\n",
      "Trained batch 8791 batch loss 5.51649857 epoch total loss 5.84759283\n",
      "Trained batch 8792 batch loss 5.15953398 epoch total loss 5.84751463\n",
      "Trained batch 8793 batch loss 5.39527702 epoch total loss 5.84746313\n",
      "Trained batch 8794 batch loss 5.69102192 epoch total loss 5.84744549\n",
      "Trained batch 8795 batch loss 5.67691231 epoch total loss 5.84742594\n",
      "Trained batch 8796 batch loss 4.40497398 epoch total loss 5.84726191\n",
      "Trained batch 8797 batch loss 5.22417164 epoch total loss 5.84719086\n",
      "Trained batch 8798 batch loss 5.364604 epoch total loss 5.84713602\n",
      "Trained batch 8799 batch loss 5.35920763 epoch total loss 5.84708\n",
      "Trained batch 8800 batch loss 4.47614861 epoch total loss 5.84692478\n",
      "Trained batch 8801 batch loss 4.89228725 epoch total loss 5.84681606\n",
      "Trained batch 8802 batch loss 5.11781263 epoch total loss 5.84673309\n",
      "Trained batch 8803 batch loss 4.58303547 epoch total loss 5.84658957\n",
      "Trained batch 8804 batch loss 4.77505207 epoch total loss 5.84646749\n",
      "Trained batch 8805 batch loss 4.50751448 epoch total loss 5.84631538\n",
      "Trained batch 8806 batch loss 4.27437782 epoch total loss 5.84613705\n",
      "Trained batch 8807 batch loss 4.81471586 epoch total loss 5.84602\n",
      "Trained batch 8808 batch loss 4.53681374 epoch total loss 5.84587097\n",
      "Trained batch 8809 batch loss 4.68667078 epoch total loss 5.84574\n",
      "Trained batch 8810 batch loss 5.17810726 epoch total loss 5.84566402\n",
      "Trained batch 8811 batch loss 5.19623089 epoch total loss 5.84559\n",
      "Trained batch 8812 batch loss 4.90722704 epoch total loss 5.84548378\n",
      "Trained batch 8813 batch loss 5.01664305 epoch total loss 5.84538937\n",
      "Trained batch 8814 batch loss 4.73022938 epoch total loss 5.845263\n",
      "Trained batch 8815 batch loss 4.56148529 epoch total loss 5.84511757\n",
      "Trained batch 8816 batch loss 6.3334589 epoch total loss 5.84517288\n",
      "Trained batch 8817 batch loss 6.3385148 epoch total loss 5.84522867\n",
      "Trained batch 8818 batch loss 6.57096863 epoch total loss 5.84531116\n",
      "Trained batch 8819 batch loss 6.07073402 epoch total loss 5.84533644\n",
      "Trained batch 8820 batch loss 6.57423496 epoch total loss 5.84541941\n",
      "Trained batch 8821 batch loss 5.74159908 epoch total loss 5.84540749\n",
      "Trained batch 8822 batch loss 6.3866396 epoch total loss 5.845469\n",
      "Trained batch 8823 batch loss 6.54650831 epoch total loss 5.84554815\n",
      "Trained batch 8824 batch loss 7.11637878 epoch total loss 5.84569263\n",
      "Trained batch 8825 batch loss 5.56921959 epoch total loss 5.84566116\n",
      "Trained batch 8826 batch loss 7.13218498 epoch total loss 5.84580708\n",
      "Trained batch 8827 batch loss 6.02397823 epoch total loss 5.8458271\n",
      "Trained batch 8828 batch loss 6.13311863 epoch total loss 5.84585953\n",
      "Trained batch 8829 batch loss 5.24162531 epoch total loss 5.84579134\n",
      "Trained batch 8830 batch loss 5.41181 epoch total loss 5.84574223\n",
      "Trained batch 8831 batch loss 5.83474541 epoch total loss 5.8457408\n",
      "Trained batch 8832 batch loss 5.62327 epoch total loss 5.845716\n",
      "Trained batch 8833 batch loss 6.44313192 epoch total loss 5.84578323\n",
      "Trained batch 8834 batch loss 6.03369045 epoch total loss 5.84580469\n",
      "Trained batch 8835 batch loss 6.73011875 epoch total loss 5.84590483\n",
      "Trained batch 8836 batch loss 5.80219 epoch total loss 5.8459\n",
      "Trained batch 8837 batch loss 6.75341 epoch total loss 5.84600258\n",
      "Trained batch 8838 batch loss 6.88751793 epoch total loss 5.84612036\n",
      "Trained batch 8839 batch loss 6.65466118 epoch total loss 5.84621191\n",
      "Trained batch 8840 batch loss 6.83219194 epoch total loss 5.84632349\n",
      "Trained batch 8841 batch loss 6.38492489 epoch total loss 5.84638453\n",
      "Trained batch 8842 batch loss 6.40530872 epoch total loss 5.84644794\n",
      "Trained batch 8843 batch loss 6.47610378 epoch total loss 5.84651899\n",
      "Trained batch 8844 batch loss 6.21908188 epoch total loss 5.84656143\n",
      "Trained batch 8845 batch loss 5.5378685 epoch total loss 5.84652662\n",
      "Trained batch 8846 batch loss 5.56385899 epoch total loss 5.84649467\n",
      "Trained batch 8847 batch loss 6.15143871 epoch total loss 5.84652901\n",
      "Trained batch 8848 batch loss 6.34780121 epoch total loss 5.84658575\n",
      "Trained batch 8849 batch loss 5.67511463 epoch total loss 5.8465662\n",
      "Trained batch 8850 batch loss 4.83761787 epoch total loss 5.84645224\n",
      "Trained batch 8851 batch loss 4.73078632 epoch total loss 5.84632587\n",
      "Trained batch 8852 batch loss 5.11091423 epoch total loss 5.8462429\n",
      "Trained batch 8853 batch loss 5.71893406 epoch total loss 5.8462286\n",
      "Trained batch 8854 batch loss 5.19163227 epoch total loss 5.84615469\n",
      "Trained batch 8855 batch loss 4.9992857 epoch total loss 5.84605885\n",
      "Trained batch 8856 batch loss 5.22579861 epoch total loss 5.84598875\n",
      "Trained batch 8857 batch loss 5.43935871 epoch total loss 5.84594297\n",
      "Trained batch 8858 batch loss 5.90165377 epoch total loss 5.84594917\n",
      "Trained batch 8859 batch loss 4.93368053 epoch total loss 5.84584618\n",
      "Trained batch 8860 batch loss 4.80371475 epoch total loss 5.84572887\n",
      "Trained batch 8861 batch loss 4.67484093 epoch total loss 5.84559679\n",
      "Trained batch 8862 batch loss 4.82439423 epoch total loss 5.8454814\n",
      "Trained batch 8863 batch loss 4.81593 epoch total loss 5.84536552\n",
      "Trained batch 8864 batch loss 5.06703091 epoch total loss 5.84527731\n",
      "Trained batch 8865 batch loss 5.40643024 epoch total loss 5.84522772\n",
      "Trained batch 8866 batch loss 5.6223259 epoch total loss 5.84520245\n",
      "Trained batch 8867 batch loss 5.32072496 epoch total loss 5.84514332\n",
      "Trained batch 8868 batch loss 5.58566904 epoch total loss 5.84511423\n",
      "Trained batch 8869 batch loss 6.07123089 epoch total loss 5.8451395\n",
      "Trained batch 8870 batch loss 6.31135511 epoch total loss 5.84519243\n",
      "Trained batch 8871 batch loss 5.74423885 epoch total loss 5.84518099\n",
      "Trained batch 8872 batch loss 6.18290043 epoch total loss 5.84521914\n",
      "Trained batch 8873 batch loss 6.1661644 epoch total loss 5.84525585\n",
      "Trained batch 8874 batch loss 6.17633 epoch total loss 5.84529305\n",
      "Trained batch 8875 batch loss 6.21628189 epoch total loss 5.84533453\n",
      "Trained batch 8876 batch loss 6.22601223 epoch total loss 5.84537745\n",
      "Trained batch 8877 batch loss 6.17789364 epoch total loss 5.84541512\n",
      "Trained batch 8878 batch loss 6.24367142 epoch total loss 5.84546\n",
      "Trained batch 8879 batch loss 6.22515249 epoch total loss 5.84550285\n",
      "Trained batch 8880 batch loss 6.06953764 epoch total loss 5.84552813\n",
      "Trained batch 8881 batch loss 4.56289196 epoch total loss 5.84538364\n",
      "Trained batch 8882 batch loss 5.63725853 epoch total loss 5.84536028\n",
      "Trained batch 8883 batch loss 5.7273035 epoch total loss 5.84534693\n",
      "Trained batch 8884 batch loss 5.93006516 epoch total loss 5.84535599\n",
      "Trained batch 8885 batch loss 5.9845109 epoch total loss 5.84537172\n",
      "Trained batch 8886 batch loss 5.4215126 epoch total loss 5.84532404\n",
      "Trained batch 8887 batch loss 5.9026413 epoch total loss 5.84533072\n",
      "Trained batch 8888 batch loss 6.71166515 epoch total loss 5.84542799\n",
      "Trained batch 8889 batch loss 5.32036781 epoch total loss 5.84536886\n",
      "Trained batch 8890 batch loss 5.21464777 epoch total loss 5.84529781\n",
      "Trained batch 8891 batch loss 6.03282642 epoch total loss 5.84531879\n",
      "Trained batch 8892 batch loss 6.19732952 epoch total loss 5.84535885\n",
      "Trained batch 8893 batch loss 6.90977478 epoch total loss 5.84547853\n",
      "Trained batch 8894 batch loss 6.14264679 epoch total loss 5.84551191\n",
      "Trained batch 8895 batch loss 5.26006079 epoch total loss 5.84544659\n",
      "Trained batch 8896 batch loss 5.65479517 epoch total loss 5.84542513\n",
      "Trained batch 8897 batch loss 7.5241394 epoch total loss 5.84561396\n",
      "Trained batch 8898 batch loss 5.50991535 epoch total loss 5.84557629\n",
      "Trained batch 8899 batch loss 4.79486656 epoch total loss 5.84545803\n",
      "Trained batch 8900 batch loss 5.65790367 epoch total loss 5.84543657\n",
      "Trained batch 8901 batch loss 5.84134769 epoch total loss 5.8454361\n",
      "Trained batch 8902 batch loss 4.90859747 epoch total loss 5.84533119\n",
      "Trained batch 8903 batch loss 5.43165 epoch total loss 5.84528494\n",
      "Trained batch 8904 batch loss 5.77357674 epoch total loss 5.84527683\n",
      "Trained batch 8905 batch loss 5.60696697 epoch total loss 5.84524965\n",
      "Trained batch 8906 batch loss 5.48893118 epoch total loss 5.8452096\n",
      "Trained batch 8907 batch loss 4.57121563 epoch total loss 5.84506655\n",
      "Trained batch 8908 batch loss 4.49496412 epoch total loss 5.84491491\n",
      "Trained batch 8909 batch loss 4.99653625 epoch total loss 5.84482\n",
      "Trained batch 8910 batch loss 5.60011292 epoch total loss 5.84479237\n",
      "Trained batch 8911 batch loss 4.97629595 epoch total loss 5.84469509\n",
      "Trained batch 8912 batch loss 4.99800682 epoch total loss 5.84459972\n",
      "Trained batch 8913 batch loss 5.50478554 epoch total loss 5.84456158\n",
      "Trained batch 8914 batch loss 5.72989559 epoch total loss 5.8445487\n",
      "Trained batch 8915 batch loss 5.63704586 epoch total loss 5.84452534\n",
      "Trained batch 8916 batch loss 5.56226158 epoch total loss 5.84449387\n",
      "Trained batch 8917 batch loss 5.41585255 epoch total loss 5.84444571\n",
      "Trained batch 8918 batch loss 6.13743973 epoch total loss 5.84447861\n",
      "Trained batch 8919 batch loss 6.11717558 epoch total loss 5.84450912\n",
      "Trained batch 8920 batch loss 5.79874086 epoch total loss 5.84450388\n",
      "Trained batch 8921 batch loss 5.58023453 epoch total loss 5.84447432\n",
      "Trained batch 8922 batch loss 6.12453794 epoch total loss 5.84450579\n",
      "Trained batch 8923 batch loss 6.41415501 epoch total loss 5.84456968\n",
      "Trained batch 8924 batch loss 5.51715374 epoch total loss 5.84453249\n",
      "Trained batch 8925 batch loss 6.13743258 epoch total loss 5.84456539\n",
      "Trained batch 8926 batch loss 6.24439192 epoch total loss 5.84461\n",
      "Trained batch 8927 batch loss 6.43603468 epoch total loss 5.84467697\n",
      "Trained batch 8928 batch loss 5.8352809 epoch total loss 5.84467602\n",
      "Trained batch 8929 batch loss 6.93113613 epoch total loss 5.84479713\n",
      "Trained batch 8930 batch loss 5.76809692 epoch total loss 5.84478903\n",
      "Trained batch 8931 batch loss 5.63634729 epoch total loss 5.84476566\n",
      "Trained batch 8932 batch loss 5.52844429 epoch total loss 5.84473\n",
      "Trained batch 8933 batch loss 5.92982483 epoch total loss 5.84473944\n",
      "Trained batch 8934 batch loss 5.2683115 epoch total loss 5.84467506\n",
      "Trained batch 8935 batch loss 6.0585022 epoch total loss 5.84469891\n",
      "Trained batch 8936 batch loss 5.73950291 epoch total loss 5.84468699\n",
      "Trained batch 8937 batch loss 5.82839823 epoch total loss 5.84468555\n",
      "Trained batch 8938 batch loss 5.67710161 epoch total loss 5.84466648\n",
      "Trained batch 8939 batch loss 5.5473032 epoch total loss 5.8446331\n",
      "Trained batch 8940 batch loss 5.84634781 epoch total loss 5.84463358\n",
      "Trained batch 8941 batch loss 5.00018597 epoch total loss 5.84453917\n",
      "Trained batch 8942 batch loss 5.37568808 epoch total loss 5.84448671\n",
      "Trained batch 8943 batch loss 5.60184622 epoch total loss 5.84445953\n",
      "Trained batch 8944 batch loss 5.32170391 epoch total loss 5.84440088\n",
      "Trained batch 8945 batch loss 5.64821911 epoch total loss 5.84437895\n",
      "Trained batch 8946 batch loss 5.51083374 epoch total loss 5.84434175\n",
      "Trained batch 8947 batch loss 4.96330261 epoch total loss 5.84424353\n",
      "Trained batch 8948 batch loss 5.83499146 epoch total loss 5.84424257\n",
      "Trained batch 8949 batch loss 4.33398104 epoch total loss 5.8440733\n",
      "Trained batch 8950 batch loss 5.40228701 epoch total loss 5.84402418\n",
      "Trained batch 8951 batch loss 5.50532055 epoch total loss 5.84398603\n",
      "Trained batch 8952 batch loss 5.36745548 epoch total loss 5.84393263\n",
      "Trained batch 8953 batch loss 6.12045 epoch total loss 5.84396362\n",
      "Trained batch 8954 batch loss 6.40432072 epoch total loss 5.84402657\n",
      "Trained batch 8955 batch loss 5.96447468 epoch total loss 5.84404\n",
      "Trained batch 8956 batch loss 5.97198248 epoch total loss 5.84405422\n",
      "Trained batch 8957 batch loss 6.22537613 epoch total loss 5.84409714\n",
      "Trained batch 8958 batch loss 4.44099426 epoch total loss 5.84394073\n",
      "Trained batch 8959 batch loss 6.57656097 epoch total loss 5.84402227\n",
      "Trained batch 8960 batch loss 6.22547913 epoch total loss 5.84406519\n",
      "Trained batch 8961 batch loss 3.16791916 epoch total loss 5.84376669\n",
      "Trained batch 8962 batch loss 6.73986721 epoch total loss 5.84386635\n",
      "Trained batch 8963 batch loss 6.70056343 epoch total loss 5.84396172\n",
      "Trained batch 8964 batch loss 6.19894314 epoch total loss 5.84400129\n",
      "Trained batch 8965 batch loss 4.30844975 epoch total loss 5.84383\n",
      "Trained batch 8966 batch loss 5.41725683 epoch total loss 5.8437829\n",
      "Trained batch 8967 batch loss 6.0958519 epoch total loss 5.84381104\n",
      "Trained batch 8968 batch loss 6.60482359 epoch total loss 5.84389591\n",
      "Trained batch 8969 batch loss 6.88818932 epoch total loss 5.84401226\n",
      "Trained batch 8970 batch loss 6.04965305 epoch total loss 5.84403515\n",
      "Trained batch 8971 batch loss 6.37973 epoch total loss 5.84409475\n",
      "Trained batch 8972 batch loss 6.19836 epoch total loss 5.84413433\n",
      "Trained batch 8973 batch loss 6.30147266 epoch total loss 5.84418535\n",
      "Trained batch 8974 batch loss 6.00775623 epoch total loss 5.84420347\n",
      "Trained batch 8975 batch loss 7.42392349 epoch total loss 5.84438\n",
      "Trained batch 8976 batch loss 6.10794735 epoch total loss 5.84440947\n",
      "Trained batch 8977 batch loss 6.57233715 epoch total loss 5.84449053\n",
      "Trained batch 8978 batch loss 5.82980061 epoch total loss 5.84448862\n",
      "Trained batch 8979 batch loss 5.1496 epoch total loss 5.84441137\n",
      "Trained batch 8980 batch loss 6.1293211 epoch total loss 5.84444284\n",
      "Trained batch 8981 batch loss 5.89887953 epoch total loss 5.84444904\n",
      "Trained batch 8982 batch loss 5.83828449 epoch total loss 5.84444857\n",
      "Trained batch 8983 batch loss 5.80145836 epoch total loss 5.8444438\n",
      "Trained batch 8984 batch loss 5.69623709 epoch total loss 5.84442711\n",
      "Trained batch 8985 batch loss 6.17152548 epoch total loss 5.84446335\n",
      "Trained batch 8986 batch loss 5.72883224 epoch total loss 5.84445095\n",
      "Trained batch 8987 batch loss 6.04917049 epoch total loss 5.84447384\n",
      "Trained batch 8988 batch loss 6.29116583 epoch total loss 5.84452343\n",
      "Trained batch 8989 batch loss 4.76633835 epoch total loss 5.84440374\n",
      "Trained batch 8990 batch loss 4.9748292 epoch total loss 5.84430695\n",
      "Trained batch 8991 batch loss 5.37380171 epoch total loss 5.84425497\n",
      "Trained batch 8992 batch loss 5.02971125 epoch total loss 5.84416437\n",
      "Trained batch 8993 batch loss 5.56424046 epoch total loss 5.8441329\n",
      "Trained batch 8994 batch loss 5.67280388 epoch total loss 5.84411383\n",
      "Trained batch 8995 batch loss 5.79159737 epoch total loss 5.8441081\n",
      "Trained batch 8996 batch loss 5.14892769 epoch total loss 5.84403086\n",
      "Trained batch 8997 batch loss 5.23936 epoch total loss 5.84396362\n",
      "Trained batch 8998 batch loss 5.21271229 epoch total loss 5.84389305\n",
      "Trained batch 8999 batch loss 5.1842823 epoch total loss 5.84382\n",
      "Trained batch 9000 batch loss 5.29568577 epoch total loss 5.84375906\n",
      "Trained batch 9001 batch loss 5.82324696 epoch total loss 5.84375715\n",
      "Trained batch 9002 batch loss 5.21190405 epoch total loss 5.84368658\n",
      "Trained batch 9003 batch loss 4.8967495 epoch total loss 5.84358168\n",
      "Trained batch 9004 batch loss 5.77761555 epoch total loss 5.84357452\n",
      "Trained batch 9005 batch loss 5.46374607 epoch total loss 5.84353209\n",
      "Trained batch 9006 batch loss 5.19256353 epoch total loss 5.84345961\n",
      "Trained batch 9007 batch loss 4.87690639 epoch total loss 5.84335232\n",
      "Trained batch 9008 batch loss 4.98791552 epoch total loss 5.84325743\n",
      "Trained batch 9009 batch loss 5.55556 epoch total loss 5.84322548\n",
      "Trained batch 9010 batch loss 5.54011059 epoch total loss 5.84319162\n",
      "Trained batch 9011 batch loss 5.57375908 epoch total loss 5.84316158\n",
      "Trained batch 9012 batch loss 5.46370125 epoch total loss 5.84311962\n",
      "Trained batch 9013 batch loss 5.09258127 epoch total loss 5.84303665\n",
      "Trained batch 9014 batch loss 4.97991848 epoch total loss 5.84294081\n",
      "Trained batch 9015 batch loss 5.49115467 epoch total loss 5.84290218\n",
      "Trained batch 9016 batch loss 5.13341427 epoch total loss 5.84282303\n",
      "Trained batch 9017 batch loss 6.0506258 epoch total loss 5.84284639\n",
      "Trained batch 9018 batch loss 5.81383419 epoch total loss 5.84284306\n",
      "Trained batch 9019 batch loss 5.58635 epoch total loss 5.84281445\n",
      "Trained batch 9020 batch loss 4.96970129 epoch total loss 5.84271765\n",
      "Trained batch 9021 batch loss 5.5434804 epoch total loss 5.84268427\n",
      "Trained batch 9022 batch loss 5.6743803 epoch total loss 5.84266567\n",
      "Trained batch 9023 batch loss 5.73513174 epoch total loss 5.84265375\n",
      "Trained batch 9024 batch loss 6.14815807 epoch total loss 5.84268761\n",
      "Trained batch 9025 batch loss 5.67444801 epoch total loss 5.84266901\n",
      "Trained batch 9026 batch loss 6.11487341 epoch total loss 5.84269905\n",
      "Trained batch 9027 batch loss 5.88387251 epoch total loss 5.84270382\n",
      "Trained batch 9028 batch loss 5.13818359 epoch total loss 5.84262562\n",
      "Trained batch 9029 batch loss 6.02700901 epoch total loss 5.84264612\n",
      "Trained batch 9030 batch loss 6.23426437 epoch total loss 5.84268951\n",
      "Trained batch 9031 batch loss 5.55181408 epoch total loss 5.84265709\n",
      "Trained batch 9032 batch loss 5.97728205 epoch total loss 5.84267187\n",
      "Trained batch 9033 batch loss 5.99132442 epoch total loss 5.84268856\n",
      "Trained batch 9034 batch loss 5.49740887 epoch total loss 5.84265\n",
      "Trained batch 9035 batch loss 6.4921422 epoch total loss 5.84272194\n",
      "Trained batch 9036 batch loss 6.35300636 epoch total loss 5.84277821\n",
      "Trained batch 9037 batch loss 5.9783783 epoch total loss 5.84279299\n",
      "Trained batch 9038 batch loss 6.26000929 epoch total loss 5.84283924\n",
      "Trained batch 9039 batch loss 5.76574516 epoch total loss 5.84283066\n",
      "Trained batch 9040 batch loss 6.05465031 epoch total loss 5.84285402\n",
      "Trained batch 9041 batch loss 5.44426632 epoch total loss 5.84281\n",
      "Trained batch 9042 batch loss 4.58301449 epoch total loss 5.84267092\n",
      "Trained batch 9043 batch loss 5.17773867 epoch total loss 5.84259748\n",
      "Trained batch 9044 batch loss 5.2407918 epoch total loss 5.8425312\n",
      "Trained batch 9045 batch loss 5.0600338 epoch total loss 5.84244442\n",
      "Trained batch 9046 batch loss 6.02796268 epoch total loss 5.84246492\n",
      "Trained batch 9047 batch loss 6.33467197 epoch total loss 5.84251928\n",
      "Trained batch 9048 batch loss 5.99842072 epoch total loss 5.84253693\n",
      "Trained batch 9049 batch loss 5.68129635 epoch total loss 5.84251881\n",
      "Trained batch 9050 batch loss 5.39063883 epoch total loss 5.84246874\n",
      "Trained batch 9051 batch loss 6.03743267 epoch total loss 5.84249067\n",
      "Trained batch 9052 batch loss 5.3919 epoch total loss 5.84244061\n",
      "Trained batch 9053 batch loss 5.61322308 epoch total loss 5.84241533\n",
      "Trained batch 9054 batch loss 6.26267338 epoch total loss 5.84246159\n",
      "Trained batch 9055 batch loss 6.86301 epoch total loss 5.8425746\n",
      "Trained batch 9056 batch loss 6.07755327 epoch total loss 5.84260035\n",
      "Trained batch 9057 batch loss 6.06026459 epoch total loss 5.84262419\n",
      "Trained batch 9058 batch loss 5.63957357 epoch total loss 5.84260225\n",
      "Trained batch 9059 batch loss 6.11972761 epoch total loss 5.84263277\n",
      "Trained batch 9060 batch loss 6.44072676 epoch total loss 5.84269905\n",
      "Trained batch 9061 batch loss 6.4461112 epoch total loss 5.84276533\n",
      "Trained batch 9062 batch loss 6.63958549 epoch total loss 5.84285355\n",
      "Trained batch 9063 batch loss 6.59053802 epoch total loss 5.84293604\n",
      "Trained batch 9064 batch loss 6.1009326 epoch total loss 5.84296417\n",
      "Trained batch 9065 batch loss 5.45831203 epoch total loss 5.84292173\n",
      "Trained batch 9066 batch loss 5.46508551 epoch total loss 5.84288025\n",
      "Trained batch 9067 batch loss 6.14670753 epoch total loss 5.84291363\n",
      "Trained batch 9068 batch loss 5.00649738 epoch total loss 5.8428216\n",
      "Trained batch 9069 batch loss 5.44794178 epoch total loss 5.84277821\n",
      "Trained batch 9070 batch loss 5.67099285 epoch total loss 5.84275961\n",
      "Trained batch 9071 batch loss 4.07041264 epoch total loss 5.84256411\n",
      "Trained batch 9072 batch loss 6.40508461 epoch total loss 5.84262609\n",
      "Trained batch 9073 batch loss 6.02145672 epoch total loss 5.84264565\n",
      "Trained batch 9074 batch loss 4.96726131 epoch total loss 5.84254932\n",
      "Trained batch 9075 batch loss 5.80806923 epoch total loss 5.84254551\n",
      "Trained batch 9076 batch loss 5.48092842 epoch total loss 5.84250593\n",
      "Trained batch 9077 batch loss 6.45013189 epoch total loss 5.84257269\n",
      "Trained batch 9078 batch loss 5.27619028 epoch total loss 5.84251\n",
      "Trained batch 9079 batch loss 6.05064964 epoch total loss 5.84253311\n",
      "Trained batch 9080 batch loss 6.29271507 epoch total loss 5.8425827\n",
      "Trained batch 9081 batch loss 6.22870445 epoch total loss 5.84262562\n",
      "Trained batch 9082 batch loss 5.61000681 epoch total loss 5.8426\n",
      "Trained batch 9083 batch loss 4.74924946 epoch total loss 5.84247971\n",
      "Trained batch 9084 batch loss 4.96031952 epoch total loss 5.84238243\n",
      "Trained batch 9085 batch loss 5.55235291 epoch total loss 5.84235048\n",
      "Trained batch 9086 batch loss 6.72861958 epoch total loss 5.84244823\n",
      "Trained batch 9087 batch loss 6.05009 epoch total loss 5.84247112\n",
      "Trained batch 9088 batch loss 5.22664452 epoch total loss 5.84240341\n",
      "Trained batch 9089 batch loss 5.88300037 epoch total loss 5.8424077\n",
      "Trained batch 9090 batch loss 5.1088686 epoch total loss 5.84232712\n",
      "Trained batch 9091 batch loss 6.1128931 epoch total loss 5.84235716\n",
      "Trained batch 9092 batch loss 3.62223148 epoch total loss 5.84211254\n",
      "Trained batch 9093 batch loss 6.13344383 epoch total loss 5.84214449\n",
      "Trained batch 9094 batch loss 5.91992378 epoch total loss 5.84215355\n",
      "Trained batch 9095 batch loss 5.50572348 epoch total loss 5.84211636\n",
      "Trained batch 9096 batch loss 4.52537441 epoch total loss 5.8419714\n",
      "Trained batch 9097 batch loss 5.75109053 epoch total loss 5.84196091\n",
      "Trained batch 9098 batch loss 6.40425539 epoch total loss 5.8420229\n",
      "Trained batch 9099 batch loss 6.02268124 epoch total loss 5.84204245\n",
      "Trained batch 9100 batch loss 5.2464 epoch total loss 5.84197712\n",
      "Trained batch 9101 batch loss 5.27412128 epoch total loss 5.84191465\n",
      "Trained batch 9102 batch loss 4.82814503 epoch total loss 5.84180355\n",
      "Trained batch 9103 batch loss 5.56155157 epoch total loss 5.84177256\n",
      "Trained batch 9104 batch loss 5.76137686 epoch total loss 5.84176397\n",
      "Trained batch 9105 batch loss 5.8024292 epoch total loss 5.8417592\n",
      "Trained batch 9106 batch loss 5.60578632 epoch total loss 5.84173346\n",
      "Trained batch 9107 batch loss 5.31352711 epoch total loss 5.84167528\n",
      "Trained batch 9108 batch loss 5.98184443 epoch total loss 5.84169054\n",
      "Trained batch 9109 batch loss 5.84174919 epoch total loss 5.84169054\n",
      "Trained batch 9110 batch loss 5.80335283 epoch total loss 5.84168625\n",
      "Trained batch 9111 batch loss 5.98204756 epoch total loss 5.84170151\n",
      "Trained batch 9112 batch loss 5.50443745 epoch total loss 5.84166431\n",
      "Trained batch 9113 batch loss 6.00718164 epoch total loss 5.84168243\n",
      "Trained batch 9114 batch loss 5.77135086 epoch total loss 5.8416748\n",
      "Trained batch 9115 batch loss 5.80391788 epoch total loss 5.84167051\n",
      "Trained batch 9116 batch loss 5.74438047 epoch total loss 5.84166\n",
      "Trained batch 9117 batch loss 5.62885523 epoch total loss 5.84163666\n",
      "Trained batch 9118 batch loss 5.59657669 epoch total loss 5.84161\n",
      "Trained batch 9119 batch loss 5.66351032 epoch total loss 5.8415904\n",
      "Trained batch 9120 batch loss 6.23301792 epoch total loss 5.8416338\n",
      "Trained batch 9121 batch loss 5.61522675 epoch total loss 5.84160852\n",
      "Trained batch 9122 batch loss 5.71466684 epoch total loss 5.8415947\n",
      "Trained batch 9123 batch loss 6.50055361 epoch total loss 5.8416667\n",
      "Trained batch 9124 batch loss 6.39117336 epoch total loss 5.84172726\n",
      "Trained batch 9125 batch loss 6.99685192 epoch total loss 5.84185362\n",
      "Trained batch 9126 batch loss 6.13020039 epoch total loss 5.84188509\n",
      "Trained batch 9127 batch loss 6.09822178 epoch total loss 5.84191322\n",
      "Trained batch 9128 batch loss 6.92463493 epoch total loss 5.84203196\n",
      "Trained batch 9129 batch loss 6.32164431 epoch total loss 5.84208441\n",
      "Trained batch 9130 batch loss 6.41193199 epoch total loss 5.8421464\n",
      "Trained batch 9131 batch loss 6.43309498 epoch total loss 5.84221125\n",
      "Trained batch 9132 batch loss 6.60909176 epoch total loss 5.84229517\n",
      "Trained batch 9133 batch loss 6.26911259 epoch total loss 5.8423419\n",
      "Trained batch 9134 batch loss 7.66156673 epoch total loss 5.84254122\n",
      "Trained batch 9135 batch loss 7.16157961 epoch total loss 5.84268522\n",
      "Trained batch 9136 batch loss 5.33260727 epoch total loss 5.84262943\n",
      "Trained batch 9137 batch loss 6.41709518 epoch total loss 5.84269238\n",
      "Trained batch 9138 batch loss 7.07685041 epoch total loss 5.84282732\n",
      "Trained batch 9139 batch loss 6.91300774 epoch total loss 5.84294462\n",
      "Trained batch 9140 batch loss 5.91614056 epoch total loss 5.84295273\n",
      "Trained batch 9141 batch loss 6.52023458 epoch total loss 5.84302711\n",
      "Trained batch 9142 batch loss 6.73443174 epoch total loss 5.84312439\n",
      "Trained batch 9143 batch loss 6.18556881 epoch total loss 5.84316206\n",
      "Trained batch 9144 batch loss 6.82547283 epoch total loss 5.84326935\n",
      "Trained batch 9145 batch loss 6.13734913 epoch total loss 5.8433013\n",
      "Trained batch 9146 batch loss 6.37679338 epoch total loss 5.84335947\n",
      "Trained batch 9147 batch loss 6.446 epoch total loss 5.84342527\n",
      "Trained batch 9148 batch loss 5.62574482 epoch total loss 5.84340143\n",
      "Trained batch 9149 batch loss 7.04938412 epoch total loss 5.84353352\n",
      "Trained batch 9150 batch loss 5.44355488 epoch total loss 5.84349\n",
      "Trained batch 9151 batch loss 6.54781342 epoch total loss 5.84356689\n",
      "Trained batch 9152 batch loss 5.71688366 epoch total loss 5.84355307\n",
      "Trained batch 9153 batch loss 6.1034627 epoch total loss 5.8435812\n",
      "Trained batch 9154 batch loss 6.70740509 epoch total loss 5.84367561\n",
      "Trained batch 9155 batch loss 6.06773472 epoch total loss 5.8437\n",
      "Trained batch 9156 batch loss 6.46721268 epoch total loss 5.84376812\n",
      "Trained batch 9157 batch loss 6.86896515 epoch total loss 5.84388\n",
      "Trained batch 9158 batch loss 7.05772305 epoch total loss 5.84401274\n",
      "Trained batch 9159 batch loss 6.04835033 epoch total loss 5.84403467\n",
      "Trained batch 9160 batch loss 5.32073116 epoch total loss 5.84397793\n",
      "Trained batch 9161 batch loss 4.59353352 epoch total loss 5.84384108\n",
      "Trained batch 9162 batch loss 4.90331841 epoch total loss 5.84373856\n",
      "Trained batch 9163 batch loss 5.02967834 epoch total loss 5.84365\n",
      "Trained batch 9164 batch loss 4.99532413 epoch total loss 5.84355736\n",
      "Trained batch 9165 batch loss 5.47297764 epoch total loss 5.84351683\n",
      "Trained batch 9166 batch loss 5.40158415 epoch total loss 5.84346867\n",
      "Trained batch 9167 batch loss 4.8024826 epoch total loss 5.84335518\n",
      "Trained batch 9168 batch loss 5.06352806 epoch total loss 5.84327\n",
      "Trained batch 9169 batch loss 5.0771246 epoch total loss 5.84318638\n",
      "Trained batch 9170 batch loss 4.00994444 epoch total loss 5.84298658\n",
      "Trained batch 9171 batch loss 4.49676704 epoch total loss 5.84283972\n",
      "Trained batch 9172 batch loss 4.90355253 epoch total loss 5.8427372\n",
      "Trained batch 9173 batch loss 5.46597433 epoch total loss 5.84269619\n",
      "Trained batch 9174 batch loss 5.50358486 epoch total loss 5.842659\n",
      "Trained batch 9175 batch loss 4.57826233 epoch total loss 5.84252119\n",
      "Trained batch 9176 batch loss 4.17235 epoch total loss 5.84233904\n",
      "Trained batch 9177 batch loss 4.94946384 epoch total loss 5.84224176\n",
      "Trained batch 9178 batch loss 4.55549908 epoch total loss 5.84210157\n",
      "Trained batch 9179 batch loss 4.55194139 epoch total loss 5.84196091\n",
      "Trained batch 9180 batch loss 4.40455294 epoch total loss 5.8418045\n",
      "Trained batch 9181 batch loss 4.51068163 epoch total loss 5.84165955\n",
      "Trained batch 9182 batch loss 4.72091675 epoch total loss 5.84153795\n",
      "Trained batch 9183 batch loss 4.1798625 epoch total loss 5.84135675\n",
      "Trained batch 9184 batch loss 4.56706572 epoch total loss 5.84121799\n",
      "Trained batch 9185 batch loss 4.18062115 epoch total loss 5.84103727\n",
      "Trained batch 9186 batch loss 4.5128336 epoch total loss 5.84089231\n",
      "Trained batch 9187 batch loss 5.12922049 epoch total loss 5.84081507\n",
      "Trained batch 9188 batch loss 4.66887951 epoch total loss 5.84068727\n",
      "Trained batch 9189 batch loss 5.02674818 epoch total loss 5.84059858\n",
      "Trained batch 9190 batch loss 4.57855654 epoch total loss 5.84046125\n",
      "Trained batch 9191 batch loss 4.20121431 epoch total loss 5.84028339\n",
      "Trained batch 9192 batch loss 4.23501205 epoch total loss 5.84010839\n",
      "Trained batch 9193 batch loss 4.88963795 epoch total loss 5.8400054\n",
      "Trained batch 9194 batch loss 4.82036304 epoch total loss 5.83989429\n",
      "Trained batch 9195 batch loss 6.35771179 epoch total loss 5.83995104\n",
      "Trained batch 9196 batch loss 5.75333261 epoch total loss 5.8399415\n",
      "Trained batch 9197 batch loss 5.34057045 epoch total loss 5.83988714\n",
      "Trained batch 9198 batch loss 5.53656244 epoch total loss 5.83985376\n",
      "Trained batch 9199 batch loss 4.96672964 epoch total loss 5.83975887\n",
      "Trained batch 9200 batch loss 5.94689751 epoch total loss 5.83977032\n",
      "Trained batch 9201 batch loss 5.80632973 epoch total loss 5.8397665\n",
      "Trained batch 9202 batch loss 6.5731287 epoch total loss 5.83984613\n",
      "Trained batch 9203 batch loss 5.67029285 epoch total loss 5.83982801\n",
      "Trained batch 9204 batch loss 5.20799923 epoch total loss 5.83975935\n",
      "Trained batch 9205 batch loss 5.16870213 epoch total loss 5.83968639\n",
      "Trained batch 9206 batch loss 5.65038633 epoch total loss 5.83966541\n",
      "Trained batch 9207 batch loss 5.47383308 epoch total loss 5.83962584\n",
      "Trained batch 9208 batch loss 6.59546614 epoch total loss 5.83970737\n",
      "Trained batch 9209 batch loss 4.28500557 epoch total loss 5.83953857\n",
      "Trained batch 9210 batch loss 4.25943279 epoch total loss 5.83936691\n",
      "Trained batch 9211 batch loss 6.60358524 epoch total loss 5.83945036\n",
      "Trained batch 9212 batch loss 5.98008728 epoch total loss 5.83946562\n",
      "Trained batch 9213 batch loss 5.6031456 epoch total loss 5.83944\n",
      "Trained batch 9214 batch loss 5.7633667 epoch total loss 5.83943129\n",
      "Trained batch 9215 batch loss 5.47771645 epoch total loss 5.83939171\n",
      "Trained batch 9216 batch loss 5.73034048 epoch total loss 5.83938026\n",
      "Trained batch 9217 batch loss 6.43041372 epoch total loss 5.83944416\n",
      "Trained batch 9218 batch loss 6.06020927 epoch total loss 5.839468\n",
      "Trained batch 9219 batch loss 5.77554226 epoch total loss 5.83946133\n",
      "Trained batch 9220 batch loss 6.68677902 epoch total loss 5.83955288\n",
      "Trained batch 9221 batch loss 6.17609167 epoch total loss 5.8395896\n",
      "Trained batch 9222 batch loss 6.31305552 epoch total loss 5.83964062\n",
      "Trained batch 9223 batch loss 5.9957304 epoch total loss 5.83965778\n",
      "Trained batch 9224 batch loss 6.04580784 epoch total loss 5.83968\n",
      "Trained batch 9225 batch loss 4.97145844 epoch total loss 5.83958626\n",
      "Trained batch 9226 batch loss 5.14784956 epoch total loss 5.83951139\n",
      "Trained batch 9227 batch loss 6.29166222 epoch total loss 5.83956051\n",
      "Trained batch 9228 batch loss 6.17057896 epoch total loss 5.83959675\n",
      "Trained batch 9229 batch loss 5.36977482 epoch total loss 5.83954573\n",
      "Trained batch 9230 batch loss 6.20103455 epoch total loss 5.83958483\n",
      "Trained batch 9231 batch loss 6.08912754 epoch total loss 5.83961201\n",
      "Trained batch 9232 batch loss 6.23507786 epoch total loss 5.83965445\n",
      "Trained batch 9233 batch loss 5.60112906 epoch total loss 5.8396287\n",
      "Trained batch 9234 batch loss 6.07237053 epoch total loss 5.83965445\n",
      "Trained batch 9235 batch loss 5.91807461 epoch total loss 5.83966255\n",
      "Trained batch 9236 batch loss 5.659338 epoch total loss 5.83964348\n",
      "Trained batch 9237 batch loss 5.94469833 epoch total loss 5.83965492\n",
      "Trained batch 9238 batch loss 5.35200882 epoch total loss 5.83960199\n",
      "Trained batch 9239 batch loss 6.09766817 epoch total loss 5.83962965\n",
      "Trained batch 9240 batch loss 5.68931723 epoch total loss 5.83961344\n",
      "Trained batch 9241 batch loss 5.61561918 epoch total loss 5.83958912\n",
      "Trained batch 9242 batch loss 6.20013142 epoch total loss 5.83962822\n",
      "Trained batch 9243 batch loss 5.00868416 epoch total loss 5.8395381\n",
      "Trained batch 9244 batch loss 5.89902782 epoch total loss 5.83954477\n",
      "Trained batch 9245 batch loss 4.69584751 epoch total loss 5.8394208\n",
      "Trained batch 9246 batch loss 5.97083855 epoch total loss 5.8394351\n",
      "Trained batch 9247 batch loss 5.54661369 epoch total loss 5.83940363\n",
      "Trained batch 9248 batch loss 4.54714108 epoch total loss 5.83926392\n",
      "Trained batch 9249 batch loss 5.33892536 epoch total loss 5.83921\n",
      "Trained batch 9250 batch loss 5.21126556 epoch total loss 5.83914185\n",
      "Trained batch 9251 batch loss 5.19811869 epoch total loss 5.8390727\n",
      "Trained batch 9252 batch loss 4.93488503 epoch total loss 5.83897495\n",
      "Trained batch 9253 batch loss 5.12232494 epoch total loss 5.83889723\n",
      "Trained batch 9254 batch loss 4.80396509 epoch total loss 5.83878565\n",
      "Trained batch 9255 batch loss 6.69231462 epoch total loss 5.83887768\n",
      "Trained batch 9256 batch loss 5.24826431 epoch total loss 5.83881378\n",
      "Trained batch 9257 batch loss 6.18239594 epoch total loss 5.83885145\n",
      "Trained batch 9258 batch loss 5.93691206 epoch total loss 5.83886194\n",
      "Trained batch 9259 batch loss 5.30583668 epoch total loss 5.83880424\n",
      "Trained batch 9260 batch loss 6.29372501 epoch total loss 5.83885336\n",
      "Trained batch 9261 batch loss 5.73390865 epoch total loss 5.83884192\n",
      "Trained batch 9262 batch loss 5.79193926 epoch total loss 5.83883715\n",
      "Trained batch 9263 batch loss 6.14800835 epoch total loss 5.83887053\n",
      "Trained batch 9264 batch loss 5.06917095 epoch total loss 5.83878756\n",
      "Trained batch 9265 batch loss 4.72772 epoch total loss 5.83866739\n",
      "Trained batch 9266 batch loss 5.64897585 epoch total loss 5.83864689\n",
      "Trained batch 9267 batch loss 5.4517231 epoch total loss 5.8386054\n",
      "Trained batch 9268 batch loss 5.71098423 epoch total loss 5.83859158\n",
      "Trained batch 9269 batch loss 5.20887089 epoch total loss 5.83852339\n",
      "Trained batch 9270 batch loss 5.33857679 epoch total loss 5.83846951\n",
      "Trained batch 9271 batch loss 4.98796272 epoch total loss 5.83837795\n",
      "Trained batch 9272 batch loss 5.36827469 epoch total loss 5.83832693\n",
      "Trained batch 9273 batch loss 5.12489033 epoch total loss 5.83825\n",
      "Trained batch 9274 batch loss 5.11442 epoch total loss 5.83817196\n",
      "Trained batch 9275 batch loss 5.05111408 epoch total loss 5.83808708\n",
      "Trained batch 9276 batch loss 5.7269783 epoch total loss 5.83807516\n",
      "Trained batch 9277 batch loss 5.57980633 epoch total loss 5.83804703\n",
      "Trained batch 9278 batch loss 5.46867943 epoch total loss 5.83800745\n",
      "Trained batch 9279 batch loss 5.5701704 epoch total loss 5.83797836\n",
      "Trained batch 9280 batch loss 5.31106138 epoch total loss 5.83792162\n",
      "Trained batch 9281 batch loss 4.92619419 epoch total loss 5.83782339\n",
      "Trained batch 9282 batch loss 5.14213562 epoch total loss 5.83774853\n",
      "Trained batch 9283 batch loss 6.19728947 epoch total loss 5.83778715\n",
      "Trained batch 9284 batch loss 6.25152493 epoch total loss 5.8378315\n",
      "Trained batch 9285 batch loss 5.79840422 epoch total loss 5.83782721\n",
      "Trained batch 9286 batch loss 6.60020208 epoch total loss 5.8379097\n",
      "Trained batch 9287 batch loss 5.94994259 epoch total loss 5.83792162\n",
      "Trained batch 9288 batch loss 5.64564228 epoch total loss 5.83790064\n",
      "Trained batch 9289 batch loss 5.31550026 epoch total loss 5.83784437\n",
      "Trained batch 9290 batch loss 5.00059509 epoch total loss 5.83775425\n",
      "Trained batch 9291 batch loss 4.81696558 epoch total loss 5.83764458\n",
      "Trained batch 9292 batch loss 5.16842175 epoch total loss 5.83757257\n",
      "Trained batch 9293 batch loss 5.51703024 epoch total loss 5.83753777\n",
      "Trained batch 9294 batch loss 5.07661486 epoch total loss 5.83745623\n",
      "Trained batch 9295 batch loss 4.99346352 epoch total loss 5.83736515\n",
      "Trained batch 9296 batch loss 4.71083927 epoch total loss 5.83724403\n",
      "Trained batch 9297 batch loss 6.03524208 epoch total loss 5.83726501\n",
      "Trained batch 9298 batch loss 5.36770535 epoch total loss 5.83721447\n",
      "Trained batch 9299 batch loss 6.14514685 epoch total loss 5.83724785\n",
      "Trained batch 9300 batch loss 5.83290291 epoch total loss 5.83724737\n",
      "Trained batch 9301 batch loss 5.14243221 epoch total loss 5.83717203\n",
      "Trained batch 9302 batch loss 5.96270752 epoch total loss 5.83718538\n",
      "Trained batch 9303 batch loss 6.20190668 epoch total loss 5.83722496\n",
      "Trained batch 9304 batch loss 5.30136108 epoch total loss 5.83716726\n",
      "Trained batch 9305 batch loss 4.78738785 epoch total loss 5.83705473\n",
      "Trained batch 9306 batch loss 5.10370827 epoch total loss 5.83697605\n",
      "Trained batch 9307 batch loss 5.0285759 epoch total loss 5.83688879\n",
      "Trained batch 9308 batch loss 4.47107315 epoch total loss 5.8367424\n",
      "Trained batch 9309 batch loss 4.07488823 epoch total loss 5.8365531\n",
      "Trained batch 9310 batch loss 5.01935339 epoch total loss 5.83646536\n",
      "Trained batch 9311 batch loss 4.51136923 epoch total loss 5.83632326\n",
      "Trained batch 9312 batch loss 6.00268078 epoch total loss 5.8363409\n",
      "Trained batch 9313 batch loss 6.00595188 epoch total loss 5.8363595\n",
      "Trained batch 9314 batch loss 5.7899847 epoch total loss 5.83635426\n",
      "Trained batch 9315 batch loss 5.03316498 epoch total loss 5.83626795\n",
      "Trained batch 9316 batch loss 5.20957756 epoch total loss 5.83620071\n",
      "Trained batch 9317 batch loss 5.39900875 epoch total loss 5.83615398\n",
      "Trained batch 9318 batch loss 6.49449 epoch total loss 5.83622456\n",
      "Trained batch 9319 batch loss 5.90600109 epoch total loss 5.83623219\n",
      "Trained batch 9320 batch loss 5.79757881 epoch total loss 5.83622789\n",
      "Trained batch 9321 batch loss 6.50520086 epoch total loss 5.83629942\n",
      "Trained batch 9322 batch loss 4.89593124 epoch total loss 5.83619833\n",
      "Trained batch 9323 batch loss 6.39444542 epoch total loss 5.83625841\n",
      "Trained batch 9324 batch loss 5.54009295 epoch total loss 5.83622646\n",
      "Trained batch 9325 batch loss 5.98099566 epoch total loss 5.8362422\n",
      "Trained batch 9326 batch loss 5.75094223 epoch total loss 5.83623266\n",
      "Trained batch 9327 batch loss 5.96854591 epoch total loss 5.83624697\n",
      "Trained batch 9328 batch loss 5.27924109 epoch total loss 5.83618689\n",
      "Trained batch 9329 batch loss 4.80226231 epoch total loss 5.83607626\n",
      "Trained batch 9330 batch loss 4.44390106 epoch total loss 5.83592701\n",
      "Trained batch 9331 batch loss 6.2404232 epoch total loss 5.8359704\n",
      "Trained batch 9332 batch loss 6.47387838 epoch total loss 5.83603859\n",
      "Trained batch 9333 batch loss 4.70708656 epoch total loss 5.83591795\n",
      "Trained batch 9334 batch loss 4.94554138 epoch total loss 5.83582258\n",
      "Trained batch 9335 batch loss 5.20275164 epoch total loss 5.83575487\n",
      "Trained batch 9336 batch loss 5.39496136 epoch total loss 5.83570719\n",
      "Trained batch 9337 batch loss 5.48819828 epoch total loss 5.83567\n",
      "Trained batch 9338 batch loss 6.40016413 epoch total loss 5.83573055\n",
      "Trained batch 9339 batch loss 6.30486965 epoch total loss 5.83578062\n",
      "Trained batch 9340 batch loss 6.58032 epoch total loss 5.83586073\n",
      "Trained batch 9341 batch loss 6.34309292 epoch total loss 5.83591509\n",
      "Trained batch 9342 batch loss 6.17176247 epoch total loss 5.83595085\n",
      "Trained batch 9343 batch loss 6.67808914 epoch total loss 5.83604097\n",
      "Trained batch 9344 batch loss 6.18283749 epoch total loss 5.83607817\n",
      "Trained batch 9345 batch loss 5.77136421 epoch total loss 5.83607149\n",
      "Trained batch 9346 batch loss 6.34893608 epoch total loss 5.83612585\n",
      "Trained batch 9347 batch loss 5.66865635 epoch total loss 5.83610821\n",
      "Trained batch 9348 batch loss 5.52105904 epoch total loss 5.83607435\n",
      "Trained batch 9349 batch loss 5.88719893 epoch total loss 5.8360796\n",
      "Trained batch 9350 batch loss 5.70240641 epoch total loss 5.83606529\n",
      "Trained batch 9351 batch loss 6.43595028 epoch total loss 5.83612967\n",
      "Trained batch 9352 batch loss 6.27341509 epoch total loss 5.8361764\n",
      "Trained batch 9353 batch loss 5.99771309 epoch total loss 5.83619356\n",
      "Trained batch 9354 batch loss 5.89064455 epoch total loss 5.83619928\n",
      "Trained batch 9355 batch loss 6.12462807 epoch total loss 5.83623028\n",
      "Trained batch 9356 batch loss 5.76302624 epoch total loss 5.83622217\n",
      "Trained batch 9357 batch loss 5.63961124 epoch total loss 5.83620119\n",
      "Trained batch 9358 batch loss 5.58338737 epoch total loss 5.83617401\n",
      "Trained batch 9359 batch loss 5.9803896 epoch total loss 5.83618975\n",
      "Trained batch 9360 batch loss 5.695261 epoch total loss 5.83617449\n",
      "Trained batch 9361 batch loss 5.43928432 epoch total loss 5.83613205\n",
      "Trained batch 9362 batch loss 5.96974277 epoch total loss 5.83614635\n",
      "Trained batch 9363 batch loss 5.60830688 epoch total loss 5.83612204\n",
      "Trained batch 9364 batch loss 5.35583735 epoch total loss 5.83607054\n",
      "Trained batch 9365 batch loss 5.68298054 epoch total loss 5.83605433\n",
      "Trained batch 9366 batch loss 5.85245037 epoch total loss 5.83605576\n",
      "Trained batch 9367 batch loss 6.5811882 epoch total loss 5.83613539\n",
      "Trained batch 9368 batch loss 5.27312183 epoch total loss 5.83607531\n",
      "Trained batch 9369 batch loss 5.77649 epoch total loss 5.83606911\n",
      "Trained batch 9370 batch loss 4.83121967 epoch total loss 5.8359623\n",
      "Trained batch 9371 batch loss 6.13713837 epoch total loss 5.83599424\n",
      "Trained batch 9372 batch loss 6.49161196 epoch total loss 5.83606434\n",
      "Trained batch 9373 batch loss 6.37392855 epoch total loss 5.83612156\n",
      "Trained batch 9374 batch loss 6.32058907 epoch total loss 5.83617353\n",
      "Trained batch 9375 batch loss 5.89508486 epoch total loss 5.83617973\n",
      "Trained batch 9376 batch loss 6.1343956 epoch total loss 5.8362112\n",
      "Trained batch 9377 batch loss 6.2596755 epoch total loss 5.83625603\n",
      "Trained batch 9378 batch loss 6.31413841 epoch total loss 5.83630705\n",
      "Trained batch 9379 batch loss 5.93306637 epoch total loss 5.83631754\n",
      "Trained batch 9380 batch loss 5.82869816 epoch total loss 5.83631659\n",
      "Trained batch 9381 batch loss 5.90160322 epoch total loss 5.83632374\n",
      "Trained batch 9382 batch loss 6.20507383 epoch total loss 5.83636236\n",
      "Trained batch 9383 batch loss 5.86010504 epoch total loss 5.83636522\n",
      "Trained batch 9384 batch loss 5.50330496 epoch total loss 5.83632946\n",
      "Trained batch 9385 batch loss 5.97006702 epoch total loss 5.83634377\n",
      "Trained batch 9386 batch loss 6.21170616 epoch total loss 5.83638382\n",
      "Trained batch 9387 batch loss 6.23304272 epoch total loss 5.83642626\n",
      "Trained batch 9388 batch loss 5.02155352 epoch total loss 5.83633947\n",
      "Trained batch 9389 batch loss 5.97212362 epoch total loss 5.83635378\n",
      "Trained batch 9390 batch loss 5.21449614 epoch total loss 5.83628798\n",
      "Trained batch 9391 batch loss 5.57275677 epoch total loss 5.83626\n",
      "Trained batch 9392 batch loss 6.18334675 epoch total loss 5.83629704\n",
      "Trained batch 9393 batch loss 5.19361448 epoch total loss 5.83622837\n",
      "Trained batch 9394 batch loss 3.84994602 epoch total loss 5.83601713\n",
      "Trained batch 9395 batch loss 5.4601388 epoch total loss 5.83597755\n",
      "Trained batch 9396 batch loss 5.39937 epoch total loss 5.83593082\n",
      "Trained batch 9397 batch loss 5.67678499 epoch total loss 5.83591366\n",
      "Trained batch 9398 batch loss 5.26944542 epoch total loss 5.83585358\n",
      "Trained batch 9399 batch loss 4.83905506 epoch total loss 5.83574772\n",
      "Trained batch 9400 batch loss 4.3369112 epoch total loss 5.83558798\n",
      "Trained batch 9401 batch loss 5.48708344 epoch total loss 5.83555126\n",
      "Trained batch 9402 batch loss 4.84066963 epoch total loss 5.8354454\n",
      "Trained batch 9403 batch loss 5.52532768 epoch total loss 5.83541203\n",
      "Trained batch 9404 batch loss 5.38294888 epoch total loss 5.83536386\n",
      "Trained batch 9405 batch loss 5.67522717 epoch total loss 5.8353467\n",
      "Trained batch 9406 batch loss 6.01912403 epoch total loss 5.83536625\n",
      "Trained batch 9407 batch loss 5.68361855 epoch total loss 5.83535051\n",
      "Trained batch 9408 batch loss 5.87264252 epoch total loss 5.83535433\n",
      "Trained batch 9409 batch loss 5.57267952 epoch total loss 5.83532619\n",
      "Trained batch 9410 batch loss 5.32410145 epoch total loss 5.83527184\n",
      "Trained batch 9411 batch loss 5.15390873 epoch total loss 5.83519936\n",
      "Trained batch 9412 batch loss 5.49302 epoch total loss 5.83516312\n",
      "Trained batch 9413 batch loss 5.6889782 epoch total loss 5.83514738\n",
      "Trained batch 9414 batch loss 5.5144453 epoch total loss 5.83511353\n",
      "Trained batch 9415 batch loss 5.52780151 epoch total loss 5.83508062\n",
      "Trained batch 9416 batch loss 5.89881611 epoch total loss 5.8350873\n",
      "Trained batch 9417 batch loss 5.34812975 epoch total loss 5.8350358\n",
      "Trained batch 9418 batch loss 5.35051346 epoch total loss 5.8349843\n",
      "Trained batch 9419 batch loss 5.81598568 epoch total loss 5.8349824\n",
      "Trained batch 9420 batch loss 5.58821964 epoch total loss 5.83495617\n",
      "Trained batch 9421 batch loss 5.72947264 epoch total loss 5.8349452\n",
      "Trained batch 9422 batch loss 5.7196312 epoch total loss 5.8349328\n",
      "Trained batch 9423 batch loss 5.55446911 epoch total loss 5.83490324\n",
      "Trained batch 9424 batch loss 5.95668077 epoch total loss 5.83491611\n",
      "Trained batch 9425 batch loss 4.67549515 epoch total loss 5.83479309\n",
      "Trained batch 9426 batch loss 6.18732452 epoch total loss 5.83483076\n",
      "Trained batch 9427 batch loss 6.36628914 epoch total loss 5.83488703\n",
      "Trained batch 9428 batch loss 5.76608181 epoch total loss 5.83488\n",
      "Trained batch 9429 batch loss 4.95951796 epoch total loss 5.83478689\n",
      "Trained batch 9430 batch loss 5.16672468 epoch total loss 5.83471632\n",
      "Trained batch 9431 batch loss 4.931077 epoch total loss 5.83462048\n",
      "Trained batch 9432 batch loss 5.41477966 epoch total loss 5.83457565\n",
      "Trained batch 9433 batch loss 4.68386364 epoch total loss 5.83445358\n",
      "Trained batch 9434 batch loss 6.33233309 epoch total loss 5.83450651\n",
      "Trained batch 9435 batch loss 5.33186626 epoch total loss 5.83445311\n",
      "Trained batch 9436 batch loss 5.94519472 epoch total loss 5.83446503\n",
      "Trained batch 9437 batch loss 6.72921658 epoch total loss 5.83456\n",
      "Trained batch 9438 batch loss 6.68287 epoch total loss 5.83465\n",
      "Trained batch 9439 batch loss 6.30569267 epoch total loss 5.83469963\n",
      "Trained batch 9440 batch loss 5.87563419 epoch total loss 5.83470392\n",
      "Trained batch 9441 batch loss 6.21489716 epoch total loss 5.83474445\n",
      "Trained batch 9442 batch loss 6.5828476 epoch total loss 5.83482361\n",
      "Trained batch 9443 batch loss 5.58192158 epoch total loss 5.83479643\n",
      "Trained batch 9444 batch loss 6.94899273 epoch total loss 5.83491468\n",
      "Trained batch 9445 batch loss 6.05933809 epoch total loss 5.83493853\n",
      "Trained batch 9446 batch loss 5.87973642 epoch total loss 5.83494282\n",
      "Trained batch 9447 batch loss 2.16790533 epoch total loss 5.83455467\n",
      "Trained batch 9448 batch loss 5.82193184 epoch total loss 5.83455324\n",
      "Trained batch 9449 batch loss 5.63565683 epoch total loss 5.83453226\n",
      "Trained batch 9450 batch loss 5.9277277 epoch total loss 5.8345418\n",
      "Trained batch 9451 batch loss 5.55125427 epoch total loss 5.83451176\n",
      "Trained batch 9452 batch loss 5.43832207 epoch total loss 5.83447\n",
      "Trained batch 9453 batch loss 5.64847279 epoch total loss 5.83445024\n",
      "Trained batch 9454 batch loss 5.4888792 epoch total loss 5.83441353\n",
      "Trained batch 9455 batch loss 6.15522099 epoch total loss 5.83444786\n",
      "Trained batch 9456 batch loss 6.41121674 epoch total loss 5.83450842\n",
      "Trained batch 9457 batch loss 5.98554039 epoch total loss 5.83452463\n",
      "Trained batch 9458 batch loss 5.86688519 epoch total loss 5.83452797\n",
      "Trained batch 9459 batch loss 5.93751812 epoch total loss 5.83453894\n",
      "Trained batch 9460 batch loss 6.53733349 epoch total loss 5.83461332\n",
      "Trained batch 9461 batch loss 6.05571175 epoch total loss 5.83463669\n",
      "Trained batch 9462 batch loss 6.37433672 epoch total loss 5.83469343\n",
      "Trained batch 9463 batch loss 5.0442028 epoch total loss 5.83461\n",
      "Trained batch 9464 batch loss 6.43757629 epoch total loss 5.83467388\n",
      "Trained batch 9465 batch loss 6.36815596 epoch total loss 5.83473\n",
      "Trained batch 9466 batch loss 5.6415081 epoch total loss 5.83470964\n",
      "Trained batch 9467 batch loss 5.60644913 epoch total loss 5.83468533\n",
      "Trained batch 9468 batch loss 5.9637785 epoch total loss 5.83469915\n",
      "Trained batch 9469 batch loss 6.14198875 epoch total loss 5.8347311\n",
      "Trained batch 9470 batch loss 5.78726292 epoch total loss 5.83472633\n",
      "Trained batch 9471 batch loss 5.9077239 epoch total loss 5.83473396\n",
      "Trained batch 9472 batch loss 6.10044098 epoch total loss 5.8347621\n",
      "Trained batch 9473 batch loss 6.43498898 epoch total loss 5.83482552\n",
      "Trained batch 9474 batch loss 6.29712 epoch total loss 5.83487415\n",
      "Trained batch 9475 batch loss 6.25517893 epoch total loss 5.8349185\n",
      "Trained batch 9476 batch loss 6.16112375 epoch total loss 5.83495283\n",
      "Trained batch 9477 batch loss 6.04668522 epoch total loss 5.83497524\n",
      "Trained batch 9478 batch loss 6.69624233 epoch total loss 5.83506584\n",
      "Trained batch 9479 batch loss 6.33461 epoch total loss 5.83511877\n",
      "Trained batch 9480 batch loss 5.96246433 epoch total loss 5.83513212\n",
      "Trained batch 9481 batch loss 6.73559284 epoch total loss 5.83522701\n",
      "Trained batch 9482 batch loss 6.99835873 epoch total loss 5.83534956\n",
      "Trained batch 9483 batch loss 7.10547543 epoch total loss 5.83548355\n",
      "Trained batch 9484 batch loss 6.72321367 epoch total loss 5.83557701\n",
      "Trained batch 9485 batch loss 6.75505066 epoch total loss 5.83567381\n",
      "Trained batch 9486 batch loss 6.66428947 epoch total loss 5.83576107\n",
      "Trained batch 9487 batch loss 6.34189749 epoch total loss 5.83581495\n",
      "Trained batch 9488 batch loss 6.64415932 epoch total loss 5.8359\n",
      "Trained batch 9489 batch loss 4.72621918 epoch total loss 5.835783\n",
      "Trained batch 9490 batch loss 6.00229597 epoch total loss 5.83580065\n",
      "Trained batch 9491 batch loss 6.56737518 epoch total loss 5.8358779\n",
      "Trained batch 9492 batch loss 6.51186085 epoch total loss 5.83594894\n",
      "Trained batch 9493 batch loss 6.84389687 epoch total loss 5.83605528\n",
      "Trained batch 9494 batch loss 6.49812937 epoch total loss 5.8361249\n",
      "Trained batch 9495 batch loss 6.19237947 epoch total loss 5.83616257\n",
      "Trained batch 9496 batch loss 6.05122 epoch total loss 5.83618498\n",
      "Trained batch 9497 batch loss 5.84297 epoch total loss 5.83618593\n",
      "Trained batch 9498 batch loss 5.99305916 epoch total loss 5.83620214\n",
      "Trained batch 9499 batch loss 5.68540716 epoch total loss 5.83618641\n",
      "Trained batch 9500 batch loss 3.98085308 epoch total loss 5.83599091\n",
      "Trained batch 9501 batch loss 6.42185974 epoch total loss 5.83605242\n",
      "Trained batch 9502 batch loss 5.7480793 epoch total loss 5.83604336\n",
      "Trained batch 9503 batch loss 4.75690937 epoch total loss 5.83593\n",
      "Trained batch 9504 batch loss 4.88153028 epoch total loss 5.83582973\n",
      "Trained batch 9505 batch loss 3.8637619 epoch total loss 5.83562231\n",
      "Trained batch 9506 batch loss 4.96421289 epoch total loss 5.83553076\n",
      "Trained batch 9507 batch loss 5.36697674 epoch total loss 5.83548164\n",
      "Trained batch 9508 batch loss 4.97818661 epoch total loss 5.83539104\n",
      "Trained batch 9509 batch loss 4.90816879 epoch total loss 5.83529329\n",
      "Trained batch 9510 batch loss 5.17151451 epoch total loss 5.83522367\n",
      "Trained batch 9511 batch loss 4.35069752 epoch total loss 5.83506775\n",
      "Trained batch 9512 batch loss 4.13150787 epoch total loss 5.83488846\n",
      "Trained batch 9513 batch loss 3.64988542 epoch total loss 5.83465862\n",
      "Trained batch 9514 batch loss 5.3206048 epoch total loss 5.83460474\n",
      "Trained batch 9515 batch loss 6.76629925 epoch total loss 5.83470249\n",
      "Trained batch 9516 batch loss 6.66700459 epoch total loss 5.83479\n",
      "Trained batch 9517 batch loss 5.21029758 epoch total loss 5.83472443\n",
      "Trained batch 9518 batch loss 6.23949051 epoch total loss 5.83476686\n",
      "Trained batch 9519 batch loss 6.12403679 epoch total loss 5.83479738\n",
      "Trained batch 9520 batch loss 6.63853312 epoch total loss 5.83488178\n",
      "Trained batch 9521 batch loss 6.36862278 epoch total loss 5.83493757\n",
      "Trained batch 9522 batch loss 6.280581 epoch total loss 5.8349843\n",
      "Trained batch 9523 batch loss 6.45192528 epoch total loss 5.83504963\n",
      "Trained batch 9524 batch loss 6.42469501 epoch total loss 5.83511162\n",
      "Trained batch 9525 batch loss 6.36136627 epoch total loss 5.83516693\n",
      "Trained batch 9526 batch loss 6.40174198 epoch total loss 5.83522654\n",
      "Trained batch 9527 batch loss 6.47969151 epoch total loss 5.83529425\n",
      "Trained batch 9528 batch loss 6.47245646 epoch total loss 5.835361\n",
      "Trained batch 9529 batch loss 6.96554565 epoch total loss 5.83547974\n",
      "Trained batch 9530 batch loss 5.9183 epoch total loss 5.83548832\n",
      "Trained batch 9531 batch loss 6.75221205 epoch total loss 5.83558464\n",
      "Trained batch 9532 batch loss 5.41137028 epoch total loss 5.83554\n",
      "Trained batch 9533 batch loss 6.07690334 epoch total loss 5.83556557\n",
      "Trained batch 9534 batch loss 6.95799112 epoch total loss 5.83568287\n",
      "Trained batch 9535 batch loss 6.65164661 epoch total loss 5.8357687\n",
      "Trained batch 9536 batch loss 5.55750561 epoch total loss 5.83573961\n",
      "Trained batch 9537 batch loss 6.07169056 epoch total loss 5.83576441\n",
      "Trained batch 9538 batch loss 5.60822296 epoch total loss 5.83574057\n",
      "Trained batch 9539 batch loss 5.58158779 epoch total loss 5.83571386\n",
      "Trained batch 9540 batch loss 5.64681673 epoch total loss 5.83569431\n",
      "Trained batch 9541 batch loss 5.55278492 epoch total loss 5.83566475\n",
      "Trained batch 9542 batch loss 6.07635307 epoch total loss 5.83569\n",
      "Trained batch 9543 batch loss 6.23204327 epoch total loss 5.83573151\n",
      "Trained batch 9544 batch loss 5.80573797 epoch total loss 5.83572817\n",
      "Trained batch 9545 batch loss 5.83665943 epoch total loss 5.83572817\n",
      "Trained batch 9546 batch loss 5.87732744 epoch total loss 5.83573294\n",
      "Trained batch 9547 batch loss 6.18777847 epoch total loss 5.83576965\n",
      "Trained batch 9548 batch loss 6.13166475 epoch total loss 5.83580065\n",
      "Trained batch 9549 batch loss 5.49946547 epoch total loss 5.83576584\n",
      "Trained batch 9550 batch loss 5.55156279 epoch total loss 5.8357358\n",
      "Trained batch 9551 batch loss 5.92107201 epoch total loss 5.83574486\n",
      "Trained batch 9552 batch loss 5.61699581 epoch total loss 5.83572197\n",
      "Trained batch 9553 batch loss 5.82839203 epoch total loss 5.83572102\n",
      "Trained batch 9554 batch loss 5.61916542 epoch total loss 5.8356986\n",
      "Trained batch 9555 batch loss 5.83850813 epoch total loss 5.83569908\n",
      "Trained batch 9556 batch loss 6.13550568 epoch total loss 5.83573055\n",
      "Trained batch 9557 batch loss 5.56090641 epoch total loss 5.83570194\n",
      "Trained batch 9558 batch loss 5.70363808 epoch total loss 5.83568811\n",
      "Trained batch 9559 batch loss 5.65525627 epoch total loss 5.83566952\n",
      "Trained batch 9560 batch loss 6.10920811 epoch total loss 5.83569813\n",
      "Trained batch 9561 batch loss 5.7459712 epoch total loss 5.83568859\n",
      "Trained batch 9562 batch loss 6.86942959 epoch total loss 5.83579683\n",
      "Trained batch 9563 batch loss 6.57438421 epoch total loss 5.83587408\n",
      "Trained batch 9564 batch loss 6.93193817 epoch total loss 5.835989\n",
      "Trained batch 9565 batch loss 6.09211826 epoch total loss 5.8360157\n",
      "Trained batch 9566 batch loss 5.62245 epoch total loss 5.83599329\n",
      "Trained batch 9567 batch loss 6.17551422 epoch total loss 5.83602905\n",
      "Trained batch 9568 batch loss 6.12014103 epoch total loss 5.83605862\n",
      "Trained batch 9569 batch loss 5.83848429 epoch total loss 5.83605909\n",
      "Trained batch 9570 batch loss 5.62417793 epoch total loss 5.83603716\n",
      "Trained batch 9571 batch loss 6.90282726 epoch total loss 5.83614874\n",
      "Trained batch 9572 batch loss 6.54513264 epoch total loss 5.83622265\n",
      "Trained batch 9573 batch loss 6.76812267 epoch total loss 5.8363204\n",
      "Trained batch 9574 batch loss 7.10423 epoch total loss 5.83645296\n",
      "Trained batch 9575 batch loss 6.7251997 epoch total loss 5.83654594\n",
      "Trained batch 9576 batch loss 6.37008762 epoch total loss 5.83660173\n",
      "Trained batch 9577 batch loss 6.64941883 epoch total loss 5.83668613\n",
      "Trained batch 9578 batch loss 5.80562449 epoch total loss 5.8366828\n",
      "Trained batch 9579 batch loss 6.54159069 epoch total loss 5.83675671\n",
      "Trained batch 9580 batch loss 6.86669636 epoch total loss 5.83686447\n",
      "Trained batch 9581 batch loss 6.0645771 epoch total loss 5.83688831\n",
      "Trained batch 9582 batch loss 6.18883276 epoch total loss 5.83692503\n",
      "Trained batch 9583 batch loss 5.92106056 epoch total loss 5.83693361\n",
      "Trained batch 9584 batch loss 5.952847 epoch total loss 5.83694601\n",
      "Trained batch 9585 batch loss 6.14000034 epoch total loss 5.83697748\n",
      "Trained batch 9586 batch loss 6.44664145 epoch total loss 5.8370409\n",
      "Trained batch 9587 batch loss 6.12741423 epoch total loss 5.83707142\n",
      "Trained batch 9588 batch loss 6.60785437 epoch total loss 5.837152\n",
      "Trained batch 9589 batch loss 6.46557 epoch total loss 5.83721733\n",
      "Trained batch 9590 batch loss 6.03457689 epoch total loss 5.83723831\n",
      "Trained batch 9591 batch loss 5.76407337 epoch total loss 5.83723068\n",
      "Trained batch 9592 batch loss 5.73814964 epoch total loss 5.83722\n",
      "Trained batch 9593 batch loss 4.60501766 epoch total loss 5.83709192\n",
      "Trained batch 9594 batch loss 5.33089542 epoch total loss 5.83703947\n",
      "Trained batch 9595 batch loss 5.00500393 epoch total loss 5.83695221\n",
      "Trained batch 9596 batch loss 4.55195045 epoch total loss 5.83681822\n",
      "Trained batch 9597 batch loss 4.63578796 epoch total loss 5.83669329\n",
      "Trained batch 9598 batch loss 5.2396 epoch total loss 5.83663082\n",
      "Trained batch 9599 batch loss 6.11739254 epoch total loss 5.83666039\n",
      "Trained batch 9600 batch loss 6.55450726 epoch total loss 5.83673525\n",
      "Trained batch 9601 batch loss 6.64639473 epoch total loss 5.83681917\n",
      "Trained batch 9602 batch loss 6.00165701 epoch total loss 5.83683634\n",
      "Trained batch 9603 batch loss 6.07978058 epoch total loss 5.83686113\n",
      "Trained batch 9604 batch loss 5.66739 epoch total loss 5.83684349\n",
      "Trained batch 9605 batch loss 5.46695948 epoch total loss 5.83680534\n",
      "Trained batch 9606 batch loss 5.58539104 epoch total loss 5.83677912\n",
      "Trained batch 9607 batch loss 4.91650534 epoch total loss 5.83668375\n",
      "Trained batch 9608 batch loss 5.80613756 epoch total loss 5.83668041\n",
      "Trained batch 9609 batch loss 5.20731354 epoch total loss 5.83661461\n",
      "Trained batch 9610 batch loss 6.39615631 epoch total loss 5.83667278\n",
      "Trained batch 9611 batch loss 5.67445087 epoch total loss 5.83665609\n",
      "Trained batch 9612 batch loss 6.01221371 epoch total loss 5.83667421\n",
      "Trained batch 9613 batch loss 6.30908203 epoch total loss 5.83672333\n",
      "Trained batch 9614 batch loss 6.27126455 epoch total loss 5.83676863\n",
      "Trained batch 9615 batch loss 6.27766037 epoch total loss 5.8368144\n",
      "Trained batch 9616 batch loss 6.05482483 epoch total loss 5.83683681\n",
      "Trained batch 9617 batch loss 6.43228626 epoch total loss 5.8368988\n",
      "Trained batch 9618 batch loss 6.41276 epoch total loss 5.83695889\n",
      "Trained batch 9619 batch loss 5.15682936 epoch total loss 5.83688831\n",
      "Trained batch 9620 batch loss 5.32222271 epoch total loss 5.83683443\n",
      "Trained batch 9621 batch loss 5.34282398 epoch total loss 5.83678341\n",
      "Trained batch 9622 batch loss 5.02359629 epoch total loss 5.83669853\n",
      "Trained batch 9623 batch loss 7.48523712 epoch total loss 5.83686972\n",
      "Trained batch 9624 batch loss 6.48921824 epoch total loss 5.83693743\n",
      "Trained batch 9625 batch loss 6.25371933 epoch total loss 5.83698082\n",
      "Trained batch 9626 batch loss 5.86222839 epoch total loss 5.83698368\n",
      "Trained batch 9627 batch loss 5.79622 epoch total loss 5.83697939\n",
      "Trained batch 9628 batch loss 5.78382587 epoch total loss 5.83697414\n",
      "Trained batch 9629 batch loss 5.85444641 epoch total loss 5.83697605\n",
      "Trained batch 9630 batch loss 5.65315437 epoch total loss 5.83695698\n",
      "Trained batch 9631 batch loss 4.95085716 epoch total loss 5.83686447\n",
      "Trained batch 9632 batch loss 5.43838215 epoch total loss 5.83682299\n",
      "Trained batch 9633 batch loss 5.5109973 epoch total loss 5.83678961\n",
      "Trained batch 9634 batch loss 4.98113346 epoch total loss 5.83670044\n",
      "Trained batch 9635 batch loss 6.17358446 epoch total loss 5.83673525\n",
      "Trained batch 9636 batch loss 6.75396585 epoch total loss 5.83683062\n",
      "Trained batch 9637 batch loss 6.65341187 epoch total loss 5.83691502\n",
      "Trained batch 9638 batch loss 6.72251749 epoch total loss 5.83700705\n",
      "Trained batch 9639 batch loss 6.30486536 epoch total loss 5.83705568\n",
      "Trained batch 9640 batch loss 6.06937695 epoch total loss 5.83708\n",
      "Trained batch 9641 batch loss 6.5878706 epoch total loss 5.83715725\n",
      "Trained batch 9642 batch loss 6.38802624 epoch total loss 5.83721447\n",
      "Trained batch 9643 batch loss 6.63628387 epoch total loss 5.83729744\n",
      "Trained batch 9644 batch loss 6.48330212 epoch total loss 5.83736467\n",
      "Trained batch 9645 batch loss 4.6512804 epoch total loss 5.83724165\n",
      "Trained batch 9646 batch loss 6.02280855 epoch total loss 5.83726072\n",
      "Trained batch 9647 batch loss 4.62367058 epoch total loss 5.83713531\n",
      "Trained batch 9648 batch loss 4.93360758 epoch total loss 5.83704138\n",
      "Trained batch 9649 batch loss 5.26412439 epoch total loss 5.83698225\n",
      "Trained batch 9650 batch loss 6.23222637 epoch total loss 5.83702326\n",
      "Trained batch 9651 batch loss 4.74222 epoch total loss 5.83691\n",
      "Trained batch 9652 batch loss 6.10713339 epoch total loss 5.83693743\n",
      "Trained batch 9653 batch loss 5.42035866 epoch total loss 5.83689451\n",
      "Trained batch 9654 batch loss 4.75162792 epoch total loss 5.83678198\n",
      "Trained batch 9655 batch loss 5.07642174 epoch total loss 5.8367033\n",
      "Trained batch 9656 batch loss 6.40780687 epoch total loss 5.83676243\n",
      "Trained batch 9657 batch loss 5.973948 epoch total loss 5.83677626\n",
      "Trained batch 9658 batch loss 6.71522713 epoch total loss 5.83686733\n",
      "Trained batch 9659 batch loss 5.86813879 epoch total loss 5.83687067\n",
      "Trained batch 9660 batch loss 5.86923409 epoch total loss 5.83687401\n",
      "Trained batch 9661 batch loss 6.72061968 epoch total loss 5.83696508\n",
      "Trained batch 9662 batch loss 6.36989164 epoch total loss 5.8370204\n",
      "Trained batch 9663 batch loss 6.25948524 epoch total loss 5.83706427\n",
      "Trained batch 9664 batch loss 6.69788074 epoch total loss 5.83715343\n",
      "Trained batch 9665 batch loss 6.38221931 epoch total loss 5.8372097\n",
      "Trained batch 9666 batch loss 6.73699856 epoch total loss 5.83730316\n",
      "Trained batch 9667 batch loss 6.30636883 epoch total loss 5.83735132\n",
      "Trained batch 9668 batch loss 5.39335108 epoch total loss 5.83730555\n",
      "Trained batch 9669 batch loss 5.27273273 epoch total loss 5.83724737\n",
      "Trained batch 9670 batch loss 5.85720205 epoch total loss 5.83724928\n",
      "Trained batch 9671 batch loss 6.1580987 epoch total loss 5.83728218\n",
      "Trained batch 9672 batch loss 6.19568396 epoch total loss 5.83731937\n",
      "Trained batch 9673 batch loss 6.39367628 epoch total loss 5.83737659\n",
      "Trained batch 9674 batch loss 5.79405832 epoch total loss 5.8373723\n",
      "Trained batch 9675 batch loss 5.34676647 epoch total loss 5.83732176\n",
      "Trained batch 9676 batch loss 5.42419481 epoch total loss 5.83727884\n",
      "Trained batch 9677 batch loss 5.13917065 epoch total loss 5.83720684\n",
      "Trained batch 9678 batch loss 5.1979537 epoch total loss 5.83714104\n",
      "Trained batch 9679 batch loss 6.29500484 epoch total loss 5.83718872\n",
      "Trained batch 9680 batch loss 5.32549572 epoch total loss 5.83713579\n",
      "Trained batch 9681 batch loss 5.7014246 epoch total loss 5.83712196\n",
      "Trained batch 9682 batch loss 5.94451761 epoch total loss 5.83713293\n",
      "Trained batch 9683 batch loss 5.91541672 epoch total loss 5.83714104\n",
      "Trained batch 9684 batch loss 5.7767539 epoch total loss 5.83713484\n",
      "Trained batch 9685 batch loss 6.29974127 epoch total loss 5.83718252\n",
      "Trained batch 9686 batch loss 6.34187794 epoch total loss 5.83723497\n",
      "Trained batch 9687 batch loss 5.70205927 epoch total loss 5.83722115\n",
      "Trained batch 9688 batch loss 6.49256039 epoch total loss 5.83728886\n",
      "Trained batch 9689 batch loss 5.81425381 epoch total loss 5.837286\n",
      "Trained batch 9690 batch loss 5.60761356 epoch total loss 5.83726263\n",
      "Trained batch 9691 batch loss 5.71664715 epoch total loss 5.83724976\n",
      "Trained batch 9692 batch loss 6.18349934 epoch total loss 5.83728552\n",
      "Trained batch 9693 batch loss 5.79442215 epoch total loss 5.83728123\n",
      "Trained batch 9694 batch loss 5.89579964 epoch total loss 5.83728695\n",
      "Trained batch 9695 batch loss 5.97020245 epoch total loss 5.83730078\n",
      "Trained batch 9696 batch loss 5.36356068 epoch total loss 5.83725166\n",
      "Trained batch 9697 batch loss 5.14504623 epoch total loss 5.83718\n",
      "Trained batch 9698 batch loss 5.90046215 epoch total loss 5.83718681\n",
      "Trained batch 9699 batch loss 5.75554752 epoch total loss 5.83717823\n",
      "Trained batch 9700 batch loss 5.729918 epoch total loss 5.83716726\n",
      "Trained batch 9701 batch loss 5.52863264 epoch total loss 5.83713531\n",
      "Trained batch 9702 batch loss 5.5052042 epoch total loss 5.83710098\n",
      "Trained batch 9703 batch loss 5.49902344 epoch total loss 5.83706617\n",
      "Trained batch 9704 batch loss 5.23853779 epoch total loss 5.83700466\n",
      "Trained batch 9705 batch loss 5.49719715 epoch total loss 5.83696938\n",
      "Trained batch 9706 batch loss 5.07114506 epoch total loss 5.8368907\n",
      "Trained batch 9707 batch loss 5.55752468 epoch total loss 5.83686209\n",
      "Trained batch 9708 batch loss 4.76605415 epoch total loss 5.83675146\n",
      "Trained batch 9709 batch loss 5.00056076 epoch total loss 5.83666515\n",
      "Trained batch 9710 batch loss 5.51713 epoch total loss 5.83663225\n",
      "Trained batch 9711 batch loss 5.94316864 epoch total loss 5.83664322\n",
      "Trained batch 9712 batch loss 6.06068516 epoch total loss 5.83666611\n",
      "Trained batch 9713 batch loss 5.42640352 epoch total loss 5.83662415\n",
      "Trained batch 9714 batch loss 4.99272966 epoch total loss 5.83653688\n",
      "Trained batch 9715 batch loss 5.69651079 epoch total loss 5.83652258\n",
      "Trained batch 9716 batch loss 5.74859858 epoch total loss 5.83651352\n",
      "Trained batch 9717 batch loss 5.05853796 epoch total loss 5.83643341\n",
      "Trained batch 9718 batch loss 5.15701199 epoch total loss 5.83636379\n",
      "Trained batch 9719 batch loss 6.02770662 epoch total loss 5.83638334\n",
      "Trained batch 9720 batch loss 5.89683819 epoch total loss 5.83638954\n",
      "Trained batch 9721 batch loss 5.69033766 epoch total loss 5.83637476\n",
      "Trained batch 9722 batch loss 5.33940506 epoch total loss 5.83632374\n",
      "Trained batch 9723 batch loss 5.95203781 epoch total loss 5.83633566\n",
      "Trained batch 9724 batch loss 4.73080063 epoch total loss 5.83622169\n",
      "Trained batch 9725 batch loss 5.71280432 epoch total loss 5.83620882\n",
      "Trained batch 9726 batch loss 5.57526398 epoch total loss 5.83618212\n",
      "Trained batch 9727 batch loss 6.31044912 epoch total loss 5.83623075\n",
      "Trained batch 9728 batch loss 6.44847727 epoch total loss 5.8362937\n",
      "Trained batch 9729 batch loss 5.75362349 epoch total loss 5.83628511\n",
      "Trained batch 9730 batch loss 6.21871948 epoch total loss 5.83632469\n",
      "Trained batch 9731 batch loss 5.90631104 epoch total loss 5.83633184\n",
      "Trained batch 9732 batch loss 5.84789944 epoch total loss 5.8363328\n",
      "Trained batch 9733 batch loss 6.22948217 epoch total loss 5.83637333\n",
      "Trained batch 9734 batch loss 6.01915932 epoch total loss 5.8363924\n",
      "Trained batch 9735 batch loss 6.26075935 epoch total loss 5.83643579\n",
      "Trained batch 9736 batch loss 5.7110672 epoch total loss 5.83642292\n",
      "Trained batch 9737 batch loss 5.7978158 epoch total loss 5.83641911\n",
      "Trained batch 9738 batch loss 5.06024361 epoch total loss 5.836339\n",
      "Trained batch 9739 batch loss 5.95231533 epoch total loss 5.83635092\n",
      "Trained batch 9740 batch loss 5.88257551 epoch total loss 5.83635569\n",
      "Trained batch 9741 batch loss 5.6667366 epoch total loss 5.83633852\n",
      "Trained batch 9742 batch loss 5.7877512 epoch total loss 5.83633375\n",
      "Trained batch 9743 batch loss 5.73341131 epoch total loss 5.83632326\n",
      "Trained batch 9744 batch loss 5.73680592 epoch total loss 5.83631325\n",
      "Trained batch 9745 batch loss 5.65859795 epoch total loss 5.83629513\n",
      "Trained batch 9746 batch loss 5.89196301 epoch total loss 5.83630085\n",
      "Trained batch 9747 batch loss 5.39862823 epoch total loss 5.83625555\n",
      "Trained batch 9748 batch loss 6.05692101 epoch total loss 5.83627844\n",
      "Trained batch 9749 batch loss 6.46592903 epoch total loss 5.83634281\n",
      "Trained batch 9750 batch loss 5.9554081 epoch total loss 5.83635521\n",
      "Trained batch 9751 batch loss 5.31027031 epoch total loss 5.83630133\n",
      "Trained batch 9752 batch loss 5.78793097 epoch total loss 5.83629656\n",
      "Trained batch 9753 batch loss 5.39573908 epoch total loss 5.83625126\n",
      "Trained batch 9754 batch loss 5.71823311 epoch total loss 5.83623886\n",
      "Trained batch 9755 batch loss 6.36388493 epoch total loss 5.83629322\n",
      "Trained batch 9756 batch loss 5.96396589 epoch total loss 5.8363061\n",
      "Trained batch 9757 batch loss 5.49512768 epoch total loss 5.83627129\n",
      "Trained batch 9758 batch loss 5.91574 epoch total loss 5.83627939\n",
      "Trained batch 9759 batch loss 5.86939526 epoch total loss 5.83628273\n",
      "Trained batch 9760 batch loss 5.58651304 epoch total loss 5.83625746\n",
      "Trained batch 9761 batch loss 6.03770065 epoch total loss 5.83627796\n",
      "Trained batch 9762 batch loss 5.33702707 epoch total loss 5.83622694\n",
      "Trained batch 9763 batch loss 5.91499233 epoch total loss 5.83623457\n",
      "Trained batch 9764 batch loss 5.53124714 epoch total loss 5.83620358\n",
      "Trained batch 9765 batch loss 5.36792564 epoch total loss 5.83615541\n",
      "Trained batch 9766 batch loss 5.49361134 epoch total loss 5.83612\n",
      "Trained batch 9767 batch loss 5.38219881 epoch total loss 5.83607388\n",
      "Trained batch 9768 batch loss 5.1877861 epoch total loss 5.8360076\n",
      "Trained batch 9769 batch loss 5.2366786 epoch total loss 5.83594608\n",
      "Trained batch 9770 batch loss 5.64173126 epoch total loss 5.83592653\n",
      "Trained batch 9771 batch loss 5.64760971 epoch total loss 5.83590698\n",
      "Trained batch 9772 batch loss 5.11989498 epoch total loss 5.83583403\n",
      "Trained batch 9773 batch loss 4.95494699 epoch total loss 5.83574343\n",
      "Trained batch 9774 batch loss 5.99222374 epoch total loss 5.83575964\n",
      "Trained batch 9775 batch loss 5.50036144 epoch total loss 5.83572531\n",
      "Trained batch 9776 batch loss 5.86425114 epoch total loss 5.83572817\n",
      "Trained batch 9777 batch loss 6.19597769 epoch total loss 5.83576488\n",
      "Trained batch 9778 batch loss 6.06976604 epoch total loss 5.83578873\n",
      "Trained batch 9779 batch loss 6.23990822 epoch total loss 5.83583\n",
      "Trained batch 9780 batch loss 5.89069462 epoch total loss 5.83583546\n",
      "Trained batch 9781 batch loss 6.12013865 epoch total loss 5.83586502\n",
      "Trained batch 9782 batch loss 6.17743969 epoch total loss 5.83589935\n",
      "Trained batch 9783 batch loss 6.01079226 epoch total loss 5.83591747\n",
      "Trained batch 9784 batch loss 4.82735538 epoch total loss 5.83581448\n",
      "Trained batch 9785 batch loss 6.05706596 epoch total loss 5.83583736\n",
      "Trained batch 9786 batch loss 5.76202679 epoch total loss 5.83582973\n",
      "Trained batch 9787 batch loss 5.93376064 epoch total loss 5.83583975\n",
      "Trained batch 9788 batch loss 6.5561676 epoch total loss 5.83591318\n",
      "Trained batch 9789 batch loss 6.64601278 epoch total loss 5.83599567\n",
      "Trained batch 9790 batch loss 6.21119308 epoch total loss 5.83603382\n",
      "Trained batch 9791 batch loss 5.43416739 epoch total loss 5.83599281\n",
      "Trained batch 9792 batch loss 5.69557 epoch total loss 5.83597851\n",
      "Trained batch 9793 batch loss 5.85088921 epoch total loss 5.83598042\n",
      "Trained batch 9794 batch loss 6.18817902 epoch total loss 5.83601618\n",
      "Trained batch 9795 batch loss 6.45534134 epoch total loss 5.8360796\n",
      "Trained batch 9796 batch loss 6.37470627 epoch total loss 5.83613443\n",
      "Trained batch 9797 batch loss 6.14456129 epoch total loss 5.8361659\n",
      "Trained batch 9798 batch loss 6.29609871 epoch total loss 5.83621311\n",
      "Trained batch 9799 batch loss 5.23009 epoch total loss 5.83615112\n",
      "Trained batch 9800 batch loss 5.88866138 epoch total loss 5.83615637\n",
      "Trained batch 9801 batch loss 5.32911968 epoch total loss 5.83610439\n",
      "Trained batch 9802 batch loss 4.47158957 epoch total loss 5.83596563\n",
      "Trained batch 9803 batch loss 5.20871639 epoch total loss 5.83590126\n",
      "Trained batch 9804 batch loss 5.63253927 epoch total loss 5.83588076\n",
      "Trained batch 9805 batch loss 5.79063034 epoch total loss 5.83587551\n",
      "Trained batch 9806 batch loss 6.41049385 epoch total loss 5.83593416\n",
      "Trained batch 9807 batch loss 5.7467947 epoch total loss 5.8359251\n",
      "Trained batch 9808 batch loss 6.22618866 epoch total loss 5.83596516\n",
      "Trained batch 9809 batch loss 6.46331692 epoch total loss 5.83602905\n",
      "Trained batch 9810 batch loss 5.5505681 epoch total loss 5.836\n",
      "Trained batch 9811 batch loss 5.24672937 epoch total loss 5.83594\n",
      "Trained batch 9812 batch loss 6.30235386 epoch total loss 5.83598709\n",
      "Trained batch 9813 batch loss 6.21647406 epoch total loss 5.83602571\n",
      "Trained batch 9814 batch loss 5.67496729 epoch total loss 5.8360095\n",
      "Trained batch 9815 batch loss 6.45756531 epoch total loss 5.83607292\n",
      "Trained batch 9816 batch loss 6.9116044 epoch total loss 5.83618212\n",
      "Trained batch 9817 batch loss 5.91948891 epoch total loss 5.8361907\n",
      "Trained batch 9818 batch loss 6.00327206 epoch total loss 5.83620787\n",
      "Trained batch 9819 batch loss 5.87934589 epoch total loss 5.83621216\n",
      "Trained batch 9820 batch loss 5.71595573 epoch total loss 5.83619976\n",
      "Trained batch 9821 batch loss 5.62640667 epoch total loss 5.8361783\n",
      "Trained batch 9822 batch loss 5.67693186 epoch total loss 5.83616161\n",
      "Trained batch 9823 batch loss 5.48134756 epoch total loss 5.83612537\n",
      "Trained batch 9824 batch loss 7.36508369 epoch total loss 5.83628082\n",
      "Trained batch 9825 batch loss 7.1535368 epoch total loss 5.83641481\n",
      "Trained batch 9826 batch loss 5.83791447 epoch total loss 5.83641529\n",
      "Trained batch 9827 batch loss 5.84532261 epoch total loss 5.83641624\n",
      "Trained batch 9828 batch loss 5.26624107 epoch total loss 5.83635807\n",
      "Trained batch 9829 batch loss 5.40067673 epoch total loss 5.83631372\n",
      "Trained batch 9830 batch loss 5.63016701 epoch total loss 5.83629274\n",
      "Trained batch 9831 batch loss 4.50427341 epoch total loss 5.83615732\n",
      "Trained batch 9832 batch loss 5.43953228 epoch total loss 5.83611727\n",
      "Trained batch 9833 batch loss 4.52755642 epoch total loss 5.83598375\n",
      "Trained batch 9834 batch loss 5.67497921 epoch total loss 5.83596754\n",
      "Trained batch 9835 batch loss 5.89388466 epoch total loss 5.83597374\n",
      "Trained batch 9836 batch loss 6.66495705 epoch total loss 5.83605766\n",
      "Trained batch 9837 batch loss 6.02531147 epoch total loss 5.83607674\n",
      "Trained batch 9838 batch loss 6.3420887 epoch total loss 5.83612871\n",
      "Trained batch 9839 batch loss 6.78142405 epoch total loss 5.83622456\n",
      "Trained batch 9840 batch loss 5.91786337 epoch total loss 5.83623266\n",
      "Trained batch 9841 batch loss 5.74689579 epoch total loss 5.8362236\n",
      "Trained batch 9842 batch loss 6.06054878 epoch total loss 5.83624649\n",
      "Trained batch 9843 batch loss 6.0654707 epoch total loss 5.83627\n",
      "Trained batch 9844 batch loss 5.87234211 epoch total loss 5.83627367\n",
      "Trained batch 9845 batch loss 5.59670353 epoch total loss 5.83624935\n",
      "Trained batch 9846 batch loss 5.41248083 epoch total loss 5.83620644\n",
      "Trained batch 9847 batch loss 6.18725586 epoch total loss 5.8362422\n",
      "Trained batch 9848 batch loss 5.73084211 epoch total loss 5.83623123\n",
      "Trained batch 9849 batch loss 5.57360077 epoch total loss 5.83620501\n",
      "Trained batch 9850 batch loss 5.96595573 epoch total loss 5.83621788\n",
      "Trained batch 9851 batch loss 4.26002645 epoch total loss 5.83605814\n",
      "Trained batch 9852 batch loss 4.86555386 epoch total loss 5.83596\n",
      "Trained batch 9853 batch loss 6.30104446 epoch total loss 5.83600664\n",
      "Trained batch 9854 batch loss 5.68433189 epoch total loss 5.83599138\n",
      "Trained batch 9855 batch loss 6.79248333 epoch total loss 5.83608866\n",
      "Trained batch 9856 batch loss 6.02856064 epoch total loss 5.83610773\n",
      "Trained batch 9857 batch loss 6.40054226 epoch total loss 5.83616543\n",
      "Trained batch 9858 batch loss 5.29337788 epoch total loss 5.83611\n",
      "Trained batch 9859 batch loss 6.03394032 epoch total loss 5.83613062\n",
      "Trained batch 9860 batch loss 5.57423306 epoch total loss 5.83610392\n",
      "Trained batch 9861 batch loss 5.57283592 epoch total loss 5.83607721\n",
      "Trained batch 9862 batch loss 5.74848604 epoch total loss 5.83606863\n",
      "Trained batch 9863 batch loss 6.14115524 epoch total loss 5.83609962\n",
      "Trained batch 9864 batch loss 6.65967178 epoch total loss 5.83618307\n",
      "Trained batch 9865 batch loss 6.55880928 epoch total loss 5.83625603\n",
      "Trained batch 9866 batch loss 6.11564827 epoch total loss 5.83628464\n",
      "Trained batch 9867 batch loss 6.90066385 epoch total loss 5.83639288\n",
      "Trained batch 9868 batch loss 6.84692574 epoch total loss 5.8364954\n",
      "Trained batch 9869 batch loss 5.71181059 epoch total loss 5.83648252\n",
      "Trained batch 9870 batch loss 6.43550825 epoch total loss 5.83654308\n",
      "Trained batch 9871 batch loss 6.15389824 epoch total loss 5.83657503\n",
      "Trained batch 9872 batch loss 6.56722832 epoch total loss 5.83664894\n",
      "Trained batch 9873 batch loss 6.53085423 epoch total loss 5.83671951\n",
      "Trained batch 9874 batch loss 4.46871376 epoch total loss 5.83658075\n",
      "Trained batch 9875 batch loss 4.32824945 epoch total loss 5.83642817\n",
      "Trained batch 9876 batch loss 5.47872353 epoch total loss 5.83639193\n",
      "Trained batch 9877 batch loss 4.90513515 epoch total loss 5.83629799\n",
      "Trained batch 9878 batch loss 5.7212286 epoch total loss 5.83628607\n",
      "Trained batch 9879 batch loss 5.53681374 epoch total loss 5.83625603\n",
      "Trained batch 9880 batch loss 4.62549448 epoch total loss 5.836133\n",
      "Trained batch 9881 batch loss 6.17763186 epoch total loss 5.83616781\n",
      "Trained batch 9882 batch loss 5.56452227 epoch total loss 5.83614\n",
      "Trained batch 9883 batch loss 6.90614414 epoch total loss 5.8362484\n",
      "Trained batch 9884 batch loss 6.77633238 epoch total loss 5.83634377\n",
      "Trained batch 9885 batch loss 4.62131739 epoch total loss 5.83622074\n",
      "Trained batch 9886 batch loss 6.59152269 epoch total loss 5.83629704\n",
      "Trained batch 9887 batch loss 6.53730631 epoch total loss 5.83636808\n",
      "Trained batch 9888 batch loss 6.68734 epoch total loss 5.83645439\n",
      "Trained batch 9889 batch loss 4.89414406 epoch total loss 5.83635902\n",
      "Trained batch 9890 batch loss 5.78193569 epoch total loss 5.8363533\n",
      "Trained batch 9891 batch loss 6.11583233 epoch total loss 5.83638191\n",
      "Trained batch 9892 batch loss 4.76055622 epoch total loss 5.83627319\n",
      "Trained batch 9893 batch loss 5.15258217 epoch total loss 5.83620405\n",
      "Trained batch 9894 batch loss 5.70083809 epoch total loss 5.83619\n",
      "Trained batch 9895 batch loss 5.36677122 epoch total loss 5.83614302\n",
      "Trained batch 9896 batch loss 5.84494686 epoch total loss 5.83614349\n",
      "Trained batch 9897 batch loss 5.8446269 epoch total loss 5.83614445\n",
      "Trained batch 9898 batch loss 5.1116991 epoch total loss 5.83607149\n",
      "Trained batch 9899 batch loss 5.85058641 epoch total loss 5.83607292\n",
      "Trained batch 9900 batch loss 6.62457514 epoch total loss 5.83615255\n",
      "Trained batch 9901 batch loss 6.35748196 epoch total loss 5.83620548\n",
      "Trained batch 9902 batch loss 5.32775879 epoch total loss 5.83615398\n",
      "Trained batch 9903 batch loss 5.4078207 epoch total loss 5.83611059\n",
      "Trained batch 9904 batch loss 7.29606581 epoch total loss 5.83625793\n",
      "Trained batch 9905 batch loss 7.40311813 epoch total loss 5.83641624\n",
      "Trained batch 9906 batch loss 6.0661912 epoch total loss 5.83643961\n",
      "Trained batch 9907 batch loss 6.42228508 epoch total loss 5.83649874\n",
      "Trained batch 9908 batch loss 6.24919319 epoch total loss 5.83654\n",
      "Trained batch 9909 batch loss 6.45563316 epoch total loss 5.83660316\n",
      "Trained batch 9910 batch loss 5.96289921 epoch total loss 5.83661604\n",
      "Trained batch 9911 batch loss 6.06283569 epoch total loss 5.83663845\n",
      "Trained batch 9912 batch loss 6.87141514 epoch total loss 5.83674288\n",
      "Trained batch 9913 batch loss 5.82221413 epoch total loss 5.83674145\n",
      "Trained batch 9914 batch loss 6.45411777 epoch total loss 5.83680344\n",
      "Trained batch 9915 batch loss 6.7826786 epoch total loss 5.8368988\n",
      "Trained batch 9916 batch loss 5.94047165 epoch total loss 5.83690929\n",
      "Trained batch 9917 batch loss 5.92815447 epoch total loss 5.83691883\n",
      "Trained batch 9918 batch loss 6.08638 epoch total loss 5.83694363\n",
      "Trained batch 9919 batch loss 5.44113684 epoch total loss 5.83690405\n",
      "Trained batch 9920 batch loss 4.73197508 epoch total loss 5.83679247\n",
      "Trained batch 9921 batch loss 5.77751637 epoch total loss 5.83678627\n",
      "Trained batch 9922 batch loss 6.10799122 epoch total loss 5.83681393\n",
      "Trained batch 9923 batch loss 5.48825264 epoch total loss 5.83677864\n",
      "Trained batch 9924 batch loss 5.30243301 epoch total loss 5.83672476\n",
      "Trained batch 9925 batch loss 5.7283268 epoch total loss 5.83671379\n",
      "Trained batch 9926 batch loss 3.86597586 epoch total loss 5.83651543\n",
      "Trained batch 9927 batch loss 6.44553041 epoch total loss 5.83657646\n",
      "Trained batch 9928 batch loss 6.02171516 epoch total loss 5.83659554\n",
      "Trained batch 9929 batch loss 5.92333031 epoch total loss 5.83660412\n",
      "Trained batch 9930 batch loss 5.72219658 epoch total loss 5.83659267\n",
      "Trained batch 9931 batch loss 5.88091373 epoch total loss 5.83659697\n",
      "Trained batch 9932 batch loss 5.19219875 epoch total loss 5.83653212\n",
      "Trained batch 9933 batch loss 6.03152752 epoch total loss 5.83655167\n",
      "Trained batch 9934 batch loss 6.76914883 epoch total loss 5.8366456\n",
      "Trained batch 9935 batch loss 5.3478241 epoch total loss 5.83659649\n",
      "Trained batch 9936 batch loss 4.98606586 epoch total loss 5.83651066\n",
      "Trained batch 9937 batch loss 5.8335495 epoch total loss 5.83651\n",
      "Trained batch 9938 batch loss 5.77280378 epoch total loss 5.83650398\n",
      "Trained batch 9939 batch loss 6.68096638 epoch total loss 5.83658886\n",
      "Trained batch 9940 batch loss 7.61371231 epoch total loss 5.83676767\n",
      "Trained batch 9941 batch loss 4.4114151 epoch total loss 5.83662415\n",
      "Trained batch 9942 batch loss 5.4649229 epoch total loss 5.83658648\n",
      "Trained batch 9943 batch loss 5.76759291 epoch total loss 5.83658\n",
      "Trained batch 9944 batch loss 5.247365 epoch total loss 5.83652067\n",
      "Trained batch 9945 batch loss 5.29902315 epoch total loss 5.83646679\n",
      "Trained batch 9946 batch loss 5.93555641 epoch total loss 5.8364768\n",
      "Trained batch 9947 batch loss 4.94574928 epoch total loss 5.83638716\n",
      "Trained batch 9948 batch loss 5.13325691 epoch total loss 5.83631659\n",
      "Trained batch 9949 batch loss 5.72825336 epoch total loss 5.83630562\n",
      "Trained batch 9950 batch loss 5.02535105 epoch total loss 5.8362236\n",
      "Trained batch 9951 batch loss 4.72756672 epoch total loss 5.83611202\n",
      "Trained batch 9952 batch loss 4.71231556 epoch total loss 5.83599901\n",
      "Trained batch 9953 batch loss 4.63487625 epoch total loss 5.83587885\n",
      "Trained batch 9954 batch loss 6.46144295 epoch total loss 5.83594131\n",
      "Trained batch 9955 batch loss 5.42302275 epoch total loss 5.8359\n",
      "Trained batch 9956 batch loss 6.00964212 epoch total loss 5.835917\n",
      "Trained batch 9957 batch loss 5.27318859 epoch total loss 5.83586073\n",
      "Trained batch 9958 batch loss 4.90925598 epoch total loss 5.83576775\n",
      "Trained batch 9959 batch loss 4.67111444 epoch total loss 5.83565092\n",
      "Trained batch 9960 batch loss 6.03820848 epoch total loss 5.83567142\n",
      "Trained batch 9961 batch loss 6.18018866 epoch total loss 5.83570576\n",
      "Trained batch 9962 batch loss 5.79899549 epoch total loss 5.83570242\n",
      "Trained batch 9963 batch loss 5.50112104 epoch total loss 5.83566856\n",
      "Trained batch 9964 batch loss 5.82260847 epoch total loss 5.83566761\n",
      "Trained batch 9965 batch loss 5.78240871 epoch total loss 5.83566189\n",
      "Trained batch 9966 batch loss 6.47766685 epoch total loss 5.83572626\n",
      "Trained batch 9967 batch loss 4.58961058 epoch total loss 5.83560133\n",
      "Trained batch 9968 batch loss 5.7592268 epoch total loss 5.83559322\n",
      "Trained batch 9969 batch loss 6.50834227 epoch total loss 5.83566093\n",
      "Trained batch 9970 batch loss 5.76623249 epoch total loss 5.83565378\n",
      "Trained batch 9971 batch loss 5.30573177 epoch total loss 5.83560038\n",
      "Trained batch 9972 batch loss 5.73492146 epoch total loss 5.83559036\n",
      "Trained batch 9973 batch loss 5.9710207 epoch total loss 5.83560419\n",
      "Trained batch 9974 batch loss 5.85338545 epoch total loss 5.83560562\n",
      "Trained batch 9975 batch loss 5.8536787 epoch total loss 5.83560801\n",
      "Trained batch 9976 batch loss 5.96590328 epoch total loss 5.83562088\n",
      "Trained batch 9977 batch loss 5.74739075 epoch total loss 5.83561182\n",
      "Trained batch 9978 batch loss 5.9241457 epoch total loss 5.83562088\n",
      "Trained batch 9979 batch loss 5.83089447 epoch total loss 5.8356204\n",
      "Trained batch 9980 batch loss 5.11715031 epoch total loss 5.8355484\n",
      "Trained batch 9981 batch loss 5.96038151 epoch total loss 5.8355608\n",
      "Trained batch 9982 batch loss 5.66149807 epoch total loss 5.83554363\n",
      "Trained batch 9983 batch loss 5.17400074 epoch total loss 5.83547735\n",
      "Trained batch 9984 batch loss 6.17312622 epoch total loss 5.83551121\n",
      "Trained batch 9985 batch loss 6.6513772 epoch total loss 5.83559275\n",
      "Trained batch 9986 batch loss 5.62682915 epoch total loss 5.83557177\n",
      "Trained batch 9987 batch loss 5.13789749 epoch total loss 5.83550167\n",
      "Trained batch 9988 batch loss 5.75201178 epoch total loss 5.83549356\n",
      "Trained batch 9989 batch loss 6.32424164 epoch total loss 5.83554268\n",
      "Trained batch 9990 batch loss 4.39591312 epoch total loss 5.8353982\n",
      "Trained batch 9991 batch loss 5.92834187 epoch total loss 5.83540773\n",
      "Trained batch 9992 batch loss 5.51255608 epoch total loss 5.83537531\n",
      "Trained batch 9993 batch loss 6.27664185 epoch total loss 5.83541965\n",
      "Trained batch 9994 batch loss 5.67401028 epoch total loss 5.83540344\n",
      "Trained batch 9995 batch loss 6.04725266 epoch total loss 5.8354249\n",
      "Trained batch 9996 batch loss 6.5007205 epoch total loss 5.83549118\n",
      "Trained batch 9997 batch loss 6.12328529 epoch total loss 5.83552027\n",
      "Trained batch 9998 batch loss 6.09712315 epoch total loss 5.83554649\n",
      "Trained batch 9999 batch loss 6.09885216 epoch total loss 5.83557272\n",
      "Trained batch 10000 batch loss 5.76986265 epoch total loss 5.83556604\n",
      "Trained batch 10001 batch loss 5.51566744 epoch total loss 5.8355341\n",
      "Trained batch 10002 batch loss 5.69377899 epoch total loss 5.83552\n",
      "Trained batch 10003 batch loss 6.42454243 epoch total loss 5.83557892\n",
      "Trained batch 10004 batch loss 6.06578064 epoch total loss 5.83560228\n",
      "Trained batch 10005 batch loss 5.9996953 epoch total loss 5.8356185\n",
      "Trained batch 10006 batch loss 5.72595596 epoch total loss 5.83560753\n",
      "Trained batch 10007 batch loss 6.04469919 epoch total loss 5.83562851\n",
      "Trained batch 10008 batch loss 5.19096184 epoch total loss 5.83556414\n",
      "Trained batch 10009 batch loss 5.88985348 epoch total loss 5.83556938\n",
      "Trained batch 10010 batch loss 5.29594421 epoch total loss 5.8355155\n",
      "Trained batch 10011 batch loss 4.77201462 epoch total loss 5.83540964\n",
      "Trained batch 10012 batch loss 5.23951197 epoch total loss 5.83535\n",
      "Trained batch 10013 batch loss 4.60948896 epoch total loss 5.83522749\n",
      "Trained batch 10014 batch loss 4.9705596 epoch total loss 5.83514118\n",
      "Trained batch 10015 batch loss 4.95585346 epoch total loss 5.83505344\n",
      "Trained batch 10016 batch loss 5.93149185 epoch total loss 5.8350625\n",
      "Trained batch 10017 batch loss 5.85054779 epoch total loss 5.83506441\n",
      "Trained batch 10018 batch loss 5.34281874 epoch total loss 5.8350153\n",
      "Trained batch 10019 batch loss 5.36946774 epoch total loss 5.83496904\n",
      "Trained batch 10020 batch loss 5.62017488 epoch total loss 5.83494759\n",
      "Trained batch 10021 batch loss 5.84684372 epoch total loss 5.83494902\n",
      "Trained batch 10022 batch loss 5.67836 epoch total loss 5.83493328\n",
      "Trained batch 10023 batch loss 5.56904411 epoch total loss 5.83490705\n",
      "Trained batch 10024 batch loss 5.75573063 epoch total loss 5.83489895\n",
      "Trained batch 10025 batch loss 5.31987953 epoch total loss 5.83484745\n",
      "Trained batch 10026 batch loss 6.02447891 epoch total loss 5.83486652\n",
      "Trained batch 10027 batch loss 5.73877621 epoch total loss 5.83485699\n",
      "Trained batch 10028 batch loss 6.32725143 epoch total loss 5.8349061\n",
      "Trained batch 10029 batch loss 5.71956348 epoch total loss 5.83489466\n",
      "Trained batch 10030 batch loss 6.41396427 epoch total loss 5.83495235\n",
      "Trained batch 10031 batch loss 7.02991867 epoch total loss 5.83507156\n",
      "Trained batch 10032 batch loss 5.9005475 epoch total loss 5.83507824\n",
      "Trained batch 10033 batch loss 5.3253274 epoch total loss 5.83502722\n",
      "Trained batch 10034 batch loss 6.8683219 epoch total loss 5.83513\n",
      "Trained batch 10035 batch loss 6.24953079 epoch total loss 5.83517122\n",
      "Trained batch 10036 batch loss 5.57670259 epoch total loss 5.83514595\n",
      "Trained batch 10037 batch loss 4.52753401 epoch total loss 5.8350153\n",
      "Trained batch 10038 batch loss 5.61789608 epoch total loss 5.83499384\n",
      "Trained batch 10039 batch loss 5.43342495 epoch total loss 5.83495378\n",
      "Trained batch 10040 batch loss 6.13953733 epoch total loss 5.8349843\n",
      "Trained batch 10041 batch loss 5.82127094 epoch total loss 5.83498287\n",
      "Trained batch 10042 batch loss 6.11089945 epoch total loss 5.83501\n",
      "Trained batch 10043 batch loss 5.78361416 epoch total loss 5.83500528\n",
      "Trained batch 10044 batch loss 5.84198141 epoch total loss 5.83500624\n",
      "Trained batch 10045 batch loss 6.39846897 epoch total loss 5.83506203\n",
      "Trained batch 10046 batch loss 5.10888958 epoch total loss 5.83499\n",
      "Trained batch 10047 batch loss 6.72755623 epoch total loss 5.83507872\n",
      "Trained batch 10048 batch loss 6.45889759 epoch total loss 5.83514071\n",
      "Trained batch 10049 batch loss 6.26425457 epoch total loss 5.83518362\n",
      "Trained batch 10050 batch loss 6.06485415 epoch total loss 5.83520651\n",
      "Trained batch 10051 batch loss 5.63509178 epoch total loss 5.83518648\n",
      "Trained batch 10052 batch loss 6.07150793 epoch total loss 5.83521\n",
      "Trained batch 10053 batch loss 6.62635422 epoch total loss 5.83528852\n",
      "Trained batch 10054 batch loss 5.66080093 epoch total loss 5.83527136\n",
      "Trained batch 10055 batch loss 5.42915916 epoch total loss 5.83523083\n",
      "Trained batch 10056 batch loss 5.62000704 epoch total loss 5.83520937\n",
      "Trained batch 10057 batch loss 5.61107445 epoch total loss 5.83518696\n",
      "Trained batch 10058 batch loss 5.76527 epoch total loss 5.83518028\n",
      "Trained batch 10059 batch loss 5.62207222 epoch total loss 5.83515882\n",
      "Trained batch 10060 batch loss 5.63170481 epoch total loss 5.8351388\n",
      "Trained batch 10061 batch loss 5.10017729 epoch total loss 5.83506584\n",
      "Trained batch 10062 batch loss 5.14457083 epoch total loss 5.83499718\n",
      "Trained batch 10063 batch loss 4.71453381 epoch total loss 5.83488607\n",
      "Trained batch 10064 batch loss 5.31384754 epoch total loss 5.8348341\n",
      "Trained batch 10065 batch loss 4.87863064 epoch total loss 5.83473921\n",
      "Trained batch 10066 batch loss 4.75423241 epoch total loss 5.83463144\n",
      "Trained batch 10067 batch loss 4.32102966 epoch total loss 5.83448124\n",
      "Trained batch 10068 batch loss 4.84219551 epoch total loss 5.83438301\n",
      "Trained batch 10069 batch loss 5.0618453 epoch total loss 5.83430624\n",
      "Trained batch 10070 batch loss 5.45547342 epoch total loss 5.83426857\n",
      "Trained batch 10071 batch loss 6.9575305 epoch total loss 5.83438\n",
      "Trained batch 10072 batch loss 5.56747866 epoch total loss 5.83435345\n",
      "Trained batch 10073 batch loss 6.60432911 epoch total loss 5.83443\n",
      "Trained batch 10074 batch loss 5.03987026 epoch total loss 5.83435106\n",
      "Trained batch 10075 batch loss 5.5431757 epoch total loss 5.83432245\n",
      "Trained batch 10076 batch loss 5.82917213 epoch total loss 5.8343215\n",
      "Trained batch 10077 batch loss 6.406847 epoch total loss 5.83437824\n",
      "Trained batch 10078 batch loss 6.73533869 epoch total loss 5.83446789\n",
      "Trained batch 10079 batch loss 6.13321877 epoch total loss 5.83449745\n",
      "Trained batch 10080 batch loss 6.47057915 epoch total loss 5.83456039\n",
      "Trained batch 10081 batch loss 5.96647787 epoch total loss 5.83457327\n",
      "Trained batch 10082 batch loss 5.90638542 epoch total loss 5.83458042\n",
      "Trained batch 10083 batch loss 5.45398331 epoch total loss 5.83454227\n",
      "Trained batch 10084 batch loss 5.64497137 epoch total loss 5.83452368\n",
      "Trained batch 10085 batch loss 6.00334644 epoch total loss 5.83454037\n",
      "Trained batch 10086 batch loss 6.24657822 epoch total loss 5.83458138\n",
      "Trained batch 10087 batch loss 5.41115618 epoch total loss 5.83453894\n",
      "Trained batch 10088 batch loss 5.89417934 epoch total loss 5.83454514\n",
      "Trained batch 10089 batch loss 6.34320641 epoch total loss 5.83459568\n",
      "Trained batch 10090 batch loss 5.99947691 epoch total loss 5.83461189\n",
      "Trained batch 10091 batch loss 4.83315277 epoch total loss 5.83451271\n",
      "Trained batch 10092 batch loss 5.38022804 epoch total loss 5.83446741\n",
      "Trained batch 10093 batch loss 5.97376108 epoch total loss 5.83448124\n",
      "Trained batch 10094 batch loss 5.8488574 epoch total loss 5.83448219\n",
      "Trained batch 10095 batch loss 5.89041 epoch total loss 5.83448792\n",
      "Trained batch 10096 batch loss 4.5547843 epoch total loss 5.83436108\n",
      "Trained batch 10097 batch loss 5.64146423 epoch total loss 5.834342\n",
      "Trained batch 10098 batch loss 5.54773426 epoch total loss 5.83431339\n",
      "Trained batch 10099 batch loss 5.77627563 epoch total loss 5.83430815\n",
      "Trained batch 10100 batch loss 5.51410961 epoch total loss 5.8342762\n",
      "Trained batch 10101 batch loss 5.31440735 epoch total loss 5.8342247\n",
      "Trained batch 10102 batch loss 5.90455627 epoch total loss 5.83423185\n",
      "Trained batch 10103 batch loss 5.7393074 epoch total loss 5.83422232\n",
      "Trained batch 10104 batch loss 4.9175663 epoch total loss 5.83413172\n",
      "Trained batch 10105 batch loss 5.67651272 epoch total loss 5.83411598\n",
      "Trained batch 10106 batch loss 5.50477934 epoch total loss 5.83408356\n",
      "Trained batch 10107 batch loss 5.96987152 epoch total loss 5.83409643\n",
      "Trained batch 10108 batch loss 4.75402164 epoch total loss 5.83398962\n",
      "Trained batch 10109 batch loss 5.64294052 epoch total loss 5.83397102\n",
      "Trained batch 10110 batch loss 6.07831717 epoch total loss 5.83399534\n",
      "Trained batch 10111 batch loss 5.83816338 epoch total loss 5.83399582\n",
      "Trained batch 10112 batch loss 6.08912373 epoch total loss 5.83402109\n",
      "Trained batch 10113 batch loss 5.25422192 epoch total loss 5.83396387\n",
      "Trained batch 10114 batch loss 6.06338501 epoch total loss 5.83398628\n",
      "Trained batch 10115 batch loss 5.01531792 epoch total loss 5.83390522\n",
      "Trained batch 10116 batch loss 5.13061619 epoch total loss 5.8338356\n",
      "Trained batch 10117 batch loss 5.51307297 epoch total loss 5.83380365\n",
      "Trained batch 10118 batch loss 5.73015308 epoch total loss 5.83379364\n",
      "Trained batch 10119 batch loss 5.92876911 epoch total loss 5.83380318\n",
      "Trained batch 10120 batch loss 6.20075417 epoch total loss 5.83383942\n",
      "Trained batch 10121 batch loss 5.34698296 epoch total loss 5.83379126\n",
      "Trained batch 10122 batch loss 6.050529 epoch total loss 5.83381271\n",
      "Trained batch 10123 batch loss 6.57702255 epoch total loss 5.83388615\n",
      "Trained batch 10124 batch loss 5.82261181 epoch total loss 5.83388519\n",
      "Trained batch 10125 batch loss 6.23082829 epoch total loss 5.83392429\n",
      "Trained batch 10126 batch loss 5.68529844 epoch total loss 5.83390951\n",
      "Trained batch 10127 batch loss 4.89836073 epoch total loss 5.83381701\n",
      "Trained batch 10128 batch loss 4.89387465 epoch total loss 5.8337245\n",
      "Trained batch 10129 batch loss 4.90969276 epoch total loss 5.83363342\n",
      "Trained batch 10130 batch loss 5.69809198 epoch total loss 5.83362\n",
      "Trained batch 10131 batch loss 4.29153538 epoch total loss 5.83346796\n",
      "Trained batch 10132 batch loss 5.10805368 epoch total loss 5.83339643\n",
      "Trained batch 10133 batch loss 6.0037818 epoch total loss 5.83341312\n",
      "Trained batch 10134 batch loss 5.23250294 epoch total loss 5.833354\n",
      "Trained batch 10135 batch loss 5.11556435 epoch total loss 5.83328342\n",
      "Trained batch 10136 batch loss 5.47390938 epoch total loss 5.83324814\n",
      "Trained batch 10137 batch loss 5.97017193 epoch total loss 5.83326149\n",
      "Trained batch 10138 batch loss 5.93878508 epoch total loss 5.8332715\n",
      "Trained batch 10139 batch loss 6.02645826 epoch total loss 5.83329058\n",
      "Trained batch 10140 batch loss 5.55537701 epoch total loss 5.8332634\n",
      "Trained batch 10141 batch loss 6.12506199 epoch total loss 5.83329201\n",
      "Trained batch 10142 batch loss 5.52295113 epoch total loss 5.83326149\n",
      "Trained batch 10143 batch loss 5.9716568 epoch total loss 5.83327532\n",
      "Trained batch 10144 batch loss 5.31734133 epoch total loss 5.8332243\n",
      "Trained batch 10145 batch loss 5.46210289 epoch total loss 5.83318758\n",
      "Trained batch 10146 batch loss 6.03920364 epoch total loss 5.83320761\n",
      "Trained batch 10147 batch loss 6.27631378 epoch total loss 5.83325148\n",
      "Trained batch 10148 batch loss 5.49048185 epoch total loss 5.8332181\n",
      "Trained batch 10149 batch loss 5.61236286 epoch total loss 5.83319616\n",
      "Trained batch 10150 batch loss 6.41496897 epoch total loss 5.83325338\n",
      "Trained batch 10151 batch loss 5.53001881 epoch total loss 5.83322382\n",
      "Trained batch 10152 batch loss 5.87634373 epoch total loss 5.83322811\n",
      "Trained batch 10153 batch loss 5.47521973 epoch total loss 5.83319283\n",
      "Trained batch 10154 batch loss 5.97156334 epoch total loss 5.83320665\n",
      "Trained batch 10155 batch loss 6.25372887 epoch total loss 5.83324814\n",
      "Trained batch 10156 batch loss 5.93841648 epoch total loss 5.83325815\n",
      "Trained batch 10157 batch loss 4.31426525 epoch total loss 5.83310843\n",
      "Trained batch 10158 batch loss 5.95262861 epoch total loss 5.83312035\n",
      "Trained batch 10159 batch loss 5.3197546 epoch total loss 5.83307\n",
      "Trained batch 10160 batch loss 6.00367594 epoch total loss 5.83308649\n",
      "Trained batch 10161 batch loss 5.83228588 epoch total loss 5.83308649\n",
      "Trained batch 10162 batch loss 5.96614647 epoch total loss 5.83309937\n",
      "Trained batch 10163 batch loss 5.97733402 epoch total loss 5.83311367\n",
      "Trained batch 10164 batch loss 5.9491663 epoch total loss 5.83312511\n",
      "Trained batch 10165 batch loss 5.55935 epoch total loss 5.83309793\n",
      "Trained batch 10166 batch loss 5.82111788 epoch total loss 5.83309698\n",
      "Trained batch 10167 batch loss 5.58661747 epoch total loss 5.83307266\n",
      "Trained batch 10168 batch loss 5.65707874 epoch total loss 5.83305502\n",
      "Trained batch 10169 batch loss 4.97818661 epoch total loss 5.83297062\n",
      "Trained batch 10170 batch loss 5.19944763 epoch total loss 5.83290863\n",
      "Trained batch 10171 batch loss 5.94377136 epoch total loss 5.8329196\n",
      "Trained batch 10172 batch loss 5.90942669 epoch total loss 5.83292723\n",
      "Trained batch 10173 batch loss 5.72410583 epoch total loss 5.83291626\n",
      "Trained batch 10174 batch loss 6.16681719 epoch total loss 5.83294916\n",
      "Trained batch 10175 batch loss 5.7816515 epoch total loss 5.83294439\n",
      "Trained batch 10176 batch loss 6.12555885 epoch total loss 5.832973\n",
      "Trained batch 10177 batch loss 5.30610275 epoch total loss 5.83292103\n",
      "Trained batch 10178 batch loss 5.77820873 epoch total loss 5.83291531\n",
      "Trained batch 10179 batch loss 5.85756969 epoch total loss 5.83291817\n",
      "Trained batch 10180 batch loss 5.61169338 epoch total loss 5.83289671\n",
      "Trained batch 10181 batch loss 5.49047089 epoch total loss 5.83286285\n",
      "Trained batch 10182 batch loss 5.00327492 epoch total loss 5.83278179\n",
      "Trained batch 10183 batch loss 5.18419 epoch total loss 5.8327179\n",
      "Trained batch 10184 batch loss 4.86266232 epoch total loss 5.83262253\n",
      "Trained batch 10185 batch loss 4.25774193 epoch total loss 5.83246803\n",
      "Trained batch 10186 batch loss 5.96099091 epoch total loss 5.83248091\n",
      "Trained batch 10187 batch loss 6.28060389 epoch total loss 5.83252478\n",
      "Trained batch 10188 batch loss 5.75123882 epoch total loss 5.83251667\n",
      "Trained batch 10189 batch loss 6.25727463 epoch total loss 5.83255863\n",
      "Trained batch 10190 batch loss 5.92036057 epoch total loss 5.83256721\n",
      "Trained batch 10191 batch loss 6.60341263 epoch total loss 5.83264256\n",
      "Trained batch 10192 batch loss 6.51232719 epoch total loss 5.83270931\n",
      "Trained batch 10193 batch loss 6.78714 epoch total loss 5.83280325\n",
      "Trained batch 10194 batch loss 6.24442196 epoch total loss 5.83284378\n",
      "Trained batch 10195 batch loss 7.13426495 epoch total loss 5.8329711\n",
      "Trained batch 10196 batch loss 5.78587151 epoch total loss 5.83296633\n",
      "Trained batch 10197 batch loss 6.39790487 epoch total loss 5.83302212\n",
      "Trained batch 10198 batch loss 6.69005775 epoch total loss 5.83310604\n",
      "Trained batch 10199 batch loss 7.06206083 epoch total loss 5.83322668\n",
      "Trained batch 10200 batch loss 5.89156723 epoch total loss 5.8332324\n",
      "Trained batch 10201 batch loss 6.27903223 epoch total loss 5.83327579\n",
      "Trained batch 10202 batch loss 6.3448143 epoch total loss 5.83332586\n",
      "Trained batch 10203 batch loss 6.06098366 epoch total loss 5.83334827\n",
      "Trained batch 10204 batch loss 5.87495041 epoch total loss 5.83335257\n",
      "Trained batch 10205 batch loss 6.23909092 epoch total loss 5.83339214\n",
      "Trained batch 10206 batch loss 5.86958885 epoch total loss 5.83339596\n",
      "Trained batch 10207 batch loss 6.41695595 epoch total loss 5.83345318\n",
      "Trained batch 10208 batch loss 5.4548521 epoch total loss 5.83341551\n",
      "Trained batch 10209 batch loss 4.96926975 epoch total loss 5.83333111\n",
      "Trained batch 10210 batch loss 5.82087469 epoch total loss 5.83332968\n",
      "Trained batch 10211 batch loss 5.69559765 epoch total loss 5.83331633\n",
      "Trained batch 10212 batch loss 6.90703058 epoch total loss 5.83342123\n",
      "Trained batch 10213 batch loss 6.44273 epoch total loss 5.83348083\n",
      "Trained batch 10214 batch loss 6.77547359 epoch total loss 5.83357334\n",
      "Trained batch 10215 batch loss 6.20721436 epoch total loss 5.83360958\n",
      "Trained batch 10216 batch loss 6.31776094 epoch total loss 5.83365726\n",
      "Trained batch 10217 batch loss 6.41633844 epoch total loss 5.83371449\n",
      "Trained batch 10218 batch loss 6.7316308 epoch total loss 5.83380222\n",
      "Trained batch 10219 batch loss 7.15255594 epoch total loss 5.83393097\n",
      "Trained batch 10220 batch loss 6.61265421 epoch total loss 5.83400726\n",
      "Trained batch 10221 batch loss 7.06963 epoch total loss 5.83412838\n",
      "Trained batch 10222 batch loss 6.42225218 epoch total loss 5.8341856\n",
      "Trained batch 10223 batch loss 6.66726494 epoch total loss 5.83426714\n",
      "Trained batch 10224 batch loss 6.55437374 epoch total loss 5.83433771\n",
      "Trained batch 10225 batch loss 6.79234791 epoch total loss 5.83443165\n",
      "Trained batch 10226 batch loss 6.73161221 epoch total loss 5.83451939\n",
      "Trained batch 10227 batch loss 6.54762602 epoch total loss 5.834589\n",
      "Trained batch 10228 batch loss 6.54447508 epoch total loss 5.83465815\n",
      "Trained batch 10229 batch loss 6.56412458 epoch total loss 5.83472919\n",
      "Trained batch 10230 batch loss 6.12465143 epoch total loss 5.8347578\n",
      "Trained batch 10231 batch loss 5.63689423 epoch total loss 5.83473825\n",
      "Trained batch 10232 batch loss 6.33090496 epoch total loss 5.83478689\n",
      "Trained batch 10233 batch loss 5.9865036 epoch total loss 5.83480167\n",
      "Trained batch 10234 batch loss 6.37588406 epoch total loss 5.8348546\n",
      "Trained batch 10235 batch loss 6.54360723 epoch total loss 5.83492374\n",
      "Trained batch 10236 batch loss 6.23670244 epoch total loss 5.83496332\n",
      "Trained batch 10237 batch loss 6.43198156 epoch total loss 5.8350215\n",
      "Trained batch 10238 batch loss 5.87418747 epoch total loss 5.83502579\n",
      "Trained batch 10239 batch loss 6.1607914 epoch total loss 5.83505726\n",
      "Trained batch 10240 batch loss 6.39131355 epoch total loss 5.83511162\n",
      "Trained batch 10241 batch loss 6.94403267 epoch total loss 5.83522\n",
      "Trained batch 10242 batch loss 6.54804325 epoch total loss 5.83528948\n",
      "Trained batch 10243 batch loss 5.4975419 epoch total loss 5.83525658\n",
      "Trained batch 10244 batch loss 6.27928925 epoch total loss 5.83529949\n",
      "Trained batch 10245 batch loss 6.82870722 epoch total loss 5.83539629\n",
      "Trained batch 10246 batch loss 6.52560377 epoch total loss 5.835464\n",
      "Trained batch 10247 batch loss 5.6196413 epoch total loss 5.83544302\n",
      "Trained batch 10248 batch loss 6.26873207 epoch total loss 5.83548546\n",
      "Trained batch 10249 batch loss 4.57222271 epoch total loss 5.83536196\n",
      "Trained batch 10250 batch loss 5.2924633 epoch total loss 5.83530903\n",
      "Trained batch 10251 batch loss 5.09424305 epoch total loss 5.83523655\n",
      "Trained batch 10252 batch loss 4.65482521 epoch total loss 5.83512163\n",
      "Trained batch 10253 batch loss 5.15970421 epoch total loss 5.83505583\n",
      "Trained batch 10254 batch loss 5.29397202 epoch total loss 5.8350029\n",
      "Trained batch 10255 batch loss 5.49888468 epoch total loss 5.83497047\n",
      "Trained batch 10256 batch loss 6.6044054 epoch total loss 5.83504534\n",
      "Trained batch 10257 batch loss 6.50497437 epoch total loss 5.83511066\n",
      "Trained batch 10258 batch loss 6.33261061 epoch total loss 5.8351593\n",
      "Trained batch 10259 batch loss 5.78436661 epoch total loss 5.83515406\n",
      "Trained batch 10260 batch loss 6.97864628 epoch total loss 5.83526611\n",
      "Trained batch 10261 batch loss 6.43163967 epoch total loss 5.83532381\n",
      "Trained batch 10262 batch loss 6.55136728 epoch total loss 5.83539343\n",
      "Trained batch 10263 batch loss 6.13291645 epoch total loss 5.83542252\n",
      "Trained batch 10264 batch loss 6.41346598 epoch total loss 5.83547878\n",
      "Trained batch 10265 batch loss 6.42071629 epoch total loss 5.835536\n",
      "Trained batch 10266 batch loss 6.36101103 epoch total loss 5.83558702\n",
      "Trained batch 10267 batch loss 6.07246494 epoch total loss 5.83561039\n",
      "Trained batch 10268 batch loss 5.18597507 epoch total loss 5.83554697\n",
      "Trained batch 10269 batch loss 5.0208807 epoch total loss 5.83546782\n",
      "Trained batch 10270 batch loss 5.85557938 epoch total loss 5.83546972\n",
      "Trained batch 10271 batch loss 5.96609 epoch total loss 5.83548212\n",
      "Trained batch 10272 batch loss 6.06297827 epoch total loss 5.83550453\n",
      "Trained batch 10273 batch loss 4.97786188 epoch total loss 5.83542061\n",
      "Trained batch 10274 batch loss 6.41108131 epoch total loss 5.83547688\n",
      "Trained batch 10275 batch loss 6.84479666 epoch total loss 5.83557463\n",
      "Trained batch 10276 batch loss 5.3292141 epoch total loss 5.83552551\n",
      "Trained batch 10277 batch loss 6.2795496 epoch total loss 5.8355689\n",
      "Trained batch 10278 batch loss 5.66298199 epoch total loss 5.83555222\n",
      "Trained batch 10279 batch loss 6.38821697 epoch total loss 5.83560562\n",
      "Trained batch 10280 batch loss 5.67511272 epoch total loss 5.83559036\n",
      "Trained batch 10281 batch loss 5.12957 epoch total loss 5.83552122\n",
      "Trained batch 10282 batch loss 5.36063957 epoch total loss 5.83547497\n",
      "Trained batch 10283 batch loss 5.99707222 epoch total loss 5.8354907\n",
      "Trained batch 10284 batch loss 5.84029675 epoch total loss 5.83549118\n",
      "Trained batch 10285 batch loss 5.39680243 epoch total loss 5.83544874\n",
      "Trained batch 10286 batch loss 6.79177427 epoch total loss 5.83554173\n",
      "Trained batch 10287 batch loss 5.56510925 epoch total loss 5.8355155\n",
      "Trained batch 10288 batch loss 5.90359068 epoch total loss 5.83552217\n",
      "Trained batch 10289 batch loss 5.88561392 epoch total loss 5.83552694\n",
      "Trained batch 10290 batch loss 5.51050854 epoch total loss 5.83549547\n",
      "Trained batch 10291 batch loss 5.80844784 epoch total loss 5.83549309\n",
      "Trained batch 10292 batch loss 6.26594639 epoch total loss 5.83553457\n",
      "Trained batch 10293 batch loss 5.63252831 epoch total loss 5.83551502\n",
      "Trained batch 10294 batch loss 6.01289177 epoch total loss 5.83553219\n",
      "Trained batch 10295 batch loss 5.45812702 epoch total loss 5.83549547\n",
      "Trained batch 10296 batch loss 5.88825607 epoch total loss 5.83550024\n",
      "Trained batch 10297 batch loss 6.12530804 epoch total loss 5.83552837\n",
      "Trained batch 10298 batch loss 6.1049962 epoch total loss 5.8355546\n",
      "Trained batch 10299 batch loss 5.75167704 epoch total loss 5.83554649\n",
      "Trained batch 10300 batch loss 5.74920368 epoch total loss 5.83553839\n",
      "Trained batch 10301 batch loss 6.71806717 epoch total loss 5.83562374\n",
      "Trained batch 10302 batch loss 6.54923105 epoch total loss 5.83569336\n",
      "Trained batch 10303 batch loss 6.12422752 epoch total loss 5.83572149\n",
      "Trained batch 10304 batch loss 5.4608345 epoch total loss 5.83568525\n",
      "Trained batch 10305 batch loss 5.70048523 epoch total loss 5.8356719\n",
      "Trained batch 10306 batch loss 5.53535175 epoch total loss 5.83564281\n",
      "Trained batch 10307 batch loss 6.08888197 epoch total loss 5.83566713\n",
      "Trained batch 10308 batch loss 5.78606558 epoch total loss 5.83566236\n",
      "Trained batch 10309 batch loss 5.07044792 epoch total loss 5.83558798\n",
      "Trained batch 10310 batch loss 5.92590904 epoch total loss 5.83559704\n",
      "Trained batch 10311 batch loss 5.00182152 epoch total loss 5.83551598\n",
      "Trained batch 10312 batch loss 5.46227694 epoch total loss 5.83547974\n",
      "Trained batch 10313 batch loss 6.07101393 epoch total loss 5.83550215\n",
      "Trained batch 10314 batch loss 6.0131135 epoch total loss 5.83551931\n",
      "Trained batch 10315 batch loss 5.52042055 epoch total loss 5.8354888\n",
      "Trained batch 10316 batch loss 6.00433207 epoch total loss 5.83550501\n",
      "Trained batch 10317 batch loss 5.5680418 epoch total loss 5.83547878\n",
      "Trained batch 10318 batch loss 5.76141596 epoch total loss 5.83547163\n",
      "Trained batch 10319 batch loss 5.75842905 epoch total loss 5.83546448\n",
      "Trained batch 10320 batch loss 5.71507359 epoch total loss 5.83545256\n",
      "Trained batch 10321 batch loss 5.71036243 epoch total loss 5.83544064\n",
      "Trained batch 10322 batch loss 5.38781071 epoch total loss 5.83539724\n",
      "Trained batch 10323 batch loss 5.36113548 epoch total loss 5.83535099\n",
      "Trained batch 10324 batch loss 4.5645113 epoch total loss 5.83522797\n",
      "Trained batch 10325 batch loss 4.44929838 epoch total loss 5.83509398\n",
      "Trained batch 10326 batch loss 5.35832787 epoch total loss 5.83504772\n",
      "Trained batch 10327 batch loss 5.84106541 epoch total loss 5.8350482\n",
      "Trained batch 10328 batch loss 5.40191221 epoch total loss 5.83500624\n",
      "Trained batch 10329 batch loss 5.00712299 epoch total loss 5.83492613\n",
      "Trained batch 10330 batch loss 4.88724232 epoch total loss 5.83483458\n",
      "Trained batch 10331 batch loss 4.73476601 epoch total loss 5.83472776\n",
      "Trained batch 10332 batch loss 4.29728031 epoch total loss 5.83457899\n",
      "Trained batch 10333 batch loss 4.36241865 epoch total loss 5.83443689\n",
      "Trained batch 10334 batch loss 5.04369354 epoch total loss 5.83436\n",
      "Trained batch 10335 batch loss 5.45817804 epoch total loss 5.83432341\n",
      "Trained batch 10336 batch loss 5.41761208 epoch total loss 5.83428335\n",
      "Trained batch 10337 batch loss 6.98600769 epoch total loss 5.83439445\n",
      "Trained batch 10338 batch loss 4.01097918 epoch total loss 5.8342185\n",
      "Trained batch 10339 batch loss 4.56736946 epoch total loss 5.83409548\n",
      "Trained batch 10340 batch loss 4.47475719 epoch total loss 5.83396435\n",
      "Trained batch 10341 batch loss 4.56458092 epoch total loss 5.8338418\n",
      "Trained batch 10342 batch loss 5.02104378 epoch total loss 5.83376312\n",
      "Trained batch 10343 batch loss 4.1854825 epoch total loss 5.83360338\n",
      "Trained batch 10344 batch loss 4.19456482 epoch total loss 5.83344507\n",
      "Trained batch 10345 batch loss 5.57555771 epoch total loss 5.83342028\n",
      "Trained batch 10346 batch loss 4.60321808 epoch total loss 5.83330107\n",
      "Trained batch 10347 batch loss 5.39542103 epoch total loss 5.83325863\n",
      "Trained batch 10348 batch loss 5.26807785 epoch total loss 5.83320427\n",
      "Trained batch 10349 batch loss 6.28096247 epoch total loss 5.83324718\n",
      "Trained batch 10350 batch loss 5.38761806 epoch total loss 5.83320427\n",
      "Trained batch 10351 batch loss 5.24646759 epoch total loss 5.83314753\n",
      "Trained batch 10352 batch loss 5.29521608 epoch total loss 5.83309555\n",
      "Trained batch 10353 batch loss 5.33515739 epoch total loss 5.83304787\n",
      "Trained batch 10354 batch loss 5.0644846 epoch total loss 5.83297348\n",
      "Trained batch 10355 batch loss 4.58703709 epoch total loss 5.83285332\n",
      "Trained batch 10356 batch loss 5.25473881 epoch total loss 5.83279753\n",
      "Trained batch 10357 batch loss 4.73087215 epoch total loss 5.83269072\n",
      "Trained batch 10358 batch loss 4.8812604 epoch total loss 5.83259916\n",
      "Trained batch 10359 batch loss 4.54017353 epoch total loss 5.83247423\n",
      "Trained batch 10360 batch loss 4.71407604 epoch total loss 5.83236647\n",
      "Trained batch 10361 batch loss 4.59819317 epoch total loss 5.83224726\n",
      "Trained batch 10362 batch loss 4.16178417 epoch total loss 5.83208609\n",
      "Trained batch 10363 batch loss 4.48883343 epoch total loss 5.83195639\n",
      "Trained batch 10364 batch loss 3.9791882 epoch total loss 5.83177757\n",
      "Trained batch 10365 batch loss 4.76484442 epoch total loss 5.83167458\n",
      "Trained batch 10366 batch loss 4.83137226 epoch total loss 5.83157825\n",
      "Trained batch 10367 batch loss 4.53727865 epoch total loss 5.8314538\n",
      "Trained batch 10368 batch loss 3.9872241 epoch total loss 5.83127594\n",
      "Trained batch 10369 batch loss 4.29832125 epoch total loss 5.83112764\n",
      "Trained batch 10370 batch loss 4.37779427 epoch total loss 5.83098793\n",
      "Trained batch 10371 batch loss 4.00587 epoch total loss 5.83081198\n",
      "Trained batch 10372 batch loss 4.73330641 epoch total loss 5.83070612\n",
      "Trained batch 10373 batch loss 4.47525024 epoch total loss 5.83057594\n",
      "Trained batch 10374 batch loss 4.70598125 epoch total loss 5.8304677\n",
      "Trained batch 10375 batch loss 4.41542816 epoch total loss 5.83033085\n",
      "Trained batch 10376 batch loss 4.24664974 epoch total loss 5.83017826\n",
      "Trained batch 10377 batch loss 4.72398806 epoch total loss 5.83007145\n",
      "Trained batch 10378 batch loss 4.58670425 epoch total loss 5.82995176\n",
      "Trained batch 10379 batch loss 4.40607309 epoch total loss 5.82981443\n",
      "Trained batch 10380 batch loss 4.41284943 epoch total loss 5.82967806\n",
      "Trained batch 10381 batch loss 3.72796154 epoch total loss 5.8294754\n",
      "Trained batch 10382 batch loss 4.43920326 epoch total loss 5.82934141\n",
      "Trained batch 10383 batch loss 4.02438831 epoch total loss 5.82916737\n",
      "Trained batch 10384 batch loss 4.01313496 epoch total loss 5.82899237\n",
      "Trained batch 10385 batch loss 4.59593391 epoch total loss 5.82887411\n",
      "Trained batch 10386 batch loss 4.07142 epoch total loss 5.82870436\n",
      "Trained batch 10387 batch loss 4.19545 epoch total loss 5.82854748\n",
      "Trained batch 10388 batch loss 6.27545738 epoch total loss 5.82859039\n",
      "Trained batch 10389 batch loss 6.70443249 epoch total loss 5.82867479\n",
      "Trained batch 10390 batch loss 6.76475573 epoch total loss 5.82876492\n",
      "Trained batch 10391 batch loss 6.42348909 epoch total loss 5.82882214\n",
      "Trained batch 10392 batch loss 6.15497494 epoch total loss 5.82885361\n",
      "Trained batch 10393 batch loss 6.31596851 epoch total loss 5.82890034\n",
      "Trained batch 10394 batch loss 5.60621834 epoch total loss 5.82887888\n",
      "Trained batch 10395 batch loss 5.78127766 epoch total loss 5.82887411\n",
      "Trained batch 10396 batch loss 6.23350048 epoch total loss 5.82891321\n",
      "Trained batch 10397 batch loss 5.89559746 epoch total loss 5.82891941\n",
      "Trained batch 10398 batch loss 5.60511112 epoch total loss 5.82889795\n",
      "Trained batch 10399 batch loss 5.60326767 epoch total loss 5.8288765\n",
      "Trained batch 10400 batch loss 6.18450928 epoch total loss 5.82891035\n",
      "Trained batch 10401 batch loss 5.74271154 epoch total loss 5.82890224\n",
      "Trained batch 10402 batch loss 5.68994236 epoch total loss 5.82888889\n",
      "Trained batch 10403 batch loss 5.4164505 epoch total loss 5.82884932\n",
      "Trained batch 10404 batch loss 5.40871477 epoch total loss 5.82880926\n",
      "Trained batch 10405 batch loss 5.63281059 epoch total loss 5.82879\n",
      "Trained batch 10406 batch loss 5.9764595 epoch total loss 5.82880449\n",
      "Trained batch 10407 batch loss 5.48813152 epoch total loss 5.82877159\n",
      "Trained batch 10408 batch loss 6.26840973 epoch total loss 5.82881403\n",
      "Trained batch 10409 batch loss 5.7656889 epoch total loss 5.82880783\n",
      "Trained batch 10410 batch loss 4.86303139 epoch total loss 5.82871532\n",
      "Trained batch 10411 batch loss 5.92907381 epoch total loss 5.82872486\n",
      "Trained batch 10412 batch loss 5.3322773 epoch total loss 5.82867718\n",
      "Trained batch 10413 batch loss 5.60491 epoch total loss 5.82865572\n",
      "Trained batch 10414 batch loss 5.34081078 epoch total loss 5.82860899\n",
      "Trained batch 10415 batch loss 6.11920071 epoch total loss 5.82863712\n",
      "Trained batch 10416 batch loss 6.31920862 epoch total loss 5.82868433\n",
      "Trained batch 10417 batch loss 6.42884731 epoch total loss 5.82874203\n",
      "Trained batch 10418 batch loss 4.81930304 epoch total loss 5.82864523\n",
      "Trained batch 10419 batch loss 5.29001236 epoch total loss 5.82859325\n",
      "Trained batch 10420 batch loss 5.1517849 epoch total loss 5.8285284\n",
      "Trained batch 10421 batch loss 5.27132511 epoch total loss 5.82847452\n",
      "Trained batch 10422 batch loss 4.95195675 epoch total loss 5.8283906\n",
      "Trained batch 10423 batch loss 5.44394112 epoch total loss 5.82835388\n",
      "Trained batch 10424 batch loss 5.80613232 epoch total loss 5.8283515\n",
      "Trained batch 10425 batch loss 5.63615322 epoch total loss 5.82833338\n",
      "Trained batch 10426 batch loss 5.83279085 epoch total loss 5.82833385\n",
      "Trained batch 10427 batch loss 6.2681818 epoch total loss 5.82837582\n",
      "Trained batch 10428 batch loss 6.4212923 epoch total loss 5.82843304\n",
      "Trained batch 10429 batch loss 4.52006054 epoch total loss 5.82830763\n",
      "Trained batch 10430 batch loss 6.10142326 epoch total loss 5.82833338\n",
      "Trained batch 10431 batch loss 5.10519123 epoch total loss 5.82826424\n",
      "Trained batch 10432 batch loss 6.01379585 epoch total loss 5.82828236\n",
      "Trained batch 10433 batch loss 5.46555948 epoch total loss 5.82824755\n",
      "Trained batch 10434 batch loss 6.01832676 epoch total loss 5.82826567\n",
      "Trained batch 10435 batch loss 5.81174326 epoch total loss 5.82826424\n",
      "Trained batch 10436 batch loss 5.23544312 epoch total loss 5.82820749\n",
      "Trained batch 10437 batch loss 5.57254601 epoch total loss 5.82818317\n",
      "Trained batch 10438 batch loss 5.55925751 epoch total loss 5.82815695\n",
      "Trained batch 10439 batch loss 5.74042416 epoch total loss 5.82814884\n",
      "Trained batch 10440 batch loss 5.90684748 epoch total loss 5.82815647\n",
      "Trained batch 10441 batch loss 5.12129593 epoch total loss 5.82808876\n",
      "Trained batch 10442 batch loss 6.11746788 epoch total loss 5.82811642\n",
      "Trained batch 10443 batch loss 5.49974966 epoch total loss 5.82808495\n",
      "Trained batch 10444 batch loss 5.98603439 epoch total loss 5.82809973\n",
      "Trained batch 10445 batch loss 5.28395557 epoch total loss 5.82804775\n",
      "Trained batch 10446 batch loss 5.08161068 epoch total loss 5.8279767\n",
      "Trained batch 10447 batch loss 5.54965401 epoch total loss 5.82795\n",
      "Trained batch 10448 batch loss 4.89951706 epoch total loss 5.82786083\n",
      "Trained batch 10449 batch loss 5.52080441 epoch total loss 5.82783175\n",
      "Trained batch 10450 batch loss 6.08528423 epoch total loss 5.82785606\n",
      "Trained batch 10451 batch loss 6.22058678 epoch total loss 5.82789373\n",
      "Trained batch 10452 batch loss 5.63945866 epoch total loss 5.82787561\n",
      "Trained batch 10453 batch loss 5.64481449 epoch total loss 5.82785797\n",
      "Trained batch 10454 batch loss 6.1689291 epoch total loss 5.82789087\n",
      "Trained batch 10455 batch loss 5.9501977 epoch total loss 5.82790232\n",
      "Trained batch 10456 batch loss 5.76601219 epoch total loss 5.8278966\n",
      "Trained batch 10457 batch loss 5.69656086 epoch total loss 5.82788372\n",
      "Trained batch 10458 batch loss 5.51124477 epoch total loss 5.82785368\n",
      "Trained batch 10459 batch loss 5.54900551 epoch total loss 5.82782698\n",
      "Trained batch 10460 batch loss 5.20303965 epoch total loss 5.82776737\n",
      "Trained batch 10461 batch loss 5.55795813 epoch total loss 5.82774162\n",
      "Trained batch 10462 batch loss 5.84739399 epoch total loss 5.82774353\n",
      "Trained batch 10463 batch loss 5.9977026 epoch total loss 5.82775927\n",
      "Trained batch 10464 batch loss 5.34777546 epoch total loss 5.82771349\n",
      "Trained batch 10465 batch loss 5.23618317 epoch total loss 5.82765675\n",
      "Trained batch 10466 batch loss 5.96157742 epoch total loss 5.82766962\n",
      "Trained batch 10467 batch loss 6.2980442 epoch total loss 5.82771444\n",
      "Trained batch 10468 batch loss 5.31281948 epoch total loss 5.82766533\n",
      "Trained batch 10469 batch loss 5.45596457 epoch total loss 5.82763\n",
      "Trained batch 10470 batch loss 5.29193974 epoch total loss 5.82757902\n",
      "Trained batch 10471 batch loss 5.59070444 epoch total loss 5.82755613\n",
      "Trained batch 10472 batch loss 5.34969187 epoch total loss 5.82751083\n",
      "Trained batch 10473 batch loss 5.28763151 epoch total loss 5.82745934\n",
      "Trained batch 10474 batch loss 5.99417496 epoch total loss 5.82747555\n",
      "Trained batch 10475 batch loss 6.09850359 epoch total loss 5.8275013\n",
      "Trained batch 10476 batch loss 5.03023577 epoch total loss 5.827425\n",
      "Trained batch 10477 batch loss 6.39144087 epoch total loss 5.82747889\n",
      "Trained batch 10478 batch loss 6.29631186 epoch total loss 5.82752371\n",
      "Trained batch 10479 batch loss 6.28207731 epoch total loss 5.8275671\n",
      "Trained batch 10480 batch loss 5.68004131 epoch total loss 5.8275528\n",
      "Trained batch 10481 batch loss 5.79644 epoch total loss 5.82755\n",
      "Trained batch 10482 batch loss 6.88063526 epoch total loss 5.82765\n",
      "Trained batch 10483 batch loss 4.28370953 epoch total loss 5.8275032\n",
      "Trained batch 10484 batch loss 5.85203218 epoch total loss 5.82750559\n",
      "Trained batch 10485 batch loss 6.46595621 epoch total loss 5.82756615\n",
      "Trained batch 10486 batch loss 6.43718767 epoch total loss 5.82762432\n",
      "Trained batch 10487 batch loss 5.89604282 epoch total loss 5.82763052\n",
      "Trained batch 10488 batch loss 6.11939383 epoch total loss 5.82765865\n",
      "Trained batch 10489 batch loss 6.37368679 epoch total loss 5.82771111\n",
      "Trained batch 10490 batch loss 5.60553455 epoch total loss 5.82768965\n",
      "Trained batch 10491 batch loss 5.86481094 epoch total loss 5.82769299\n",
      "Trained batch 10492 batch loss 6.00970936 epoch total loss 5.82771\n",
      "Trained batch 10493 batch loss 5.72903633 epoch total loss 5.82770109\n",
      "Trained batch 10494 batch loss 5.78076935 epoch total loss 5.82769632\n",
      "Trained batch 10495 batch loss 5.91513157 epoch total loss 5.82770491\n",
      "Trained batch 10496 batch loss 5.56751156 epoch total loss 5.82768\n",
      "Trained batch 10497 batch loss 6.11124945 epoch total loss 5.82770681\n",
      "Trained batch 10498 batch loss 6.50504255 epoch total loss 5.82777119\n",
      "Trained batch 10499 batch loss 5.9671 epoch total loss 5.82778454\n",
      "Trained batch 10500 batch loss 6.16532898 epoch total loss 5.82781649\n",
      "Trained batch 10501 batch loss 6.28122807 epoch total loss 5.82786\n",
      "Trained batch 10502 batch loss 6.13562298 epoch total loss 5.82788897\n",
      "Trained batch 10503 batch loss 5.77268887 epoch total loss 5.8278842\n",
      "Trained batch 10504 batch loss 6.52467251 epoch total loss 5.82795\n",
      "Trained batch 10505 batch loss 5.432127 epoch total loss 5.82791281\n",
      "Trained batch 10506 batch loss 6.23704147 epoch total loss 5.82795191\n",
      "Trained batch 10507 batch loss 5.59456635 epoch total loss 5.8279295\n",
      "Trained batch 10508 batch loss 5.25250483 epoch total loss 5.82787466\n",
      "Trained batch 10509 batch loss 5.76681185 epoch total loss 5.82786894\n",
      "Trained batch 10510 batch loss 5.85645866 epoch total loss 5.82787132\n",
      "Trained batch 10511 batch loss 5.77627897 epoch total loss 5.82786655\n",
      "Trained batch 10512 batch loss 5.67856503 epoch total loss 5.82785273\n",
      "Trained batch 10513 batch loss 5.78168821 epoch total loss 5.82784796\n",
      "Trained batch 10514 batch loss 5.6189394 epoch total loss 5.82782793\n",
      "Trained batch 10515 batch loss 5.95451403 epoch total loss 5.82784\n",
      "Trained batch 10516 batch loss 5.82880592 epoch total loss 5.82784033\n",
      "Trained batch 10517 batch loss 5.60256624 epoch total loss 5.82781839\n",
      "Trained batch 10518 batch loss 5.76215315 epoch total loss 5.82781219\n",
      "Trained batch 10519 batch loss 6.46360493 epoch total loss 5.82787275\n",
      "Trained batch 10520 batch loss 4.5163784 epoch total loss 5.8277483\n",
      "Trained batch 10521 batch loss 6.02273178 epoch total loss 5.8277669\n",
      "Trained batch 10522 batch loss 6.08341789 epoch total loss 5.82779074\n",
      "Trained batch 10523 batch loss 5.61153269 epoch total loss 5.82777071\n",
      "Trained batch 10524 batch loss 4.8774538 epoch total loss 5.82768\n",
      "Trained batch 10525 batch loss 3.40331364 epoch total loss 5.82745\n",
      "Trained batch 10526 batch loss 4.81978416 epoch total loss 5.82735443\n",
      "Trained batch 10527 batch loss 4.45336771 epoch total loss 5.82722378\n",
      "Trained batch 10528 batch loss 4.63120174 epoch total loss 5.82711029\n",
      "Trained batch 10529 batch loss 4.77374363 epoch total loss 5.82701\n",
      "Trained batch 10530 batch loss 4.75705051 epoch total loss 5.82690859\n",
      "Trained batch 10531 batch loss 4.15988302 epoch total loss 5.82675028\n",
      "Trained batch 10532 batch loss 4.28749752 epoch total loss 5.82660437\n",
      "Trained batch 10533 batch loss 5.42213631 epoch total loss 5.82656574\n",
      "Trained batch 10534 batch loss 5.97202921 epoch total loss 5.82657957\n",
      "Trained batch 10535 batch loss 4.83319521 epoch total loss 5.82648516\n",
      "Trained batch 10536 batch loss 4.89715481 epoch total loss 5.82639742\n",
      "Trained batch 10537 batch loss 5.18188381 epoch total loss 5.82633638\n",
      "Trained batch 10538 batch loss 4.56831646 epoch total loss 5.8262167\n",
      "Trained batch 10539 batch loss 4.76352024 epoch total loss 5.82611561\n",
      "Trained batch 10540 batch loss 4.3715539 epoch total loss 5.8259778\n",
      "Trained batch 10541 batch loss 4.69952965 epoch total loss 5.82587099\n",
      "Trained batch 10542 batch loss 5.99589348 epoch total loss 5.82588673\n",
      "Trained batch 10543 batch loss 4.90377522 epoch total loss 5.82579947\n",
      "Trained batch 10544 batch loss 6.38441372 epoch total loss 5.82585239\n",
      "Trained batch 10545 batch loss 6.10038567 epoch total loss 5.82587814\n",
      "Trained batch 10546 batch loss 5.15693665 epoch total loss 5.82581472\n",
      "Trained batch 10547 batch loss 4.57607365 epoch total loss 5.82569599\n",
      "Trained batch 10548 batch loss 5.10826588 epoch total loss 5.82562828\n",
      "Trained batch 10549 batch loss 6.71082115 epoch total loss 5.8257122\n",
      "Trained batch 10550 batch loss 5.01114 epoch total loss 5.82563496\n",
      "Trained batch 10551 batch loss 5.99779034 epoch total loss 5.82565117\n",
      "Trained batch 10552 batch loss 6.39406872 epoch total loss 5.82570505\n",
      "Trained batch 10553 batch loss 6.06985235 epoch total loss 5.82572842\n",
      "Trained batch 10554 batch loss 5.34418201 epoch total loss 5.82568264\n",
      "Trained batch 10555 batch loss 6.35511589 epoch total loss 5.82573271\n",
      "Trained batch 10556 batch loss 5.74447346 epoch total loss 5.82572508\n",
      "Trained batch 10557 batch loss 6.34939432 epoch total loss 5.82577467\n",
      "Trained batch 10558 batch loss 6.39732265 epoch total loss 5.82582903\n",
      "Trained batch 10559 batch loss 5.81571674 epoch total loss 5.82582808\n",
      "Trained batch 10560 batch loss 5.88071 epoch total loss 5.82583284\n",
      "Trained batch 10561 batch loss 5.90559483 epoch total loss 5.82584047\n",
      "Trained batch 10562 batch loss 4.7860465 epoch total loss 5.82574224\n",
      "Trained batch 10563 batch loss 5.6177249 epoch total loss 5.82572222\n",
      "Trained batch 10564 batch loss 5.6692667 epoch total loss 5.82570744\n",
      "Trained batch 10565 batch loss 4.80101395 epoch total loss 5.82561064\n",
      "Trained batch 10566 batch loss 5.48169518 epoch total loss 5.82557774\n",
      "Trained batch 10567 batch loss 6.03154421 epoch total loss 5.82559729\n",
      "Trained batch 10568 batch loss 5.7178793 epoch total loss 5.82558727\n",
      "Trained batch 10569 batch loss 4.93765879 epoch total loss 5.82550287\n",
      "Trained batch 10570 batch loss 5.58796644 epoch total loss 5.82548094\n",
      "Trained batch 10571 batch loss 5.10837507 epoch total loss 5.82541323\n",
      "Trained batch 10572 batch loss 5.51989794 epoch total loss 5.82538414\n",
      "Trained batch 10573 batch loss 5.84602261 epoch total loss 5.82538605\n",
      "Trained batch 10574 batch loss 4.99259424 epoch total loss 5.82530737\n",
      "Trained batch 10575 batch loss 5.73205423 epoch total loss 5.82529831\n",
      "Trained batch 10576 batch loss 4.71456623 epoch total loss 5.82519341\n",
      "Trained batch 10577 batch loss 5.672997 epoch total loss 5.8251791\n",
      "Trained batch 10578 batch loss 5.16374445 epoch total loss 5.82511663\n",
      "Trained batch 10579 batch loss 6.08420086 epoch total loss 5.82514095\n",
      "Trained batch 10580 batch loss 4.54669857 epoch total loss 5.82502031\n",
      "Trained batch 10581 batch loss 5.71190548 epoch total loss 5.82500935\n",
      "Trained batch 10582 batch loss 5.55679893 epoch total loss 5.82498455\n",
      "Trained batch 10583 batch loss 5.22470808 epoch total loss 5.82492781\n",
      "Trained batch 10584 batch loss 5.50370407 epoch total loss 5.82489729\n",
      "Trained batch 10585 batch loss 5.4713521 epoch total loss 5.82486439\n",
      "Trained batch 10586 batch loss 6.37008905 epoch total loss 5.82491589\n",
      "Trained batch 10587 batch loss 5.33836317 epoch total loss 5.82487\n",
      "Trained batch 10588 batch loss 6.23689032 epoch total loss 5.82490921\n",
      "Trained batch 10589 batch loss 5.86313534 epoch total loss 5.82491255\n",
      "Trained batch 10590 batch loss 5.82009506 epoch total loss 5.82491207\n",
      "Trained batch 10591 batch loss 5.78293228 epoch total loss 5.82490826\n",
      "Trained batch 10592 batch loss 5.31013298 epoch total loss 5.82485914\n",
      "Trained batch 10593 batch loss 6.11934948 epoch total loss 5.82488728\n",
      "Trained batch 10594 batch loss 5.7468524 epoch total loss 5.82487965\n",
      "Trained batch 10595 batch loss 5.39903069 epoch total loss 5.82483959\n",
      "Trained batch 10596 batch loss 3.35354519 epoch total loss 5.82460642\n",
      "Trained batch 10597 batch loss 3.79960251 epoch total loss 5.82441568\n",
      "Trained batch 10598 batch loss 4.68596315 epoch total loss 5.8243084\n",
      "Trained batch 10599 batch loss 4.10427713 epoch total loss 5.82414627\n",
      "Trained batch 10600 batch loss 5.38847256 epoch total loss 5.82410479\n",
      "Trained batch 10601 batch loss 4.92607 epoch total loss 5.82402\n",
      "Trained batch 10602 batch loss 5.08582211 epoch total loss 5.82395029\n",
      "Trained batch 10603 batch loss 5.02401257 epoch total loss 5.82387495\n",
      "Trained batch 10604 batch loss 4.57354259 epoch total loss 5.82375717\n",
      "Trained batch 10605 batch loss 4.51370668 epoch total loss 5.82363367\n",
      "Trained batch 10606 batch loss 5.17289495 epoch total loss 5.82357216\n",
      "Trained batch 10607 batch loss 5.29083061 epoch total loss 5.82352209\n",
      "Trained batch 10608 batch loss 5.08370495 epoch total loss 5.823452\n",
      "Trained batch 10609 batch loss 4.61236286 epoch total loss 5.82333803\n",
      "Trained batch 10610 batch loss 5.14889717 epoch total loss 5.82327461\n",
      "Trained batch 10611 batch loss 5.2108655 epoch total loss 5.82321692\n",
      "Trained batch 10612 batch loss 5.29091835 epoch total loss 5.82316637\n",
      "Trained batch 10613 batch loss 4.7115097 epoch total loss 5.82306147\n",
      "Trained batch 10614 batch loss 4.50643826 epoch total loss 5.82293749\n",
      "Trained batch 10615 batch loss 4.53870392 epoch total loss 5.82281685\n",
      "Trained batch 10616 batch loss 4.61100388 epoch total loss 5.82270241\n",
      "Trained batch 10617 batch loss 4.17644739 epoch total loss 5.82254744\n",
      "Trained batch 10618 batch loss 4.79301739 epoch total loss 5.82245\n",
      "Trained batch 10619 batch loss 5.16895866 epoch total loss 5.82238865\n",
      "Trained batch 10620 batch loss 5.34965038 epoch total loss 5.8223443\n",
      "Trained batch 10621 batch loss 6.1125474 epoch total loss 5.82237196\n",
      "Trained batch 10622 batch loss 6.13690901 epoch total loss 5.82240152\n",
      "Trained batch 10623 batch loss 5.68892765 epoch total loss 5.82238865\n",
      "Trained batch 10624 batch loss 5.87379837 epoch total loss 5.82239342\n",
      "Trained batch 10625 batch loss 4.52673244 epoch total loss 5.82227182\n",
      "Trained batch 10626 batch loss 6.23709631 epoch total loss 5.82231092\n",
      "Trained batch 10627 batch loss 5.95788431 epoch total loss 5.82232332\n",
      "Trained batch 10628 batch loss 6.57835102 epoch total loss 5.82239485\n",
      "Trained batch 10629 batch loss 5.62977266 epoch total loss 5.82237625\n",
      "Trained batch 10630 batch loss 6.55466843 epoch total loss 5.82244539\n",
      "Trained batch 10631 batch loss 5.97514725 epoch total loss 5.8224597\n",
      "Trained batch 10632 batch loss 6.80796623 epoch total loss 5.82255268\n",
      "Trained batch 10633 batch loss 5.14467239 epoch total loss 5.82248878\n",
      "Trained batch 10634 batch loss 5.88015652 epoch total loss 5.82249403\n",
      "Trained batch 10635 batch loss 5.29872847 epoch total loss 5.82244492\n",
      "Trained batch 10636 batch loss 5.59706688 epoch total loss 5.82242346\n",
      "Trained batch 10637 batch loss 5.17683601 epoch total loss 5.8223629\n",
      "Trained batch 10638 batch loss 5.74114037 epoch total loss 5.82235527\n",
      "Trained batch 10639 batch loss 6.02600336 epoch total loss 5.82237434\n",
      "Trained batch 10640 batch loss 5.42576885 epoch total loss 5.82233715\n",
      "Trained batch 10641 batch loss 5.29696751 epoch total loss 5.82228804\n",
      "Trained batch 10642 batch loss 5.94250393 epoch total loss 5.822299\n",
      "Trained batch 10643 batch loss 5.39445591 epoch total loss 5.82225895\n",
      "Trained batch 10644 batch loss 5.36201191 epoch total loss 5.82221556\n",
      "Trained batch 10645 batch loss 5.90537834 epoch total loss 5.82222366\n",
      "Trained batch 10646 batch loss 5.59425735 epoch total loss 5.82220221\n",
      "Trained batch 10647 batch loss 5.91109562 epoch total loss 5.82221031\n",
      "Trained batch 10648 batch loss 5.59248924 epoch total loss 5.82218885\n",
      "Trained batch 10649 batch loss 6.49944 epoch total loss 5.82225275\n",
      "Trained batch 10650 batch loss 5.90726757 epoch total loss 5.82226038\n",
      "Trained batch 10651 batch loss 5.35941362 epoch total loss 5.82221699\n",
      "Trained batch 10652 batch loss 6.22557259 epoch total loss 5.82225513\n",
      "Trained batch 10653 batch loss 5.81433964 epoch total loss 5.82225418\n",
      "Trained batch 10654 batch loss 5.71157265 epoch total loss 5.82224369\n",
      "Trained batch 10655 batch loss 6.41123295 epoch total loss 5.822299\n",
      "Trained batch 10656 batch loss 5.25760841 epoch total loss 5.8222456\n",
      "Trained batch 10657 batch loss 5.45877075 epoch total loss 5.82221174\n",
      "Trained batch 10658 batch loss 5.88599586 epoch total loss 5.82221746\n",
      "Trained batch 10659 batch loss 6.06323576 epoch total loss 5.82224035\n",
      "Trained batch 10660 batch loss 6.01551056 epoch total loss 5.82225847\n",
      "Trained batch 10661 batch loss 6.05000401 epoch total loss 5.82228\n",
      "Trained batch 10662 batch loss 6.29465675 epoch total loss 5.8223238\n",
      "Trained batch 10663 batch loss 5.70136356 epoch total loss 5.82231283\n",
      "Trained batch 10664 batch loss 5.83991241 epoch total loss 5.82231426\n",
      "Trained batch 10665 batch loss 5.90395927 epoch total loss 5.82232189\n",
      "Trained batch 10666 batch loss 5.05814457 epoch total loss 5.82225037\n",
      "Trained batch 10667 batch loss 6.87375069 epoch total loss 5.82234907\n",
      "Trained batch 10668 batch loss 5.99490929 epoch total loss 5.82236528\n",
      "Trained batch 10669 batch loss 6.17415714 epoch total loss 5.82239819\n",
      "Trained batch 10670 batch loss 6.27797508 epoch total loss 5.8224411\n",
      "Trained batch 10671 batch loss 6.10924911 epoch total loss 5.8224678\n",
      "Trained batch 10672 batch loss 6.2616024 epoch total loss 5.82250881\n",
      "Trained batch 10673 batch loss 6.70218 epoch total loss 5.8225913\n",
      "Trained batch 10674 batch loss 6.03124142 epoch total loss 5.82261086\n",
      "Trained batch 10675 batch loss 6.18865871 epoch total loss 5.82264519\n",
      "Trained batch 10676 batch loss 5.75929737 epoch total loss 5.82263899\n",
      "Trained batch 10677 batch loss 6.07684851 epoch total loss 5.82266331\n",
      "Trained batch 10678 batch loss 6.27195024 epoch total loss 5.82270527\n",
      "Trained batch 10679 batch loss 6.61229849 epoch total loss 5.82277918\n",
      "Trained batch 10680 batch loss 5.18939543 epoch total loss 5.82272\n",
      "Trained batch 10681 batch loss 6.66053724 epoch total loss 5.82279825\n",
      "Trained batch 10682 batch loss 5.80558586 epoch total loss 5.82279682\n",
      "Trained batch 10683 batch loss 6.3593235 epoch total loss 5.82284689\n",
      "Trained batch 10684 batch loss 6.36676788 epoch total loss 5.82289791\n",
      "Trained batch 10685 batch loss 6.24926281 epoch total loss 5.82293797\n",
      "Trained batch 10686 batch loss 6.43378162 epoch total loss 5.82299471\n",
      "Trained batch 10687 batch loss 6.06964684 epoch total loss 5.82301807\n",
      "Trained batch 10688 batch loss 5.79110527 epoch total loss 5.82301521\n",
      "Trained batch 10689 batch loss 5.7130022 epoch total loss 5.8230052\n",
      "Trained batch 10690 batch loss 6.14698792 epoch total loss 5.82303572\n",
      "Trained batch 10691 batch loss 4.92304325 epoch total loss 5.82295132\n",
      "Trained batch 10692 batch loss 5.39047909 epoch total loss 5.82291079\n",
      "Trained batch 10693 batch loss 5.31302214 epoch total loss 5.8228631\n",
      "Trained batch 10694 batch loss 5.67954063 epoch total loss 5.82284975\n",
      "Trained batch 10695 batch loss 5.4853611 epoch total loss 5.82281828\n",
      "Trained batch 10696 batch loss 5.45073509 epoch total loss 5.82278299\n",
      "Trained batch 10697 batch loss 5.6865778 epoch total loss 5.8227706\n",
      "Trained batch 10698 batch loss 5.34289742 epoch total loss 5.82272577\n",
      "Trained batch 10699 batch loss 5.72230816 epoch total loss 5.82271624\n",
      "Trained batch 10700 batch loss 5.01490402 epoch total loss 5.8226409\n",
      "Trained batch 10701 batch loss 5.42626476 epoch total loss 5.8226037\n",
      "Trained batch 10702 batch loss 5.60860729 epoch total loss 5.82258368\n",
      "Trained batch 10703 batch loss 6.05535412 epoch total loss 5.82260561\n",
      "Trained batch 10704 batch loss 5.92535 epoch total loss 5.82261515\n",
      "Trained batch 10705 batch loss 6.36027 epoch total loss 5.82266521\n",
      "Trained batch 10706 batch loss 6.06344509 epoch total loss 5.82268763\n",
      "Trained batch 10707 batch loss 5.99890661 epoch total loss 5.82270432\n",
      "Trained batch 10708 batch loss 6.82729149 epoch total loss 5.82279825\n",
      "Trained batch 10709 batch loss 6.53073215 epoch total loss 5.82286453\n",
      "Trained batch 10710 batch loss 5.52180767 epoch total loss 5.8228364\n",
      "Trained batch 10711 batch loss 5.32969332 epoch total loss 5.82279\n",
      "Trained batch 10712 batch loss 5.89430809 epoch total loss 5.82279682\n",
      "Trained batch 10713 batch loss 5.49682331 epoch total loss 5.8227663\n",
      "Trained batch 10714 batch loss 6.48874807 epoch total loss 5.82282877\n",
      "Trained batch 10715 batch loss 7.68191719 epoch total loss 5.82300234\n",
      "Trained batch 10716 batch loss 7.68498659 epoch total loss 5.82317591\n",
      "Trained batch 10717 batch loss 6.34807205 epoch total loss 5.82322502\n",
      "Trained batch 10718 batch loss 5.97239447 epoch total loss 5.82323885\n",
      "Trained batch 10719 batch loss 5.50219536 epoch total loss 5.82320881\n",
      "Trained batch 10720 batch loss 5.33984947 epoch total loss 5.82316399\n",
      "Trained batch 10721 batch loss 5.23888302 epoch total loss 5.82310915\n",
      "Trained batch 10722 batch loss 5.32457781 epoch total loss 5.8230629\n",
      "Trained batch 10723 batch loss 6.11214256 epoch total loss 5.8230896\n",
      "Trained batch 10724 batch loss 6.51032448 epoch total loss 5.82315397\n",
      "Trained batch 10725 batch loss 5.92468834 epoch total loss 5.82316351\n",
      "Trained batch 10726 batch loss 5.02143097 epoch total loss 5.82308865\n",
      "Trained batch 10727 batch loss 4.98416519 epoch total loss 5.82301044\n",
      "Trained batch 10728 batch loss 5.65166426 epoch total loss 5.82299471\n",
      "Trained batch 10729 batch loss 5.1076417 epoch total loss 5.82292795\n",
      "Trained batch 10730 batch loss 6.5019083 epoch total loss 5.82299137\n",
      "Trained batch 10731 batch loss 5.52972221 epoch total loss 5.82296419\n",
      "Trained batch 10732 batch loss 5.59292793 epoch total loss 5.82294273\n",
      "Trained batch 10733 batch loss 5.8665638 epoch total loss 5.82294655\n",
      "Trained batch 10734 batch loss 5.38072872 epoch total loss 5.82290554\n",
      "Trained batch 10735 batch loss 5.61492443 epoch total loss 5.82288599\n",
      "Trained batch 10736 batch loss 5.65745068 epoch total loss 5.82287025\n",
      "Trained batch 10737 batch loss 5.43891811 epoch total loss 5.82283449\n",
      "Trained batch 10738 batch loss 5.6957159 epoch total loss 5.82282257\n",
      "Trained batch 10739 batch loss 5.71427822 epoch total loss 5.82281256\n",
      "Trained batch 10740 batch loss 5.85609722 epoch total loss 5.82281542\n",
      "Trained batch 10741 batch loss 5.54009151 epoch total loss 5.82278919\n",
      "Trained batch 10742 batch loss 5.4581356 epoch total loss 5.82275486\n",
      "Trained batch 10743 batch loss 5.70212078 epoch total loss 5.82274389\n",
      "Trained batch 10744 batch loss 5.66296911 epoch total loss 5.82272911\n",
      "Trained batch 10745 batch loss 5.80168056 epoch total loss 5.8227272\n",
      "Trained batch 10746 batch loss 5.78604794 epoch total loss 5.82272387\n",
      "Trained batch 10747 batch loss 6.04097652 epoch total loss 5.82274389\n",
      "Trained batch 10748 batch loss 5.34283543 epoch total loss 5.82269907\n",
      "Trained batch 10749 batch loss 5.56834936 epoch total loss 5.82267523\n",
      "Trained batch 10750 batch loss 5.56920862 epoch total loss 5.82265186\n",
      "Trained batch 10751 batch loss 5.59750748 epoch total loss 5.82263088\n",
      "Trained batch 10752 batch loss 5.28791523 epoch total loss 5.82258129\n",
      "Trained batch 10753 batch loss 6.07581186 epoch total loss 5.82260466\n",
      "Trained batch 10754 batch loss 5.41959333 epoch total loss 5.82256699\n",
      "Trained batch 10755 batch loss 5.65362597 epoch total loss 5.82255125\n",
      "Trained batch 10756 batch loss 5.39284515 epoch total loss 5.82251167\n",
      "Trained batch 10757 batch loss 6.19313717 epoch total loss 5.82254601\n",
      "Trained batch 10758 batch loss 5.49311924 epoch total loss 5.82251501\n",
      "Trained batch 10759 batch loss 5.78606701 epoch total loss 5.82251167\n",
      "Trained batch 10760 batch loss 5.0988059 epoch total loss 5.82244444\n",
      "Trained batch 10761 batch loss 5.67052317 epoch total loss 5.82243\n",
      "Trained batch 10762 batch loss 5.80458641 epoch total loss 5.8224287\n",
      "Trained batch 10763 batch loss 5.95301533 epoch total loss 5.82244062\n",
      "Trained batch 10764 batch loss 5.9611516 epoch total loss 5.8224535\n",
      "Trained batch 10765 batch loss 6.51878071 epoch total loss 5.82251835\n",
      "Trained batch 10766 batch loss 5.98240376 epoch total loss 5.82253313\n",
      "Trained batch 10767 batch loss 6.64806318 epoch total loss 5.82261\n",
      "Trained batch 10768 batch loss 5.62359428 epoch total loss 5.8225913\n",
      "Trained batch 10769 batch loss 5.27852535 epoch total loss 5.82254076\n",
      "Trained batch 10770 batch loss 5.90796757 epoch total loss 5.82254839\n",
      "Trained batch 10771 batch loss 5.39458752 epoch total loss 5.82250881\n",
      "Trained batch 10772 batch loss 5.62992287 epoch total loss 5.82249069\n",
      "Trained batch 10773 batch loss 6.19249773 epoch total loss 5.82252502\n",
      "Trained batch 10774 batch loss 5.53980827 epoch total loss 5.8224988\n",
      "Trained batch 10775 batch loss 5.84011507 epoch total loss 5.8225\n",
      "Trained batch 10776 batch loss 5.97852135 epoch total loss 5.82251501\n",
      "Trained batch 10777 batch loss 6.15891552 epoch total loss 5.82254648\n",
      "Trained batch 10778 batch loss 5.79443836 epoch total loss 5.82254362\n",
      "Trained batch 10779 batch loss 5.96449471 epoch total loss 5.82255697\n",
      "Trained batch 10780 batch loss 5.9713316 epoch total loss 5.8225708\n",
      "Trained batch 10781 batch loss 6.64812565 epoch total loss 5.82264757\n",
      "Trained batch 10782 batch loss 6.04331 epoch total loss 5.8226676\n",
      "Trained batch 10783 batch loss 5.25228882 epoch total loss 5.82261515\n",
      "Trained batch 10784 batch loss 6.3127203 epoch total loss 5.82266045\n",
      "Trained batch 10785 batch loss 5.24715614 epoch total loss 5.82260704\n",
      "Trained batch 10786 batch loss 5.80625296 epoch total loss 5.82260513\n",
      "Trained batch 10787 batch loss 5.81379128 epoch total loss 5.82260418\n",
      "Trained batch 10788 batch loss 6.20869732 epoch total loss 5.82264\n",
      "Trained batch 10789 batch loss 7.02144384 epoch total loss 5.82275105\n",
      "Trained batch 10790 batch loss 5.85671425 epoch total loss 5.82275391\n",
      "Trained batch 10791 batch loss 5.56287861 epoch total loss 5.82273\n",
      "Trained batch 10792 batch loss 5.24846363 epoch total loss 5.82267666\n",
      "Trained batch 10793 batch loss 5.53447628 epoch total loss 5.82265\n",
      "Trained batch 10794 batch loss 5.42785358 epoch total loss 5.82261372\n",
      "Trained batch 10795 batch loss 6.34134 epoch total loss 5.82266188\n",
      "Trained batch 10796 batch loss 6.14546204 epoch total loss 5.82269144\n",
      "Trained batch 10797 batch loss 5.61421251 epoch total loss 5.82267189\n",
      "Trained batch 10798 batch loss 4.40301037 epoch total loss 5.82254076\n",
      "Trained batch 10799 batch loss 5.58696461 epoch total loss 5.82251883\n",
      "Trained batch 10800 batch loss 5.87836266 epoch total loss 5.82252407\n",
      "Trained batch 10801 batch loss 4.97261858 epoch total loss 5.82244539\n",
      "Trained batch 10802 batch loss 5.32185173 epoch total loss 5.82239866\n",
      "Trained batch 10803 batch loss 5.75343513 epoch total loss 5.82239246\n",
      "Trained batch 10804 batch loss 5.08437347 epoch total loss 5.82232428\n",
      "Trained batch 10805 batch loss 5.22510338 epoch total loss 5.82226896\n",
      "Trained batch 10806 batch loss 7.35480928 epoch total loss 5.82241106\n",
      "Trained batch 10807 batch loss 5.76834965 epoch total loss 5.82240582\n",
      "Trained batch 10808 batch loss 5.29001141 epoch total loss 5.8223567\n",
      "Trained batch 10809 batch loss 5.39560795 epoch total loss 5.82231712\n",
      "Trained batch 10810 batch loss 5.70712233 epoch total loss 5.82230663\n",
      "Trained batch 10811 batch loss 5.42661953 epoch total loss 5.82227\n",
      "Trained batch 10812 batch loss 6.61461353 epoch total loss 5.82234287\n",
      "Trained batch 10813 batch loss 5.87824917 epoch total loss 5.82234812\n",
      "Trained batch 10814 batch loss 5.91613865 epoch total loss 5.82235718\n",
      "Trained batch 10815 batch loss 5.50989819 epoch total loss 5.82232809\n",
      "Trained batch 10816 batch loss 5.59091854 epoch total loss 5.82230663\n",
      "Trained batch 10817 batch loss 5.26808167 epoch total loss 5.82225561\n",
      "Trained batch 10818 batch loss 5.26929569 epoch total loss 5.82220459\n",
      "Trained batch 10819 batch loss 4.91740417 epoch total loss 5.82212114\n",
      "Trained batch 10820 batch loss 4.77039 epoch total loss 5.82202387\n",
      "Trained batch 10821 batch loss 5.20791054 epoch total loss 5.82196712\n",
      "Trained batch 10822 batch loss 4.69776726 epoch total loss 5.82186317\n",
      "Trained batch 10823 batch loss 4.71340466 epoch total loss 5.82176065\n",
      "Trained batch 10824 batch loss 5.25223446 epoch total loss 5.8217082\n",
      "Trained batch 10825 batch loss 5.15505 epoch total loss 5.82164717\n",
      "Trained batch 10826 batch loss 5.16291952 epoch total loss 5.82158613\n",
      "Trained batch 10827 batch loss 5.05848789 epoch total loss 5.82151556\n",
      "Trained batch 10828 batch loss 4.91472054 epoch total loss 5.82143211\n",
      "Trained batch 10829 batch loss 5.68316078 epoch total loss 5.82141924\n",
      "Trained batch 10830 batch loss 5.33592796 epoch total loss 5.82137442\n",
      "Trained batch 10831 batch loss 5.65869141 epoch total loss 5.82135963\n",
      "Trained batch 10832 batch loss 5.66387 epoch total loss 5.82134485\n",
      "Trained batch 10833 batch loss 5.84396267 epoch total loss 5.82134724\n",
      "Trained batch 10834 batch loss 5.54447556 epoch total loss 5.82132149\n",
      "Trained batch 10835 batch loss 5.83065319 epoch total loss 5.82132244\n",
      "Trained batch 10836 batch loss 5.27458382 epoch total loss 5.8212719\n",
      "Trained batch 10837 batch loss 4.79845524 epoch total loss 5.82117701\n",
      "Trained batch 10838 batch loss 5.08156872 epoch total loss 5.82110882\n",
      "Trained batch 10839 batch loss 5.15989494 epoch total loss 5.82104826\n",
      "Trained batch 10840 batch loss 5.2496748 epoch total loss 5.82099533\n",
      "Trained batch 10841 batch loss 5.35981131 epoch total loss 5.82095289\n",
      "Trained batch 10842 batch loss 5.58365822 epoch total loss 5.82093096\n",
      "Trained batch 10843 batch loss 5.50675058 epoch total loss 5.82090187\n",
      "Trained batch 10844 batch loss 5.17794895 epoch total loss 5.82084274\n",
      "Trained batch 10845 batch loss 4.61816692 epoch total loss 5.82073164\n",
      "Trained batch 10846 batch loss 5.55217934 epoch total loss 5.82070684\n",
      "Trained batch 10847 batch loss 5.4465704 epoch total loss 5.82067204\n",
      "Trained batch 10848 batch loss 5.73242617 epoch total loss 5.82066441\n",
      "Trained batch 10849 batch loss 5.97363091 epoch total loss 5.82067823\n",
      "Trained batch 10850 batch loss 5.86107445 epoch total loss 5.82068205\n",
      "Trained batch 10851 batch loss 5.93038177 epoch total loss 5.82069206\n",
      "Trained batch 10852 batch loss 6.33695793 epoch total loss 5.82073927\n",
      "Trained batch 10853 batch loss 5.49585247 epoch total loss 5.82070971\n",
      "Trained batch 10854 batch loss 5.02102423 epoch total loss 5.8206358\n",
      "Trained batch 10855 batch loss 5.49142838 epoch total loss 5.82060528\n",
      "Trained batch 10856 batch loss 5.37303162 epoch total loss 5.82056379\n",
      "Trained batch 10857 batch loss 5.56565523 epoch total loss 5.82054043\n",
      "Trained batch 10858 batch loss 5.10158253 epoch total loss 5.82047462\n",
      "Trained batch 10859 batch loss 4.97018099 epoch total loss 5.82039595\n",
      "Trained batch 10860 batch loss 5.04639101 epoch total loss 5.8203249\n",
      "Trained batch 10861 batch loss 5.35496283 epoch total loss 5.82028198\n",
      "Trained batch 10862 batch loss 5.37435913 epoch total loss 5.82024097\n",
      "Trained batch 10863 batch loss 5.55947208 epoch total loss 5.82021666\n",
      "Trained batch 10864 batch loss 5.40373325 epoch total loss 5.82017851\n",
      "Trained batch 10865 batch loss 6.4461937 epoch total loss 5.82023573\n",
      "Trained batch 10866 batch loss 6.68808413 epoch total loss 5.82031584\n",
      "Trained batch 10867 batch loss 6.68207932 epoch total loss 5.82039499\n",
      "Trained batch 10868 batch loss 6.19756508 epoch total loss 5.82043\n",
      "Trained batch 10869 batch loss 6.24002 epoch total loss 5.82046843\n",
      "Trained batch 10870 batch loss 4.82405 epoch total loss 5.82037687\n",
      "Trained batch 10871 batch loss 4.8217144 epoch total loss 5.82028484\n",
      "Trained batch 10872 batch loss 5.52944756 epoch total loss 5.82025814\n",
      "Trained batch 10873 batch loss 4.94721842 epoch total loss 5.82017756\n",
      "Trained batch 10874 batch loss 5.1764884 epoch total loss 5.82011843\n",
      "Trained batch 10875 batch loss 5.97697687 epoch total loss 5.82013273\n",
      "Trained batch 10876 batch loss 5.67847 epoch total loss 5.82012\n",
      "Trained batch 10877 batch loss 5.52043915 epoch total loss 5.8200922\n",
      "Trained batch 10878 batch loss 5.91894722 epoch total loss 5.82010126\n",
      "Trained batch 10879 batch loss 5.69080162 epoch total loss 5.82008934\n",
      "Trained batch 10880 batch loss 6.28038597 epoch total loss 5.82013178\n",
      "Trained batch 10881 batch loss 4.84570265 epoch total loss 5.82004213\n",
      "Trained batch 10882 batch loss 6.78033257 epoch total loss 5.82013035\n",
      "Trained batch 10883 batch loss 5.93897343 epoch total loss 5.82014132\n",
      "Trained batch 10884 batch loss 6.51959658 epoch total loss 5.82020569\n",
      "Trained batch 10885 batch loss 6.01821232 epoch total loss 5.82022381\n",
      "Trained batch 10886 batch loss 5.72171068 epoch total loss 5.82021475\n",
      "Trained batch 10887 batch loss 5.52214956 epoch total loss 5.82018757\n",
      "Trained batch 10888 batch loss 7.13521338 epoch total loss 5.82030869\n",
      "Trained batch 10889 batch loss 6.88980055 epoch total loss 5.82040691\n",
      "Trained batch 10890 batch loss 6.56669712 epoch total loss 5.82047558\n",
      "Trained batch 10891 batch loss 6.14179516 epoch total loss 5.82050467\n",
      "Trained batch 10892 batch loss 5.53345585 epoch total loss 5.82047844\n",
      "Trained batch 10893 batch loss 5.72566795 epoch total loss 5.82047\n",
      "Trained batch 10894 batch loss 5.27183819 epoch total loss 5.82042\n",
      "Trained batch 10895 batch loss 6.92613411 epoch total loss 5.82052135\n",
      "Trained batch 10896 batch loss 5.67867708 epoch total loss 5.82050848\n",
      "Trained batch 10897 batch loss 4.80218077 epoch total loss 5.82041454\n",
      "Trained batch 10898 batch loss 5.47294521 epoch total loss 5.8203826\n",
      "Trained batch 10899 batch loss 5.67770481 epoch total loss 5.82036972\n",
      "Trained batch 10900 batch loss 5.39209652 epoch total loss 5.82033\n",
      "Trained batch 10901 batch loss 4.37111139 epoch total loss 5.82019711\n",
      "Trained batch 10902 batch loss 4.4992 epoch total loss 5.82007599\n",
      "Trained batch 10903 batch loss 6.47435713 epoch total loss 5.82013607\n",
      "Trained batch 10904 batch loss 5.47906685 epoch total loss 5.8201046\n",
      "Trained batch 10905 batch loss 6.1262064 epoch total loss 5.82013273\n",
      "Trained batch 10906 batch loss 4.01742125 epoch total loss 5.81996727\n",
      "Trained batch 10907 batch loss 5.44631958 epoch total loss 5.81993294\n",
      "Trained batch 10908 batch loss 6.84299183 epoch total loss 5.82002687\n",
      "Trained batch 10909 batch loss 6.12480545 epoch total loss 5.82005453\n",
      "Trained batch 10910 batch loss 6.08184719 epoch total loss 5.82007885\n",
      "Trained batch 10911 batch loss 6.55431461 epoch total loss 5.82014608\n",
      "Trained batch 10912 batch loss 6.31328392 epoch total loss 5.82019138\n",
      "Trained batch 10913 batch loss 5.82515526 epoch total loss 5.82019138\n",
      "Trained batch 10914 batch loss 6.09200191 epoch total loss 5.82021666\n",
      "Trained batch 10915 batch loss 5.52617645 epoch total loss 5.82019\n",
      "Trained batch 10916 batch loss 5.26072311 epoch total loss 5.82013845\n",
      "Trained batch 10917 batch loss 5.4178896 epoch total loss 5.82010174\n",
      "Trained batch 10918 batch loss 6.22055626 epoch total loss 5.82013845\n",
      "Trained batch 10919 batch loss 6.28323507 epoch total loss 5.82018089\n",
      "Trained batch 10920 batch loss 5.5401516 epoch total loss 5.82015514\n",
      "Trained batch 10921 batch loss 5.25658417 epoch total loss 5.82010365\n",
      "Trained batch 10922 batch loss 6.54176807 epoch total loss 5.82017\n",
      "Trained batch 10923 batch loss 5.98537636 epoch total loss 5.82018471\n",
      "Trained batch 10924 batch loss 5.82108498 epoch total loss 5.82018471\n",
      "Trained batch 10925 batch loss 5.78154564 epoch total loss 5.82018137\n",
      "Trained batch 10926 batch loss 5.42960119 epoch total loss 5.82014561\n",
      "Trained batch 10927 batch loss 5.38595867 epoch total loss 5.82010603\n",
      "Trained batch 10928 batch loss 6.0033145 epoch total loss 5.82012272\n",
      "Trained batch 10929 batch loss 5.6604929 epoch total loss 5.82010794\n",
      "Trained batch 10930 batch loss 5.83551073 epoch total loss 5.82010937\n",
      "Trained batch 10931 batch loss 5.62259912 epoch total loss 5.82009125\n",
      "Trained batch 10932 batch loss 5.4583025 epoch total loss 5.82005787\n",
      "Trained batch 10933 batch loss 5.58787584 epoch total loss 5.82003689\n",
      "Trained batch 10934 batch loss 6.13199711 epoch total loss 5.8200655\n",
      "Trained batch 10935 batch loss 6.2681036 epoch total loss 5.82010651\n",
      "Trained batch 10936 batch loss 5.49986172 epoch total loss 5.82007694\n",
      "Trained batch 10937 batch loss 6.74655676 epoch total loss 5.82016182\n",
      "Trained batch 10938 batch loss 6.28145885 epoch total loss 5.82020378\n",
      "Trained batch 10939 batch loss 7.04348326 epoch total loss 5.82031584\n",
      "Trained batch 10940 batch loss 7.3307724 epoch total loss 5.82045412\n",
      "Trained batch 10941 batch loss 6.34574556 epoch total loss 5.82050228\n",
      "Trained batch 10942 batch loss 6.51464367 epoch total loss 5.8205657\n",
      "Trained batch 10943 batch loss 5.55325127 epoch total loss 5.82054138\n",
      "Trained batch 10944 batch loss 6.30525303 epoch total loss 5.82058573\n",
      "Trained batch 10945 batch loss 6.38652039 epoch total loss 5.82063723\n",
      "Trained batch 10946 batch loss 6.34301281 epoch total loss 5.82068491\n",
      "Trained batch 10947 batch loss 5.54917336 epoch total loss 5.82066059\n",
      "Trained batch 10948 batch loss 4.98023558 epoch total loss 5.82058382\n",
      "Trained batch 10949 batch loss 7.1145649 epoch total loss 5.8207016\n",
      "Trained batch 10950 batch loss 6.4287262 epoch total loss 5.82075739\n",
      "Trained batch 10951 batch loss 5.91957664 epoch total loss 5.82076645\n",
      "Trained batch 10952 batch loss 5.76762772 epoch total loss 5.82076168\n",
      "Trained batch 10953 batch loss 6.08292818 epoch total loss 5.82078552\n",
      "Trained batch 10954 batch loss 5.79517269 epoch total loss 5.82078314\n",
      "Trained batch 10955 batch loss 5.37348604 epoch total loss 5.82074261\n",
      "Trained batch 10956 batch loss 7.05087 epoch total loss 5.82085466\n",
      "Trained batch 10957 batch loss 5.99033451 epoch total loss 5.8208704\n",
      "Trained batch 10958 batch loss 6.85745144 epoch total loss 5.82096529\n",
      "Trained batch 10959 batch loss 6.18431234 epoch total loss 5.82099819\n",
      "Trained batch 10960 batch loss 5.30756 epoch total loss 5.82095146\n",
      "Trained batch 10961 batch loss 5.03543806 epoch total loss 5.82088\n",
      "Trained batch 10962 batch loss 5.9651823 epoch total loss 5.82089281\n",
      "Trained batch 10963 batch loss 5.55040741 epoch total loss 5.82086849\n",
      "Trained batch 10964 batch loss 4.56861687 epoch total loss 5.82075405\n",
      "Trained batch 10965 batch loss 5.41004753 epoch total loss 5.82071686\n",
      "Trained batch 10966 batch loss 4.70611858 epoch total loss 5.82061529\n",
      "Trained batch 10967 batch loss 5.15359116 epoch total loss 5.82055426\n",
      "Trained batch 10968 batch loss 6.97698927 epoch total loss 5.82065964\n",
      "Trained batch 10969 batch loss 6.69154739 epoch total loss 5.82073927\n",
      "Trained batch 10970 batch loss 6.13565683 epoch total loss 5.82076788\n",
      "Trained batch 10971 batch loss 6.22142792 epoch total loss 5.8208046\n",
      "Trained batch 10972 batch loss 6.07485485 epoch total loss 5.82082748\n",
      "Trained batch 10973 batch loss 4.8909049 epoch total loss 5.82074308\n",
      "Trained batch 10974 batch loss 5.36163616 epoch total loss 5.82070112\n",
      "Trained batch 10975 batch loss 6.32707119 epoch total loss 5.82074738\n",
      "Trained batch 10976 batch loss 5.92232418 epoch total loss 5.82075644\n",
      "Trained batch 10977 batch loss 5.85814238 epoch total loss 5.82076025\n",
      "Trained batch 10978 batch loss 5.4493165 epoch total loss 5.82072639\n",
      "Trained batch 10979 batch loss 5.45583153 epoch total loss 5.82069302\n",
      "Trained batch 10980 batch loss 4.91722775 epoch total loss 5.820611\n",
      "Trained batch 10981 batch loss 4.98333311 epoch total loss 5.82053471\n",
      "Trained batch 10982 batch loss 5.3586278 epoch total loss 5.82049274\n",
      "Trained batch 10983 batch loss 5.23367786 epoch total loss 5.82043934\n",
      "Trained batch 10984 batch loss 5.54300547 epoch total loss 5.82041407\n",
      "Trained batch 10985 batch loss 5.3577404 epoch total loss 5.8203721\n",
      "Trained batch 10986 batch loss 5.49949121 epoch total loss 5.82034302\n",
      "Trained batch 10987 batch loss 5.33897 epoch total loss 5.82029915\n",
      "Trained batch 10988 batch loss 6.25545406 epoch total loss 5.82033873\n",
      "Trained batch 10989 batch loss 6.27454567 epoch total loss 5.82038\n",
      "Trained batch 10990 batch loss 6.04301262 epoch total loss 5.8204\n",
      "Trained batch 10991 batch loss 6.47470474 epoch total loss 5.82046\n",
      "Trained batch 10992 batch loss 5.62598419 epoch total loss 5.8204422\n",
      "Trained batch 10993 batch loss 6.06010342 epoch total loss 5.82046366\n",
      "Trained batch 10994 batch loss 5.73352528 epoch total loss 5.82045603\n",
      "Trained batch 10995 batch loss 5.82946682 epoch total loss 5.8204565\n",
      "Trained batch 10996 batch loss 5.92878294 epoch total loss 5.82046652\n",
      "Trained batch 10997 batch loss 5.81538 epoch total loss 5.82046652\n",
      "Trained batch 10998 batch loss 5.54830408 epoch total loss 5.82044125\n",
      "Trained batch 10999 batch loss 5.50982571 epoch total loss 5.82041359\n",
      "Trained batch 11000 batch loss 4.58004189 epoch total loss 5.82030058\n",
      "Trained batch 11001 batch loss 5.88724232 epoch total loss 5.8203063\n",
      "Trained batch 11002 batch loss 4.978652 epoch total loss 5.82023\n",
      "Trained batch 11003 batch loss 5.75359726 epoch total loss 5.82022429\n",
      "Trained batch 11004 batch loss 5.15190792 epoch total loss 5.82016325\n",
      "Trained batch 11005 batch loss 4.87879372 epoch total loss 5.8200779\n",
      "Trained batch 11006 batch loss 5.65667152 epoch total loss 5.82006311\n",
      "Trained batch 11007 batch loss 5.82675171 epoch total loss 5.82006359\n",
      "Trained batch 11008 batch loss 4.29577923 epoch total loss 5.81992531\n",
      "Trained batch 11009 batch loss 5.30873489 epoch total loss 5.81987906\n",
      "Trained batch 11010 batch loss 6.3460722 epoch total loss 5.81992674\n",
      "Trained batch 11011 batch loss 5.60637951 epoch total loss 5.81990719\n",
      "Trained batch 11012 batch loss 6.17721319 epoch total loss 5.81993961\n",
      "Trained batch 11013 batch loss 5.08639145 epoch total loss 5.81987286\n",
      "Trained batch 11014 batch loss 5.11536932 epoch total loss 5.81980944\n",
      "Trained batch 11015 batch loss 5.20225859 epoch total loss 5.81975317\n",
      "Trained batch 11016 batch loss 6.44675493 epoch total loss 5.81981\n",
      "Trained batch 11017 batch loss 5.31289577 epoch total loss 5.81976414\n",
      "Trained batch 11018 batch loss 5.54571438 epoch total loss 5.81973934\n",
      "Trained batch 11019 batch loss 5.2387948 epoch total loss 5.81968641\n",
      "Trained batch 11020 batch loss 6.32180309 epoch total loss 5.81973171\n",
      "Trained batch 11021 batch loss 5.71306658 epoch total loss 5.81972218\n",
      "Trained batch 11022 batch loss 5.27401447 epoch total loss 5.81967258\n",
      "Trained batch 11023 batch loss 6.71639442 epoch total loss 5.81975412\n",
      "Trained batch 11024 batch loss 6.77399635 epoch total loss 5.81984043\n",
      "Trained batch 11025 batch loss 6.98312092 epoch total loss 5.81994629\n",
      "Trained batch 11026 batch loss 6.45206976 epoch total loss 5.82000351\n",
      "Trained batch 11027 batch loss 6.31275177 epoch total loss 5.82004833\n",
      "Trained batch 11028 batch loss 6.69289923 epoch total loss 5.82012701\n",
      "Trained batch 11029 batch loss 4.36610222 epoch total loss 5.8199954\n",
      "Trained batch 11030 batch loss 5.04731131 epoch total loss 5.81992531\n",
      "Trained batch 11031 batch loss 5.51971769 epoch total loss 5.81989813\n",
      "Trained batch 11032 batch loss 5.9494257 epoch total loss 5.81991\n",
      "Trained batch 11033 batch loss 5.50274 epoch total loss 5.81988144\n",
      "Trained batch 11034 batch loss 5.70727634 epoch total loss 5.81987095\n",
      "Trained batch 11035 batch loss 5.238235 epoch total loss 5.8198185\n",
      "Trained batch 11036 batch loss 6.53703 epoch total loss 5.81988335\n",
      "Trained batch 11037 batch loss 5.8986392 epoch total loss 5.8198905\n",
      "Trained batch 11038 batch loss 6.63264847 epoch total loss 5.81996393\n",
      "Trained batch 11039 batch loss 5.78894615 epoch total loss 5.81996107\n",
      "Trained batch 11040 batch loss 6.52659225 epoch total loss 5.82002497\n",
      "Trained batch 11041 batch loss 6.51590443 epoch total loss 5.82008839\n",
      "Trained batch 11042 batch loss 4.52739716 epoch total loss 5.81997108\n",
      "Trained batch 11043 batch loss 6.48119926 epoch total loss 5.82003117\n",
      "Trained batch 11044 batch loss 6.26239777 epoch total loss 5.82007074\n",
      "Trained batch 11045 batch loss 5.88324642 epoch total loss 5.82007647\n",
      "Trained batch 11046 batch loss 6.21999836 epoch total loss 5.82011271\n",
      "Trained batch 11047 batch loss 5.60659122 epoch total loss 5.82009315\n",
      "Trained batch 11048 batch loss 6.63138866 epoch total loss 5.82016706\n",
      "Trained batch 11049 batch loss 6.03812218 epoch total loss 5.82018661\n",
      "Trained batch 11050 batch loss 6.47685337 epoch total loss 5.82024622\n",
      "Trained batch 11051 batch loss 6.0790863 epoch total loss 5.82026958\n",
      "Trained batch 11052 batch loss 6.144526 epoch total loss 5.82029867\n",
      "Trained batch 11053 batch loss 5.54410362 epoch total loss 5.8202734\n",
      "Trained batch 11054 batch loss 6.02207375 epoch total loss 5.820292\n",
      "Trained batch 11055 batch loss 6.08914042 epoch total loss 5.82031631\n",
      "Trained batch 11056 batch loss 6.14230537 epoch total loss 5.8203454\n",
      "Trained batch 11057 batch loss 5.31083632 epoch total loss 5.82029963\n",
      "Trained batch 11058 batch loss 5.74702644 epoch total loss 5.82029295\n",
      "Trained batch 11059 batch loss 5.75666618 epoch total loss 5.82028723\n",
      "Trained batch 11060 batch loss 6.18594265 epoch total loss 5.82032\n",
      "Trained batch 11061 batch loss 6.04953098 epoch total loss 5.82034111\n",
      "Trained batch 11062 batch loss 6.33836 epoch total loss 5.82038784\n",
      "Trained batch 11063 batch loss 5.39609623 epoch total loss 5.82034969\n",
      "Trained batch 11064 batch loss 5.88294125 epoch total loss 5.82035542\n",
      "Trained batch 11065 batch loss 5.43338585 epoch total loss 5.82032\n",
      "Trained batch 11066 batch loss 5.97204494 epoch total loss 5.82033396\n",
      "Trained batch 11067 batch loss 5.59559155 epoch total loss 5.82031345\n",
      "Trained batch 11068 batch loss 6.34981823 epoch total loss 5.82036161\n",
      "Trained batch 11069 batch loss 6.05225 epoch total loss 5.8203826\n",
      "Trained batch 11070 batch loss 6.22877836 epoch total loss 5.82041931\n",
      "Trained batch 11071 batch loss 6.02772427 epoch total loss 5.82043791\n",
      "Trained batch 11072 batch loss 6.88447094 epoch total loss 5.82053423\n",
      "Trained batch 11073 batch loss 6.15342331 epoch total loss 5.82056379\n",
      "Trained batch 11074 batch loss 6.61615276 epoch total loss 5.8206358\n",
      "Trained batch 11075 batch loss 6.35483742 epoch total loss 5.82068443\n",
      "Trained batch 11076 batch loss 5.94681072 epoch total loss 5.8206954\n",
      "Trained batch 11077 batch loss 6.61328363 epoch total loss 5.82076693\n",
      "Trained batch 11078 batch loss 5.73653412 epoch total loss 5.82076\n",
      "Trained batch 11079 batch loss 5.70879745 epoch total loss 5.82074928\n",
      "Trained batch 11080 batch loss 6.01566219 epoch total loss 5.82076693\n",
      "Trained batch 11081 batch loss 4.73790598 epoch total loss 5.82066917\n",
      "Trained batch 11082 batch loss 5.37983274 epoch total loss 5.8206296\n",
      "Trained batch 11083 batch loss 5.75274181 epoch total loss 5.8206234\n",
      "Trained batch 11084 batch loss 6.76193762 epoch total loss 5.82070827\n",
      "Trained batch 11085 batch loss 6.15558338 epoch total loss 5.82073832\n",
      "Trained batch 11086 batch loss 6.54457283 epoch total loss 5.82080364\n",
      "Trained batch 11087 batch loss 5.78647089 epoch total loss 5.8208003\n",
      "Trained batch 11088 batch loss 6.60737801 epoch total loss 5.82087135\n",
      "Trained batch 11089 batch loss 6.15276337 epoch total loss 5.82090092\n",
      "Trained batch 11090 batch loss 6.6083703 epoch total loss 5.82097244\n",
      "Trained batch 11091 batch loss 5.81769657 epoch total loss 5.82097197\n",
      "Trained batch 11092 batch loss 5.68742323 epoch total loss 5.82095957\n",
      "Trained batch 11093 batch loss 5.95011044 epoch total loss 5.82097149\n",
      "Trained batch 11094 batch loss 4.42666912 epoch total loss 5.8208456\n",
      "Trained batch 11095 batch loss 5.4521122 epoch total loss 5.82081223\n",
      "Trained batch 11096 batch loss 5.72395706 epoch total loss 5.82080364\n",
      "Trained batch 11097 batch loss 5.17544794 epoch total loss 5.82074547\n",
      "Trained batch 11098 batch loss 5.5583477 epoch total loss 5.82072163\n",
      "Trained batch 11099 batch loss 6.05771828 epoch total loss 5.82074308\n",
      "Trained batch 11100 batch loss 5.73179245 epoch total loss 5.82073498\n",
      "Trained batch 11101 batch loss 5.69946098 epoch total loss 5.82072401\n",
      "Trained batch 11102 batch loss 5.54535 epoch total loss 5.82069969\n",
      "Trained batch 11103 batch loss 5.42224693 epoch total loss 5.82066345\n",
      "Trained batch 11104 batch loss 5.22745657 epoch total loss 5.82061\n",
      "Trained batch 11105 batch loss 4.92692184 epoch total loss 5.82052946\n",
      "Trained batch 11106 batch loss 5.99293518 epoch total loss 5.8205452\n",
      "Trained batch 11107 batch loss 6.10662174 epoch total loss 5.82057047\n",
      "Trained batch 11108 batch loss 5.31518221 epoch total loss 5.82052517\n",
      "Trained batch 11109 batch loss 6.01875496 epoch total loss 5.82054329\n",
      "Trained batch 11110 batch loss 5.44126 epoch total loss 5.82050896\n",
      "Trained batch 11111 batch loss 5.77198029 epoch total loss 5.82050467\n",
      "Trained batch 11112 batch loss 4.34002495 epoch total loss 5.82037163\n",
      "Trained batch 11113 batch loss 4.74534607 epoch total loss 5.82027483\n",
      "Trained batch 11114 batch loss 4.56830835 epoch total loss 5.8201623\n",
      "Trained batch 11115 batch loss 4.61496496 epoch total loss 5.82005358\n",
      "Trained batch 11116 batch loss 3.91620755 epoch total loss 5.81988239\n",
      "Trained batch 11117 batch loss 5.3139 epoch total loss 5.81983662\n",
      "Trained batch 11118 batch loss 4.8714695 epoch total loss 5.81975126\n",
      "Trained batch 11119 batch loss 5.88142681 epoch total loss 5.81975698\n",
      "Trained batch 11120 batch loss 6.59231234 epoch total loss 5.8198266\n",
      "Trained batch 11121 batch loss 5.64347267 epoch total loss 5.81981087\n",
      "Trained batch 11122 batch loss 6.76386356 epoch total loss 5.81989574\n",
      "Trained batch 11123 batch loss 5.24514961 epoch total loss 5.81984425\n",
      "Epoch 2 train loss 5.8198442459106445\n",
      "Validated batch 1 batch loss 6.33680248\n",
      "Validated batch 2 batch loss 5.76981544\n",
      "Validated batch 3 batch loss 6.20200253\n",
      "Validated batch 4 batch loss 6.10552311\n",
      "Validated batch 5 batch loss 6.50060654\n",
      "Validated batch 6 batch loss 5.5710125\n",
      "Validated batch 7 batch loss 7.1365633\n",
      "Validated batch 8 batch loss 6.90308475\n",
      "Validated batch 9 batch loss 6.93777943\n",
      "Validated batch 10 batch loss 7.11027336\n",
      "Validated batch 11 batch loss 6.89287901\n",
      "Validated batch 12 batch loss 5.74181938\n",
      "Validated batch 13 batch loss 5.94148159\n",
      "Validated batch 14 batch loss 6.76576805\n",
      "Validated batch 15 batch loss 5.5257206\n",
      "Validated batch 16 batch loss 5.6569376\n",
      "Validated batch 17 batch loss 4.99950171\n",
      "Validated batch 18 batch loss 5.13634396\n",
      "Validated batch 19 batch loss 5.66842175\n",
      "Validated batch 20 batch loss 6.56899691\n",
      "Validated batch 21 batch loss 6.64534\n",
      "Validated batch 22 batch loss 5.79101086\n",
      "Validated batch 23 batch loss 5.83804703\n",
      "Validated batch 24 batch loss 5.78799725\n",
      "Validated batch 25 batch loss 6.18244\n",
      "Validated batch 26 batch loss 6.04385424\n",
      "Validated batch 27 batch loss 5.48862743\n",
      "Validated batch 28 batch loss 3.78707266\n",
      "Validated batch 29 batch loss 6.44538212\n",
      "Validated batch 30 batch loss 5.73489761\n",
      "Validated batch 31 batch loss 4.98034859\n",
      "Validated batch 32 batch loss 6.14778662\n",
      "Validated batch 33 batch loss 6.13316774\n",
      "Validated batch 34 batch loss 5.98417\n",
      "Validated batch 35 batch loss 6.72311497\n",
      "Validated batch 36 batch loss 6.41496944\n",
      "Validated batch 37 batch loss 6.58150768\n",
      "Validated batch 38 batch loss 6.73310518\n",
      "Validated batch 39 batch loss 5.14943886\n",
      "Validated batch 40 batch loss 4.6010828\n",
      "Validated batch 41 batch loss 5.93747425\n",
      "Validated batch 42 batch loss 6.65326214\n",
      "Validated batch 43 batch loss 6.43636036\n",
      "Validated batch 44 batch loss 6.60746574\n",
      "Validated batch 45 batch loss 6.92006302\n",
      "Validated batch 46 batch loss 5.98734236\n",
      "Validated batch 47 batch loss 5.62563896\n",
      "Validated batch 48 batch loss 5.64768934\n",
      "Validated batch 49 batch loss 5.31154633\n",
      "Validated batch 50 batch loss 6.10826302\n",
      "Validated batch 51 batch loss 6.56546783\n",
      "Validated batch 52 batch loss 5.98040438\n",
      "Validated batch 53 batch loss 4.96593952\n",
      "Validated batch 54 batch loss 5.91686535\n",
      "Validated batch 55 batch loss 5.57707882\n",
      "Validated batch 56 batch loss 6.14983606\n",
      "Validated batch 57 batch loss 5.21088123\n",
      "Validated batch 58 batch loss 5.58755589\n",
      "Validated batch 59 batch loss 5.63946199\n",
      "Validated batch 60 batch loss 6.62427711\n",
      "Validated batch 61 batch loss 6.69358921\n",
      "Validated batch 62 batch loss 6.47675562\n",
      "Validated batch 63 batch loss 5.12166691\n",
      "Validated batch 64 batch loss 4.96371174\n",
      "Validated batch 65 batch loss 4.57767582\n",
      "Validated batch 66 batch loss 4.31337261\n",
      "Validated batch 67 batch loss 6.93520927\n",
      "Validated batch 68 batch loss 6.15275908\n",
      "Validated batch 69 batch loss 6.50623465\n",
      "Validated batch 70 batch loss 6.76684618\n",
      "Validated batch 71 batch loss 6.44658756\n",
      "Validated batch 72 batch loss 7.65470839\n",
      "Validated batch 73 batch loss 6.50526428\n",
      "Validated batch 74 batch loss 6.1435\n",
      "Validated batch 75 batch loss 5.34297\n",
      "Validated batch 76 batch loss 6.46694136\n",
      "Validated batch 77 batch loss 5.99386406\n",
      "Validated batch 78 batch loss 6.37768173\n",
      "Validated batch 79 batch loss 6.65690041\n",
      "Validated batch 80 batch loss 5.52632856\n",
      "Validated batch 81 batch loss 6.39815092\n",
      "Validated batch 82 batch loss 5.29862261\n",
      "Validated batch 83 batch loss 6.04239559\n",
      "Validated batch 84 batch loss 6.26038742\n",
      "Validated batch 85 batch loss 5.02009106\n",
      "Validated batch 86 batch loss 6.13520336\n",
      "Validated batch 87 batch loss 5.61041451\n",
      "Validated batch 88 batch loss 6.4431448\n",
      "Validated batch 89 batch loss 5.14415264\n",
      "Validated batch 90 batch loss 5.40294552\n",
      "Validated batch 91 batch loss 6.30090141\n",
      "Validated batch 92 batch loss 6.18547916\n",
      "Validated batch 93 batch loss 6.13995123\n",
      "Validated batch 94 batch loss 5.76188183\n",
      "Validated batch 95 batch loss 4.40456486\n",
      "Validated batch 96 batch loss 6.37722206\n",
      "Validated batch 97 batch loss 5.32874632\n",
      "Validated batch 98 batch loss 6.57308674\n",
      "Validated batch 99 batch loss 5.54732704\n",
      "Validated batch 100 batch loss 4.83980274\n",
      "Validated batch 101 batch loss 6.29745722\n",
      "Validated batch 102 batch loss 6.51107693\n",
      "Validated batch 103 batch loss 7.15738869\n",
      "Validated batch 104 batch loss 5.75049973\n",
      "Validated batch 105 batch loss 6.3473587\n",
      "Validated batch 106 batch loss 6.23027182\n",
      "Validated batch 107 batch loss 6.65740585\n",
      "Validated batch 108 batch loss 7.40600157\n",
      "Validated batch 109 batch loss 6.68614101\n",
      "Validated batch 110 batch loss 5.41629362\n",
      "Validated batch 111 batch loss 6.02308512\n",
      "Validated batch 112 batch loss 5.00034332\n",
      "Validated batch 113 batch loss 5.54147148\n",
      "Validated batch 114 batch loss 6.10559607\n",
      "Validated batch 115 batch loss 6.76236057\n",
      "Validated batch 116 batch loss 5.67931509\n",
      "Validated batch 117 batch loss 6.28479624\n",
      "Validated batch 118 batch loss 6.24527931\n",
      "Validated batch 119 batch loss 6.14664078\n",
      "Validated batch 120 batch loss 5.58732605\n",
      "Validated batch 121 batch loss 6.74537373\n",
      "Validated batch 122 batch loss 5.34022522\n",
      "Validated batch 123 batch loss 6.45794106\n",
      "Validated batch 124 batch loss 6.09337902\n",
      "Validated batch 125 batch loss 6.36896181\n",
      "Validated batch 126 batch loss 6.54871941\n",
      "Validated batch 127 batch loss 6.16253853\n",
      "Validated batch 128 batch loss 5.88988972\n",
      "Validated batch 129 batch loss 5.9858284\n",
      "Validated batch 130 batch loss 5.84639454\n",
      "Validated batch 131 batch loss 6.02204847\n",
      "Validated batch 132 batch loss 5.60847282\n",
      "Validated batch 133 batch loss 6.6441927\n",
      "Validated batch 134 batch loss 6.46997261\n",
      "Validated batch 135 batch loss 6.32586336\n",
      "Validated batch 136 batch loss 6.16030741\n",
      "Validated batch 137 batch loss 7.12522364\n",
      "Validated batch 138 batch loss 7.06875944\n",
      "Validated batch 139 batch loss 6.194664\n",
      "Validated batch 140 batch loss 6.9117\n",
      "Validated batch 141 batch loss 5.51882601\n",
      "Validated batch 142 batch loss 6.49205112\n",
      "Validated batch 143 batch loss 6.28977\n",
      "Validated batch 144 batch loss 5.86917257\n",
      "Validated batch 145 batch loss 5.47261333\n",
      "Validated batch 146 batch loss 5.44768143\n",
      "Validated batch 147 batch loss 6.17934513\n",
      "Validated batch 148 batch loss 6.11638927\n",
      "Validated batch 149 batch loss 5.73378\n",
      "Validated batch 150 batch loss 6.39737129\n",
      "Validated batch 151 batch loss 6.33068657\n",
      "Validated batch 152 batch loss 7.05667591\n",
      "Validated batch 153 batch loss 7.30814648\n",
      "Validated batch 154 batch loss 6.86626387\n",
      "Validated batch 155 batch loss 5.52556324\n",
      "Validated batch 156 batch loss 5.60530853\n",
      "Validated batch 157 batch loss 5.92232513\n",
      "Validated batch 158 batch loss 5.71153784\n",
      "Validated batch 159 batch loss 5.5078249\n",
      "Validated batch 160 batch loss 5.91440821\n",
      "Validated batch 161 batch loss 7.37301207\n",
      "Validated batch 162 batch loss 5.52525187\n",
      "Validated batch 163 batch loss 6.99305105\n",
      "Validated batch 164 batch loss 6.8754487\n",
      "Validated batch 165 batch loss 6.09978\n",
      "Validated batch 166 batch loss 5.8759675\n",
      "Validated batch 167 batch loss 5.44250488\n",
      "Validated batch 168 batch loss 6.07900667\n",
      "Validated batch 169 batch loss 5.17100525\n",
      "Validated batch 170 batch loss 5.05222416\n",
      "Validated batch 171 batch loss 5.43251514\n",
      "Validated batch 172 batch loss 4.52054262\n",
      "Validated batch 173 batch loss 4.68208504\n",
      "Validated batch 174 batch loss 5.18078613\n",
      "Validated batch 175 batch loss 5.20998621\n",
      "Validated batch 176 batch loss 6.14896488\n",
      "Validated batch 177 batch loss 5.33645296\n",
      "Validated batch 178 batch loss 6.26616859\n",
      "Validated batch 179 batch loss 5.94372845\n",
      "Validated batch 180 batch loss 6.40383\n",
      "Validated batch 181 batch loss 6.95038939\n",
      "Validated batch 182 batch loss 7.0617466\n",
      "Validated batch 183 batch loss 6.37375927\n",
      "Validated batch 184 batch loss 6.53842926\n",
      "Validated batch 185 batch loss 6.6642251\n",
      "Validated batch 186 batch loss 6.13877869\n",
      "Validated batch 187 batch loss 5.81941795\n",
      "Validated batch 188 batch loss 6.76772976\n",
      "Validated batch 189 batch loss 6.05932617\n",
      "Validated batch 190 batch loss 6.97545147\n",
      "Validated batch 191 batch loss 6.85907888\n",
      "Validated batch 192 batch loss 5.74655247\n",
      "Validated batch 193 batch loss 5.56617\n",
      "Validated batch 194 batch loss 5.88801527\n",
      "Validated batch 195 batch loss 6.52092457\n",
      "Validated batch 196 batch loss 5.60368919\n",
      "Validated batch 197 batch loss 5.52860355\n",
      "Validated batch 198 batch loss 6.88063431\n",
      "Validated batch 199 batch loss 6.10125828\n",
      "Validated batch 200 batch loss 7.11764288\n",
      "Validated batch 201 batch loss 5.4651432\n",
      "Validated batch 202 batch loss 6.11069679\n",
      "Validated batch 203 batch loss 5.47596169\n",
      "Validated batch 204 batch loss 5.34362411\n",
      "Validated batch 205 batch loss 6.15084076\n",
      "Validated batch 206 batch loss 5.82022095\n",
      "Validated batch 207 batch loss 6.07371902\n",
      "Validated batch 208 batch loss 5.87518501\n",
      "Validated batch 209 batch loss 5.84173059\n",
      "Validated batch 210 batch loss 4.83408356\n",
      "Validated batch 211 batch loss 6.04602957\n",
      "Validated batch 212 batch loss 5.18144417\n",
      "Validated batch 213 batch loss 6.67542648\n",
      "Validated batch 214 batch loss 6.09323597\n",
      "Validated batch 215 batch loss 5.99013281\n",
      "Validated batch 216 batch loss 6.23575687\n",
      "Validated batch 217 batch loss 5.09567833\n",
      "Validated batch 218 batch loss 6.44859123\n",
      "Validated batch 219 batch loss 6.11996365\n",
      "Validated batch 220 batch loss 5.48507595\n",
      "Validated batch 221 batch loss 6.33014679\n",
      "Validated batch 222 batch loss 6.02242136\n",
      "Validated batch 223 batch loss 5.32275391\n",
      "Validated batch 224 batch loss 6.84093142\n",
      "Validated batch 225 batch loss 7.10052061\n",
      "Validated batch 226 batch loss 7.23437357\n",
      "Validated batch 227 batch loss 6.37860966\n",
      "Validated batch 228 batch loss 6.76086521\n",
      "Validated batch 229 batch loss 5.63074207\n",
      "Validated batch 230 batch loss 5.53309202\n",
      "Validated batch 231 batch loss 5.74636889\n",
      "Validated batch 232 batch loss 6.27869368\n",
      "Validated batch 233 batch loss 5.46119976\n",
      "Validated batch 234 batch loss 5.41660786\n",
      "Validated batch 235 batch loss 6.26853085\n",
      "Validated batch 236 batch loss 6.41702509\n",
      "Validated batch 237 batch loss 6.58268547\n",
      "Validated batch 238 batch loss 5.75889874\n",
      "Validated batch 239 batch loss 6.14226723\n",
      "Validated batch 240 batch loss 6.27646303\n",
      "Validated batch 241 batch loss 5.51192141\n",
      "Validated batch 242 batch loss 5.48437214\n",
      "Validated batch 243 batch loss 6.60981083\n",
      "Validated batch 244 batch loss 6.77208185\n",
      "Validated batch 245 batch loss 6.07257\n",
      "Validated batch 246 batch loss 6.70292473\n",
      "Validated batch 247 batch loss 6.36252403\n",
      "Validated batch 248 batch loss 6.4545\n",
      "Validated batch 249 batch loss 6.04991436\n",
      "Validated batch 250 batch loss 5.65450573\n",
      "Validated batch 251 batch loss 5.92888355\n",
      "Validated batch 252 batch loss 6.17712593\n",
      "Validated batch 253 batch loss 5.70775\n",
      "Validated batch 254 batch loss 6.72205973\n",
      "Validated batch 255 batch loss 6.69535\n",
      "Validated batch 256 batch loss 6.57226849\n",
      "Validated batch 257 batch loss 6.22854424\n",
      "Validated batch 258 batch loss 7.25388336\n",
      "Validated batch 259 batch loss 4.77628517\n",
      "Validated batch 260 batch loss 5.78285122\n",
      "Validated batch 261 batch loss 6.18923\n",
      "Validated batch 262 batch loss 5.98832\n",
      "Validated batch 263 batch loss 6.07122183\n",
      "Validated batch 264 batch loss 6.55931759\n",
      "Validated batch 265 batch loss 5.291471\n",
      "Validated batch 266 batch loss 5.67841053\n",
      "Validated batch 267 batch loss 6.52928448\n",
      "Validated batch 268 batch loss 5.77117348\n",
      "Validated batch 269 batch loss 5.59462547\n",
      "Validated batch 270 batch loss 6.52251816\n",
      "Validated batch 271 batch loss 6.19746971\n",
      "Validated batch 272 batch loss 6.09044\n",
      "Validated batch 273 batch loss 5.73499203\n",
      "Validated batch 274 batch loss 5.71042061\n",
      "Validated batch 275 batch loss 6.32617378\n",
      "Validated batch 276 batch loss 5.2814827\n",
      "Validated batch 277 batch loss 5.55225754\n",
      "Validated batch 278 batch loss 4.84428024\n",
      "Validated batch 279 batch loss 5.0486269\n",
      "Validated batch 280 batch loss 6.0431695\n",
      "Validated batch 281 batch loss 5.85481644\n",
      "Validated batch 282 batch loss 5.7605772\n",
      "Validated batch 283 batch loss 5.49080181\n",
      "Validated batch 284 batch loss 6.64496231\n",
      "Validated batch 285 batch loss 7.60310125\n",
      "Validated batch 286 batch loss 6.50303364\n",
      "Validated batch 287 batch loss 7.56783533\n",
      "Validated batch 288 batch loss 7.24271584\n",
      "Validated batch 289 batch loss 7.01759863\n",
      "Validated batch 290 batch loss 6.62882519\n",
      "Validated batch 291 batch loss 6.42939949\n",
      "Validated batch 292 batch loss 5.63399792\n",
      "Validated batch 293 batch loss 4.31400681\n",
      "Validated batch 294 batch loss 6.42076778\n",
      "Validated batch 295 batch loss 5.8405571\n",
      "Validated batch 296 batch loss 4.84919882\n",
      "Validated batch 297 batch loss 6.18129349\n",
      "Validated batch 298 batch loss 6.48762321\n",
      "Validated batch 299 batch loss 6.42509413\n",
      "Validated batch 300 batch loss 5.81763697\n",
      "Validated batch 301 batch loss 6.46565866\n",
      "Validated batch 302 batch loss 7.14366913\n",
      "Validated batch 303 batch loss 5.95167065\n",
      "Validated batch 304 batch loss 5.4255681\n",
      "Validated batch 305 batch loss 5.57688475\n",
      "Validated batch 306 batch loss 6.41058445\n",
      "Validated batch 307 batch loss 5.94013214\n",
      "Validated batch 308 batch loss 6.09892654\n",
      "Validated batch 309 batch loss 6.76693869\n",
      "Validated batch 310 batch loss 6.43348598\n",
      "Validated batch 311 batch loss 6.05408955\n",
      "Validated batch 312 batch loss 5.54996204\n",
      "Validated batch 313 batch loss 7.10206\n",
      "Validated batch 314 batch loss 6.17983675\n",
      "Validated batch 315 batch loss 6.23309231\n",
      "Validated batch 316 batch loss 5.58599186\n",
      "Validated batch 317 batch loss 6.99478722\n",
      "Validated batch 318 batch loss 5.23134947\n",
      "Validated batch 319 batch loss 6.03954744\n",
      "Validated batch 320 batch loss 6.61114216\n",
      "Validated batch 321 batch loss 6.59467316\n",
      "Validated batch 322 batch loss 6.77164698\n",
      "Validated batch 323 batch loss 5.45784\n",
      "Validated batch 324 batch loss 4.75786114\n",
      "Validated batch 325 batch loss 4.40202141\n",
      "Validated batch 326 batch loss 5.38458061\n",
      "Validated batch 327 batch loss 5.10224533\n",
      "Validated batch 328 batch loss 5.40983105\n",
      "Validated batch 329 batch loss 6.20334148\n",
      "Validated batch 330 batch loss 5.70931911\n",
      "Validated batch 331 batch loss 6.22231293\n",
      "Validated batch 332 batch loss 5.66045952\n",
      "Validated batch 333 batch loss 4.86926126\n",
      "Validated batch 334 batch loss 5.4398222\n",
      "Validated batch 335 batch loss 6.75930882\n",
      "Validated batch 336 batch loss 5.35553455\n",
      "Validated batch 337 batch loss 6.149\n",
      "Validated batch 338 batch loss 6.28073788\n",
      "Validated batch 339 batch loss 6.34763527\n",
      "Validated batch 340 batch loss 6.28524876\n",
      "Validated batch 341 batch loss 6.15512037\n",
      "Validated batch 342 batch loss 6.16177607\n",
      "Validated batch 343 batch loss 6.16839027\n",
      "Validated batch 344 batch loss 5.44878769\n",
      "Validated batch 345 batch loss 6.2131567\n",
      "Validated batch 346 batch loss 5.70791245\n",
      "Validated batch 347 batch loss 6.69731617\n",
      "Validated batch 348 batch loss 6.50022316\n",
      "Validated batch 349 batch loss 5.2306776\n",
      "Validated batch 350 batch loss 4.72505951\n",
      "Validated batch 351 batch loss 5.79329538\n",
      "Validated batch 352 batch loss 6.49833107\n",
      "Validated batch 353 batch loss 6.09978056\n",
      "Validated batch 354 batch loss 6.07783\n",
      "Validated batch 355 batch loss 5.65508175\n",
      "Validated batch 356 batch loss 5.37457\n",
      "Validated batch 357 batch loss 4.97355843\n",
      "Validated batch 358 batch loss 7.45595169\n",
      "Validated batch 359 batch loss 6.78199434\n",
      "Validated batch 360 batch loss 6.71534538\n",
      "Validated batch 361 batch loss 6.4376421\n",
      "Validated batch 362 batch loss 6.18463135\n",
      "Validated batch 363 batch loss 5.13035297\n",
      "Validated batch 364 batch loss 6.99792099\n",
      "Validated batch 365 batch loss 5.65038061\n",
      "Validated batch 366 batch loss 5.92983532\n",
      "Validated batch 367 batch loss 6.34806919\n",
      "Validated batch 368 batch loss 6.79058361\n",
      "Validated batch 369 batch loss 6.88829613\n",
      "Validated batch 370 batch loss 4.99764109\n",
      "Validated batch 371 batch loss 6.5326314\n",
      "Validated batch 372 batch loss 7.00389814\n",
      "Validated batch 373 batch loss 5.66548777\n",
      "Validated batch 374 batch loss 5.17117214\n",
      "Validated batch 375 batch loss 6.73164845\n",
      "Validated batch 376 batch loss 6.42199421\n",
      "Validated batch 377 batch loss 6.38468456\n",
      "Validated batch 378 batch loss 6.42366028\n",
      "Validated batch 379 batch loss 6.00701523\n",
      "Validated batch 380 batch loss 5.73122787\n",
      "Validated batch 381 batch loss 5.72770882\n",
      "Validated batch 382 batch loss 6.31852865\n",
      "Validated batch 383 batch loss 5.97997284\n",
      "Validated batch 384 batch loss 5.93826866\n",
      "Validated batch 385 batch loss 5.50521803\n",
      "Validated batch 386 batch loss 5.84154892\n",
      "Validated batch 387 batch loss 5.97073889\n",
      "Validated batch 388 batch loss 6.7669\n",
      "Validated batch 389 batch loss 5.93999958\n",
      "Validated batch 390 batch loss 6.47257233\n",
      "Validated batch 391 batch loss 4.39965916\n",
      "Validated batch 392 batch loss 6.1483345\n",
      "Validated batch 393 batch loss 5.00076199\n",
      "Validated batch 394 batch loss 5.89727497\n",
      "Validated batch 395 batch loss 6.98264217\n",
      "Validated batch 396 batch loss 6.4254775\n",
      "Validated batch 397 batch loss 6.88526583\n",
      "Validated batch 398 batch loss 6.19082355\n",
      "Validated batch 399 batch loss 7.27271605\n",
      "Validated batch 400 batch loss 7.32538176\n",
      "Validated batch 401 batch loss 5.92906094\n",
      "Validated batch 402 batch loss 6.07263\n",
      "Validated batch 403 batch loss 6.02001667\n",
      "Validated batch 404 batch loss 5.94517517\n",
      "Validated batch 405 batch loss 6.87247801\n",
      "Validated batch 406 batch loss 6.17875\n",
      "Validated batch 407 batch loss 6.54281616\n",
      "Validated batch 408 batch loss 6.64460278\n",
      "Validated batch 409 batch loss 5.95803881\n",
      "Validated batch 410 batch loss 5.87146187\n",
      "Validated batch 411 batch loss 6.15094328\n",
      "Validated batch 412 batch loss 5.84173393\n",
      "Validated batch 413 batch loss 6.68597\n",
      "Validated batch 414 batch loss 5.48338223\n",
      "Validated batch 415 batch loss 6.31333256\n",
      "Validated batch 416 batch loss 6.43033361\n",
      "Validated batch 417 batch loss 6.57094336\n",
      "Validated batch 418 batch loss 6.30187\n",
      "Validated batch 419 batch loss 6.3044343\n",
      "Validated batch 420 batch loss 6.39006805\n",
      "Validated batch 421 batch loss 6.17709923\n",
      "Validated batch 422 batch loss 6.467\n",
      "Validated batch 423 batch loss 6.34187317\n",
      "Validated batch 424 batch loss 6.4723959\n",
      "Validated batch 425 batch loss 6.85187292\n",
      "Validated batch 426 batch loss 4.70231152\n",
      "Validated batch 427 batch loss 6.03489399\n",
      "Validated batch 428 batch loss 6.73695374\n",
      "Validated batch 429 batch loss 6.95529413\n",
      "Validated batch 430 batch loss 6.71305561\n",
      "Validated batch 431 batch loss 5.72002697\n",
      "Validated batch 432 batch loss 6.02401781\n",
      "Validated batch 433 batch loss 6.09295273\n",
      "Validated batch 434 batch loss 6.6052494\n",
      "Validated batch 435 batch loss 5.41763258\n",
      "Validated batch 436 batch loss 7.65359306\n",
      "Validated batch 437 batch loss 6.12985182\n",
      "Validated batch 438 batch loss 6.46267796\n",
      "Validated batch 439 batch loss 6.25187445\n",
      "Validated batch 440 batch loss 5.6733532\n",
      "Validated batch 441 batch loss 6.00920868\n",
      "Validated batch 442 batch loss 6.11958504\n",
      "Validated batch 443 batch loss 6.03880548\n",
      "Validated batch 444 batch loss 6.11257076\n",
      "Validated batch 445 batch loss 6.49490595\n",
      "Validated batch 446 batch loss 6.21291065\n",
      "Validated batch 447 batch loss 6.15678883\n",
      "Validated batch 448 batch loss 6.6831007\n",
      "Validated batch 449 batch loss 7.05506372\n",
      "Validated batch 450 batch loss 6.17568779\n",
      "Validated batch 451 batch loss 6.18175364\n",
      "Validated batch 452 batch loss 6.56404\n",
      "Validated batch 453 batch loss 6.2030015\n",
      "Validated batch 454 batch loss 6.94921541\n",
      "Validated batch 455 batch loss 6.45016527\n",
      "Validated batch 456 batch loss 7.07109594\n",
      "Validated batch 457 batch loss 6.8310585\n",
      "Validated batch 458 batch loss 6.02037191\n",
      "Validated batch 459 batch loss 7.44324732\n",
      "Validated batch 460 batch loss 6.13888884\n",
      "Validated batch 461 batch loss 5.92206478\n",
      "Validated batch 462 batch loss 6.54748631\n",
      "Validated batch 463 batch loss 5.7849946\n",
      "Validated batch 464 batch loss 5.5443635\n",
      "Validated batch 465 batch loss 5.54970169\n",
      "Validated batch 466 batch loss 5.16559601\n",
      "Validated batch 467 batch loss 5.73243189\n",
      "Validated batch 468 batch loss 5.60426474\n",
      "Validated batch 469 batch loss 6.88192797\n",
      "Validated batch 470 batch loss 6.51626968\n",
      "Validated batch 471 batch loss 6.12707\n",
      "Validated batch 472 batch loss 6.62358284\n",
      "Validated batch 473 batch loss 6.0094986\n",
      "Validated batch 474 batch loss 6.83536339\n",
      "Validated batch 475 batch loss 6.91242886\n",
      "Validated batch 476 batch loss 5.81097031\n",
      "Validated batch 477 batch loss 5.87408733\n",
      "Validated batch 478 batch loss 6.59146\n",
      "Validated batch 479 batch loss 6.1878047\n",
      "Validated batch 480 batch loss 6.38608742\n",
      "Validated batch 481 batch loss 6.30133581\n",
      "Validated batch 482 batch loss 6.52902269\n",
      "Validated batch 483 batch loss 5.57881927\n",
      "Validated batch 484 batch loss 5.8477335\n",
      "Validated batch 485 batch loss 6.43540764\n",
      "Validated batch 486 batch loss 5.55112076\n",
      "Validated batch 487 batch loss 5.55113316\n",
      "Validated batch 488 batch loss 6.20130253\n",
      "Validated batch 489 batch loss 6.95848799\n",
      "Validated batch 490 batch loss 5.44243622\n",
      "Validated batch 491 batch loss 5.94880676\n",
      "Validated batch 492 batch loss 6.68087959\n",
      "Validated batch 493 batch loss 6.95769787\n",
      "Validated batch 494 batch loss 5.79569\n",
      "Validated batch 495 batch loss 7.11235\n",
      "Validated batch 496 batch loss 5.0590353\n",
      "Validated batch 497 batch loss 6.32560158\n",
      "Validated batch 498 batch loss 6.00198174\n",
      "Validated batch 499 batch loss 6.94849968\n",
      "Validated batch 500 batch loss 6.50222635\n",
      "Validated batch 501 batch loss 6.22549629\n",
      "Validated batch 502 batch loss 7.25720406\n",
      "Validated batch 503 batch loss 7.50978565\n",
      "Validated batch 504 batch loss 6.18318081\n",
      "Validated batch 505 batch loss 5.50957489\n",
      "Validated batch 506 batch loss 5.20329857\n",
      "Validated batch 507 batch loss 5.57129765\n",
      "Validated batch 508 batch loss 5.56693745\n",
      "Validated batch 509 batch loss 5.14120674\n",
      "Validated batch 510 batch loss 5.91560936\n",
      "Validated batch 511 batch loss 5.22599125\n",
      "Validated batch 512 batch loss 5.52985573\n",
      "Validated batch 513 batch loss 6.73868179\n",
      "Validated batch 514 batch loss 6.639781\n",
      "Validated batch 515 batch loss 6.23065758\n",
      "Validated batch 516 batch loss 4.70017815\n",
      "Validated batch 517 batch loss 6.15353537\n",
      "Validated batch 518 batch loss 6.01946735\n",
      "Validated batch 519 batch loss 6.68536758\n",
      "Validated batch 520 batch loss 6.65782166\n",
      "Validated batch 521 batch loss 6.71175861\n",
      "Validated batch 522 batch loss 5.69782734\n",
      "Validated batch 523 batch loss 6.35612106\n",
      "Validated batch 524 batch loss 5.69579411\n",
      "Validated batch 525 batch loss 6.01481056\n",
      "Validated batch 526 batch loss 6.20895576\n",
      "Validated batch 527 batch loss 6.16058731\n",
      "Validated batch 528 batch loss 5.84981346\n",
      "Validated batch 529 batch loss 6.18409967\n",
      "Validated batch 530 batch loss 5.16194\n",
      "Validated batch 531 batch loss 5.69728947\n",
      "Validated batch 532 batch loss 5.93878794\n",
      "Validated batch 533 batch loss 6.35886383\n",
      "Validated batch 534 batch loss 6.20673132\n",
      "Validated batch 535 batch loss 5.74264765\n",
      "Validated batch 536 batch loss 6.88140774\n",
      "Validated batch 537 batch loss 6.87391901\n",
      "Validated batch 538 batch loss 7.45056725\n",
      "Validated batch 539 batch loss 5.92822695\n",
      "Validated batch 540 batch loss 7.03120375\n",
      "Validated batch 541 batch loss 6.89024\n",
      "Validated batch 542 batch loss 6.55513477\n",
      "Validated batch 543 batch loss 7.06336308\n",
      "Validated batch 544 batch loss 6.4095459\n",
      "Validated batch 545 batch loss 6.06512499\n",
      "Validated batch 546 batch loss 5.46785498\n",
      "Validated batch 547 batch loss 5.33195496\n",
      "Validated batch 548 batch loss 5.15487766\n",
      "Validated batch 549 batch loss 5.63104439\n",
      "Validated batch 550 batch loss 6.10999727\n",
      "Validated batch 551 batch loss 6.05182648\n",
      "Validated batch 552 batch loss 6.58872747\n",
      "Validated batch 553 batch loss 5.72110701\n",
      "Validated batch 554 batch loss 5.6427269\n",
      "Validated batch 555 batch loss 6.42083073\n",
      "Validated batch 556 batch loss 6.06563139\n",
      "Validated batch 557 batch loss 5.47856331\n",
      "Validated batch 558 batch loss 6.78290844\n",
      "Validated batch 559 batch loss 5.01001072\n",
      "Validated batch 560 batch loss 6.54503822\n",
      "Validated batch 561 batch loss 6.53679276\n",
      "Validated batch 562 batch loss 6.19339752\n",
      "Validated batch 563 batch loss 5.28943777\n",
      "Validated batch 564 batch loss 4.98513079\n",
      "Validated batch 565 batch loss 5.50327253\n",
      "Validated batch 566 batch loss 5.65458059\n",
      "Validated batch 567 batch loss 5.27916956\n",
      "Validated batch 568 batch loss 6.53007317\n",
      "Validated batch 569 batch loss 5.74778\n",
      "Validated batch 570 batch loss 5.7126646\n",
      "Validated batch 571 batch loss 6.56454706\n",
      "Validated batch 572 batch loss 5.9366641\n",
      "Validated batch 573 batch loss 6.53742838\n",
      "Validated batch 574 batch loss 6.58334\n",
      "Validated batch 575 batch loss 5.41996\n",
      "Validated batch 576 batch loss 6.10942841\n",
      "Validated batch 577 batch loss 5.8763032\n",
      "Validated batch 578 batch loss 5.76342249\n",
      "Validated batch 579 batch loss 6.09588528\n",
      "Validated batch 580 batch loss 6.1274457\n",
      "Validated batch 581 batch loss 6.17777157\n",
      "Validated batch 582 batch loss 6.48729849\n",
      "Validated batch 583 batch loss 5.04331303\n",
      "Validated batch 584 batch loss 5.68814659\n",
      "Validated batch 585 batch loss 6.21187\n",
      "Validated batch 586 batch loss 6.38025284\n",
      "Validated batch 587 batch loss 6.14016914\n",
      "Validated batch 588 batch loss 5.49403095\n",
      "Validated batch 589 batch loss 5.39075851\n",
      "Validated batch 590 batch loss 5.23259354\n",
      "Validated batch 591 batch loss 4.72661304\n",
      "Validated batch 592 batch loss 6.89228868\n",
      "Validated batch 593 batch loss 6.43619728\n",
      "Validated batch 594 batch loss 5.71157789\n",
      "Validated batch 595 batch loss 5.85659313\n",
      "Validated batch 596 batch loss 6.83171129\n",
      "Validated batch 597 batch loss 6.9021244\n",
      "Validated batch 598 batch loss 6.37126\n",
      "Validated batch 599 batch loss 6.43163061\n",
      "Validated batch 600 batch loss 6.19149399\n",
      "Validated batch 601 batch loss 6.07225323\n",
      "Validated batch 602 batch loss 5.71892643\n",
      "Validated batch 603 batch loss 5.53118467\n",
      "Validated batch 604 batch loss 5.76228857\n",
      "Validated batch 605 batch loss 6.99928856\n",
      "Validated batch 606 batch loss 6.17599773\n",
      "Validated batch 607 batch loss 6.35479355\n",
      "Validated batch 608 batch loss 6.37697\n",
      "Validated batch 609 batch loss 5.82465553\n",
      "Validated batch 610 batch loss 6.10521793\n",
      "Validated batch 611 batch loss 5.53851128\n",
      "Validated batch 612 batch loss 6.1225276\n",
      "Validated batch 613 batch loss 6.1704216\n",
      "Validated batch 614 batch loss 6.26860046\n",
      "Validated batch 615 batch loss 6.31494713\n",
      "Validated batch 616 batch loss 7.64559174\n",
      "Validated batch 617 batch loss 6.98555279\n",
      "Validated batch 618 batch loss 6.76573181\n",
      "Validated batch 619 batch loss 5.83051205\n",
      "Validated batch 620 batch loss 6.03731155\n",
      "Validated batch 621 batch loss 5.26123905\n",
      "Validated batch 622 batch loss 6.38815117\n",
      "Validated batch 623 batch loss 6.20876789\n",
      "Validated batch 624 batch loss 6.74177074\n",
      "Validated batch 625 batch loss 7.04195833\n",
      "Validated batch 626 batch loss 7.21737289\n",
      "Validated batch 627 batch loss 6.73754597\n",
      "Validated batch 628 batch loss 6.17542362\n",
      "Validated batch 629 batch loss 5.97802353\n",
      "Validated batch 630 batch loss 6.09010649\n",
      "Validated batch 631 batch loss 5.62345219\n",
      "Validated batch 632 batch loss 5.25894928\n",
      "Validated batch 633 batch loss 6.57989597\n",
      "Validated batch 634 batch loss 6.3999424\n",
      "Validated batch 635 batch loss 6.46948242\n",
      "Validated batch 636 batch loss 5.29343224\n",
      "Validated batch 637 batch loss 6.99777603\n",
      "Validated batch 638 batch loss 5.46715879\n",
      "Validated batch 639 batch loss 5.61501503\n",
      "Validated batch 640 batch loss 5.54409885\n",
      "Validated batch 641 batch loss 6.75660658\n",
      "Validated batch 642 batch loss 6.0116272\n",
      "Validated batch 643 batch loss 6.06226\n",
      "Validated batch 644 batch loss 5.92247\n",
      "Validated batch 645 batch loss 6.89249325\n",
      "Validated batch 646 batch loss 7.2333312\n",
      "Validated batch 647 batch loss 6.7840457\n",
      "Validated batch 648 batch loss 5.36996841\n",
      "Validated batch 649 batch loss 5.52617\n",
      "Validated batch 650 batch loss 6.26922798\n",
      "Validated batch 651 batch loss 6.60842609\n",
      "Validated batch 652 batch loss 6.58640289\n",
      "Validated batch 653 batch loss 5.63820314\n",
      "Validated batch 654 batch loss 6.03788853\n",
      "Validated batch 655 batch loss 5.14699316\n",
      "Validated batch 656 batch loss 6.38249207\n",
      "Validated batch 657 batch loss 6.9069252\n",
      "Validated batch 658 batch loss 6.79987192\n",
      "Validated batch 659 batch loss 6.80916262\n",
      "Validated batch 660 batch loss 6.26447725\n",
      "Validated batch 661 batch loss 6.44311428\n",
      "Validated batch 662 batch loss 6.03273535\n",
      "Validated batch 663 batch loss 6.11404848\n",
      "Validated batch 664 batch loss 6.30810213\n",
      "Validated batch 665 batch loss 6.83778477\n",
      "Validated batch 666 batch loss 6.57595062\n",
      "Validated batch 667 batch loss 6.31957865\n",
      "Validated batch 668 batch loss 6.59756613\n",
      "Validated batch 669 batch loss 6.18865108\n",
      "Validated batch 670 batch loss 6.04184628\n",
      "Validated batch 671 batch loss 6.54771662\n",
      "Validated batch 672 batch loss 6.3954134\n",
      "Validated batch 673 batch loss 5.37027\n",
      "Validated batch 674 batch loss 6.73213434\n",
      "Validated batch 675 batch loss 6.52034616\n",
      "Validated batch 676 batch loss 5.93145466\n",
      "Validated batch 677 batch loss 6.73871565\n",
      "Validated batch 678 batch loss 6.48658657\n",
      "Validated batch 679 batch loss 6.14437866\n",
      "Validated batch 680 batch loss 5.23256397\n",
      "Validated batch 681 batch loss 5.74496031\n",
      "Validated batch 682 batch loss 6.75120163\n",
      "Validated batch 683 batch loss 6.50762177\n",
      "Validated batch 684 batch loss 6.33071041\n",
      "Validated batch 685 batch loss 5.46479416\n",
      "Validated batch 686 batch loss 6.01361275\n",
      "Validated batch 687 batch loss 6.23646355\n",
      "Validated batch 688 batch loss 6.25215\n",
      "Validated batch 689 batch loss 6.60973835\n",
      "Validated batch 690 batch loss 5.51290751\n",
      "Validated batch 691 batch loss 6.59328842\n",
      "Validated batch 692 batch loss 5.65535212\n",
      "Validated batch 693 batch loss 4.44220734\n",
      "Validated batch 694 batch loss 5.31909466\n",
      "Validated batch 695 batch loss 5.44527817\n",
      "Validated batch 696 batch loss 5.84691525\n",
      "Validated batch 697 batch loss 6.1605072\n",
      "Validated batch 698 batch loss 5.73512411\n",
      "Validated batch 699 batch loss 6.45131636\n",
      "Validated batch 700 batch loss 5.75765419\n",
      "Validated batch 701 batch loss 5.83405733\n",
      "Validated batch 702 batch loss 6.14833927\n",
      "Validated batch 703 batch loss 6.45531321\n",
      "Validated batch 704 batch loss 5.87443829\n",
      "Validated batch 705 batch loss 6.19055\n",
      "Validated batch 706 batch loss 6.18739223\n",
      "Validated batch 707 batch loss 6.38028622\n",
      "Validated batch 708 batch loss 6.01766109\n",
      "Validated batch 709 batch loss 5.99193335\n",
      "Validated batch 710 batch loss 6.20978975\n",
      "Validated batch 711 batch loss 6.58513641\n",
      "Validated batch 712 batch loss 6.2860775\n",
      "Validated batch 713 batch loss 6.31073093\n",
      "Validated batch 714 batch loss 6.15989733\n",
      "Validated batch 715 batch loss 6.97999334\n",
      "Validated batch 716 batch loss 6.17201424\n",
      "Validated batch 717 batch loss 5.57170582\n",
      "Validated batch 718 batch loss 6.64548683\n",
      "Validated batch 719 batch loss 5.51222897\n",
      "Validated batch 720 batch loss 6.09623337\n",
      "Validated batch 721 batch loss 6.02642822\n",
      "Validated batch 722 batch loss 5.67069626\n",
      "Validated batch 723 batch loss 6.25173\n",
      "Validated batch 724 batch loss 6.04299498\n",
      "Validated batch 725 batch loss 6.35567284\n",
      "Validated batch 726 batch loss 6.49380732\n",
      "Validated batch 727 batch loss 6.00562191\n",
      "Validated batch 728 batch loss 6.25160789\n",
      "Validated batch 729 batch loss 6.78809547\n",
      "Validated batch 730 batch loss 6.2203927\n",
      "Validated batch 731 batch loss 5.48977\n",
      "Validated batch 732 batch loss 6.39810848\n",
      "Validated batch 733 batch loss 5.90684462\n",
      "Validated batch 734 batch loss 5.19278717\n",
      "Validated batch 735 batch loss 5.39597225\n",
      "Validated batch 736 batch loss 5.63004541\n",
      "Validated batch 737 batch loss 6.768116\n",
      "Validated batch 738 batch loss 6.5414443\n",
      "Validated batch 739 batch loss 7.15031385\n",
      "Validated batch 740 batch loss 5.927495\n",
      "Validated batch 741 batch loss 6.836483\n",
      "Validated batch 742 batch loss 7.14336395\n",
      "Validated batch 743 batch loss 6.44451046\n",
      "Validated batch 744 batch loss 6.96275854\n",
      "Validated batch 745 batch loss 6.66180038\n",
      "Validated batch 746 batch loss 5.83617306\n",
      "Validated batch 747 batch loss 5.69515324\n",
      "Validated batch 748 batch loss 6.01314259\n",
      "Validated batch 749 batch loss 5.56252098\n",
      "Validated batch 750 batch loss 6.1583004\n",
      "Validated batch 751 batch loss 5.91215229\n",
      "Validated batch 752 batch loss 6.76727676\n",
      "Validated batch 753 batch loss 6.86871052\n",
      "Validated batch 754 batch loss 6.51499414\n",
      "Validated batch 755 batch loss 5.42733955\n",
      "Validated batch 756 batch loss 6.80856609\n",
      "Validated batch 757 batch loss 6.5286932\n",
      "Validated batch 758 batch loss 5.39182901\n",
      "Validated batch 759 batch loss 5.80684376\n",
      "Validated batch 760 batch loss 6.23464489\n",
      "Validated batch 761 batch loss 6.3906579\n",
      "Validated batch 762 batch loss 5.97600937\n",
      "Validated batch 763 batch loss 5.91579199\n",
      "Validated batch 764 batch loss 5.11604214\n",
      "Validated batch 765 batch loss 5.97506857\n",
      "Validated batch 766 batch loss 6.27976894\n",
      "Validated batch 767 batch loss 6.25950718\n",
      "Validated batch 768 batch loss 6.05476904\n",
      "Validated batch 769 batch loss 6.30159235\n",
      "Validated batch 770 batch loss 6.79322529\n",
      "Validated batch 771 batch loss 5.67433167\n",
      "Validated batch 772 batch loss 5.50386524\n",
      "Validated batch 773 batch loss 5.71532917\n",
      "Validated batch 774 batch loss 5.88310337\n",
      "Validated batch 775 batch loss 6.08568621\n",
      "Validated batch 776 batch loss 6.38309717\n",
      "Validated batch 777 batch loss 6.19611931\n",
      "Validated batch 778 batch loss 6.33087158\n",
      "Validated batch 779 batch loss 7.11546087\n",
      "Validated batch 780 batch loss 6.80272865\n",
      "Validated batch 781 batch loss 6.36576462\n",
      "Validated batch 782 batch loss 6.39885044\n",
      "Validated batch 783 batch loss 5.72966909\n",
      "Validated batch 784 batch loss 6.4789257\n",
      "Validated batch 785 batch loss 5.52449894\n",
      "Validated batch 786 batch loss 5.97524548\n",
      "Validated batch 787 batch loss 6.34252644\n",
      "Validated batch 788 batch loss 6.51226616\n",
      "Validated batch 789 batch loss 6.30439949\n",
      "Validated batch 790 batch loss 6.3550725\n",
      "Validated batch 791 batch loss 5.44438362\n",
      "Validated batch 792 batch loss 6.12382412\n",
      "Validated batch 793 batch loss 6.65563679\n",
      "Validated batch 794 batch loss 6.34364319\n",
      "Validated batch 795 batch loss 5.4379158\n",
      "Validated batch 796 batch loss 5.87622213\n",
      "Validated batch 797 batch loss 6.03349113\n",
      "Validated batch 798 batch loss 5.13086939\n",
      "Validated batch 799 batch loss 5.1726532\n",
      "Validated batch 800 batch loss 6.20837784\n",
      "Validated batch 801 batch loss 4.98607063\n",
      "Validated batch 802 batch loss 5.76081181\n",
      "Validated batch 803 batch loss 5.53970432\n",
      "Validated batch 804 batch loss 5.85023451\n",
      "Validated batch 805 batch loss 5.54992294\n",
      "Validated batch 806 batch loss 5.63259029\n",
      "Validated batch 807 batch loss 6.17588139\n",
      "Validated batch 808 batch loss 6.28962421\n",
      "Validated batch 809 batch loss 7.02636242\n",
      "Validated batch 810 batch loss 6.34294939\n",
      "Validated batch 811 batch loss 5.71996641\n",
      "Validated batch 812 batch loss 6.1473484\n",
      "Validated batch 813 batch loss 7.20324278\n",
      "Validated batch 814 batch loss 5.05958271\n",
      "Validated batch 815 batch loss 6.00813723\n",
      "Validated batch 816 batch loss 5.27213097\n",
      "Validated batch 817 batch loss 6.230762\n",
      "Validated batch 818 batch loss 6.51767635\n",
      "Validated batch 819 batch loss 6.3518281\n",
      "Validated batch 820 batch loss 6.45830059\n",
      "Validated batch 821 batch loss 5.08956194\n",
      "Validated batch 822 batch loss 6.52203417\n",
      "Validated batch 823 batch loss 6.54898167\n",
      "Validated batch 824 batch loss 6.41557121\n",
      "Validated batch 825 batch loss 6.11144352\n",
      "Validated batch 826 batch loss 6.61815786\n",
      "Validated batch 827 batch loss 6.26545668\n",
      "Validated batch 828 batch loss 5.74402761\n",
      "Validated batch 829 batch loss 6.23890352\n",
      "Validated batch 830 batch loss 5.65417814\n",
      "Validated batch 831 batch loss 6.27553558\n",
      "Validated batch 832 batch loss 6.82430172\n",
      "Validated batch 833 batch loss 4.86405373\n",
      "Validated batch 834 batch loss 5.82854939\n",
      "Validated batch 835 batch loss 7.25162029\n",
      "Validated batch 836 batch loss 6.14775181\n",
      "Validated batch 837 batch loss 6.13863134\n",
      "Validated batch 838 batch loss 5.88716125\n",
      "Validated batch 839 batch loss 6.44017506\n",
      "Validated batch 840 batch loss 5.45413351\n",
      "Validated batch 841 batch loss 5.28646\n",
      "Validated batch 842 batch loss 5.70925188\n",
      "Validated batch 843 batch loss 5.99078083\n",
      "Validated batch 844 batch loss 5.16433525\n",
      "Validated batch 845 batch loss 5.99929619\n",
      "Validated batch 846 batch loss 5.52309132\n",
      "Validated batch 847 batch loss 5.9333334\n",
      "Validated batch 848 batch loss 5.67942286\n",
      "Validated batch 849 batch loss 5.70073032\n",
      "Validated batch 850 batch loss 6.60464382\n",
      "Validated batch 851 batch loss 6.50115204\n",
      "Validated batch 852 batch loss 5.92690659\n",
      "Validated batch 853 batch loss 6.63331413\n",
      "Validated batch 854 batch loss 6.08763361\n",
      "Validated batch 855 batch loss 4.83626\n",
      "Validated batch 856 batch loss 5.71183825\n",
      "Validated batch 857 batch loss 6.17525864\n",
      "Validated batch 858 batch loss 6.07858801\n",
      "Validated batch 859 batch loss 7.05154037\n",
      "Validated batch 860 batch loss 6.27594948\n",
      "Validated batch 861 batch loss 5.80863857\n",
      "Validated batch 862 batch loss 6.1582222\n",
      "Validated batch 863 batch loss 6.56188583\n",
      "Validated batch 864 batch loss 6.49459314\n",
      "Validated batch 865 batch loss 5.40566874\n",
      "Validated batch 866 batch loss 6.18314934\n",
      "Validated batch 867 batch loss 6.21382141\n",
      "Validated batch 868 batch loss 6.31224489\n",
      "Validated batch 869 batch loss 6.80657387\n",
      "Validated batch 870 batch loss 5.75204182\n",
      "Validated batch 871 batch loss 5.82897758\n",
      "Validated batch 872 batch loss 6.21967268\n",
      "Validated batch 873 batch loss 6.3703537\n",
      "Validated batch 874 batch loss 5.50002241\n",
      "Validated batch 875 batch loss 6.43831825\n",
      "Validated batch 876 batch loss 6.66874552\n",
      "Validated batch 877 batch loss 6.73013067\n",
      "Validated batch 878 batch loss 6.52736378\n",
      "Validated batch 879 batch loss 6.15644264\n",
      "Validated batch 880 batch loss 6.44066048\n",
      "Validated batch 881 batch loss 6.64909744\n",
      "Validated batch 882 batch loss 6.69345522\n",
      "Validated batch 883 batch loss 6.57039833\n",
      "Validated batch 884 batch loss 6.3550415\n",
      "Validated batch 885 batch loss 6.85350418\n",
      "Validated batch 886 batch loss 6.67187595\n",
      "Validated batch 887 batch loss 6.99287224\n",
      "Validated batch 888 batch loss 6.53204727\n",
      "Validated batch 889 batch loss 6.88789654\n",
      "Validated batch 890 batch loss 7.16771793\n",
      "Validated batch 891 batch loss 7.65048313\n",
      "Validated batch 892 batch loss 6.02705383\n",
      "Validated batch 893 batch loss 7.11356306\n",
      "Validated batch 894 batch loss 7.44180202\n",
      "Validated batch 895 batch loss 7.69635582\n",
      "Validated batch 896 batch loss 6.59701633\n",
      "Validated batch 897 batch loss 7.71929932\n",
      "Validated batch 898 batch loss 6.66379213\n",
      "Validated batch 899 batch loss 6.88083076\n",
      "Validated batch 900 batch loss 7.6770153\n",
      "Validated batch 901 batch loss 6.57617474\n",
      "Validated batch 902 batch loss 6.24915457\n",
      "Validated batch 903 batch loss 6.12434435\n",
      "Validated batch 904 batch loss 6.39845848\n",
      "Validated batch 905 batch loss 5.60104084\n",
      "Validated batch 906 batch loss 6.46085\n",
      "Validated batch 907 batch loss 5.01940918\n",
      "Validated batch 908 batch loss 6.45732546\n",
      "Validated batch 909 batch loss 6.12526321\n",
      "Validated batch 910 batch loss 6.25880051\n",
      "Validated batch 911 batch loss 6.81821156\n",
      "Validated batch 912 batch loss 5.86627579\n",
      "Validated batch 913 batch loss 6.81455851\n",
      "Validated batch 914 batch loss 6.19630432\n",
      "Validated batch 915 batch loss 4.43832254\n",
      "Validated batch 916 batch loss 5.7407589\n",
      "Validated batch 917 batch loss 5.49479961\n",
      "Validated batch 918 batch loss 5.0268364\n",
      "Validated batch 919 batch loss 6.5065341\n",
      "Validated batch 920 batch loss 6.16528797\n",
      "Validated batch 921 batch loss 5.76502275\n",
      "Validated batch 922 batch loss 5.3505435\n",
      "Validated batch 923 batch loss 5.19206619\n",
      "Validated batch 924 batch loss 7.0098505\n",
      "Validated batch 925 batch loss 6.06463909\n",
      "Validated batch 926 batch loss 5.07177401\n",
      "Validated batch 927 batch loss 5.39504623\n",
      "Validated batch 928 batch loss 4.43752575\n",
      "Validated batch 929 batch loss 4.98040915\n",
      "Validated batch 930 batch loss 7.38557577\n",
      "Validated batch 931 batch loss 6.2857151\n",
      "Validated batch 932 batch loss 5.50815916\n",
      "Validated batch 933 batch loss 6.36280918\n",
      "Validated batch 934 batch loss 7.40242672\n",
      "Validated batch 935 batch loss 6.31225443\n",
      "Validated batch 936 batch loss 6.45999622\n",
      "Validated batch 937 batch loss 6.02885246\n",
      "Validated batch 938 batch loss 6.4082346\n",
      "Validated batch 939 batch loss 5.88469696\n",
      "Validated batch 940 batch loss 7.0701828\n",
      "Validated batch 941 batch loss 6.735919\n",
      "Validated batch 942 batch loss 5.65826416\n",
      "Validated batch 943 batch loss 5.63849926\n",
      "Validated batch 944 batch loss 5.63733101\n",
      "Validated batch 945 batch loss 6.15307331\n",
      "Validated batch 946 batch loss 5.59966946\n",
      "Validated batch 947 batch loss 7.38157415\n",
      "Validated batch 948 batch loss 6.5397892\n",
      "Validated batch 949 batch loss 6.65379524\n",
      "Validated batch 950 batch loss 6.92315\n",
      "Validated batch 951 batch loss 5.61351967\n",
      "Validated batch 952 batch loss 5.00762177\n",
      "Validated batch 953 batch loss 6.48192453\n",
      "Validated batch 954 batch loss 5.93532705\n",
      "Validated batch 955 batch loss 6.86464119\n",
      "Validated batch 956 batch loss 5.51991892\n",
      "Validated batch 957 batch loss 6.19218397\n",
      "Validated batch 958 batch loss 6.1369276\n",
      "Validated batch 959 batch loss 6.60438633\n",
      "Validated batch 960 batch loss 5.98746586\n",
      "Validated batch 961 batch loss 6.24041367\n",
      "Validated batch 962 batch loss 6.43475819\n",
      "Validated batch 963 batch loss 7.51846123\n",
      "Validated batch 964 batch loss 6.7968092\n",
      "Validated batch 965 batch loss 6.56759548\n",
      "Validated batch 966 batch loss 6.02173758\n",
      "Validated batch 967 batch loss 6.78246117\n",
      "Validated batch 968 batch loss 6.51819801\n",
      "Validated batch 969 batch loss 6.23316908\n",
      "Validated batch 970 batch loss 6.54108334\n",
      "Validated batch 971 batch loss 5.9134903\n",
      "Validated batch 972 batch loss 6.66603374\n",
      "Validated batch 973 batch loss 5.34793758\n",
      "Validated batch 974 batch loss 4.68999624\n",
      "Validated batch 975 batch loss 5.66670418\n",
      "Validated batch 976 batch loss 5.94931459\n",
      "Validated batch 977 batch loss 5.52455568\n",
      "Validated batch 978 batch loss 6.87830591\n",
      "Validated batch 979 batch loss 6.76836109\n",
      "Validated batch 980 batch loss 6.02186823\n",
      "Validated batch 981 batch loss 6.48498821\n",
      "Validated batch 982 batch loss 5.87904739\n",
      "Validated batch 983 batch loss 6.85971642\n",
      "Validated batch 984 batch loss 6.44493294\n",
      "Validated batch 985 batch loss 6.32454205\n",
      "Validated batch 986 batch loss 6.01719666\n",
      "Validated batch 987 batch loss 6.13096476\n",
      "Validated batch 988 batch loss 6.46047926\n",
      "Validated batch 989 batch loss 6.18221951\n",
      "Validated batch 990 batch loss 5.40617\n",
      "Validated batch 991 batch loss 5.48872232\n",
      "Validated batch 992 batch loss 5.57632589\n",
      "Validated batch 993 batch loss 5.7604847\n",
      "Validated batch 994 batch loss 6.1500864\n",
      "Validated batch 995 batch loss 6.62930775\n",
      "Validated batch 996 batch loss 5.57690573\n",
      "Validated batch 997 batch loss 6.59479332\n",
      "Validated batch 998 batch loss 7.50653553\n",
      "Validated batch 999 batch loss 5.78932285\n",
      "Validated batch 1000 batch loss 6.13616467\n",
      "Validated batch 1001 batch loss 6.26955128\n",
      "Validated batch 1002 batch loss 6.50817871\n",
      "Validated batch 1003 batch loss 5.72327805\n",
      "Validated batch 1004 batch loss 6.4185729\n",
      "Validated batch 1005 batch loss 6.1450758\n",
      "Validated batch 1006 batch loss 5.95530033\n",
      "Validated batch 1007 batch loss 6.970994\n",
      "Validated batch 1008 batch loss 6.29188538\n",
      "Validated batch 1009 batch loss 6.46515656\n",
      "Validated batch 1010 batch loss 6.46096563\n",
      "Validated batch 1011 batch loss 5.2794528\n",
      "Validated batch 1012 batch loss 6.09234238\n",
      "Validated batch 1013 batch loss 5.4093585\n",
      "Validated batch 1014 batch loss 6.12915039\n",
      "Validated batch 1015 batch loss 5.78698158\n",
      "Validated batch 1016 batch loss 5.47491264\n",
      "Validated batch 1017 batch loss 5.32203484\n",
      "Validated batch 1018 batch loss 4.85817385\n",
      "Validated batch 1019 batch loss 4.52677965\n",
      "Validated batch 1020 batch loss 4.48427248\n",
      "Validated batch 1021 batch loss 5.06088114\n",
      "Validated batch 1022 batch loss 4.27987385\n",
      "Validated batch 1023 batch loss 6.572474\n",
      "Validated batch 1024 batch loss 6.48159409\n",
      "Validated batch 1025 batch loss 6.27324104\n",
      "Validated batch 1026 batch loss 6.26425791\n",
      "Validated batch 1027 batch loss 6.58579731\n",
      "Validated batch 1028 batch loss 6.38920259\n",
      "Validated batch 1029 batch loss 6.28638697\n",
      "Validated batch 1030 batch loss 5.96866894\n",
      "Validated batch 1031 batch loss 6.25943089\n",
      "Validated batch 1032 batch loss 6.26788521\n",
      "Validated batch 1033 batch loss 6.49772263\n",
      "Validated batch 1034 batch loss 6.20434141\n",
      "Validated batch 1035 batch loss 6.06647873\n",
      "Validated batch 1036 batch loss 6.16149426\n",
      "Validated batch 1037 batch loss 6.41076\n",
      "Validated batch 1038 batch loss 5.90418816\n",
      "Validated batch 1039 batch loss 5.93728065\n",
      "Validated batch 1040 batch loss 6.19654846\n",
      "Validated batch 1041 batch loss 6.57783031\n",
      "Validated batch 1042 batch loss 6.61739254\n",
      "Validated batch 1043 batch loss 5.99947309\n",
      "Validated batch 1044 batch loss 5.88110971\n",
      "Validated batch 1045 batch loss 6.36234\n",
      "Validated batch 1046 batch loss 5.66938114\n",
      "Validated batch 1047 batch loss 6.22904\n",
      "Validated batch 1048 batch loss 5.94568157\n",
      "Validated batch 1049 batch loss 5.34394741\n",
      "Validated batch 1050 batch loss 5.94518089\n",
      "Validated batch 1051 batch loss 6.8854475\n",
      "Validated batch 1052 batch loss 6.91074753\n",
      "Validated batch 1053 batch loss 5.8130765\n",
      "Validated batch 1054 batch loss 6.73841715\n",
      "Validated batch 1055 batch loss 6.34161282\n",
      "Validated batch 1056 batch loss 5.96305847\n",
      "Validated batch 1057 batch loss 5.85601568\n",
      "Validated batch 1058 batch loss 5.99952221\n",
      "Validated batch 1059 batch loss 6.98368\n",
      "Validated batch 1060 batch loss 6.69369698\n",
      "Validated batch 1061 batch loss 5.43823051\n",
      "Validated batch 1062 batch loss 5.13211441\n",
      "Validated batch 1063 batch loss 5.519804\n",
      "Validated batch 1064 batch loss 6.32844114\n",
      "Validated batch 1065 batch loss 6.41251898\n",
      "Validated batch 1066 batch loss 5.67487335\n",
      "Validated batch 1067 batch loss 7.5911622\n",
      "Validated batch 1068 batch loss 6.70089626\n",
      "Validated batch 1069 batch loss 6.0693922\n",
      "Validated batch 1070 batch loss 6.00836754\n",
      "Validated batch 1071 batch loss 5.57476473\n",
      "Validated batch 1072 batch loss 5.53809357\n",
      "Validated batch 1073 batch loss 5.86049175\n",
      "Validated batch 1074 batch loss 6.36465168\n",
      "Validated batch 1075 batch loss 6.85240841\n",
      "Validated batch 1076 batch loss 6.65734434\n",
      "Validated batch 1077 batch loss 6.7232008\n",
      "Validated batch 1078 batch loss 6.69526434\n",
      "Validated batch 1079 batch loss 6.52253866\n",
      "Validated batch 1080 batch loss 6.39901304\n",
      "Validated batch 1081 batch loss 6.15977\n",
      "Validated batch 1082 batch loss 6.17623043\n",
      "Validated batch 1083 batch loss 6.41048813\n",
      "Validated batch 1084 batch loss 6.11668\n",
      "Validated batch 1085 batch loss 6.4644289\n",
      "Validated batch 1086 batch loss 5.94464779\n",
      "Validated batch 1087 batch loss 6.41365242\n",
      "Validated batch 1088 batch loss 6.39586067\n",
      "Validated batch 1089 batch loss 6.59936333\n",
      "Validated batch 1090 batch loss 6.98326111\n",
      "Validated batch 1091 batch loss 6.30794811\n",
      "Validated batch 1092 batch loss 6.15542412\n",
      "Validated batch 1093 batch loss 6.61411572\n",
      "Validated batch 1094 batch loss 5.94262791\n",
      "Validated batch 1095 batch loss 5.85279\n",
      "Validated batch 1096 batch loss 6.17455721\n",
      "Validated batch 1097 batch loss 5.55570221\n",
      "Validated batch 1098 batch loss 6.24078894\n",
      "Validated batch 1099 batch loss 5.78078508\n",
      "Validated batch 1100 batch loss 4.92289352\n",
      "Validated batch 1101 batch loss 5.59860611\n",
      "Validated batch 1102 batch loss 5.3367219\n",
      "Validated batch 1103 batch loss 5.54758263\n",
      "Validated batch 1104 batch loss 6.19787264\n",
      "Validated batch 1105 batch loss 6.40149069\n",
      "Validated batch 1106 batch loss 6.32778358\n",
      "Validated batch 1107 batch loss 5.66734838\n",
      "Validated batch 1108 batch loss 6.47549\n",
      "Validated batch 1109 batch loss 5.89126444\n",
      "Validated batch 1110 batch loss 6.59556437\n",
      "Validated batch 1111 batch loss 6.64018631\n",
      "Validated batch 1112 batch loss 5.87024879\n",
      "Validated batch 1113 batch loss 6.03678894\n",
      "Validated batch 1114 batch loss 5.78889\n",
      "Validated batch 1115 batch loss 5.81931782\n",
      "Validated batch 1116 batch loss 6.70762205\n",
      "Validated batch 1117 batch loss 5.98798943\n",
      "Validated batch 1118 batch loss 5.52919197\n",
      "Validated batch 1119 batch loss 5.93434\n",
      "Validated batch 1120 batch loss 5.39625454\n",
      "Validated batch 1121 batch loss 6.73714447\n",
      "Validated batch 1122 batch loss 6.52153397\n",
      "Validated batch 1123 batch loss 6.18937969\n",
      "Validated batch 1124 batch loss 5.65307236\n",
      "Validated batch 1125 batch loss 6.56698608\n",
      "Validated batch 1126 batch loss 5.77116203\n",
      "Validated batch 1127 batch loss 5.89259815\n",
      "Validated batch 1128 batch loss 5.21231318\n",
      "Validated batch 1129 batch loss 6.08861065\n",
      "Validated batch 1130 batch loss 5.32843208\n",
      "Validated batch 1131 batch loss 5.69763374\n",
      "Validated batch 1132 batch loss 6.13382721\n",
      "Validated batch 1133 batch loss 6.38470554\n",
      "Validated batch 1134 batch loss 5.51632\n",
      "Validated batch 1135 batch loss 6.52928686\n",
      "Validated batch 1136 batch loss 5.16386843\n",
      "Validated batch 1137 batch loss 5.2227\n",
      "Validated batch 1138 batch loss 6.39312649\n",
      "Validated batch 1139 batch loss 6.12415123\n",
      "Validated batch 1140 batch loss 5.90358305\n",
      "Validated batch 1141 batch loss 6.31768\n",
      "Validated batch 1142 batch loss 5.9011755\n",
      "Validated batch 1143 batch loss 5.71557045\n",
      "Validated batch 1144 batch loss 5.97885132\n",
      "Validated batch 1145 batch loss 6.84269905\n",
      "Validated batch 1146 batch loss 6.57188225\n",
      "Validated batch 1147 batch loss 6.08135223\n",
      "Validated batch 1148 batch loss 6.14753628\n",
      "Validated batch 1149 batch loss 5.6520443\n",
      "Validated batch 1150 batch loss 5.48534727\n",
      "Validated batch 1151 batch loss 6.24389076\n",
      "Validated batch 1152 batch loss 6.14592361\n",
      "Validated batch 1153 batch loss 6.39747143\n",
      "Validated batch 1154 batch loss 6.33845425\n",
      "Validated batch 1155 batch loss 6.70873451\n",
      "Validated batch 1156 batch loss 5.94435835\n",
      "Validated batch 1157 batch loss 5.58848381\n",
      "Validated batch 1158 batch loss 5.67434454\n",
      "Validated batch 1159 batch loss 4.81954193\n",
      "Validated batch 1160 batch loss 6.59974384\n",
      "Validated batch 1161 batch loss 7.62670612\n",
      "Validated batch 1162 batch loss 6.26417303\n",
      "Validated batch 1163 batch loss 5.20209694\n",
      "Validated batch 1164 batch loss 6.47224236\n",
      "Validated batch 1165 batch loss 6.28581619\n",
      "Validated batch 1166 batch loss 5.88668871\n",
      "Validated batch 1167 batch loss 5.93157196\n",
      "Validated batch 1168 batch loss 6.06951809\n",
      "Validated batch 1169 batch loss 6.30550671\n",
      "Validated batch 1170 batch loss 5.25880527\n",
      "Validated batch 1171 batch loss 6.44964027\n",
      "Validated batch 1172 batch loss 6.78829241\n",
      "Validated batch 1173 batch loss 5.95813274\n",
      "Validated batch 1174 batch loss 6.54700947\n",
      "Validated batch 1175 batch loss 5.75233841\n",
      "Validated batch 1176 batch loss 6.20631504\n",
      "Validated batch 1177 batch loss 4.67950773\n",
      "Validated batch 1178 batch loss 5.97946644\n",
      "Validated batch 1179 batch loss 6.50128\n",
      "Validated batch 1180 batch loss 6.01090145\n",
      "Validated batch 1181 batch loss 6.05368423\n",
      "Validated batch 1182 batch loss 5.8464241\n",
      "Validated batch 1183 batch loss 5.74105215\n",
      "Validated batch 1184 batch loss 6.53552151\n",
      "Validated batch 1185 batch loss 6.86508226\n",
      "Validated batch 1186 batch loss 7.33312893\n",
      "Validated batch 1187 batch loss 7.65172625\n",
      "Validated batch 1188 batch loss 6.34272623\n",
      "Validated batch 1189 batch loss 5.8197546\n",
      "Validated batch 1190 batch loss 6.36973238\n",
      "Validated batch 1191 batch loss 6.22973633\n",
      "Validated batch 1192 batch loss 5.99005127\n",
      "Validated batch 1193 batch loss 5.94086552\n",
      "Validated batch 1194 batch loss 5.74435043\n",
      "Validated batch 1195 batch loss 6.59877491\n",
      "Validated batch 1196 batch loss 6.95368099\n",
      "Validated batch 1197 batch loss 7.13877726\n",
      "Validated batch 1198 batch loss 6.22153473\n",
      "Validated batch 1199 batch loss 5.27647591\n",
      "Validated batch 1200 batch loss 5.63109875\n",
      "Validated batch 1201 batch loss 5.69586134\n",
      "Validated batch 1202 batch loss 6.36582613\n",
      "Validated batch 1203 batch loss 5.42703247\n",
      "Validated batch 1204 batch loss 6.27528191\n",
      "Validated batch 1205 batch loss 4.8747468\n",
      "Validated batch 1206 batch loss 6.2687149\n",
      "Validated batch 1207 batch loss 6.30010653\n",
      "Validated batch 1208 batch loss 5.9910841\n",
      "Validated batch 1209 batch loss 7.08750725\n",
      "Validated batch 1210 batch loss 6.17853832\n",
      "Validated batch 1211 batch loss 6.42073441\n",
      "Validated batch 1212 batch loss 6.29145145\n",
      "Validated batch 1213 batch loss 5.6205759\n",
      "Validated batch 1214 batch loss 5.73822737\n",
      "Validated batch 1215 batch loss 5.89882565\n",
      "Validated batch 1216 batch loss 6.22093391\n",
      "Validated batch 1217 batch loss 6.0964179\n",
      "Validated batch 1218 batch loss 6.25806713\n",
      "Validated batch 1219 batch loss 6.31137037\n",
      "Validated batch 1220 batch loss 6.57219362\n",
      "Validated batch 1221 batch loss 6.1764\n",
      "Validated batch 1222 batch loss 5.87929487\n",
      "Validated batch 1223 batch loss 6.42983\n",
      "Validated batch 1224 batch loss 6.10589123\n",
      "Validated batch 1225 batch loss 6.43083811\n",
      "Validated batch 1226 batch loss 5.78163719\n",
      "Validated batch 1227 batch loss 6.67635679\n",
      "Validated batch 1228 batch loss 6.44641542\n",
      "Validated batch 1229 batch loss 5.02647734\n",
      "Validated batch 1230 batch loss 5.77317476\n",
      "Validated batch 1231 batch loss 6.16821194\n",
      "Validated batch 1232 batch loss 5.50354099\n",
      "Validated batch 1233 batch loss 6.85399055\n",
      "Validated batch 1234 batch loss 6.68035603\n",
      "Validated batch 1235 batch loss 6.53309059\n",
      "Validated batch 1236 batch loss 7.11746454\n",
      "Validated batch 1237 batch loss 6.28424168\n",
      "Validated batch 1238 batch loss 5.81090164\n",
      "Validated batch 1239 batch loss 6.33542633\n",
      "Validated batch 1240 batch loss 6.75447273\n",
      "Validated batch 1241 batch loss 6.71888781\n",
      "Validated batch 1242 batch loss 5.48997498\n",
      "Validated batch 1243 batch loss 6.19126797\n",
      "Validated batch 1244 batch loss 4.72184372\n",
      "Validated batch 1245 batch loss 4.93123674\n",
      "Validated batch 1246 batch loss 6.12846\n",
      "Validated batch 1247 batch loss 6.43884\n",
      "Validated batch 1248 batch loss 6.82609558\n",
      "Validated batch 1249 batch loss 7.06758308\n",
      "Validated batch 1250 batch loss 7.34041309\n",
      "Validated batch 1251 batch loss 6.23048878\n",
      "Validated batch 1252 batch loss 6.44394732\n",
      "Validated batch 1253 batch loss 6.23440742\n",
      "Validated batch 1254 batch loss 5.81422234\n",
      "Validated batch 1255 batch loss 6.32578611\n",
      "Validated batch 1256 batch loss 6.08290768\n",
      "Validated batch 1257 batch loss 4.20966148\n",
      "Validated batch 1258 batch loss 4.18435431\n",
      "Validated batch 1259 batch loss 6.68235397\n",
      "Validated batch 1260 batch loss 6.91653442\n",
      "Validated batch 1261 batch loss 6.85457325\n",
      "Validated batch 1262 batch loss 5.80598974\n",
      "Validated batch 1263 batch loss 5.19854975\n",
      "Validated batch 1264 batch loss 7.0138216\n",
      "Validated batch 1265 batch loss 6.73813057\n",
      "Validated batch 1266 batch loss 5.89091969\n",
      "Validated batch 1267 batch loss 6.03116655\n",
      "Validated batch 1268 batch loss 4.83200836\n",
      "Validated batch 1269 batch loss 5.70894432\n",
      "Validated batch 1270 batch loss 6.60007381\n",
      "Validated batch 1271 batch loss 6.2674017\n",
      "Validated batch 1272 batch loss 5.71486855\n",
      "Validated batch 1273 batch loss 6.46836567\n",
      "Validated batch 1274 batch loss 5.5840044\n",
      "Validated batch 1275 batch loss 5.9915247\n",
      "Validated batch 1276 batch loss 6.89849615\n",
      "Validated batch 1277 batch loss 7.22190714\n",
      "Validated batch 1278 batch loss 6.48025513\n",
      "Validated batch 1279 batch loss 6.13338\n",
      "Validated batch 1280 batch loss 5.41772175\n",
      "Validated batch 1281 batch loss 5.29918814\n",
      "Validated batch 1282 batch loss 5.35320282\n",
      "Validated batch 1283 batch loss 6.24845648\n",
      "Validated batch 1284 batch loss 6.50870895\n",
      "Validated batch 1285 batch loss 6.1067071\n",
      "Validated batch 1286 batch loss 6.22362614\n",
      "Validated batch 1287 batch loss 5.15745068\n",
      "Validated batch 1288 batch loss 7.29043341\n",
      "Validated batch 1289 batch loss 6.21567726\n",
      "Validated batch 1290 batch loss 6.76627827\n",
      "Validated batch 1291 batch loss 6.6479044\n",
      "Validated batch 1292 batch loss 5.9465332\n",
      "Validated batch 1293 batch loss 4.83888721\n",
      "Validated batch 1294 batch loss 5.82858944\n",
      "Validated batch 1295 batch loss 5.81713295\n",
      "Validated batch 1296 batch loss 6.35958719\n",
      "Validated batch 1297 batch loss 6.38243818\n",
      "Validated batch 1298 batch loss 5.56551075\n",
      "Validated batch 1299 batch loss 6.22278881\n",
      "Validated batch 1300 batch loss 5.96496487\n",
      "Validated batch 1301 batch loss 6.56468773\n",
      "Validated batch 1302 batch loss 6.00324249\n",
      "Validated batch 1303 batch loss 7.1984787\n",
      "Validated batch 1304 batch loss 6.07050276\n",
      "Validated batch 1305 batch loss 6.18365812\n",
      "Validated batch 1306 batch loss 5.83957\n",
      "Validated batch 1307 batch loss 5.23322201\n",
      "Validated batch 1308 batch loss 6.90993404\n",
      "Validated batch 1309 batch loss 5.68929863\n",
      "Validated batch 1310 batch loss 5.83239651\n",
      "Validated batch 1311 batch loss 5.6030407\n",
      "Validated batch 1312 batch loss 6.11221075\n",
      "Validated batch 1313 batch loss 5.94998932\n",
      "Validated batch 1314 batch loss 5.25895405\n",
      "Validated batch 1315 batch loss 4.57140732\n",
      "Validated batch 1316 batch loss 4.5121665\n",
      "Validated batch 1317 batch loss 5.78385544\n",
      "Validated batch 1318 batch loss 4.88344049\n",
      "Validated batch 1319 batch loss 5.92398882\n",
      "Validated batch 1320 batch loss 6.51714659\n",
      "Validated batch 1321 batch loss 6.38760567\n",
      "Validated batch 1322 batch loss 6.26956797\n",
      "Validated batch 1323 batch loss 6.09249735\n",
      "Validated batch 1324 batch loss 6.29488182\n",
      "Validated batch 1325 batch loss 6.28661537\n",
      "Validated batch 1326 batch loss 6.27083158\n",
      "Validated batch 1327 batch loss 6.293571\n",
      "Validated batch 1328 batch loss 4.91387463\n",
      "Validated batch 1329 batch loss 4.70293951\n",
      "Validated batch 1330 batch loss 6.16918182\n",
      "Validated batch 1331 batch loss 6.45138884\n",
      "Validated batch 1332 batch loss 7.11433029\n",
      "Validated batch 1333 batch loss 6.59382343\n",
      "Validated batch 1334 batch loss 7.3377533\n",
      "Validated batch 1335 batch loss 7.44134903\n",
      "Validated batch 1336 batch loss 6.29824066\n",
      "Validated batch 1337 batch loss 5.54628849\n",
      "Validated batch 1338 batch loss 5.42028952\n",
      "Validated batch 1339 batch loss 4.76883125\n",
      "Validated batch 1340 batch loss 6.33083534\n",
      "Validated batch 1341 batch loss 6.84238815\n",
      "Validated batch 1342 batch loss 6.07577085\n",
      "Validated batch 1343 batch loss 5.39708138\n",
      "Validated batch 1344 batch loss 5.78862381\n",
      "Validated batch 1345 batch loss 5.88107777\n",
      "Validated batch 1346 batch loss 6.60581875\n",
      "Validated batch 1347 batch loss 6.169137\n",
      "Validated batch 1348 batch loss 6.01188755\n",
      "Validated batch 1349 batch loss 5.7383213\n",
      "Validated batch 1350 batch loss 6.13027191\n",
      "Validated batch 1351 batch loss 5.441998\n",
      "Validated batch 1352 batch loss 5.73932362\n",
      "Validated batch 1353 batch loss 5.05975342\n",
      "Validated batch 1354 batch loss 6.65467072\n",
      "Validated batch 1355 batch loss 6.148386\n",
      "Validated batch 1356 batch loss 5.33193398\n",
      "Validated batch 1357 batch loss 5.89613867\n",
      "Validated batch 1358 batch loss 5.7903223\n",
      "Validated batch 1359 batch loss 6.29709816\n",
      "Validated batch 1360 batch loss 6.50135517\n",
      "Validated batch 1361 batch loss 6.30875111\n",
      "Validated batch 1362 batch loss 6.06101704\n",
      "Validated batch 1363 batch loss 6.14960766\n",
      "Validated batch 1364 batch loss 6.06034231\n",
      "Validated batch 1365 batch loss 6.83805561\n",
      "Validated batch 1366 batch loss 5.6246748\n",
      "Validated batch 1367 batch loss 6.30821276\n",
      "Validated batch 1368 batch loss 6.56007195\n",
      "Validated batch 1369 batch loss 6.3020916\n",
      "Validated batch 1370 batch loss 6.28290272\n",
      "Validated batch 1371 batch loss 5.94804096\n",
      "Validated batch 1372 batch loss 6.7018981\n",
      "Validated batch 1373 batch loss 6.57986403\n",
      "Validated batch 1374 batch loss 6.44460297\n",
      "Validated batch 1375 batch loss 6.28673935\n",
      "Validated batch 1376 batch loss 5.72169495\n",
      "Validated batch 1377 batch loss 5.68315601\n",
      "Validated batch 1378 batch loss 5.32031059\n",
      "Validated batch 1379 batch loss 6.52597523\n",
      "Validated batch 1380 batch loss 5.5513525\n",
      "Validated batch 1381 batch loss 7.03764486\n",
      "Validated batch 1382 batch loss 7.69656706\n",
      "Validated batch 1383 batch loss 5.68062305\n",
      "Validated batch 1384 batch loss 4.805902\n",
      "Validated batch 1385 batch loss 5.92346\n",
      "Validated batch 1386 batch loss 4.94782448\n",
      "Validated batch 1387 batch loss 5.49934196\n",
      "Validated batch 1388 batch loss 6.27411747\n",
      "Validated batch 1389 batch loss 6.51219893\n",
      "Validated batch 1390 batch loss 6.10870361\n",
      "Validated batch 1391 batch loss 5.38668633\n",
      "Validated batch 1392 batch loss 5.64320278\n",
      "Validated batch 1393 batch loss 7.75202894\n",
      "Validated batch 1394 batch loss 5.72930717\n",
      "Validated batch 1395 batch loss 6.20620108\n",
      "Validated batch 1396 batch loss 6.50742722\n",
      "Validated batch 1397 batch loss 5.82821941\n",
      "Validated batch 1398 batch loss 5.01569271\n",
      "Validated batch 1399 batch loss 6.28938961\n",
      "Validated batch 1400 batch loss 6.73027039\n",
      "Validated batch 1401 batch loss 5.94002485\n",
      "Validated batch 1402 batch loss 6.67950821\n",
      "Validated batch 1403 batch loss 5.70852709\n",
      "Validated batch 1404 batch loss 5.88958836\n",
      "Validated batch 1405 batch loss 7.28034163\n",
      "Validated batch 1406 batch loss 5.5644803\n",
      "Validated batch 1407 batch loss 6.26188278\n",
      "Validated batch 1408 batch loss 6.38206482\n",
      "Validated batch 1409 batch loss 5.89271736\n",
      "Validated batch 1410 batch loss 7.18099785\n",
      "Validated batch 1411 batch loss 6.46554327\n",
      "Validated batch 1412 batch loss 6.40385056\n",
      "Validated batch 1413 batch loss 6.655128\n",
      "Validated batch 1414 batch loss 6.2733922\n",
      "Validated batch 1415 batch loss 6.98886299\n",
      "Validated batch 1416 batch loss 6.5315752\n",
      "Validated batch 1417 batch loss 5.88670921\n",
      "Validated batch 1418 batch loss 6.22673464\n",
      "Validated batch 1419 batch loss 6.09123898\n",
      "Validated batch 1420 batch loss 6.01964378\n",
      "Validated batch 1421 batch loss 7.049088\n",
      "Validated batch 1422 batch loss 6.45563602\n",
      "Validated batch 1423 batch loss 5.52175426\n",
      "Validated batch 1424 batch loss 5.24169588\n",
      "Validated batch 1425 batch loss 5.11095142\n",
      "Validated batch 1426 batch loss 6.38048935\n",
      "Validated batch 1427 batch loss 6.43024254\n",
      "Validated batch 1428 batch loss 6.59885836\n",
      "Validated batch 1429 batch loss 6.88397503\n",
      "Validated batch 1430 batch loss 6.87002134\n",
      "Validated batch 1431 batch loss 6.35078716\n",
      "Validated batch 1432 batch loss 6.15108919\n",
      "Validated batch 1433 batch loss 6.25672865\n",
      "Validated batch 1434 batch loss 6.23465729\n",
      "Validated batch 1435 batch loss 6.59797239\n",
      "Validated batch 1436 batch loss 5.94061947\n",
      "Validated batch 1437 batch loss 5.93977\n",
      "Validated batch 1438 batch loss 5.91232395\n",
      "Validated batch 1439 batch loss 5.57255459\n",
      "Validated batch 1440 batch loss 5.78261614\n",
      "Validated batch 1441 batch loss 5.46116257\n",
      "Validated batch 1442 batch loss 6.20485783\n",
      "Validated batch 1443 batch loss 5.65391684\n",
      "Validated batch 1444 batch loss 4.83142328\n",
      "Validated batch 1445 batch loss 6.08146286\n",
      "Validated batch 1446 batch loss 5.97382832\n",
      "Validated batch 1447 batch loss 5.99817038\n",
      "Validated batch 1448 batch loss 5.93943405\n",
      "Validated batch 1449 batch loss 6.14399242\n",
      "Validated batch 1450 batch loss 6.28148413\n",
      "Validated batch 1451 batch loss 6.42466259\n",
      "Validated batch 1452 batch loss 6.13659763\n",
      "Validated batch 1453 batch loss 5.75158596\n",
      "Validated batch 1454 batch loss 4.88925838\n",
      "Validated batch 1455 batch loss 6.46189404\n",
      "Validated batch 1456 batch loss 5.55576611\n",
      "Validated batch 1457 batch loss 5.57301283\n",
      "Validated batch 1458 batch loss 6.0639267\n",
      "Validated batch 1459 batch loss 5.94556236\n",
      "Validated batch 1460 batch loss 5.49033785\n",
      "Validated batch 1461 batch loss 6.29006\n",
      "Validated batch 1462 batch loss 6.02015877\n",
      "Validated batch 1463 batch loss 6.25736904\n",
      "Validated batch 1464 batch loss 6.13352108\n",
      "Validated batch 1465 batch loss 6.63613367\n",
      "Validated batch 1466 batch loss 6.00671577\n",
      "Validated batch 1467 batch loss 6.79778385\n",
      "Validated batch 1468 batch loss 6.85948372\n",
      "Validated batch 1469 batch loss 6.6344347\n",
      "Validated batch 1470 batch loss 6.95362949\n",
      "Validated batch 1471 batch loss 7.37446976\n",
      "Validated batch 1472 batch loss 5.18964386\n",
      "Validated batch 1473 batch loss 6.34504032\n",
      "Validated batch 1474 batch loss 6.79657269\n",
      "Validated batch 1475 batch loss 6.63251209\n",
      "Validated batch 1476 batch loss 6.36746931\n",
      "Validated batch 1477 batch loss 5.93130732\n",
      "Validated batch 1478 batch loss 6.23500156\n",
      "Validated batch 1479 batch loss 6.32058239\n",
      "Epoch 2 val loss 6.115265846252441\n",
      "Model ./models/model-v0.0.1-epoch-2-loss-6.1153.h5 saved.\n",
      "Start epoch 3 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 5.53240776 epoch total loss 5.53240776\n",
      "Trained batch 2 batch loss 5.68804932 epoch total loss 5.61022854\n",
      "Trained batch 3 batch loss 6.01404 epoch total loss 5.74483252\n",
      "Trained batch 4 batch loss 5.15519619 epoch total loss 5.59742355\n",
      "Trained batch 5 batch loss 6.29986382 epoch total loss 5.7379117\n",
      "Trained batch 6 batch loss 4.64373541 epoch total loss 5.55554914\n",
      "Trained batch 7 batch loss 6.46776772 epoch total loss 5.68586636\n",
      "Trained batch 8 batch loss 5.56846905 epoch total loss 5.67119169\n",
      "Trained batch 9 batch loss 4.91579342 epoch total loss 5.58725882\n",
      "Trained batch 10 batch loss 5.06860256 epoch total loss 5.53539324\n",
      "Trained batch 11 batch loss 5.46224 epoch total loss 5.52874279\n",
      "Trained batch 12 batch loss 6.25088024 epoch total loss 5.58892059\n",
      "Trained batch 13 batch loss 4.41946602 epoch total loss 5.4989624\n",
      "Trained batch 14 batch loss 5.6393919 epoch total loss 5.50899267\n",
      "Trained batch 15 batch loss 5.35042477 epoch total loss 5.49842167\n",
      "Trained batch 16 batch loss 6.37971115 epoch total loss 5.55350208\n",
      "Trained batch 17 batch loss 5.89220858 epoch total loss 5.57342625\n",
      "Trained batch 18 batch loss 4.68649673 epoch total loss 5.52415228\n",
      "Trained batch 19 batch loss 5.59220505 epoch total loss 5.52773428\n",
      "Trained batch 20 batch loss 6.20734262 epoch total loss 5.56171465\n",
      "Trained batch 21 batch loss 5.90925646 epoch total loss 5.57826424\n",
      "Trained batch 22 batch loss 6.01537466 epoch total loss 5.59813261\n",
      "Trained batch 23 batch loss 5.90195274 epoch total loss 5.61134195\n",
      "Trained batch 24 batch loss 5.81045818 epoch total loss 5.61963844\n",
      "Trained batch 25 batch loss 6.70532799 epoch total loss 5.66306591\n",
      "Trained batch 26 batch loss 6.455585 epoch total loss 5.69354725\n",
      "Trained batch 27 batch loss 5.71973419 epoch total loss 5.69451666\n",
      "Trained batch 28 batch loss 5.42075825 epoch total loss 5.68473959\n",
      "Trained batch 29 batch loss 5.56079626 epoch total loss 5.6804657\n",
      "Trained batch 30 batch loss 4.71492243 epoch total loss 5.64828062\n",
      "Trained batch 31 batch loss 6.23377514 epoch total loss 5.66716814\n",
      "Trained batch 32 batch loss 5.75231647 epoch total loss 5.66982889\n",
      "Trained batch 33 batch loss 5.60104656 epoch total loss 5.66774464\n",
      "Trained batch 34 batch loss 5.96204 epoch total loss 5.6764\n",
      "Trained batch 35 batch loss 6.12293768 epoch total loss 5.68915844\n",
      "Trained batch 36 batch loss 5.89498 epoch total loss 5.69487572\n",
      "Trained batch 37 batch loss 5.82893133 epoch total loss 5.69849873\n",
      "Trained batch 38 batch loss 6.10093117 epoch total loss 5.70908928\n",
      "Trained batch 39 batch loss 5.87159157 epoch total loss 5.71325588\n",
      "Trained batch 40 batch loss 5.43595791 epoch total loss 5.70632362\n",
      "Trained batch 41 batch loss 5.84221458 epoch total loss 5.70963812\n",
      "Trained batch 42 batch loss 5.7138176 epoch total loss 5.7097373\n",
      "Trained batch 43 batch loss 5.62441254 epoch total loss 5.70775318\n",
      "Trained batch 44 batch loss 5.84196711 epoch total loss 5.71080351\n",
      "Trained batch 45 batch loss 5.86596918 epoch total loss 5.71425152\n",
      "Trained batch 46 batch loss 6.67398691 epoch total loss 5.73511553\n",
      "Trained batch 47 batch loss 4.6448307 epoch total loss 5.71191788\n",
      "Trained batch 48 batch loss 5.60347176 epoch total loss 5.7096591\n",
      "Trained batch 49 batch loss 4.73847198 epoch total loss 5.68983841\n",
      "Trained batch 50 batch loss 5.9546113 epoch total loss 5.69513416\n",
      "Trained batch 51 batch loss 5.93886185 epoch total loss 5.6999135\n",
      "Trained batch 52 batch loss 4.85384083 epoch total loss 5.68364286\n",
      "Trained batch 53 batch loss 5.38242483 epoch total loss 5.67795944\n",
      "Trained batch 54 batch loss 5.092237 epoch total loss 5.66711235\n",
      "Trained batch 55 batch loss 4.44388 epoch total loss 5.64487171\n",
      "Trained batch 56 batch loss 4.87905407 epoch total loss 5.6311965\n",
      "Trained batch 57 batch loss 4.72149372 epoch total loss 5.61523724\n",
      "Trained batch 58 batch loss 5.04881287 epoch total loss 5.60547066\n",
      "Trained batch 59 batch loss 6.95846272 epoch total loss 5.62840319\n",
      "Trained batch 60 batch loss 6.55308151 epoch total loss 5.64381409\n",
      "Trained batch 61 batch loss 4.81753969 epoch total loss 5.63026857\n",
      "Trained batch 62 batch loss 5.25686073 epoch total loss 5.62424612\n",
      "Trained batch 63 batch loss 5.95217133 epoch total loss 5.62945127\n",
      "Trained batch 64 batch loss 5.63762665 epoch total loss 5.62957907\n",
      "Trained batch 65 batch loss 5.93060589 epoch total loss 5.63421\n",
      "Trained batch 66 batch loss 4.91580868 epoch total loss 5.62332535\n",
      "Trained batch 67 batch loss 5.98273134 epoch total loss 5.62868929\n",
      "Trained batch 68 batch loss 5.3575592 epoch total loss 5.62470198\n",
      "Trained batch 69 batch loss 4.74841785 epoch total loss 5.61200237\n",
      "Trained batch 70 batch loss 4.66813278 epoch total loss 5.59851837\n",
      "Trained batch 71 batch loss 5.86284971 epoch total loss 5.60224104\n",
      "Trained batch 72 batch loss 5.66260433 epoch total loss 5.60307932\n",
      "Trained batch 73 batch loss 5.59481 epoch total loss 5.60296631\n",
      "Trained batch 74 batch loss 5.22609711 epoch total loss 5.59787369\n",
      "Trained batch 75 batch loss 4.87018156 epoch total loss 5.58817101\n",
      "Trained batch 76 batch loss 4.81156921 epoch total loss 5.57795238\n",
      "Trained batch 77 batch loss 4.89998579 epoch total loss 5.56914759\n",
      "Trained batch 78 batch loss 5.22130919 epoch total loss 5.56468821\n",
      "Trained batch 79 batch loss 4.95391273 epoch total loss 5.55695677\n",
      "Trained batch 80 batch loss 4.99616909 epoch total loss 5.54994678\n",
      "Trained batch 81 batch loss 5.31836843 epoch total loss 5.54708767\n",
      "Trained batch 82 batch loss 5.35981512 epoch total loss 5.5448041\n",
      "Trained batch 83 batch loss 5.50917339 epoch total loss 5.54437494\n",
      "Trained batch 84 batch loss 5.65097332 epoch total loss 5.54564381\n",
      "Trained batch 85 batch loss 4.82316637 epoch total loss 5.53714418\n",
      "Trained batch 86 batch loss 5.26657772 epoch total loss 5.53399801\n",
      "Trained batch 87 batch loss 5.55858183 epoch total loss 5.53428078\n",
      "Trained batch 88 batch loss 5.97665691 epoch total loss 5.53930759\n",
      "Trained batch 89 batch loss 6.18885231 epoch total loss 5.54660606\n",
      "Trained batch 90 batch loss 6.35191774 epoch total loss 5.55555391\n",
      "Trained batch 91 batch loss 5.45132637 epoch total loss 5.55440855\n",
      "Trained batch 92 batch loss 6.01780272 epoch total loss 5.55944538\n",
      "Trained batch 93 batch loss 5.23093891 epoch total loss 5.55591297\n",
      "Trained batch 94 batch loss 5.21674728 epoch total loss 5.55230474\n",
      "Trained batch 95 batch loss 4.80948734 epoch total loss 5.54448557\n",
      "Trained batch 96 batch loss 4.98184443 epoch total loss 5.53862524\n",
      "Trained batch 97 batch loss 5.15599728 epoch total loss 5.53468037\n",
      "Trained batch 98 batch loss 5.64681 epoch total loss 5.5358243\n",
      "Trained batch 99 batch loss 5.12067461 epoch total loss 5.53163099\n",
      "Trained batch 100 batch loss 5.03725338 epoch total loss 5.52668715\n",
      "Trained batch 101 batch loss 4.75690937 epoch total loss 5.51906538\n",
      "Trained batch 102 batch loss 5.66491747 epoch total loss 5.52049541\n",
      "Trained batch 103 batch loss 6.13288879 epoch total loss 5.52644062\n",
      "Trained batch 104 batch loss 5.94197559 epoch total loss 5.53043604\n",
      "Trained batch 105 batch loss 5.13560486 epoch total loss 5.5266757\n",
      "Trained batch 106 batch loss 6.4566021 epoch total loss 5.53544855\n",
      "Trained batch 107 batch loss 5.74769068 epoch total loss 5.53743219\n",
      "Trained batch 108 batch loss 4.94723034 epoch total loss 5.53196716\n",
      "Trained batch 109 batch loss 5.33115387 epoch total loss 5.53012514\n",
      "Trained batch 110 batch loss 4.58678865 epoch total loss 5.52154922\n",
      "Trained batch 111 batch loss 5.04160309 epoch total loss 5.51722574\n",
      "Trained batch 112 batch loss 4.89872265 epoch total loss 5.51170349\n",
      "Trained batch 113 batch loss 4.39265919 epoch total loss 5.5018\n",
      "Trained batch 114 batch loss 3.9336772 epoch total loss 5.48804474\n",
      "Trained batch 115 batch loss 4.93373585 epoch total loss 5.48322439\n",
      "Trained batch 116 batch loss 5.59202099 epoch total loss 5.48416233\n",
      "Trained batch 117 batch loss 5.20462322 epoch total loss 5.48177338\n",
      "Trained batch 118 batch loss 6.53669119 epoch total loss 5.49071312\n",
      "Trained batch 119 batch loss 5.3381834 epoch total loss 5.48943186\n",
      "Trained batch 120 batch loss 5.14663649 epoch total loss 5.48657465\n",
      "Trained batch 121 batch loss 5.32786274 epoch total loss 5.48526335\n",
      "Trained batch 122 batch loss 5.67620087 epoch total loss 5.48682833\n",
      "Trained batch 123 batch loss 6.67219925 epoch total loss 5.49646521\n",
      "Trained batch 124 batch loss 5.92371702 epoch total loss 5.49991083\n",
      "Trained batch 125 batch loss 5.17682505 epoch total loss 5.49732637\n",
      "Trained batch 126 batch loss 5.93076324 epoch total loss 5.50076628\n",
      "Trained batch 127 batch loss 5.42282152 epoch total loss 5.50015306\n",
      "Trained batch 128 batch loss 6.17639923 epoch total loss 5.50543594\n",
      "Trained batch 129 batch loss 5.86808681 epoch total loss 5.50824738\n",
      "Trained batch 130 batch loss 5.512887 epoch total loss 5.50828314\n",
      "Trained batch 131 batch loss 5.75592804 epoch total loss 5.51017332\n",
      "Trained batch 132 batch loss 5.16910076 epoch total loss 5.50759\n",
      "Trained batch 133 batch loss 4.84382868 epoch total loss 5.50259876\n",
      "Trained batch 134 batch loss 4.922122 epoch total loss 5.4982667\n",
      "Trained batch 135 batch loss 5.94210243 epoch total loss 5.50155449\n",
      "Trained batch 136 batch loss 6.30808735 epoch total loss 5.50748491\n",
      "Trained batch 137 batch loss 5.34879398 epoch total loss 5.50632668\n",
      "Trained batch 138 batch loss 4.93984747 epoch total loss 5.50222158\n",
      "Trained batch 139 batch loss 4.89679813 epoch total loss 5.49786615\n",
      "Trained batch 140 batch loss 4.24180746 epoch total loss 5.48889399\n",
      "Trained batch 141 batch loss 5.13987494 epoch total loss 5.4864192\n",
      "Trained batch 142 batch loss 6.31524086 epoch total loss 5.49225569\n",
      "Trained batch 143 batch loss 6.02429771 epoch total loss 5.49597645\n",
      "Trained batch 144 batch loss 6.35595703 epoch total loss 5.50194836\n",
      "Trained batch 145 batch loss 6.43785858 epoch total loss 5.5084033\n",
      "Trained batch 146 batch loss 6.37791109 epoch total loss 5.51435852\n",
      "Trained batch 147 batch loss 6.18911648 epoch total loss 5.51894855\n",
      "Trained batch 148 batch loss 6.53164768 epoch total loss 5.52579165\n",
      "Trained batch 149 batch loss 5.74176836 epoch total loss 5.52724075\n",
      "Trained batch 150 batch loss 6.1099472 epoch total loss 5.53112555\n",
      "Trained batch 151 batch loss 5.92082691 epoch total loss 5.53370619\n",
      "Trained batch 152 batch loss 5.70048761 epoch total loss 5.53480387\n",
      "Trained batch 153 batch loss 5.72776079 epoch total loss 5.5360651\n",
      "Trained batch 154 batch loss 5.69666481 epoch total loss 5.53710794\n",
      "Trained batch 155 batch loss 5.82694244 epoch total loss 5.53897762\n",
      "Trained batch 156 batch loss 6.16828632 epoch total loss 5.54301167\n",
      "Trained batch 157 batch loss 5.74391365 epoch total loss 5.5442915\n",
      "Trained batch 158 batch loss 5.91383028 epoch total loss 5.54663\n",
      "Trained batch 159 batch loss 5.99159813 epoch total loss 5.54942846\n",
      "Trained batch 160 batch loss 5.50120354 epoch total loss 5.5491271\n",
      "Trained batch 161 batch loss 6.1122303 epoch total loss 5.5526247\n",
      "Trained batch 162 batch loss 5.88766098 epoch total loss 5.55469275\n",
      "Trained batch 163 batch loss 5.51970196 epoch total loss 5.55447817\n",
      "Trained batch 164 batch loss 5.53872252 epoch total loss 5.55438185\n",
      "Trained batch 165 batch loss 5.34080887 epoch total loss 5.55308771\n",
      "Trained batch 166 batch loss 5.03619 epoch total loss 5.54997396\n",
      "Trained batch 167 batch loss 5.43239 epoch total loss 5.54926968\n",
      "Trained batch 168 batch loss 5.74645615 epoch total loss 5.55044317\n",
      "Trained batch 169 batch loss 5.46741772 epoch total loss 5.54995203\n",
      "Trained batch 170 batch loss 5.55284834 epoch total loss 5.5499692\n",
      "Trained batch 171 batch loss 5.59500933 epoch total loss 5.55023241\n",
      "Trained batch 172 batch loss 6.1189518 epoch total loss 5.55353928\n",
      "Trained batch 173 batch loss 5.33175421 epoch total loss 5.55225706\n",
      "Trained batch 174 batch loss 5.39133549 epoch total loss 5.55133247\n",
      "Trained batch 175 batch loss 5.17758846 epoch total loss 5.54919672\n",
      "Trained batch 176 batch loss 6.0739069 epoch total loss 5.55217791\n",
      "Trained batch 177 batch loss 6.08208895 epoch total loss 5.55517197\n",
      "Trained batch 178 batch loss 5.2894907 epoch total loss 5.55367947\n",
      "Trained batch 179 batch loss 6.06018639 epoch total loss 5.55650902\n",
      "Trained batch 180 batch loss 5.90235472 epoch total loss 5.55843\n",
      "Trained batch 181 batch loss 5.64928818 epoch total loss 5.5589323\n",
      "Trained batch 182 batch loss 6.39145851 epoch total loss 5.5635066\n",
      "Trained batch 183 batch loss 6.17319775 epoch total loss 5.56683826\n",
      "Trained batch 184 batch loss 5.46790838 epoch total loss 5.56630087\n",
      "Trained batch 185 batch loss 5.61186886 epoch total loss 5.56654692\n",
      "Trained batch 186 batch loss 5.99854183 epoch total loss 5.56886911\n",
      "Trained batch 187 batch loss 5.69625 epoch total loss 5.56955051\n",
      "Trained batch 188 batch loss 5.59343338 epoch total loss 5.56967735\n",
      "Trained batch 189 batch loss 6.10243607 epoch total loss 5.57249641\n",
      "Trained batch 190 batch loss 5.96647501 epoch total loss 5.5745697\n",
      "Trained batch 191 batch loss 5.77087641 epoch total loss 5.57559729\n",
      "Trained batch 192 batch loss 6.18393517 epoch total loss 5.57876587\n",
      "Trained batch 193 batch loss 5.95121384 epoch total loss 5.58069563\n",
      "Trained batch 194 batch loss 6.62714291 epoch total loss 5.58608961\n",
      "Trained batch 195 batch loss 5.99148655 epoch total loss 5.58816862\n",
      "Trained batch 196 batch loss 5.70433331 epoch total loss 5.58876133\n",
      "Trained batch 197 batch loss 5.73506546 epoch total loss 5.58950424\n",
      "Trained batch 198 batch loss 5.80347347 epoch total loss 5.59058475\n",
      "Trained batch 199 batch loss 6.39685965 epoch total loss 5.59463644\n",
      "Trained batch 200 batch loss 6.32387543 epoch total loss 5.59828234\n",
      "Trained batch 201 batch loss 6.25887 epoch total loss 5.60156918\n",
      "Trained batch 202 batch loss 5.27663898 epoch total loss 5.59996033\n",
      "Trained batch 203 batch loss 5.28545618 epoch total loss 5.59841108\n",
      "Trained batch 204 batch loss 5.87826538 epoch total loss 5.59978294\n",
      "Trained batch 205 batch loss 5.6879 epoch total loss 5.60021257\n",
      "Trained batch 206 batch loss 5.54785919 epoch total loss 5.59995842\n",
      "Trained batch 207 batch loss 5.77193737 epoch total loss 5.60078955\n",
      "Trained batch 208 batch loss 5.67859364 epoch total loss 5.60116339\n",
      "Trained batch 209 batch loss 5.50835657 epoch total loss 5.60071898\n",
      "Trained batch 210 batch loss 5.75763416 epoch total loss 5.60146666\n",
      "Trained batch 211 batch loss 5.95521069 epoch total loss 5.60314322\n",
      "Trained batch 212 batch loss 5.82406616 epoch total loss 5.6041851\n",
      "Trained batch 213 batch loss 4.66774702 epoch total loss 5.59978867\n",
      "Trained batch 214 batch loss 4.84301281 epoch total loss 5.59625244\n",
      "Trained batch 215 batch loss 5.50176334 epoch total loss 5.5958128\n",
      "Trained batch 216 batch loss 5.6662693 epoch total loss 5.59613895\n",
      "Trained batch 217 batch loss 5.84837723 epoch total loss 5.59730148\n",
      "Trained batch 218 batch loss 6.04576159 epoch total loss 5.59935856\n",
      "Trained batch 219 batch loss 5.10856056 epoch total loss 5.59711742\n",
      "Trained batch 220 batch loss 5.79961824 epoch total loss 5.59803724\n",
      "Trained batch 221 batch loss 5.68860817 epoch total loss 5.59844732\n",
      "Trained batch 222 batch loss 5.22323513 epoch total loss 5.59675741\n",
      "Trained batch 223 batch loss 5.84115028 epoch total loss 5.59785318\n",
      "Trained batch 224 batch loss 5.6523447 epoch total loss 5.59809637\n",
      "Trained batch 225 batch loss 6.05955315 epoch total loss 5.60014772\n",
      "Trained batch 226 batch loss 5.41823483 epoch total loss 5.59934235\n",
      "Trained batch 227 batch loss 5.70173216 epoch total loss 5.59979391\n",
      "Trained batch 228 batch loss 5.53948069 epoch total loss 5.59952927\n",
      "Trained batch 229 batch loss 5.14760637 epoch total loss 5.59755564\n",
      "Trained batch 230 batch loss 5.76731777 epoch total loss 5.59829378\n",
      "Trained batch 231 batch loss 5.40686035 epoch total loss 5.59746504\n",
      "Trained batch 232 batch loss 5.70927048 epoch total loss 5.59794664\n",
      "Trained batch 233 batch loss 4.1557703 epoch total loss 5.59175682\n",
      "Trained batch 234 batch loss 4.41103506 epoch total loss 5.58671093\n",
      "Trained batch 235 batch loss 4.52486229 epoch total loss 5.5821929\n",
      "Trained batch 236 batch loss 6.66030264 epoch total loss 5.586761\n",
      "Trained batch 237 batch loss 5.54216242 epoch total loss 5.58657265\n",
      "Trained batch 238 batch loss 4.78933907 epoch total loss 5.58322287\n",
      "Trained batch 239 batch loss 4.80403662 epoch total loss 5.57996273\n",
      "Trained batch 240 batch loss 6.32587337 epoch total loss 5.58307076\n",
      "Trained batch 241 batch loss 6.32430458 epoch total loss 5.58614683\n",
      "Trained batch 242 batch loss 6.13297939 epoch total loss 5.58840609\n",
      "Trained batch 243 batch loss 6.03112602 epoch total loss 5.59022808\n",
      "Trained batch 244 batch loss 5.79492521 epoch total loss 5.59106684\n",
      "Trained batch 245 batch loss 5.94039726 epoch total loss 5.59249306\n",
      "Trained batch 246 batch loss 5.7458086 epoch total loss 5.59311628\n",
      "Trained batch 247 batch loss 5.09365177 epoch total loss 5.59109402\n",
      "Trained batch 248 batch loss 5.39499187 epoch total loss 5.59030342\n",
      "Trained batch 249 batch loss 5.08906555 epoch total loss 5.58829069\n",
      "Trained batch 250 batch loss 4.53908348 epoch total loss 5.58409357\n",
      "Trained batch 251 batch loss 6.0268755 epoch total loss 5.58585787\n",
      "Trained batch 252 batch loss 5.59555149 epoch total loss 5.58589649\n",
      "Trained batch 253 batch loss 5.89417744 epoch total loss 5.58711481\n",
      "Trained batch 254 batch loss 5.75807333 epoch total loss 5.58778763\n",
      "Trained batch 255 batch loss 5.31233311 epoch total loss 5.58670759\n",
      "Trained batch 256 batch loss 5.22750521 epoch total loss 5.58530474\n",
      "Trained batch 257 batch loss 4.34869957 epoch total loss 5.58049345\n",
      "Trained batch 258 batch loss 4.89974 epoch total loss 5.57785463\n",
      "Trained batch 259 batch loss 4.89473248 epoch total loss 5.57521725\n",
      "Trained batch 260 batch loss 4.39907074 epoch total loss 5.57069397\n",
      "Trained batch 261 batch loss 6.58646774 epoch total loss 5.57458544\n",
      "Trained batch 262 batch loss 6.31741333 epoch total loss 5.57742071\n",
      "Trained batch 263 batch loss 6.30691338 epoch total loss 5.580194\n",
      "Trained batch 264 batch loss 5.31836605 epoch total loss 5.57920218\n",
      "Trained batch 265 batch loss 6.37690639 epoch total loss 5.58221292\n",
      "Trained batch 266 batch loss 5.68100262 epoch total loss 5.58258438\n",
      "Trained batch 267 batch loss 5.44648457 epoch total loss 5.58207464\n",
      "Trained batch 268 batch loss 5.84355927 epoch total loss 5.58305025\n",
      "Trained batch 269 batch loss 5.39611721 epoch total loss 5.58235502\n",
      "Trained batch 270 batch loss 6.06100273 epoch total loss 5.58412838\n",
      "Trained batch 271 batch loss 5.44979858 epoch total loss 5.58363247\n",
      "Trained batch 272 batch loss 5.86803818 epoch total loss 5.58467817\n",
      "Trained batch 273 batch loss 6.00823355 epoch total loss 5.58622932\n",
      "Trained batch 274 batch loss 4.78061676 epoch total loss 5.58328915\n",
      "Trained batch 275 batch loss 5.92803764 epoch total loss 5.58454275\n",
      "Trained batch 276 batch loss 5.7310257 epoch total loss 5.58507395\n",
      "Trained batch 277 batch loss 5.73577356 epoch total loss 5.58561754\n",
      "Trained batch 278 batch loss 4.66129 epoch total loss 5.58229256\n",
      "Trained batch 279 batch loss 5.47766972 epoch total loss 5.58191729\n",
      "Trained batch 280 batch loss 5.69915771 epoch total loss 5.58233643\n",
      "Trained batch 281 batch loss 6.05126143 epoch total loss 5.58400536\n",
      "Trained batch 282 batch loss 5.70435715 epoch total loss 5.58443213\n",
      "Trained batch 283 batch loss 4.81318665 epoch total loss 5.581707\n",
      "Trained batch 284 batch loss 5.13929367 epoch total loss 5.58014917\n",
      "Trained batch 285 batch loss 5.4150691 epoch total loss 5.57957\n",
      "Trained batch 286 batch loss 5.98039913 epoch total loss 5.58097124\n",
      "Trained batch 287 batch loss 5.79131889 epoch total loss 5.58170366\n",
      "Trained batch 288 batch loss 6.23823118 epoch total loss 5.58398342\n",
      "Trained batch 289 batch loss 6.1837225 epoch total loss 5.58605862\n",
      "Trained batch 290 batch loss 5.29678583 epoch total loss 5.58506107\n",
      "Trained batch 291 batch loss 5.0817318 epoch total loss 5.58333158\n",
      "Trained batch 292 batch loss 5.54569721 epoch total loss 5.58320284\n",
      "Trained batch 293 batch loss 5.15229225 epoch total loss 5.58173227\n",
      "Trained batch 294 batch loss 4.70123577 epoch total loss 5.57873726\n",
      "Trained batch 295 batch loss 5.54641914 epoch total loss 5.57862759\n",
      "Trained batch 296 batch loss 6.6981287 epoch total loss 5.58241\n",
      "Trained batch 297 batch loss 5.67152691 epoch total loss 5.58271\n",
      "Trained batch 298 batch loss 4.95545197 epoch total loss 5.58060503\n",
      "Trained batch 299 batch loss 5.91326714 epoch total loss 5.58171749\n",
      "Trained batch 300 batch loss 4.88743782 epoch total loss 5.57940292\n",
      "Trained batch 301 batch loss 4.91573811 epoch total loss 5.57719851\n",
      "Trained batch 302 batch loss 5.93413973 epoch total loss 5.57838\n",
      "Trained batch 303 batch loss 5.31342793 epoch total loss 5.57750559\n",
      "Trained batch 304 batch loss 5.05040646 epoch total loss 5.57577181\n",
      "Trained batch 305 batch loss 5.78119278 epoch total loss 5.57644558\n",
      "Trained batch 306 batch loss 6.22528362 epoch total loss 5.57856607\n",
      "Trained batch 307 batch loss 6.20758724 epoch total loss 5.58061552\n",
      "Trained batch 308 batch loss 5.35988522 epoch total loss 5.57989836\n",
      "Trained batch 309 batch loss 4.9353323 epoch total loss 5.57781267\n",
      "Trained batch 310 batch loss 4.66958809 epoch total loss 5.57488251\n",
      "Trained batch 311 batch loss 4.58637428 epoch total loss 5.57170439\n",
      "Trained batch 312 batch loss 4.22269058 epoch total loss 5.56738043\n",
      "Trained batch 313 batch loss 5.09294033 epoch total loss 5.56586456\n",
      "Trained batch 314 batch loss 5.87087774 epoch total loss 5.56683588\n",
      "Trained batch 315 batch loss 5.85584402 epoch total loss 5.56775331\n",
      "Trained batch 316 batch loss 6.04410076 epoch total loss 5.5692606\n",
      "Trained batch 317 batch loss 5.69598722 epoch total loss 5.56966066\n",
      "Trained batch 318 batch loss 5.71694708 epoch total loss 5.57012367\n",
      "Trained batch 319 batch loss 5.76947641 epoch total loss 5.57074881\n",
      "Trained batch 320 batch loss 5.27950191 epoch total loss 5.56983852\n",
      "Trained batch 321 batch loss 6.19219398 epoch total loss 5.57177734\n",
      "Trained batch 322 batch loss 5.28803873 epoch total loss 5.57089615\n",
      "Trained batch 323 batch loss 5.36717224 epoch total loss 5.57026577\n",
      "Trained batch 324 batch loss 5.78499651 epoch total loss 5.57092857\n",
      "Trained batch 325 batch loss 6.52926159 epoch total loss 5.57387733\n",
      "Trained batch 326 batch loss 5.63006353 epoch total loss 5.57404947\n",
      "Trained batch 327 batch loss 6.01509953 epoch total loss 5.57539845\n",
      "Trained batch 328 batch loss 4.44609737 epoch total loss 5.5719552\n",
      "Trained batch 329 batch loss 4.79150391 epoch total loss 5.56958294\n",
      "Trained batch 330 batch loss 4.19626 epoch total loss 5.56542158\n",
      "Trained batch 331 batch loss 4.55479574 epoch total loss 5.56236839\n",
      "Trained batch 332 batch loss 5.54309 epoch total loss 5.56231\n",
      "Trained batch 333 batch loss 6.32865715 epoch total loss 5.56461143\n",
      "Trained batch 334 batch loss 6.66078043 epoch total loss 5.56789351\n",
      "Trained batch 335 batch loss 6.11624575 epoch total loss 5.56953\n",
      "Trained batch 336 batch loss 5.94419479 epoch total loss 5.57064533\n",
      "Trained batch 337 batch loss 5.93093491 epoch total loss 5.5717144\n",
      "Trained batch 338 batch loss 4.62056541 epoch total loss 5.56890059\n",
      "Trained batch 339 batch loss 6.46696377 epoch total loss 5.57154942\n",
      "Trained batch 340 batch loss 6.33116341 epoch total loss 5.5737834\n",
      "Trained batch 341 batch loss 5.84982109 epoch total loss 5.57459307\n",
      "Trained batch 342 batch loss 6.49502277 epoch total loss 5.57728434\n",
      "Trained batch 343 batch loss 6.95998049 epoch total loss 5.58131552\n",
      "Trained batch 344 batch loss 5.95786285 epoch total loss 5.58241034\n",
      "Trained batch 345 batch loss 5.97011 epoch total loss 5.58353376\n",
      "Trained batch 346 batch loss 5.92096806 epoch total loss 5.58450937\n",
      "Trained batch 347 batch loss 5.75897646 epoch total loss 5.58501244\n",
      "Trained batch 348 batch loss 6.10117292 epoch total loss 5.5864954\n",
      "Trained batch 349 batch loss 5.68299627 epoch total loss 5.58677197\n",
      "Trained batch 350 batch loss 5.91532898 epoch total loss 5.58771086\n",
      "Trained batch 351 batch loss 5.38167381 epoch total loss 5.58712387\n",
      "Trained batch 352 batch loss 6.10666656 epoch total loss 5.58859968\n",
      "Trained batch 353 batch loss 5.95215 epoch total loss 5.58962965\n",
      "Trained batch 354 batch loss 6.78759384 epoch total loss 5.59301376\n",
      "Trained batch 355 batch loss 6.54873943 epoch total loss 5.59570599\n",
      "Trained batch 356 batch loss 6.35619164 epoch total loss 5.59784222\n",
      "Trained batch 357 batch loss 6.41555786 epoch total loss 5.60013247\n",
      "Trained batch 358 batch loss 4.25503254 epoch total loss 5.59637499\n",
      "Trained batch 359 batch loss 4.88836908 epoch total loss 5.59440327\n",
      "Trained batch 360 batch loss 5.10990429 epoch total loss 5.59305716\n",
      "Trained batch 361 batch loss 6.02802563 epoch total loss 5.59426212\n",
      "Trained batch 362 batch loss 4.8010397 epoch total loss 5.59207106\n",
      "Trained batch 363 batch loss 6.00056648 epoch total loss 5.59319639\n",
      "Trained batch 364 batch loss 5.22040558 epoch total loss 5.59217262\n",
      "Trained batch 365 batch loss 5.28395081 epoch total loss 5.59132814\n",
      "Trained batch 366 batch loss 6.65629 epoch total loss 5.5942378\n",
      "Trained batch 367 batch loss 6.22986698 epoch total loss 5.59596968\n",
      "Trained batch 368 batch loss 6.6497612 epoch total loss 5.59883308\n",
      "Trained batch 369 batch loss 5.06814957 epoch total loss 5.59739494\n",
      "Trained batch 370 batch loss 6.12449884 epoch total loss 5.59881973\n",
      "Trained batch 371 batch loss 5.77776623 epoch total loss 5.59930229\n",
      "Trained batch 372 batch loss 5.92036629 epoch total loss 5.60016537\n",
      "Trained batch 373 batch loss 6.10863781 epoch total loss 5.60152864\n",
      "Trained batch 374 batch loss 5.90424347 epoch total loss 5.60233784\n",
      "Trained batch 375 batch loss 5.28746319 epoch total loss 5.60149813\n",
      "Trained batch 376 batch loss 5.42185783 epoch total loss 5.60102034\n",
      "Trained batch 377 batch loss 5.14721155 epoch total loss 5.5998168\n",
      "Trained batch 378 batch loss 4.58004665 epoch total loss 5.59711885\n",
      "Trained batch 379 batch loss 5.0602 epoch total loss 5.59570265\n",
      "Trained batch 380 batch loss 5.07886028 epoch total loss 5.59434223\n",
      "Trained batch 381 batch loss 6.01049614 epoch total loss 5.59543467\n",
      "Trained batch 382 batch loss 5.64301395 epoch total loss 5.59555912\n",
      "Trained batch 383 batch loss 5.13440037 epoch total loss 5.59435558\n",
      "Trained batch 384 batch loss 5.55272627 epoch total loss 5.59424734\n",
      "Trained batch 385 batch loss 5.90882492 epoch total loss 5.59506464\n",
      "Trained batch 386 batch loss 5.58832169 epoch total loss 5.595047\n",
      "Trained batch 387 batch loss 6.02215719 epoch total loss 5.59615088\n",
      "Trained batch 388 batch loss 6.11874533 epoch total loss 5.59749746\n",
      "Trained batch 389 batch loss 6.23596716 epoch total loss 5.59913921\n",
      "Trained batch 390 batch loss 5.41949654 epoch total loss 5.59867859\n",
      "Trained batch 391 batch loss 6.30657196 epoch total loss 5.60048914\n",
      "Trained batch 392 batch loss 5.88138199 epoch total loss 5.60120583\n",
      "Trained batch 393 batch loss 5.61890221 epoch total loss 5.60125065\n",
      "Trained batch 394 batch loss 5.09218931 epoch total loss 5.5999589\n",
      "Trained batch 395 batch loss 5.6053648 epoch total loss 5.59997272\n",
      "Trained batch 396 batch loss 5.65262032 epoch total loss 5.60010576\n",
      "Trained batch 397 batch loss 6.22764206 epoch total loss 5.601686\n",
      "Trained batch 398 batch loss 4.18304396 epoch total loss 5.59812164\n",
      "Trained batch 399 batch loss 5.854568 epoch total loss 5.59876442\n",
      "Trained batch 400 batch loss 4.67707539 epoch total loss 5.59646\n",
      "Trained batch 401 batch loss 4.52107334 epoch total loss 5.59377813\n",
      "Trained batch 402 batch loss 5.28169823 epoch total loss 5.59300184\n",
      "Trained batch 403 batch loss 5.62630129 epoch total loss 5.59308434\n",
      "Trained batch 404 batch loss 5.62875557 epoch total loss 5.59317207\n",
      "Trained batch 405 batch loss 4.86341047 epoch total loss 5.59137058\n",
      "Trained batch 406 batch loss 4.63408899 epoch total loss 5.58901262\n",
      "Trained batch 407 batch loss 5.77738953 epoch total loss 5.58947563\n",
      "Trained batch 408 batch loss 6.34052944 epoch total loss 5.59131622\n",
      "Trained batch 409 batch loss 5.38493156 epoch total loss 5.59081173\n",
      "Trained batch 410 batch loss 5.12379646 epoch total loss 5.58967304\n",
      "Trained batch 411 batch loss 5.413 epoch total loss 5.58924341\n",
      "Trained batch 412 batch loss 5.85527515 epoch total loss 5.58988857\n",
      "Trained batch 413 batch loss 6.26203728 epoch total loss 5.59151602\n",
      "Trained batch 414 batch loss 5.06616688 epoch total loss 5.59024715\n",
      "Trained batch 415 batch loss 5.17405176 epoch total loss 5.58924437\n",
      "Trained batch 416 batch loss 5.13558865 epoch total loss 5.58815336\n",
      "Trained batch 417 batch loss 5.94438362 epoch total loss 5.58900785\n",
      "Trained batch 418 batch loss 5.91888428 epoch total loss 5.58979702\n",
      "Trained batch 419 batch loss 5.55577755 epoch total loss 5.58971548\n",
      "Trained batch 420 batch loss 5.68457508 epoch total loss 5.5899415\n",
      "Trained batch 421 batch loss 6.49120617 epoch total loss 5.59208202\n",
      "Trained batch 422 batch loss 6.72198248 epoch total loss 5.59475946\n",
      "Trained batch 423 batch loss 7.05714226 epoch total loss 5.59821653\n",
      "Trained batch 424 batch loss 6.15670109 epoch total loss 5.59953403\n",
      "Trained batch 425 batch loss 5.5855689 epoch total loss 5.59950066\n",
      "Trained batch 426 batch loss 5.52008486 epoch total loss 5.59931421\n",
      "Trained batch 427 batch loss 4.61307383 epoch total loss 5.59700441\n",
      "Trained batch 428 batch loss 4.57272053 epoch total loss 5.59461117\n",
      "Trained batch 429 batch loss 5.32459927 epoch total loss 5.59398222\n",
      "Trained batch 430 batch loss 5.7726264 epoch total loss 5.59439802\n",
      "Trained batch 431 batch loss 5.59781027 epoch total loss 5.59440613\n",
      "Trained batch 432 batch loss 5.10648966 epoch total loss 5.5932765\n",
      "Trained batch 433 batch loss 6.52153397 epoch total loss 5.59542\n",
      "Trained batch 434 batch loss 6.54574966 epoch total loss 5.59760952\n",
      "Trained batch 435 batch loss 5.47101212 epoch total loss 5.59731817\n",
      "Trained batch 436 batch loss 5.62848616 epoch total loss 5.5973897\n",
      "Trained batch 437 batch loss 6.42128181 epoch total loss 5.59927511\n",
      "Trained batch 438 batch loss 6.71579647 epoch total loss 5.60182428\n",
      "Trained batch 439 batch loss 5.77549505 epoch total loss 5.60222\n",
      "Trained batch 440 batch loss 5.2997 epoch total loss 5.60153246\n",
      "Trained batch 441 batch loss 5.49777746 epoch total loss 5.60129738\n",
      "Trained batch 442 batch loss 5.93830109 epoch total loss 5.60206\n",
      "Trained batch 443 batch loss 5.89732361 epoch total loss 5.60272598\n",
      "Trained batch 444 batch loss 5.84982586 epoch total loss 5.60328245\n",
      "Trained batch 445 batch loss 6.56960726 epoch total loss 5.60545397\n",
      "Trained batch 446 batch loss 5.84079647 epoch total loss 5.60598183\n",
      "Trained batch 447 batch loss 5.9810133 epoch total loss 5.60682058\n",
      "Trained batch 448 batch loss 6.30907536 epoch total loss 5.60838795\n",
      "Trained batch 449 batch loss 5.55466032 epoch total loss 5.60826826\n",
      "Trained batch 450 batch loss 5.19166 epoch total loss 5.60734272\n",
      "Trained batch 451 batch loss 7.04224348 epoch total loss 5.61052418\n",
      "Trained batch 452 batch loss 5.85526 epoch total loss 5.61106539\n",
      "Trained batch 453 batch loss 5.88894558 epoch total loss 5.61167908\n",
      "Trained batch 454 batch loss 5.69696045 epoch total loss 5.61186695\n",
      "Trained batch 455 batch loss 5.93479204 epoch total loss 5.61257696\n",
      "Trained batch 456 batch loss 5.88338566 epoch total loss 5.61317062\n",
      "Trained batch 457 batch loss 6.03285408 epoch total loss 5.61408901\n",
      "Trained batch 458 batch loss 5.80565453 epoch total loss 5.6145072\n",
      "Trained batch 459 batch loss 6.59216 epoch total loss 5.61663675\n",
      "Trained batch 460 batch loss 5.52035093 epoch total loss 5.61642742\n",
      "Trained batch 461 batch loss 5.43876266 epoch total loss 5.61604214\n",
      "Trained batch 462 batch loss 5.42415714 epoch total loss 5.61562634\n",
      "Trained batch 463 batch loss 5.97885418 epoch total loss 5.61641073\n",
      "Trained batch 464 batch loss 6.03071833 epoch total loss 5.61730385\n",
      "Trained batch 465 batch loss 6.23724556 epoch total loss 5.61863708\n",
      "Trained batch 466 batch loss 6.35885715 epoch total loss 5.62022543\n",
      "Trained batch 467 batch loss 6.13129616 epoch total loss 5.62132025\n",
      "Trained batch 468 batch loss 5.26774693 epoch total loss 5.62056494\n",
      "Trained batch 469 batch loss 6.88810349 epoch total loss 5.62326765\n",
      "Trained batch 470 batch loss 6.09314775 epoch total loss 5.62426758\n",
      "Trained batch 471 batch loss 5.75597191 epoch total loss 5.624547\n",
      "Trained batch 472 batch loss 5.96584845 epoch total loss 5.62527\n",
      "Trained batch 473 batch loss 5.42481232 epoch total loss 5.62484598\n",
      "Trained batch 474 batch loss 6.19210815 epoch total loss 5.62604284\n",
      "Trained batch 475 batch loss 7.14143705 epoch total loss 5.62923336\n",
      "Trained batch 476 batch loss 5.54320288 epoch total loss 5.62905264\n",
      "Trained batch 477 batch loss 5.71015167 epoch total loss 5.62922239\n",
      "Trained batch 478 batch loss 4.53352928 epoch total loss 5.62693\n",
      "Trained batch 479 batch loss 5.55349445 epoch total loss 5.6267767\n",
      "Trained batch 480 batch loss 5.59498692 epoch total loss 5.62671041\n",
      "Trained batch 481 batch loss 6.45112467 epoch total loss 5.62842464\n",
      "Trained batch 482 batch loss 6.38859272 epoch total loss 5.63000202\n",
      "Trained batch 483 batch loss 5.87066317 epoch total loss 5.6305\n",
      "Trained batch 484 batch loss 5.85530758 epoch total loss 5.63096428\n",
      "Trained batch 485 batch loss 6.21753883 epoch total loss 5.63217354\n",
      "Trained batch 486 batch loss 5.93926048 epoch total loss 5.63280535\n",
      "Trained batch 487 batch loss 6.11896086 epoch total loss 5.63380337\n",
      "Trained batch 488 batch loss 6.07636166 epoch total loss 5.63471079\n",
      "Trained batch 489 batch loss 5.85564899 epoch total loss 5.63516235\n",
      "Trained batch 490 batch loss 6.04542446 epoch total loss 5.63599968\n",
      "Trained batch 491 batch loss 4.97396946 epoch total loss 5.63465118\n",
      "Trained batch 492 batch loss 4.57670498 epoch total loss 5.63250065\n",
      "Trained batch 493 batch loss 5.83644485 epoch total loss 5.63291454\n",
      "Trained batch 494 batch loss 5.67309427 epoch total loss 5.63299608\n",
      "Trained batch 495 batch loss 5.58998108 epoch total loss 5.6329093\n",
      "Trained batch 496 batch loss 5.82391882 epoch total loss 5.63329458\n",
      "Trained batch 497 batch loss 5.46456766 epoch total loss 5.63295507\n",
      "Trained batch 498 batch loss 5.52570915 epoch total loss 5.63273954\n",
      "Trained batch 499 batch loss 5.93172121 epoch total loss 5.63333845\n",
      "Trained batch 500 batch loss 5.55262232 epoch total loss 5.63317728\n",
      "Trained batch 501 batch loss 5.5474987 epoch total loss 5.63300657\n",
      "Trained batch 502 batch loss 5.55930805 epoch total loss 5.63285971\n",
      "Trained batch 503 batch loss 5.17652369 epoch total loss 5.63195229\n",
      "Trained batch 504 batch loss 5.70649767 epoch total loss 5.63210058\n",
      "Trained batch 505 batch loss 5.82264614 epoch total loss 5.63247776\n",
      "Trained batch 506 batch loss 5.88193703 epoch total loss 5.63297081\n",
      "Trained batch 507 batch loss 5.02171612 epoch total loss 5.63176537\n",
      "Trained batch 508 batch loss 5.41001368 epoch total loss 5.63132858\n",
      "Trained batch 509 batch loss 5.0152092 epoch total loss 5.63011789\n",
      "Trained batch 510 batch loss 4.52429485 epoch total loss 5.62794971\n",
      "Trained batch 511 batch loss 4.62487221 epoch total loss 5.62598658\n",
      "Trained batch 512 batch loss 5.11062288 epoch total loss 5.62498\n",
      "Trained batch 513 batch loss 4.00766611 epoch total loss 5.62182713\n",
      "Trained batch 514 batch loss 3.83709764 epoch total loss 5.6183548\n",
      "Trained batch 515 batch loss 4.55956745 epoch total loss 5.61629915\n",
      "Trained batch 516 batch loss 5.73370314 epoch total loss 5.6165266\n",
      "Trained batch 517 batch loss 5.92209435 epoch total loss 5.6171174\n",
      "Trained batch 518 batch loss 6.07248068 epoch total loss 5.61799669\n",
      "Trained batch 519 batch loss 5.56868124 epoch total loss 5.6179018\n",
      "Trained batch 520 batch loss 6.1546979 epoch total loss 5.61893415\n",
      "Trained batch 521 batch loss 4.83185148 epoch total loss 5.61742306\n",
      "Trained batch 522 batch loss 5.16095638 epoch total loss 5.61654854\n",
      "Trained batch 523 batch loss 5.26057196 epoch total loss 5.61586761\n",
      "Trained batch 524 batch loss 5.77173853 epoch total loss 5.61616516\n",
      "Trained batch 525 batch loss 4.61999512 epoch total loss 5.61426783\n",
      "Trained batch 526 batch loss 2.85063839 epoch total loss 5.60901403\n",
      "Trained batch 527 batch loss 4.72919369 epoch total loss 5.60734463\n",
      "Trained batch 528 batch loss 5.52592945 epoch total loss 5.60719\n",
      "Trained batch 529 batch loss 4.91890478 epoch total loss 5.60588932\n",
      "Trained batch 530 batch loss 5.87388945 epoch total loss 5.60639477\n",
      "Trained batch 531 batch loss 5.21766138 epoch total loss 5.60566282\n",
      "Trained batch 532 batch loss 4.83387709 epoch total loss 5.60421228\n",
      "Trained batch 533 batch loss 5.00284576 epoch total loss 5.60308409\n",
      "Trained batch 534 batch loss 5.78059483 epoch total loss 5.60341644\n",
      "Trained batch 535 batch loss 6.42221355 epoch total loss 5.60494661\n",
      "Trained batch 536 batch loss 6.22476578 epoch total loss 5.60610342\n",
      "Trained batch 537 batch loss 7.1324892 epoch total loss 5.60894585\n",
      "Trained batch 538 batch loss 6.78770161 epoch total loss 5.61113644\n",
      "Trained batch 539 batch loss 6.51655579 epoch total loss 5.61281633\n",
      "Trained batch 540 batch loss 6.01636887 epoch total loss 5.61356401\n",
      "Trained batch 541 batch loss 6.63094425 epoch total loss 5.61544418\n",
      "Trained batch 542 batch loss 6.70057201 epoch total loss 5.61744642\n",
      "Trained batch 543 batch loss 6.44693089 epoch total loss 5.61897421\n",
      "Trained batch 544 batch loss 6.3420639 epoch total loss 5.62030363\n",
      "Trained batch 545 batch loss 6.37413788 epoch total loss 5.62168646\n",
      "Trained batch 546 batch loss 5.91140938 epoch total loss 5.62221718\n",
      "Trained batch 547 batch loss 6.36382675 epoch total loss 5.62357283\n",
      "Trained batch 548 batch loss 6.11401081 epoch total loss 5.62446785\n",
      "Trained batch 549 batch loss 5.86271191 epoch total loss 5.62490177\n",
      "Trained batch 550 batch loss 5.43056393 epoch total loss 5.62454844\n",
      "Trained batch 551 batch loss 6.20165443 epoch total loss 5.62559605\n",
      "Trained batch 552 batch loss 6.47162771 epoch total loss 5.6271286\n",
      "Trained batch 553 batch loss 6.40476322 epoch total loss 5.62853479\n",
      "Trained batch 554 batch loss 6.44007969 epoch total loss 5.63\n",
      "Trained batch 555 batch loss 6.320117 epoch total loss 5.63124323\n",
      "Trained batch 556 batch loss 5.73365068 epoch total loss 5.63142776\n",
      "Trained batch 557 batch loss 6.03157663 epoch total loss 5.63214588\n",
      "Trained batch 558 batch loss 5.96297 epoch total loss 5.63273859\n",
      "Trained batch 559 batch loss 5.97929382 epoch total loss 5.63335848\n",
      "Trained batch 560 batch loss 4.97213602 epoch total loss 5.63217783\n",
      "Trained batch 561 batch loss 5.87398529 epoch total loss 5.63260889\n",
      "Trained batch 562 batch loss 6.60729504 epoch total loss 5.63434315\n",
      "Trained batch 563 batch loss 6.62336445 epoch total loss 5.63609934\n",
      "Trained batch 564 batch loss 6.36933422 epoch total loss 5.63739967\n",
      "Trained batch 565 batch loss 6.56728697 epoch total loss 5.63904572\n",
      "Trained batch 566 batch loss 5.48398733 epoch total loss 5.63877153\n",
      "Trained batch 567 batch loss 5.90780544 epoch total loss 5.63924599\n",
      "Trained batch 568 batch loss 6.22951269 epoch total loss 5.64028502\n",
      "Trained batch 569 batch loss 6.74373865 epoch total loss 5.64222431\n",
      "Trained batch 570 batch loss 6.66776037 epoch total loss 5.64402342\n",
      "Trained batch 571 batch loss 6.73851585 epoch total loss 5.6459403\n",
      "Trained batch 572 batch loss 6.32654047 epoch total loss 5.64713\n",
      "Trained batch 573 batch loss 6.28009081 epoch total loss 5.64823484\n",
      "Trained batch 574 batch loss 5.94477654 epoch total loss 5.64875126\n",
      "Trained batch 575 batch loss 6.16898346 epoch total loss 5.6496563\n",
      "Trained batch 576 batch loss 6.14378071 epoch total loss 5.65051413\n",
      "Trained batch 577 batch loss 5.85194397 epoch total loss 5.65086317\n",
      "Trained batch 578 batch loss 5.54798317 epoch total loss 5.65068531\n",
      "Trained batch 579 batch loss 5.57890415 epoch total loss 5.65056133\n",
      "Trained batch 580 batch loss 5.68138933 epoch total loss 5.65061474\n",
      "Trained batch 581 batch loss 5.32316113 epoch total loss 5.65005112\n",
      "Trained batch 582 batch loss 5.35540199 epoch total loss 5.64954472\n",
      "Trained batch 583 batch loss 4.96682072 epoch total loss 5.64837408\n",
      "Trained batch 584 batch loss 5.58604574 epoch total loss 5.64826679\n",
      "Trained batch 585 batch loss 5.52865696 epoch total loss 5.64806223\n",
      "Trained batch 586 batch loss 5.0140543 epoch total loss 5.64698076\n",
      "Trained batch 587 batch loss 5.45934582 epoch total loss 5.6466608\n",
      "Trained batch 588 batch loss 4.51747417 epoch total loss 5.64474058\n",
      "Trained batch 589 batch loss 5.33310461 epoch total loss 5.64421129\n",
      "Trained batch 590 batch loss 4.91381645 epoch total loss 5.64297342\n",
      "Trained batch 591 batch loss 4.65575123 epoch total loss 5.64130306\n",
      "Trained batch 592 batch loss 4.98570251 epoch total loss 5.64019537\n",
      "Trained batch 593 batch loss 5.46660614 epoch total loss 5.63990259\n",
      "Trained batch 594 batch loss 5.76565 epoch total loss 5.64011431\n",
      "Trained batch 595 batch loss 6.09035969 epoch total loss 5.64087057\n",
      "Trained batch 596 batch loss 5.86326218 epoch total loss 5.64124393\n",
      "Trained batch 597 batch loss 5.9510746 epoch total loss 5.64176321\n",
      "Trained batch 598 batch loss 5.6790781 epoch total loss 5.64182568\n",
      "Trained batch 599 batch loss 5.75187397 epoch total loss 5.64200974\n",
      "Trained batch 600 batch loss 5.83288574 epoch total loss 5.64232731\n",
      "Trained batch 601 batch loss 6.06902885 epoch total loss 5.6430378\n",
      "Trained batch 602 batch loss 5.78562832 epoch total loss 5.64327431\n",
      "Trained batch 603 batch loss 5.92369938 epoch total loss 5.64373922\n",
      "Trained batch 604 batch loss 5.9513464 epoch total loss 5.64424849\n",
      "Trained batch 605 batch loss 5.77758026 epoch total loss 5.64446926\n",
      "Trained batch 606 batch loss 5.98290682 epoch total loss 5.64502764\n",
      "Trained batch 607 batch loss 6.09717 epoch total loss 5.64577246\n",
      "Trained batch 608 batch loss 6.12860346 epoch total loss 5.64656687\n",
      "Trained batch 609 batch loss 5.45292711 epoch total loss 5.64624882\n",
      "Trained batch 610 batch loss 5.66342831 epoch total loss 5.64627647\n",
      "Trained batch 611 batch loss 5.39212704 epoch total loss 5.64586067\n",
      "Trained batch 612 batch loss 4.22223473 epoch total loss 5.64353418\n",
      "Trained batch 613 batch loss 4.92854643 epoch total loss 5.64236784\n",
      "Trained batch 614 batch loss 4.85998535 epoch total loss 5.64109325\n",
      "Trained batch 615 batch loss 5.38987255 epoch total loss 5.64068508\n",
      "Trained batch 616 batch loss 5.86982727 epoch total loss 5.64105701\n",
      "Trained batch 617 batch loss 5.52768898 epoch total loss 5.64087296\n",
      "Trained batch 618 batch loss 6.04645348 epoch total loss 5.64152908\n",
      "Trained batch 619 batch loss 6.04716492 epoch total loss 5.64218473\n",
      "Trained batch 620 batch loss 4.59286785 epoch total loss 5.64049196\n",
      "Trained batch 621 batch loss 6.30371952 epoch total loss 5.64156\n",
      "Trained batch 622 batch loss 6.49379444 epoch total loss 5.64293\n",
      "Trained batch 623 batch loss 6.17856359 epoch total loss 5.64379\n",
      "Trained batch 624 batch loss 5.73268795 epoch total loss 5.64393234\n",
      "Trained batch 625 batch loss 7.16291523 epoch total loss 5.6463623\n",
      "Trained batch 626 batch loss 6.09248829 epoch total loss 5.64707518\n",
      "Trained batch 627 batch loss 6.00871086 epoch total loss 5.64765215\n",
      "Trained batch 628 batch loss 6.22032738 epoch total loss 5.64856386\n",
      "Trained batch 629 batch loss 6.27749157 epoch total loss 5.64956379\n",
      "Trained batch 630 batch loss 6.25042248 epoch total loss 5.65051794\n",
      "Trained batch 631 batch loss 6.11842871 epoch total loss 5.65125942\n",
      "Trained batch 632 batch loss 6.15563202 epoch total loss 5.65205717\n",
      "Trained batch 633 batch loss 6.41352558 epoch total loss 5.65326\n",
      "Trained batch 634 batch loss 5.10773373 epoch total loss 5.65239954\n",
      "Trained batch 635 batch loss 4.70232391 epoch total loss 5.6509037\n",
      "Trained batch 636 batch loss 4.88495255 epoch total loss 5.64969921\n",
      "Trained batch 637 batch loss 6.1076932 epoch total loss 5.65041828\n",
      "Trained batch 638 batch loss 5.79426289 epoch total loss 5.65064335\n",
      "Trained batch 639 batch loss 4.90367746 epoch total loss 5.64947462\n",
      "Trained batch 640 batch loss 4.74581099 epoch total loss 5.64806271\n",
      "Trained batch 641 batch loss 5.84803057 epoch total loss 5.64837456\n",
      "Trained batch 642 batch loss 5.36413193 epoch total loss 5.64793158\n",
      "Trained batch 643 batch loss 6.22099924 epoch total loss 5.64882278\n",
      "Trained batch 644 batch loss 4.36254597 epoch total loss 5.64682579\n",
      "Trained batch 645 batch loss 6.299963 epoch total loss 5.64783812\n",
      "Trained batch 646 batch loss 6.2670393 epoch total loss 5.64879704\n",
      "Trained batch 647 batch loss 5.46646738 epoch total loss 5.64851522\n",
      "Trained batch 648 batch loss 5.71549034 epoch total loss 5.6486187\n",
      "Trained batch 649 batch loss 5.73400974 epoch total loss 5.64875031\n",
      "Trained batch 650 batch loss 5.60787296 epoch total loss 5.64868784\n",
      "Trained batch 651 batch loss 5.91342449 epoch total loss 5.6490941\n",
      "Trained batch 652 batch loss 4.93690109 epoch total loss 5.64800215\n",
      "Trained batch 653 batch loss 5.4245491 epoch total loss 5.64766\n",
      "Trained batch 654 batch loss 6.57832432 epoch total loss 5.64908314\n",
      "Trained batch 655 batch loss 6.33247423 epoch total loss 5.65012646\n",
      "Trained batch 656 batch loss 6.5049181 epoch total loss 5.65142918\n",
      "Trained batch 657 batch loss 6.3393116 epoch total loss 5.65247631\n",
      "Trained batch 658 batch loss 6.46559429 epoch total loss 5.65371227\n",
      "Trained batch 659 batch loss 6.31680584 epoch total loss 5.6547184\n",
      "Trained batch 660 batch loss 6.63583755 epoch total loss 5.6562047\n",
      "Trained batch 661 batch loss 6.16382742 epoch total loss 5.65697289\n",
      "Trained batch 662 batch loss 6.91165352 epoch total loss 5.65886784\n",
      "Trained batch 663 batch loss 6.28621197 epoch total loss 5.65981436\n",
      "Trained batch 664 batch loss 6.17604446 epoch total loss 5.6605916\n",
      "Trained batch 665 batch loss 6.4142518 epoch total loss 5.66172504\n",
      "Trained batch 666 batch loss 5.97836781 epoch total loss 5.66220045\n",
      "Trained batch 667 batch loss 6.30842972 epoch total loss 5.66316891\n",
      "Trained batch 668 batch loss 5.74830151 epoch total loss 5.66329622\n",
      "Trained batch 669 batch loss 6.02454329 epoch total loss 5.66383648\n",
      "Trained batch 670 batch loss 5.40566635 epoch total loss 5.66345119\n",
      "Trained batch 671 batch loss 4.64800358 epoch total loss 5.66193819\n",
      "Trained batch 672 batch loss 5.59291601 epoch total loss 5.66183567\n",
      "Trained batch 673 batch loss 5.83559322 epoch total loss 5.66209364\n",
      "Trained batch 674 batch loss 5.62229347 epoch total loss 5.66203499\n",
      "Trained batch 675 batch loss 6.47027779 epoch total loss 5.66323185\n",
      "Trained batch 676 batch loss 6.2572794 epoch total loss 5.66411066\n",
      "Trained batch 677 batch loss 5.97646809 epoch total loss 5.66457224\n",
      "Trained batch 678 batch loss 6.5097332 epoch total loss 5.66581917\n",
      "Trained batch 679 batch loss 6.08521795 epoch total loss 5.66643667\n",
      "Trained batch 680 batch loss 5.96182346 epoch total loss 5.66687107\n",
      "Trained batch 681 batch loss 6.18315887 epoch total loss 5.66762924\n",
      "Trained batch 682 batch loss 6.44129467 epoch total loss 5.66876364\n",
      "Trained batch 683 batch loss 5.91591215 epoch total loss 5.66912603\n",
      "Trained batch 684 batch loss 6.24273968 epoch total loss 5.66996431\n",
      "Trained batch 685 batch loss 5.71614504 epoch total loss 5.67003155\n",
      "Trained batch 686 batch loss 6.21067 epoch total loss 5.67081976\n",
      "Trained batch 687 batch loss 5.96790218 epoch total loss 5.67125225\n",
      "Trained batch 688 batch loss 6.17920494 epoch total loss 5.67199087\n",
      "Trained batch 689 batch loss 5.76915312 epoch total loss 5.67213154\n",
      "Trained batch 690 batch loss 6.16691828 epoch total loss 5.6728487\n",
      "Trained batch 691 batch loss 5.73012733 epoch total loss 5.67293167\n",
      "Trained batch 692 batch loss 5.72034883 epoch total loss 5.67300034\n",
      "Trained batch 693 batch loss 6.43721771 epoch total loss 5.67410326\n",
      "Trained batch 694 batch loss 5.91560221 epoch total loss 5.67445135\n",
      "Trained batch 695 batch loss 6.34086704 epoch total loss 5.67541\n",
      "Trained batch 696 batch loss 6.69214439 epoch total loss 5.67687082\n",
      "Trained batch 697 batch loss 6.3161974 epoch total loss 5.67778778\n",
      "Trained batch 698 batch loss 5.73675728 epoch total loss 5.67787266\n",
      "Trained batch 699 batch loss 5.94025469 epoch total loss 5.67824793\n",
      "Trained batch 700 batch loss 5.86245251 epoch total loss 5.67851114\n",
      "Trained batch 701 batch loss 6.20757198 epoch total loss 5.67926598\n",
      "Trained batch 702 batch loss 5.8942976 epoch total loss 5.67957211\n",
      "Trained batch 703 batch loss 5.84649849 epoch total loss 5.67980957\n",
      "Trained batch 704 batch loss 5.97649574 epoch total loss 5.68023109\n",
      "Trained batch 705 batch loss 5.53747892 epoch total loss 5.68002844\n",
      "Trained batch 706 batch loss 5.68467236 epoch total loss 5.68003511\n",
      "Trained batch 707 batch loss 6.68342 epoch total loss 5.68145418\n",
      "Trained batch 708 batch loss 5.0498395 epoch total loss 5.68056202\n",
      "Trained batch 709 batch loss 4.49678373 epoch total loss 5.67889261\n",
      "Trained batch 710 batch loss 4.83143139 epoch total loss 5.67769909\n",
      "Trained batch 711 batch loss 5.97478628 epoch total loss 5.6781168\n",
      "Trained batch 712 batch loss 5.70878839 epoch total loss 5.67815971\n",
      "Trained batch 713 batch loss 5.55222416 epoch total loss 5.67798328\n",
      "Trained batch 714 batch loss 5.83326197 epoch total loss 5.67820072\n",
      "Trained batch 715 batch loss 6.07296562 epoch total loss 5.6787529\n",
      "Trained batch 716 batch loss 6.41012 epoch total loss 5.67977428\n",
      "Trained batch 717 batch loss 5.77518463 epoch total loss 5.67990732\n",
      "Trained batch 718 batch loss 5.33893585 epoch total loss 5.67943239\n",
      "Trained batch 719 batch loss 5.68211031 epoch total loss 5.67943621\n",
      "Trained batch 720 batch loss 6.37634945 epoch total loss 5.68040419\n",
      "Trained batch 721 batch loss 5.41752815 epoch total loss 5.68004\n",
      "Trained batch 722 batch loss 5.76535225 epoch total loss 5.68015766\n",
      "Trained batch 723 batch loss 5.98506546 epoch total loss 5.68057919\n",
      "Trained batch 724 batch loss 6.19483137 epoch total loss 5.6812892\n",
      "Trained batch 725 batch loss 6.27495384 epoch total loss 5.68210793\n",
      "Trained batch 726 batch loss 5.2971735 epoch total loss 5.68157816\n",
      "Trained batch 727 batch loss 6.58383322 epoch total loss 5.68281937\n",
      "Trained batch 728 batch loss 5.73757124 epoch total loss 5.68289471\n",
      "Trained batch 729 batch loss 6.7014246 epoch total loss 5.68429232\n",
      "Trained batch 730 batch loss 6.36726427 epoch total loss 5.68522787\n",
      "Trained batch 731 batch loss 5.69167042 epoch total loss 5.68523693\n",
      "Trained batch 732 batch loss 6.23095798 epoch total loss 5.68598223\n",
      "Trained batch 733 batch loss 4.62089062 epoch total loss 5.68453\n",
      "Trained batch 734 batch loss 5.52612257 epoch total loss 5.6843133\n",
      "Trained batch 735 batch loss 6.58336639 epoch total loss 5.68553686\n",
      "Trained batch 736 batch loss 5.98662663 epoch total loss 5.68594646\n",
      "Trained batch 737 batch loss 6.58672094 epoch total loss 5.6871686\n",
      "Trained batch 738 batch loss 5.44884205 epoch total loss 5.68684578\n",
      "Trained batch 739 batch loss 5.9274 epoch total loss 5.68717098\n",
      "Trained batch 740 batch loss 4.6090107 epoch total loss 5.68571377\n",
      "Trained batch 741 batch loss 5.97278738 epoch total loss 5.68610096\n",
      "Trained batch 742 batch loss 6.02622795 epoch total loss 5.68655968\n",
      "Trained batch 743 batch loss 5.63667393 epoch total loss 5.68649244\n",
      "Trained batch 744 batch loss 5.66720772 epoch total loss 5.68646622\n",
      "Trained batch 745 batch loss 6.45086908 epoch total loss 5.68749237\n",
      "Trained batch 746 batch loss 5.73078632 epoch total loss 5.68755054\n",
      "Trained batch 747 batch loss 5.33455 epoch total loss 5.68707752\n",
      "Trained batch 748 batch loss 5.19679356 epoch total loss 5.68642235\n",
      "Trained batch 749 batch loss 5.62804794 epoch total loss 5.68634415\n",
      "Trained batch 750 batch loss 5.48269939 epoch total loss 5.68607283\n",
      "Trained batch 751 batch loss 5.16155863 epoch total loss 5.68537474\n",
      "Trained batch 752 batch loss 5.38331127 epoch total loss 5.68497276\n",
      "Trained batch 753 batch loss 5.875875 epoch total loss 5.68522644\n",
      "Trained batch 754 batch loss 6.03164291 epoch total loss 5.68568611\n",
      "Trained batch 755 batch loss 5.99546957 epoch total loss 5.68609667\n",
      "Trained batch 756 batch loss 5.74157095 epoch total loss 5.68617\n",
      "Trained batch 757 batch loss 5.80981922 epoch total loss 5.68633366\n",
      "Trained batch 758 batch loss 5.8173151 epoch total loss 5.68650675\n",
      "Trained batch 759 batch loss 5.93663692 epoch total loss 5.68683624\n",
      "Trained batch 760 batch loss 5.62473 epoch total loss 5.68675423\n",
      "Trained batch 761 batch loss 6.11273861 epoch total loss 5.68731403\n",
      "Trained batch 762 batch loss 5.41612291 epoch total loss 5.68695784\n",
      "Trained batch 763 batch loss 5.53644657 epoch total loss 5.6867609\n",
      "Trained batch 764 batch loss 5.21079159 epoch total loss 5.68613815\n",
      "Trained batch 765 batch loss 6.14173269 epoch total loss 5.68673325\n",
      "Trained batch 766 batch loss 5.36728907 epoch total loss 5.68631649\n",
      "Trained batch 767 batch loss 5.88512421 epoch total loss 5.68657541\n",
      "Trained batch 768 batch loss 5.26362038 epoch total loss 5.68602514\n",
      "Trained batch 769 batch loss 5.46994591 epoch total loss 5.68574381\n",
      "Trained batch 770 batch loss 6.20998144 epoch total loss 5.68642473\n",
      "Trained batch 771 batch loss 5.9764719 epoch total loss 5.68680096\n",
      "Trained batch 772 batch loss 5.68724155 epoch total loss 5.68680096\n",
      "Trained batch 773 batch loss 5.79713345 epoch total loss 5.68694401\n",
      "Trained batch 774 batch loss 5.96007633 epoch total loss 5.68729687\n",
      "Trained batch 775 batch loss 5.55668068 epoch total loss 5.68712807\n",
      "Trained batch 776 batch loss 6.40742207 epoch total loss 5.68805647\n",
      "Trained batch 777 batch loss 5.53557301 epoch total loss 5.68786\n",
      "Trained batch 778 batch loss 6.15248537 epoch total loss 5.68845701\n",
      "Trained batch 779 batch loss 6.18595743 epoch total loss 5.68909597\n",
      "Trained batch 780 batch loss 6.11316586 epoch total loss 5.68963957\n",
      "Trained batch 781 batch loss 5.95642614 epoch total loss 5.68998146\n",
      "Trained batch 782 batch loss 6.75611687 epoch total loss 5.69134521\n",
      "Trained batch 783 batch loss 6.2035 epoch total loss 5.69199944\n",
      "Trained batch 784 batch loss 6.49645901 epoch total loss 5.69302559\n",
      "Trained batch 785 batch loss 5.98375797 epoch total loss 5.69339609\n",
      "Trained batch 786 batch loss 6.65278053 epoch total loss 5.69461679\n",
      "Trained batch 787 batch loss 6.10557175 epoch total loss 5.69513893\n",
      "Trained batch 788 batch loss 5.7687397 epoch total loss 5.69523191\n",
      "Trained batch 789 batch loss 5.65459 epoch total loss 5.69518089\n",
      "Trained batch 790 batch loss 6.89336 epoch total loss 5.69669771\n",
      "Trained batch 791 batch loss 6.21673441 epoch total loss 5.69735527\n",
      "Trained batch 792 batch loss 6.67655563 epoch total loss 5.69859171\n",
      "Trained batch 793 batch loss 6.65413 epoch total loss 5.69979715\n",
      "Trained batch 794 batch loss 6.45199394 epoch total loss 5.70074463\n",
      "Trained batch 795 batch loss 6.21797943 epoch total loss 5.70139503\n",
      "Trained batch 796 batch loss 6.65338802 epoch total loss 5.70259094\n",
      "Trained batch 797 batch loss 5.8635869 epoch total loss 5.70279312\n",
      "Trained batch 798 batch loss 6.01550341 epoch total loss 5.70318508\n",
      "Trained batch 799 batch loss 5.97716141 epoch total loss 5.70352793\n",
      "Trained batch 800 batch loss 6.67255211 epoch total loss 5.70473862\n",
      "Trained batch 801 batch loss 5.92888117 epoch total loss 5.70501852\n",
      "Trained batch 802 batch loss 5.87671375 epoch total loss 5.70523262\n",
      "Trained batch 803 batch loss 6.09358358 epoch total loss 5.70571661\n",
      "Trained batch 804 batch loss 5.87045097 epoch total loss 5.70592165\n",
      "Trained batch 805 batch loss 5.76519108 epoch total loss 5.70599508\n",
      "Trained batch 806 batch loss 5.77765703 epoch total loss 5.70608425\n",
      "Trained batch 807 batch loss 6.18608761 epoch total loss 5.70667934\n",
      "Trained batch 808 batch loss 6.63272619 epoch total loss 5.70782518\n",
      "Trained batch 809 batch loss 6.2545414 epoch total loss 5.70850086\n",
      "Trained batch 810 batch loss 6.21813679 epoch total loss 5.70913029\n",
      "Trained batch 811 batch loss 5.40021706 epoch total loss 5.70875\n",
      "Trained batch 812 batch loss 6.0944972 epoch total loss 5.7092247\n",
      "Trained batch 813 batch loss 6.43314791 epoch total loss 5.71011543\n",
      "Trained batch 814 batch loss 6.40718079 epoch total loss 5.71097183\n",
      "Trained batch 815 batch loss 6.22294617 epoch total loss 5.7116003\n",
      "Trained batch 816 batch loss 5.93952179 epoch total loss 5.71187925\n",
      "Trained batch 817 batch loss 6.26936054 epoch total loss 5.71256208\n",
      "Trained batch 818 batch loss 6.28531599 epoch total loss 5.71326208\n",
      "Trained batch 819 batch loss 6.17280626 epoch total loss 5.71382284\n",
      "Trained batch 820 batch loss 5.81423092 epoch total loss 5.71394587\n",
      "Trained batch 821 batch loss 6.46969271 epoch total loss 5.71486616\n",
      "Trained batch 822 batch loss 6.62471485 epoch total loss 5.7159729\n",
      "Trained batch 823 batch loss 6.03453445 epoch total loss 5.71636\n",
      "Trained batch 824 batch loss 5.14766741 epoch total loss 5.71566963\n",
      "Trained batch 825 batch loss 6.39742136 epoch total loss 5.71649599\n",
      "Trained batch 826 batch loss 5.37419033 epoch total loss 5.71608162\n",
      "Trained batch 827 batch loss 6.00690746 epoch total loss 5.71643305\n",
      "Trained batch 828 batch loss 6.52095032 epoch total loss 5.71740484\n",
      "Trained batch 829 batch loss 6.33447552 epoch total loss 5.71814919\n",
      "Trained batch 830 batch loss 5.05371523 epoch total loss 5.71734858\n",
      "Trained batch 831 batch loss 6.15642071 epoch total loss 5.71787691\n",
      "Trained batch 832 batch loss 6.34025574 epoch total loss 5.71862507\n",
      "Trained batch 833 batch loss 5.06983376 epoch total loss 5.71784592\n",
      "Trained batch 834 batch loss 4.84296322 epoch total loss 5.71679688\n",
      "Trained batch 835 batch loss 5.88349485 epoch total loss 5.71699619\n",
      "Trained batch 836 batch loss 6.09652233 epoch total loss 5.71745062\n",
      "Trained batch 837 batch loss 5.92138624 epoch total loss 5.71769428\n",
      "Trained batch 838 batch loss 6.04273891 epoch total loss 5.71808243\n",
      "Trained batch 839 batch loss 6.58264589 epoch total loss 5.7191124\n",
      "Trained batch 840 batch loss 6.09497356 epoch total loss 5.71956\n",
      "Trained batch 841 batch loss 4.35038662 epoch total loss 5.7179327\n",
      "Trained batch 842 batch loss 5.73463964 epoch total loss 5.71795273\n",
      "Trained batch 843 batch loss 5.62740898 epoch total loss 5.71784544\n",
      "Trained batch 844 batch loss 6.00301361 epoch total loss 5.71818304\n",
      "Trained batch 845 batch loss 5.98558807 epoch total loss 5.71849918\n",
      "Trained batch 846 batch loss 6.1591897 epoch total loss 5.71902\n",
      "Trained batch 847 batch loss 6.18691778 epoch total loss 5.71957254\n",
      "Trained batch 848 batch loss 5.26067495 epoch total loss 5.71903133\n",
      "Trained batch 849 batch loss 5.63712883 epoch total loss 5.71893501\n",
      "Trained batch 850 batch loss 5.62629318 epoch total loss 5.71882629\n",
      "Trained batch 851 batch loss 5.92882156 epoch total loss 5.71907282\n",
      "Trained batch 852 batch loss 5.93317461 epoch total loss 5.71932411\n",
      "Trained batch 853 batch loss 5.84454441 epoch total loss 5.71947145\n",
      "Trained batch 854 batch loss 6.72562122 epoch total loss 5.72064924\n",
      "Trained batch 855 batch loss 5.3750186 epoch total loss 5.72024488\n",
      "Trained batch 856 batch loss 5.20412159 epoch total loss 5.71964216\n",
      "Trained batch 857 batch loss 5.5969677 epoch total loss 5.71949911\n",
      "Trained batch 858 batch loss 4.85371494 epoch total loss 5.71849\n",
      "Trained batch 859 batch loss 5.54323149 epoch total loss 5.71828604\n",
      "Trained batch 860 batch loss 5.75860596 epoch total loss 5.71833324\n",
      "Trained batch 861 batch loss 5.03977299 epoch total loss 5.71754503\n",
      "Trained batch 862 batch loss 6.43612289 epoch total loss 5.71837854\n",
      "Trained batch 863 batch loss 5.15272141 epoch total loss 5.71772289\n",
      "Trained batch 864 batch loss 5.35968 epoch total loss 5.717309\n",
      "Trained batch 865 batch loss 5.85104752 epoch total loss 5.71746349\n",
      "Trained batch 866 batch loss 6.21624 epoch total loss 5.71803951\n",
      "Trained batch 867 batch loss 5.82179737 epoch total loss 5.7181592\n",
      "Trained batch 868 batch loss 5.93586874 epoch total loss 5.71841\n",
      "Trained batch 869 batch loss 6.89183807 epoch total loss 5.71976042\n",
      "Trained batch 870 batch loss 7.06693745 epoch total loss 5.72130871\n",
      "Trained batch 871 batch loss 6.95149 epoch total loss 5.7227211\n",
      "Trained batch 872 batch loss 6.82039404 epoch total loss 5.72398\n",
      "Trained batch 873 batch loss 6.86595297 epoch total loss 5.72528791\n",
      "Trained batch 874 batch loss 6.97014618 epoch total loss 5.72671223\n",
      "Trained batch 875 batch loss 6.44498682 epoch total loss 5.72753286\n",
      "Trained batch 876 batch loss 5.93201637 epoch total loss 5.72776651\n",
      "Trained batch 877 batch loss 7.41601 epoch total loss 5.72969151\n",
      "Trained batch 878 batch loss 6.63361835 epoch total loss 5.730721\n",
      "Trained batch 879 batch loss 6.27291775 epoch total loss 5.73133802\n",
      "Trained batch 880 batch loss 6.64867067 epoch total loss 5.73238039\n",
      "Trained batch 881 batch loss 6.47890091 epoch total loss 5.73322773\n",
      "Trained batch 882 batch loss 6.87237644 epoch total loss 5.73451948\n",
      "Trained batch 883 batch loss 6.17518616 epoch total loss 5.73501873\n",
      "Trained batch 884 batch loss 6.41475105 epoch total loss 5.73578739\n",
      "Trained batch 885 batch loss 6.51903629 epoch total loss 5.7366724\n",
      "Trained batch 886 batch loss 6.36087799 epoch total loss 5.73737669\n",
      "Trained batch 887 batch loss 6.6966629 epoch total loss 5.73845863\n",
      "Trained batch 888 batch loss 6.09395123 epoch total loss 5.7388587\n",
      "Trained batch 889 batch loss 5.12255192 epoch total loss 5.73816538\n",
      "Trained batch 890 batch loss 5.30656099 epoch total loss 5.73768044\n",
      "Trained batch 891 batch loss 4.90907431 epoch total loss 5.7367506\n",
      "Trained batch 892 batch loss 5.08495426 epoch total loss 5.73602\n",
      "Trained batch 893 batch loss 4.65623856 epoch total loss 5.73481083\n",
      "Trained batch 894 batch loss 4.68806553 epoch total loss 5.73363972\n",
      "Trained batch 895 batch loss 5.19719696 epoch total loss 5.73304033\n",
      "Trained batch 896 batch loss 5.53502464 epoch total loss 5.73281956\n",
      "Trained batch 897 batch loss 6.15207 epoch total loss 5.73328686\n",
      "Trained batch 898 batch loss 5.81330347 epoch total loss 5.73337603\n",
      "Trained batch 899 batch loss 5.78833389 epoch total loss 5.73343754\n",
      "Trained batch 900 batch loss 5.73011875 epoch total loss 5.73343372\n",
      "Trained batch 901 batch loss 5.89416838 epoch total loss 5.73361206\n",
      "Trained batch 902 batch loss 5.82070494 epoch total loss 5.73370886\n",
      "Trained batch 903 batch loss 5.44147635 epoch total loss 5.73338509\n",
      "Trained batch 904 batch loss 4.99214458 epoch total loss 5.73256493\n",
      "Trained batch 905 batch loss 6.19098759 epoch total loss 5.73307133\n",
      "Trained batch 906 batch loss 5.48692 epoch total loss 5.73279953\n",
      "Trained batch 907 batch loss 5.27716637 epoch total loss 5.73229742\n",
      "Trained batch 908 batch loss 4.82168198 epoch total loss 5.73129463\n",
      "Trained batch 909 batch loss 5.65760899 epoch total loss 5.73121357\n",
      "Trained batch 910 batch loss 5.41337729 epoch total loss 5.73086452\n",
      "Trained batch 911 batch loss 5.09950876 epoch total loss 5.73017168\n",
      "Trained batch 912 batch loss 5.15437365 epoch total loss 5.72954035\n",
      "Trained batch 913 batch loss 6.48007393 epoch total loss 5.73036242\n",
      "Trained batch 914 batch loss 5.88788319 epoch total loss 5.73053455\n",
      "Trained batch 915 batch loss 5.75684834 epoch total loss 5.73056316\n",
      "Trained batch 916 batch loss 7.28534031 epoch total loss 5.73226\n",
      "Trained batch 917 batch loss 6.18711329 epoch total loss 5.73275614\n",
      "Trained batch 918 batch loss 4.97423 epoch total loss 5.73193\n",
      "Trained batch 919 batch loss 6.21428204 epoch total loss 5.73245478\n",
      "Trained batch 920 batch loss 4.8131156 epoch total loss 5.73145533\n",
      "Trained batch 921 batch loss 5.51788807 epoch total loss 5.73122358\n",
      "Trained batch 922 batch loss 5.20388794 epoch total loss 5.73065186\n",
      "Trained batch 923 batch loss 5.51153088 epoch total loss 5.73041487\n",
      "Trained batch 924 batch loss 5.58253765 epoch total loss 5.73025465\n",
      "Trained batch 925 batch loss 5.74194384 epoch total loss 5.73026752\n",
      "Trained batch 926 batch loss 5.91319418 epoch total loss 5.73046494\n",
      "Trained batch 927 batch loss 5.14418793 epoch total loss 5.72983265\n",
      "Trained batch 928 batch loss 5.17085648 epoch total loss 5.72923\n",
      "Trained batch 929 batch loss 5.81385469 epoch total loss 5.72932148\n",
      "Trained batch 930 batch loss 6.43637705 epoch total loss 5.73008204\n",
      "Trained batch 931 batch loss 5.50501537 epoch total loss 5.72984\n",
      "Trained batch 932 batch loss 5.92053318 epoch total loss 5.73004436\n",
      "Trained batch 933 batch loss 4.64654207 epoch total loss 5.72888279\n",
      "Trained batch 934 batch loss 6.35473585 epoch total loss 5.72955275\n",
      "Trained batch 935 batch loss 5.46509 epoch total loss 5.72927046\n",
      "Trained batch 936 batch loss 4.89828491 epoch total loss 5.72838259\n",
      "Trained batch 937 batch loss 5.56797 epoch total loss 5.7282114\n",
      "Trained batch 938 batch loss 6.16893578 epoch total loss 5.72868109\n",
      "Trained batch 939 batch loss 6.11249828 epoch total loss 5.72908974\n",
      "Trained batch 940 batch loss 5.44275522 epoch total loss 5.72878504\n",
      "Trained batch 941 batch loss 5.27291203 epoch total loss 5.72830057\n",
      "Trained batch 942 batch loss 6.15422726 epoch total loss 5.72875309\n",
      "Trained batch 943 batch loss 5.09758854 epoch total loss 5.72808361\n",
      "Trained batch 944 batch loss 4.9911828 epoch total loss 5.72730303\n",
      "Trained batch 945 batch loss 6.59484291 epoch total loss 5.72822094\n",
      "Trained batch 946 batch loss 6.60263824 epoch total loss 5.72914553\n",
      "Trained batch 947 batch loss 6.54346275 epoch total loss 5.73000526\n",
      "Trained batch 948 batch loss 6.51468 epoch total loss 5.73083305\n",
      "Trained batch 949 batch loss 6.10696411 epoch total loss 5.73122931\n",
      "Trained batch 950 batch loss 6.76160145 epoch total loss 5.73231411\n",
      "Trained batch 951 batch loss 6.66400623 epoch total loss 5.73329353\n",
      "Trained batch 952 batch loss 6.03612804 epoch total loss 5.73361158\n",
      "Trained batch 953 batch loss 5.88682842 epoch total loss 5.73377228\n",
      "Trained batch 954 batch loss 5.97827148 epoch total loss 5.73402882\n",
      "Trained batch 955 batch loss 6.36327124 epoch total loss 5.73468781\n",
      "Trained batch 956 batch loss 6.32563114 epoch total loss 5.73530626\n",
      "Trained batch 957 batch loss 6.55912256 epoch total loss 5.73616695\n",
      "Trained batch 958 batch loss 5.96280909 epoch total loss 5.73640347\n",
      "Trained batch 959 batch loss 5.67671347 epoch total loss 5.73634148\n",
      "Trained batch 960 batch loss 6.06466198 epoch total loss 5.73668337\n",
      "Trained batch 961 batch loss 6.82803631 epoch total loss 5.73781872\n",
      "Trained batch 962 batch loss 6.57969189 epoch total loss 5.73869371\n",
      "Trained batch 963 batch loss 6.42948818 epoch total loss 5.73941135\n",
      "Trained batch 964 batch loss 6.67978191 epoch total loss 5.74038696\n",
      "Trained batch 965 batch loss 6.58949661 epoch total loss 5.74126673\n",
      "Trained batch 966 batch loss 6.10903454 epoch total loss 5.74164724\n",
      "Trained batch 967 batch loss 5.63761234 epoch total loss 5.74153948\n",
      "Trained batch 968 batch loss 5.67869663 epoch total loss 5.74147463\n",
      "Trained batch 969 batch loss 5.96804428 epoch total loss 5.74170876\n",
      "Trained batch 970 batch loss 5.47582626 epoch total loss 5.74143457\n",
      "Trained batch 971 batch loss 5.72071743 epoch total loss 5.74141312\n",
      "Trained batch 972 batch loss 4.84120893 epoch total loss 5.7404871\n",
      "Trained batch 973 batch loss 5.64751673 epoch total loss 5.74039125\n",
      "Trained batch 974 batch loss 5.95279121 epoch total loss 5.74060917\n",
      "Trained batch 975 batch loss 5.85404301 epoch total loss 5.74072552\n",
      "Trained batch 976 batch loss 5.60583353 epoch total loss 5.74058771\n",
      "Trained batch 977 batch loss 5.88273954 epoch total loss 5.74073315\n",
      "Trained batch 978 batch loss 5.57210827 epoch total loss 5.74056101\n",
      "Trained batch 979 batch loss 5.49319363 epoch total loss 5.74030828\n",
      "Trained batch 980 batch loss 5.97146225 epoch total loss 5.74054432\n",
      "Trained batch 981 batch loss 5.48444653 epoch total loss 5.74028301\n",
      "Trained batch 982 batch loss 5.24778891 epoch total loss 5.73978138\n",
      "Trained batch 983 batch loss 5.7133 epoch total loss 5.73975468\n",
      "Trained batch 984 batch loss 4.8295064 epoch total loss 5.73882961\n",
      "Trained batch 985 batch loss 4.78497648 epoch total loss 5.73786116\n",
      "Trained batch 986 batch loss 5.42201233 epoch total loss 5.73754072\n",
      "Trained batch 987 batch loss 5.14840078 epoch total loss 5.7369442\n",
      "Trained batch 988 batch loss 6.24654245 epoch total loss 5.73745966\n",
      "Trained batch 989 batch loss 5.68462181 epoch total loss 5.73740625\n",
      "Trained batch 990 batch loss 5.65867043 epoch total loss 5.7373271\n",
      "Trained batch 991 batch loss 5.26661253 epoch total loss 5.73685169\n",
      "Trained batch 992 batch loss 5.33431578 epoch total loss 5.73644638\n",
      "Trained batch 993 batch loss 6.06226 epoch total loss 5.73677444\n",
      "Trained batch 994 batch loss 6.01614332 epoch total loss 5.73705578\n",
      "Trained batch 995 batch loss 6.27505589 epoch total loss 5.73759604\n",
      "Trained batch 996 batch loss 6.05172825 epoch total loss 5.7379117\n",
      "Trained batch 997 batch loss 5.90945959 epoch total loss 5.73808384\n",
      "Trained batch 998 batch loss 6.06361818 epoch total loss 5.73841\n",
      "Trained batch 999 batch loss 5.75402451 epoch total loss 5.73842525\n",
      "Trained batch 1000 batch loss 6.38855743 epoch total loss 5.73907566\n",
      "Trained batch 1001 batch loss 5.64432144 epoch total loss 5.73898125\n",
      "Trained batch 1002 batch loss 5.94375944 epoch total loss 5.73918581\n",
      "Trained batch 1003 batch loss 5.62482452 epoch total loss 5.73907185\n",
      "Trained batch 1004 batch loss 5.57156181 epoch total loss 5.73890543\n",
      "Trained batch 1005 batch loss 6.28970814 epoch total loss 5.73945332\n",
      "Trained batch 1006 batch loss 5.34335661 epoch total loss 5.73905945\n",
      "Trained batch 1007 batch loss 4.77460861 epoch total loss 5.73810148\n",
      "Trained batch 1008 batch loss 5.67635107 epoch total loss 5.73804\n",
      "Trained batch 1009 batch loss 5.85748577 epoch total loss 5.73815823\n",
      "Trained batch 1010 batch loss 5.1961813 epoch total loss 5.73762178\n",
      "Trained batch 1011 batch loss 5.49300766 epoch total loss 5.73738\n",
      "Trained batch 1012 batch loss 4.92365074 epoch total loss 5.73657608\n",
      "Trained batch 1013 batch loss 5.04607582 epoch total loss 5.7358942\n",
      "Trained batch 1014 batch loss 4.87042522 epoch total loss 5.73504114\n",
      "Trained batch 1015 batch loss 5.92973518 epoch total loss 5.73523283\n",
      "Trained batch 1016 batch loss 5.41645384 epoch total loss 5.73491907\n",
      "Trained batch 1017 batch loss 5.99561501 epoch total loss 5.73517513\n",
      "Trained batch 1018 batch loss 4.77213097 epoch total loss 5.73422909\n",
      "Trained batch 1019 batch loss 5.78158379 epoch total loss 5.73427582\n",
      "Trained batch 1020 batch loss 5.56680489 epoch total loss 5.73411179\n",
      "Trained batch 1021 batch loss 5.33553123 epoch total loss 5.73372126\n",
      "Trained batch 1022 batch loss 5.38177395 epoch total loss 5.73337698\n",
      "Trained batch 1023 batch loss 5.40381908 epoch total loss 5.73305464\n",
      "Trained batch 1024 batch loss 5.88195944 epoch total loss 5.7332\n",
      "Trained batch 1025 batch loss 5.36651182 epoch total loss 5.73284245\n",
      "Trained batch 1026 batch loss 5.81689453 epoch total loss 5.73292446\n",
      "Trained batch 1027 batch loss 4.94704914 epoch total loss 5.73215961\n",
      "Trained batch 1028 batch loss 6.04304 epoch total loss 5.73246193\n",
      "Trained batch 1029 batch loss 6.46422529 epoch total loss 5.73317289\n",
      "Trained batch 1030 batch loss 4.85890579 epoch total loss 5.73232412\n",
      "Trained batch 1031 batch loss 4.28030586 epoch total loss 5.73091602\n",
      "Trained batch 1032 batch loss 5.50537682 epoch total loss 5.73069715\n",
      "Trained batch 1033 batch loss 5.31892872 epoch total loss 5.73029852\n",
      "Trained batch 1034 batch loss 4.92320776 epoch total loss 5.72951794\n",
      "Trained batch 1035 batch loss 4.80408192 epoch total loss 5.72862434\n",
      "Trained batch 1036 batch loss 6.37623 epoch total loss 5.72924948\n",
      "Trained batch 1037 batch loss 6.26885605 epoch total loss 5.72977\n",
      "Trained batch 1038 batch loss 4.9665556 epoch total loss 5.7290349\n",
      "Trained batch 1039 batch loss 4.78489113 epoch total loss 5.72812605\n",
      "Trained batch 1040 batch loss 5.45879078 epoch total loss 5.72786713\n",
      "Trained batch 1041 batch loss 5.7706089 epoch total loss 5.72790813\n",
      "Trained batch 1042 batch loss 5.93512726 epoch total loss 5.72810698\n",
      "Trained batch 1043 batch loss 5.1536665 epoch total loss 5.72755623\n",
      "Trained batch 1044 batch loss 6.13195038 epoch total loss 5.72794342\n",
      "Trained batch 1045 batch loss 5.25230122 epoch total loss 5.72748852\n",
      "Trained batch 1046 batch loss 5.87674 epoch total loss 5.72763157\n",
      "Trained batch 1047 batch loss 5.7818346 epoch total loss 5.72768307\n",
      "Trained batch 1048 batch loss 5.96250725 epoch total loss 5.72790718\n",
      "Trained batch 1049 batch loss 6.26576138 epoch total loss 5.72842\n",
      "Trained batch 1050 batch loss 6.01175451 epoch total loss 5.72868967\n",
      "Trained batch 1051 batch loss 6.17218 epoch total loss 5.72911167\n",
      "Trained batch 1052 batch loss 5.93114758 epoch total loss 5.72930384\n",
      "Trained batch 1053 batch loss 5.95949554 epoch total loss 5.72952223\n",
      "Trained batch 1054 batch loss 6.27488136 epoch total loss 5.7300396\n",
      "Trained batch 1055 batch loss 5.88594723 epoch total loss 5.73018742\n",
      "Trained batch 1056 batch loss 5.72901058 epoch total loss 5.73018646\n",
      "Trained batch 1057 batch loss 5.61556625 epoch total loss 5.73007774\n",
      "Trained batch 1058 batch loss 5.88948536 epoch total loss 5.7302289\n",
      "Trained batch 1059 batch loss 5.90248489 epoch total loss 5.7303915\n",
      "Trained batch 1060 batch loss 5.63385344 epoch total loss 5.7303\n",
      "Trained batch 1061 batch loss 5.85894489 epoch total loss 5.73042154\n",
      "Trained batch 1062 batch loss 5.95041847 epoch total loss 5.73062849\n",
      "Trained batch 1063 batch loss 5.76068592 epoch total loss 5.73065662\n",
      "Trained batch 1064 batch loss 6.32897043 epoch total loss 5.73121929\n",
      "Trained batch 1065 batch loss 6.33119106 epoch total loss 5.73178244\n",
      "Trained batch 1066 batch loss 5.4435606 epoch total loss 5.73151159\n",
      "Trained batch 1067 batch loss 5.20909405 epoch total loss 5.73102188\n",
      "Trained batch 1068 batch loss 4.9979229 epoch total loss 5.73033571\n",
      "Trained batch 1069 batch loss 5.92480373 epoch total loss 5.73051739\n",
      "Trained batch 1070 batch loss 5.63982 epoch total loss 5.73043251\n",
      "Trained batch 1071 batch loss 5.24221754 epoch total loss 5.72997665\n",
      "Trained batch 1072 batch loss 5.21941328 epoch total loss 5.72950029\n",
      "Trained batch 1073 batch loss 5.66299248 epoch total loss 5.7294383\n",
      "Trained batch 1074 batch loss 4.632164 epoch total loss 5.72841692\n",
      "Trained batch 1075 batch loss 5.44938374 epoch total loss 5.72815704\n",
      "Trained batch 1076 batch loss 5.5463 epoch total loss 5.72798824\n",
      "Trained batch 1077 batch loss 5.16372 epoch total loss 5.7274642\n",
      "Trained batch 1078 batch loss 4.50400162 epoch total loss 5.72632933\n",
      "Trained batch 1079 batch loss 5.20909 epoch total loss 5.72584963\n",
      "Trained batch 1080 batch loss 5.22049809 epoch total loss 5.72538185\n",
      "Trained batch 1081 batch loss 5.46064806 epoch total loss 5.72513676\n",
      "Trained batch 1082 batch loss 5.18668842 epoch total loss 5.72463894\n",
      "Trained batch 1083 batch loss 5.416152 epoch total loss 5.72435427\n",
      "Trained batch 1084 batch loss 5.26520538 epoch total loss 5.72393036\n",
      "Trained batch 1085 batch loss 5.19191074 epoch total loss 5.72344\n",
      "Trained batch 1086 batch loss 5.8447876 epoch total loss 5.72355175\n",
      "Trained batch 1087 batch loss 5.54845333 epoch total loss 5.72339058\n",
      "Trained batch 1088 batch loss 5.43346691 epoch total loss 5.7231245\n",
      "Trained batch 1089 batch loss 5.22116566 epoch total loss 5.7226634\n",
      "Trained batch 1090 batch loss 4.6602211 epoch total loss 5.72168875\n",
      "Trained batch 1091 batch loss 5.24199581 epoch total loss 5.7212491\n",
      "Trained batch 1092 batch loss 6.12827635 epoch total loss 5.72162199\n",
      "Trained batch 1093 batch loss 6.30297089 epoch total loss 5.72215366\n",
      "Trained batch 1094 batch loss 6.85530615 epoch total loss 5.72319\n",
      "Trained batch 1095 batch loss 6.35512495 epoch total loss 5.7237668\n",
      "Trained batch 1096 batch loss 5.94711494 epoch total loss 5.72397041\n",
      "Trained batch 1097 batch loss 6.19945192 epoch total loss 5.72440386\n",
      "Trained batch 1098 batch loss 6.08425903 epoch total loss 5.72473145\n",
      "Trained batch 1099 batch loss 5.66012 epoch total loss 5.72467279\n",
      "Trained batch 1100 batch loss 5.98057461 epoch total loss 5.72490549\n",
      "Trained batch 1101 batch loss 5.84556 epoch total loss 5.72501516\n",
      "Trained batch 1102 batch loss 6.17362499 epoch total loss 5.72542238\n",
      "Trained batch 1103 batch loss 5.62441826 epoch total loss 5.72533083\n",
      "Trained batch 1104 batch loss 5.87525558 epoch total loss 5.72546673\n",
      "Trained batch 1105 batch loss 5.98266888 epoch total loss 5.7257\n",
      "Trained batch 1106 batch loss 5.3191185 epoch total loss 5.72533274\n",
      "Trained batch 1107 batch loss 6.12330675 epoch total loss 5.72569227\n",
      "Trained batch 1108 batch loss 6.19225359 epoch total loss 5.72611332\n",
      "Trained batch 1109 batch loss 6.36805916 epoch total loss 5.7266922\n",
      "Trained batch 1110 batch loss 6.56342936 epoch total loss 5.72744608\n",
      "Trained batch 1111 batch loss 6.69232559 epoch total loss 5.72831488\n",
      "Trained batch 1112 batch loss 7.05579 epoch total loss 5.7295084\n",
      "Trained batch 1113 batch loss 6.07561874 epoch total loss 5.7298193\n",
      "Trained batch 1114 batch loss 6.62286758 epoch total loss 5.73062134\n",
      "Trained batch 1115 batch loss 6.14229393 epoch total loss 5.73099041\n",
      "Trained batch 1116 batch loss 6.99139071 epoch total loss 5.73211956\n",
      "Trained batch 1117 batch loss 5.54534721 epoch total loss 5.73195219\n",
      "Trained batch 1118 batch loss 7.38354445 epoch total loss 5.73342943\n",
      "Trained batch 1119 batch loss 6.66993952 epoch total loss 5.73426628\n",
      "Trained batch 1120 batch loss 6.48279476 epoch total loss 5.73493481\n",
      "Trained batch 1121 batch loss 5.96429586 epoch total loss 5.73513937\n",
      "Trained batch 1122 batch loss 6.3292923 epoch total loss 5.73566866\n",
      "Trained batch 1123 batch loss 6.55202198 epoch total loss 5.73639584\n",
      "Trained batch 1124 batch loss 6.30741119 epoch total loss 5.73690414\n",
      "Trained batch 1125 batch loss 6.92770386 epoch total loss 5.73796272\n",
      "Trained batch 1126 batch loss 5.70416784 epoch total loss 5.73793268\n",
      "Trained batch 1127 batch loss 6.46241522 epoch total loss 5.73857546\n",
      "Trained batch 1128 batch loss 5.66969681 epoch total loss 5.73851442\n",
      "Trained batch 1129 batch loss 6.03485775 epoch total loss 5.73877668\n",
      "Trained batch 1130 batch loss 5.94860554 epoch total loss 5.73896265\n",
      "Trained batch 1131 batch loss 5.96392393 epoch total loss 5.73916149\n",
      "Trained batch 1132 batch loss 6.27780151 epoch total loss 5.73963737\n",
      "Trained batch 1133 batch loss 6.09780884 epoch total loss 5.73995352\n",
      "Trained batch 1134 batch loss 5.72187233 epoch total loss 5.73993731\n",
      "Trained batch 1135 batch loss 6.05226 epoch total loss 5.74021244\n",
      "Trained batch 1136 batch loss 6.17449 epoch total loss 5.74059439\n",
      "Trained batch 1137 batch loss 6.02132845 epoch total loss 5.74084139\n",
      "Trained batch 1138 batch loss 6.4366293 epoch total loss 5.74145317\n",
      "Trained batch 1139 batch loss 5.37684059 epoch total loss 5.74113274\n",
      "Trained batch 1140 batch loss 5.85248709 epoch total loss 5.74123049\n",
      "Trained batch 1141 batch loss 6.33287525 epoch total loss 5.74174929\n",
      "Trained batch 1142 batch loss 6.03005838 epoch total loss 5.74200201\n",
      "Trained batch 1143 batch loss 6.01665545 epoch total loss 5.74224234\n",
      "Trained batch 1144 batch loss 6.31334496 epoch total loss 5.74274158\n",
      "Trained batch 1145 batch loss 5.97373438 epoch total loss 5.74294329\n",
      "Trained batch 1146 batch loss 5.68062401 epoch total loss 5.74288893\n",
      "Trained batch 1147 batch loss 5.19948864 epoch total loss 5.74241543\n",
      "Trained batch 1148 batch loss 5.45265293 epoch total loss 5.7421627\n",
      "Trained batch 1149 batch loss 5.29610777 epoch total loss 5.74177456\n",
      "Trained batch 1150 batch loss 5.94583797 epoch total loss 5.74195194\n",
      "Trained batch 1151 batch loss 5.43360376 epoch total loss 5.74168396\n",
      "Trained batch 1152 batch loss 5.15892601 epoch total loss 5.74117804\n",
      "Trained batch 1153 batch loss 5.23840046 epoch total loss 5.74074173\n",
      "Trained batch 1154 batch loss 5.3749032 epoch total loss 5.74042463\n",
      "Trained batch 1155 batch loss 5.82353973 epoch total loss 5.74049711\n",
      "Trained batch 1156 batch loss 4.9243803 epoch total loss 5.73979092\n",
      "Trained batch 1157 batch loss 5.69135809 epoch total loss 5.73974895\n",
      "Trained batch 1158 batch loss 4.86228561 epoch total loss 5.73899126\n",
      "Trained batch 1159 batch loss 4.79753542 epoch total loss 5.73817873\n",
      "Trained batch 1160 batch loss 6.75034523 epoch total loss 5.73905134\n",
      "Trained batch 1161 batch loss 5.68182278 epoch total loss 5.73900223\n",
      "Trained batch 1162 batch loss 4.67617321 epoch total loss 5.73808765\n",
      "Trained batch 1163 batch loss 6.80947638 epoch total loss 5.7390089\n",
      "Trained batch 1164 batch loss 5.75471735 epoch total loss 5.73902225\n",
      "Trained batch 1165 batch loss 4.43455887 epoch total loss 5.73790264\n",
      "Trained batch 1166 batch loss 5.6432972 epoch total loss 5.73782158\n",
      "Trained batch 1167 batch loss 5.49623728 epoch total loss 5.73761415\n",
      "Trained batch 1168 batch loss 6.23565102 epoch total loss 5.73804092\n",
      "Trained batch 1169 batch loss 5.82641888 epoch total loss 5.73811674\n",
      "Trained batch 1170 batch loss 6.0975728 epoch total loss 5.73842382\n",
      "Trained batch 1171 batch loss 6.51228809 epoch total loss 5.73908472\n",
      "Trained batch 1172 batch loss 6.70569801 epoch total loss 5.73990917\n",
      "Trained batch 1173 batch loss 5.43815136 epoch total loss 5.73965216\n",
      "Trained batch 1174 batch loss 6.62838125 epoch total loss 5.7404089\n",
      "Trained batch 1175 batch loss 6.28793859 epoch total loss 5.74087524\n",
      "Trained batch 1176 batch loss 6.06646681 epoch total loss 5.74115181\n",
      "Trained batch 1177 batch loss 5.64764071 epoch total loss 5.74107218\n",
      "Trained batch 1178 batch loss 5.63038158 epoch total loss 5.74097824\n",
      "Trained batch 1179 batch loss 5.35658455 epoch total loss 5.74065208\n",
      "Trained batch 1180 batch loss 5.58522367 epoch total loss 5.74052095\n",
      "Trained batch 1181 batch loss 6.02800131 epoch total loss 5.74076414\n",
      "Trained batch 1182 batch loss 6.5360918 epoch total loss 5.74143696\n",
      "Trained batch 1183 batch loss 6.17993069 epoch total loss 5.74180746\n",
      "Trained batch 1184 batch loss 5.89065027 epoch total loss 5.74193287\n",
      "Trained batch 1185 batch loss 5.55166817 epoch total loss 5.74177265\n",
      "Trained batch 1186 batch loss 6.11977291 epoch total loss 5.74209118\n",
      "Trained batch 1187 batch loss 6.26458597 epoch total loss 5.7425313\n",
      "Trained batch 1188 batch loss 5.60428333 epoch total loss 5.74241543\n",
      "Trained batch 1189 batch loss 5.70736122 epoch total loss 5.74238586\n",
      "Trained batch 1190 batch loss 5.7603631 epoch total loss 5.74240065\n",
      "Trained batch 1191 batch loss 5.70859051 epoch total loss 5.74237251\n",
      "Trained batch 1192 batch loss 6.06095695 epoch total loss 5.74263954\n",
      "Trained batch 1193 batch loss 5.67116261 epoch total loss 5.74258\n",
      "Trained batch 1194 batch loss 6.09064674 epoch total loss 5.74287176\n",
      "Trained batch 1195 batch loss 5.92589855 epoch total loss 5.74302483\n",
      "Trained batch 1196 batch loss 5.9986167 epoch total loss 5.74323845\n",
      "Trained batch 1197 batch loss 5.69544506 epoch total loss 5.74319839\n",
      "Trained batch 1198 batch loss 5.55417824 epoch total loss 5.74304056\n",
      "Trained batch 1199 batch loss 5.6815958 epoch total loss 5.74298954\n",
      "Trained batch 1200 batch loss 5.87295628 epoch total loss 5.74309778\n",
      "Trained batch 1201 batch loss 5.86885786 epoch total loss 5.74320221\n",
      "Trained batch 1202 batch loss 5.54527378 epoch total loss 5.7430377\n",
      "Trained batch 1203 batch loss 5.2826581 epoch total loss 5.74265528\n",
      "Trained batch 1204 batch loss 5.80329895 epoch total loss 5.74270535\n",
      "Trained batch 1205 batch loss 5.74743366 epoch total loss 5.74270964\n",
      "Trained batch 1206 batch loss 5.88366413 epoch total loss 5.74282646\n",
      "Trained batch 1207 batch loss 6.0497117 epoch total loss 5.74308062\n",
      "Trained batch 1208 batch loss 5.78781557 epoch total loss 5.74311781\n",
      "Trained batch 1209 batch loss 6.16431189 epoch total loss 5.74346638\n",
      "Trained batch 1210 batch loss 4.28890896 epoch total loss 5.74226427\n",
      "Trained batch 1211 batch loss 5.98909 epoch total loss 5.74246836\n",
      "Trained batch 1212 batch loss 5.08847237 epoch total loss 5.74192858\n",
      "Trained batch 1213 batch loss 6.44958973 epoch total loss 5.74251175\n",
      "Trained batch 1214 batch loss 5.98492432 epoch total loss 5.74271154\n",
      "Trained batch 1215 batch loss 6.02479649 epoch total loss 5.74294376\n",
      "Trained batch 1216 batch loss 5.06313896 epoch total loss 5.74238443\n",
      "Trained batch 1217 batch loss 5.17531729 epoch total loss 5.74191856\n",
      "Trained batch 1218 batch loss 6.03553391 epoch total loss 5.74216\n",
      "Trained batch 1219 batch loss 5.60707569 epoch total loss 5.74204874\n",
      "Trained batch 1220 batch loss 5.68021774 epoch total loss 5.7419982\n",
      "Trained batch 1221 batch loss 5.32559395 epoch total loss 5.74165726\n",
      "Trained batch 1222 batch loss 4.64506674 epoch total loss 5.74076\n",
      "Trained batch 1223 batch loss 5.33516407 epoch total loss 5.74042797\n",
      "Trained batch 1224 batch loss 5.36298323 epoch total loss 5.74011946\n",
      "Trained batch 1225 batch loss 5.3953681 epoch total loss 5.73983812\n",
      "Trained batch 1226 batch loss 5.28240204 epoch total loss 5.73946476\n",
      "Trained batch 1227 batch loss 5.86657572 epoch total loss 5.73956871\n",
      "Trained batch 1228 batch loss 5.83006382 epoch total loss 5.73964214\n",
      "Trained batch 1229 batch loss 5.82461834 epoch total loss 5.73971176\n",
      "Trained batch 1230 batch loss 5.35204697 epoch total loss 5.73939657\n",
      "Trained batch 1231 batch loss 5.23117256 epoch total loss 5.73898315\n",
      "Trained batch 1232 batch loss 5.30336428 epoch total loss 5.73863\n",
      "Trained batch 1233 batch loss 5.35527563 epoch total loss 5.73831892\n",
      "Trained batch 1234 batch loss 5.46089697 epoch total loss 5.73809385\n",
      "Trained batch 1235 batch loss 5.78019667 epoch total loss 5.73812819\n",
      "Trained batch 1236 batch loss 5.64704514 epoch total loss 5.73805428\n",
      "Trained batch 1237 batch loss 5.15894699 epoch total loss 5.7375865\n",
      "Trained batch 1238 batch loss 5.6302743 epoch total loss 5.73749971\n",
      "Trained batch 1239 batch loss 5.80934286 epoch total loss 5.73755789\n",
      "Trained batch 1240 batch loss 6.04123831 epoch total loss 5.73780298\n",
      "Trained batch 1241 batch loss 5.90406322 epoch total loss 5.73793697\n",
      "Trained batch 1242 batch loss 5.75971317 epoch total loss 5.73795462\n",
      "Trained batch 1243 batch loss 5.84118938 epoch total loss 5.73803759\n",
      "Trained batch 1244 batch loss 5.44644117 epoch total loss 5.73780298\n",
      "Trained batch 1245 batch loss 5.18323231 epoch total loss 5.73735762\n",
      "Trained batch 1246 batch loss 6.07456446 epoch total loss 5.73762846\n",
      "Trained batch 1247 batch loss 5.83821869 epoch total loss 5.73770905\n",
      "Trained batch 1248 batch loss 5.40109396 epoch total loss 5.73743916\n",
      "Trained batch 1249 batch loss 5.52147818 epoch total loss 5.73726654\n",
      "Trained batch 1250 batch loss 5.03626204 epoch total loss 5.7367053\n",
      "Trained batch 1251 batch loss 5.56796551 epoch total loss 5.73657036\n",
      "Trained batch 1252 batch loss 5.92071 epoch total loss 5.7367177\n",
      "Trained batch 1253 batch loss 4.88738 epoch total loss 5.73603964\n",
      "Trained batch 1254 batch loss 4.99199 epoch total loss 5.73544645\n",
      "Trained batch 1255 batch loss 5.65192509 epoch total loss 5.73538\n",
      "Trained batch 1256 batch loss 5.65943813 epoch total loss 5.73531961\n",
      "Trained batch 1257 batch loss 5.30692863 epoch total loss 5.73497915\n",
      "Trained batch 1258 batch loss 5.2709074 epoch total loss 5.73461\n",
      "Trained batch 1259 batch loss 4.88911867 epoch total loss 5.73393869\n",
      "Trained batch 1260 batch loss 6.03438091 epoch total loss 5.73417711\n",
      "Trained batch 1261 batch loss 5.31955719 epoch total loss 5.73384809\n",
      "Trained batch 1262 batch loss 6.12362 epoch total loss 5.73415661\n",
      "Trained batch 1263 batch loss 5.95665264 epoch total loss 5.73433304\n",
      "Trained batch 1264 batch loss 6.16838264 epoch total loss 5.73467636\n",
      "Trained batch 1265 batch loss 5.63408852 epoch total loss 5.73459673\n",
      "Trained batch 1266 batch loss 5.65686417 epoch total loss 5.73453522\n",
      "Trained batch 1267 batch loss 5.93155575 epoch total loss 5.73469114\n",
      "Trained batch 1268 batch loss 6.36272717 epoch total loss 5.73518658\n",
      "Trained batch 1269 batch loss 4.28229523 epoch total loss 5.73404121\n",
      "Trained batch 1270 batch loss 6.61279774 epoch total loss 5.7347331\n",
      "Trained batch 1271 batch loss 5.93178558 epoch total loss 5.73488808\n",
      "Trained batch 1272 batch loss 5.96947861 epoch total loss 5.73507261\n",
      "Trained batch 1273 batch loss 6.20426846 epoch total loss 5.73544073\n",
      "Trained batch 1274 batch loss 6.13447809 epoch total loss 5.73575401\n",
      "Trained batch 1275 batch loss 6.07574701 epoch total loss 5.73602057\n",
      "Trained batch 1276 batch loss 5.68519354 epoch total loss 5.73598051\n",
      "Trained batch 1277 batch loss 5.70191 epoch total loss 5.73595428\n",
      "Trained batch 1278 batch loss 6.10975933 epoch total loss 5.73624659\n",
      "Trained batch 1279 batch loss 5.46358442 epoch total loss 5.73603344\n",
      "Trained batch 1280 batch loss 6.43074083 epoch total loss 5.73657608\n",
      "Trained batch 1281 batch loss 5.83136559 epoch total loss 5.73665\n",
      "Trained batch 1282 batch loss 5.86239529 epoch total loss 5.73674822\n",
      "Trained batch 1283 batch loss 6.23575974 epoch total loss 5.73713732\n",
      "Trained batch 1284 batch loss 5.7101903 epoch total loss 5.73711586\n",
      "Trained batch 1285 batch loss 5.92440033 epoch total loss 5.73726177\n",
      "Trained batch 1286 batch loss 6.39811039 epoch total loss 5.73777533\n",
      "Trained batch 1287 batch loss 6.06500626 epoch total loss 5.73802948\n",
      "Trained batch 1288 batch loss 5.88344479 epoch total loss 5.73814249\n",
      "Trained batch 1289 batch loss 5.98281717 epoch total loss 5.73833227\n",
      "Trained batch 1290 batch loss 5.49695587 epoch total loss 5.73814535\n",
      "Trained batch 1291 batch loss 6.62072849 epoch total loss 5.73882914\n",
      "Trained batch 1292 batch loss 5.80952311 epoch total loss 5.7388835\n",
      "Trained batch 1293 batch loss 5.75381279 epoch total loss 5.73889542\n",
      "Trained batch 1294 batch loss 5.72938395 epoch total loss 5.73888779\n",
      "Trained batch 1295 batch loss 5.61967897 epoch total loss 5.73879576\n",
      "Trained batch 1296 batch loss 5.49637604 epoch total loss 5.73860884\n",
      "Trained batch 1297 batch loss 5.65403128 epoch total loss 5.73854351\n",
      "Trained batch 1298 batch loss 5.69385195 epoch total loss 5.73850918\n",
      "Trained batch 1299 batch loss 5.40157318 epoch total loss 5.73825\n",
      "Trained batch 1300 batch loss 5.89964437 epoch total loss 5.73837376\n",
      "Trained batch 1301 batch loss 5.61915159 epoch total loss 5.7382822\n",
      "Trained batch 1302 batch loss 5.78945589 epoch total loss 5.7383213\n",
      "Trained batch 1303 batch loss 5.70373774 epoch total loss 5.7382946\n",
      "Trained batch 1304 batch loss 5.85281897 epoch total loss 5.73838282\n",
      "Trained batch 1305 batch loss 5.60498333 epoch total loss 5.7382803\n",
      "Trained batch 1306 batch loss 5.68857813 epoch total loss 5.73824215\n",
      "Trained batch 1307 batch loss 5.82646608 epoch total loss 5.73831\n",
      "Trained batch 1308 batch loss 5.57430172 epoch total loss 5.73818445\n",
      "Trained batch 1309 batch loss 5.53709364 epoch total loss 5.73803091\n",
      "Trained batch 1310 batch loss 4.80755138 epoch total loss 5.7373209\n",
      "Trained batch 1311 batch loss 3.6220727 epoch total loss 5.73570728\n",
      "Trained batch 1312 batch loss 4.35251522 epoch total loss 5.734653\n",
      "Trained batch 1313 batch loss 4.35194588 epoch total loss 5.7336\n",
      "Trained batch 1314 batch loss 5.13592243 epoch total loss 5.73314524\n",
      "Trained batch 1315 batch loss 4.5547657 epoch total loss 5.73224878\n",
      "Trained batch 1316 batch loss 3.84583473 epoch total loss 5.73081541\n",
      "Trained batch 1317 batch loss 4.69170046 epoch total loss 5.73002625\n",
      "Trained batch 1318 batch loss 4.65787172 epoch total loss 5.72921276\n",
      "Trained batch 1319 batch loss 5.39371061 epoch total loss 5.72895861\n",
      "Trained batch 1320 batch loss 5.20704 epoch total loss 5.72856283\n",
      "Trained batch 1321 batch loss 4.6589222 epoch total loss 5.72775316\n",
      "Trained batch 1322 batch loss 4.79105377 epoch total loss 5.72704458\n",
      "Trained batch 1323 batch loss 5.05643463 epoch total loss 5.7265377\n",
      "Trained batch 1324 batch loss 4.53123522 epoch total loss 5.72563505\n",
      "Trained batch 1325 batch loss 4.24470806 epoch total loss 5.72451735\n",
      "Trained batch 1326 batch loss 5.10813618 epoch total loss 5.72405243\n",
      "Trained batch 1327 batch loss 4.58649445 epoch total loss 5.72319508\n",
      "Trained batch 1328 batch loss 6.49268 epoch total loss 5.72377443\n",
      "Trained batch 1329 batch loss 4.78913212 epoch total loss 5.7230711\n",
      "Trained batch 1330 batch loss 6.06798172 epoch total loss 5.7233305\n",
      "Trained batch 1331 batch loss 5.66656 epoch total loss 5.72328758\n",
      "Trained batch 1332 batch loss 4.45152092 epoch total loss 5.72233295\n",
      "Trained batch 1333 batch loss 4.73665524 epoch total loss 5.72159338\n",
      "Trained batch 1334 batch loss 6.1316576 epoch total loss 5.72190094\n",
      "Trained batch 1335 batch loss 4.82882881 epoch total loss 5.72123194\n",
      "Trained batch 1336 batch loss 6.19363308 epoch total loss 5.72158575\n",
      "Trained batch 1337 batch loss 6.49498463 epoch total loss 5.72216415\n",
      "Trained batch 1338 batch loss 6.1865263 epoch total loss 5.72251129\n",
      "Trained batch 1339 batch loss 5.84425545 epoch total loss 5.72260237\n",
      "Trained batch 1340 batch loss 5.98272 epoch total loss 5.72279644\n",
      "Trained batch 1341 batch loss 5.8773241 epoch total loss 5.72291183\n",
      "Trained batch 1342 batch loss 5.89110565 epoch total loss 5.72303724\n",
      "Trained batch 1343 batch loss 6.2169466 epoch total loss 5.72340488\n",
      "Trained batch 1344 batch loss 6.39318943 epoch total loss 5.72390318\n",
      "Trained batch 1345 batch loss 5.82818699 epoch total loss 5.72398043\n",
      "Trained batch 1346 batch loss 5.10551119 epoch total loss 5.72352123\n",
      "Trained batch 1347 batch loss 6.08009052 epoch total loss 5.72378588\n",
      "Trained batch 1348 batch loss 5.11051178 epoch total loss 5.7233305\n",
      "Trained batch 1349 batch loss 4.91886711 epoch total loss 5.72273445\n",
      "Trained batch 1350 batch loss 5.17710876 epoch total loss 5.72233057\n",
      "Trained batch 1351 batch loss 4.75474262 epoch total loss 5.72161436\n",
      "Trained batch 1352 batch loss 6.21169758 epoch total loss 5.72197676\n",
      "Trained batch 1353 batch loss 5.81506443 epoch total loss 5.72204542\n",
      "Trained batch 1354 batch loss 5.292099 epoch total loss 5.72172785\n",
      "Trained batch 1355 batch loss 5.09319782 epoch total loss 5.72126436\n",
      "Trained batch 1356 batch loss 5.89687109 epoch total loss 5.72139359\n",
      "Trained batch 1357 batch loss 4.93290424 epoch total loss 5.7208128\n",
      "Trained batch 1358 batch loss 5.53207779 epoch total loss 5.72067404\n",
      "Trained batch 1359 batch loss 4.62572527 epoch total loss 5.71986818\n",
      "Trained batch 1360 batch loss 4.83743191 epoch total loss 5.71921921\n",
      "Trained batch 1361 batch loss 5.64373493 epoch total loss 5.71916389\n",
      "Trained batch 1362 batch loss 4.93414116 epoch total loss 5.7185874\n",
      "Trained batch 1363 batch loss 4.76259613 epoch total loss 5.71788597\n",
      "Trained batch 1364 batch loss 6.05023623 epoch total loss 5.71812963\n",
      "Trained batch 1365 batch loss 4.81731176 epoch total loss 5.71746969\n",
      "Trained batch 1366 batch loss 5.46022511 epoch total loss 5.71728182\n",
      "Trained batch 1367 batch loss 5.24694967 epoch total loss 5.71693754\n",
      "Trained batch 1368 batch loss 5.9199338 epoch total loss 5.71708584\n",
      "Trained batch 1369 batch loss 4.71321297 epoch total loss 5.71635294\n",
      "Trained batch 1370 batch loss 5.63202858 epoch total loss 5.71629095\n",
      "Trained batch 1371 batch loss 6.0228548 epoch total loss 5.71651459\n",
      "Trained batch 1372 batch loss 5.82795715 epoch total loss 5.71659613\n",
      "Trained batch 1373 batch loss 5.2225771 epoch total loss 5.71623659\n",
      "Trained batch 1374 batch loss 6.23049831 epoch total loss 5.71661043\n",
      "Trained batch 1375 batch loss 6.0334363 epoch total loss 5.71684074\n",
      "Trained batch 1376 batch loss 5.7921567 epoch total loss 5.71689558\n",
      "Trained batch 1377 batch loss 5.24590206 epoch total loss 5.71655369\n",
      "Trained batch 1378 batch loss 5.61745453 epoch total loss 5.71648169\n",
      "Trained batch 1379 batch loss 6.40994358 epoch total loss 5.71698475\n",
      "Trained batch 1380 batch loss 4.58986473 epoch total loss 5.71616793\n",
      "Trained batch 1381 batch loss 3.96065331 epoch total loss 5.71489668\n",
      "Trained batch 1382 batch loss 5.16008759 epoch total loss 5.71449518\n",
      "Trained batch 1383 batch loss 3.94934702 epoch total loss 5.71321917\n",
      "Trained batch 1384 batch loss 5.13324356 epoch total loss 5.7128\n",
      "Trained batch 1385 batch loss 4.1521616 epoch total loss 5.71167326\n",
      "Trained batch 1386 batch loss 4.8719058 epoch total loss 5.71106768\n",
      "Trained batch 1387 batch loss 4.23179197 epoch total loss 5.71000099\n",
      "Trained batch 1388 batch loss 5.65435 epoch total loss 5.70996094\n",
      "Trained batch 1389 batch loss 5.22986317 epoch total loss 5.70961523\n",
      "Trained batch 1390 batch loss 5.9375844 epoch total loss 5.70977926\n",
      "Trained batch 1391 batch loss 5.01053619 epoch total loss 5.70927668\n",
      "Trained batch 1392 batch loss 5.43029499 epoch total loss 5.7090764\n",
      "Trained batch 1393 batch loss 5.25317097 epoch total loss 5.70874882\n",
      "Trained batch 1394 batch loss 4.58838844 epoch total loss 5.70794535\n",
      "Trained batch 1395 batch loss 5.26216698 epoch total loss 5.70762539\n",
      "Trained batch 1396 batch loss 5.79335976 epoch total loss 5.7076869\n",
      "Trained batch 1397 batch loss 5.37544632 epoch total loss 5.70744944\n",
      "Trained batch 1398 batch loss 5.66982174 epoch total loss 5.70742226\n",
      "Trained batch 1399 batch loss 5.87432909 epoch total loss 5.70754194\n",
      "Trained batch 1400 batch loss 5.24616623 epoch total loss 5.70721245\n",
      "Trained batch 1401 batch loss 5.42831421 epoch total loss 5.70701313\n",
      "Trained batch 1402 batch loss 5.75803 epoch total loss 5.70704937\n",
      "Trained batch 1403 batch loss 5.98358345 epoch total loss 5.7072463\n",
      "Trained batch 1404 batch loss 5.55719376 epoch total loss 5.70713949\n",
      "Trained batch 1405 batch loss 5.65236378 epoch total loss 5.70710039\n",
      "Trained batch 1406 batch loss 5.27181053 epoch total loss 5.70679092\n",
      "Trained batch 1407 batch loss 4.11024761 epoch total loss 5.70565605\n",
      "Trained batch 1408 batch loss 5.26725101 epoch total loss 5.70534468\n",
      "Trained batch 1409 batch loss 4.73968124 epoch total loss 5.70465946\n",
      "Trained batch 1410 batch loss 6.09195805 epoch total loss 5.70493412\n",
      "Trained batch 1411 batch loss 4.83325291 epoch total loss 5.70431662\n",
      "Trained batch 1412 batch loss 5.51660585 epoch total loss 5.70418358\n",
      "Trained batch 1413 batch loss 5.55124378 epoch total loss 5.70407534\n",
      "Trained batch 1414 batch loss 6.54074574 epoch total loss 5.70466661\n",
      "Trained batch 1415 batch loss 5.62738085 epoch total loss 5.70461226\n",
      "Trained batch 1416 batch loss 6.13927937 epoch total loss 5.70491934\n",
      "Trained batch 1417 batch loss 6.08410025 epoch total loss 5.70518684\n",
      "Trained batch 1418 batch loss 5.90353632 epoch total loss 5.70532656\n",
      "Trained batch 1419 batch loss 5.87423468 epoch total loss 5.70544529\n",
      "Trained batch 1420 batch loss 6.00985336 epoch total loss 5.70565939\n",
      "Trained batch 1421 batch loss 6.21910763 epoch total loss 5.70602083\n",
      "Trained batch 1422 batch loss 5.7776823 epoch total loss 5.70607138\n",
      "Trained batch 1423 batch loss 5.39107084 epoch total loss 5.70585\n",
      "Trained batch 1424 batch loss 4.44764614 epoch total loss 5.70496655\n",
      "Trained batch 1425 batch loss 5.34336758 epoch total loss 5.70471287\n",
      "Trained batch 1426 batch loss 5.57677 epoch total loss 5.70462322\n",
      "Trained batch 1427 batch loss 5.57520771 epoch total loss 5.70453215\n",
      "Trained batch 1428 batch loss 5.65369 epoch total loss 5.70449686\n",
      "Trained batch 1429 batch loss 6.3263793 epoch total loss 5.70493174\n",
      "Trained batch 1430 batch loss 4.73561668 epoch total loss 5.70425415\n",
      "Trained batch 1431 batch loss 4.83956385 epoch total loss 5.70365\n",
      "Trained batch 1432 batch loss 4.9481678 epoch total loss 5.70312214\n",
      "Trained batch 1433 batch loss 5.78333235 epoch total loss 5.70317793\n",
      "Trained batch 1434 batch loss 5.06145048 epoch total loss 5.70273066\n",
      "Trained batch 1435 batch loss 5.31167269 epoch total loss 5.7024579\n",
      "Trained batch 1436 batch loss 6.4375596 epoch total loss 5.70297\n",
      "Trained batch 1437 batch loss 5.3768816 epoch total loss 5.70274305\n",
      "Trained batch 1438 batch loss 5.02041912 epoch total loss 5.7022686\n",
      "Trained batch 1439 batch loss 4.63001537 epoch total loss 5.7015233\n",
      "Trained batch 1440 batch loss 5.13555336 epoch total loss 5.70113039\n",
      "Trained batch 1441 batch loss 5.44836664 epoch total loss 5.70095491\n",
      "Trained batch 1442 batch loss 4.8887558 epoch total loss 5.70039177\n",
      "Trained batch 1443 batch loss 5.39855099 epoch total loss 5.70018244\n",
      "Trained batch 1444 batch loss 5.4387126 epoch total loss 5.70000124\n",
      "Trained batch 1445 batch loss 4.74113083 epoch total loss 5.69933748\n",
      "Trained batch 1446 batch loss 5.6020031 epoch total loss 5.69927025\n",
      "Trained batch 1447 batch loss 5.2197628 epoch total loss 5.69893885\n",
      "Trained batch 1448 batch loss 5.05236149 epoch total loss 5.69849253\n",
      "Trained batch 1449 batch loss 6.05377865 epoch total loss 5.69873762\n",
      "Trained batch 1450 batch loss 5.81623125 epoch total loss 5.69881868\n",
      "Trained batch 1451 batch loss 6.22359371 epoch total loss 5.6991806\n",
      "Trained batch 1452 batch loss 5.96214056 epoch total loss 5.69936132\n",
      "Trained batch 1453 batch loss 5.48497677 epoch total loss 5.69921398\n",
      "Trained batch 1454 batch loss 5.34065151 epoch total loss 5.69896746\n",
      "Trained batch 1455 batch loss 5.52611065 epoch total loss 5.69884872\n",
      "Trained batch 1456 batch loss 5.95994806 epoch total loss 5.69902849\n",
      "Trained batch 1457 batch loss 6.28233576 epoch total loss 5.69942856\n",
      "Trained batch 1458 batch loss 5.24138069 epoch total loss 5.69911432\n",
      "Trained batch 1459 batch loss 4.82010841 epoch total loss 5.69851208\n",
      "Trained batch 1460 batch loss 5.54234076 epoch total loss 5.69840479\n",
      "Trained batch 1461 batch loss 4.78213644 epoch total loss 5.69777775\n",
      "Trained batch 1462 batch loss 5.14290762 epoch total loss 5.69739771\n",
      "Trained batch 1463 batch loss 5.27864504 epoch total loss 5.69711161\n",
      "Trained batch 1464 batch loss 6.33762264 epoch total loss 5.69754934\n",
      "Trained batch 1465 batch loss 4.973104 epoch total loss 5.69705439\n",
      "Trained batch 1466 batch loss 6.57658291 epoch total loss 5.69765377\n",
      "Trained batch 1467 batch loss 6.35501385 epoch total loss 5.69810247\n",
      "Trained batch 1468 batch loss 5.69710541 epoch total loss 5.698102\n",
      "Trained batch 1469 batch loss 6.3432703 epoch total loss 5.69854116\n",
      "Trained batch 1470 batch loss 6.65451717 epoch total loss 5.69919157\n",
      "Trained batch 1471 batch loss 6.3446455 epoch total loss 5.69963026\n",
      "Trained batch 1472 batch loss 6.16909599 epoch total loss 5.69994926\n",
      "Trained batch 1473 batch loss 5.95038223 epoch total loss 5.70011902\n",
      "Trained batch 1474 batch loss 6.04953575 epoch total loss 5.70035648\n",
      "Trained batch 1475 batch loss 6.24825478 epoch total loss 5.70072746\n",
      "Trained batch 1476 batch loss 5.16107 epoch total loss 5.70036221\n",
      "Trained batch 1477 batch loss 5.89561939 epoch total loss 5.70049429\n",
      "Trained batch 1478 batch loss 6.07682753 epoch total loss 5.70074892\n",
      "Trained batch 1479 batch loss 6.16720295 epoch total loss 5.70106411\n",
      "Trained batch 1480 batch loss 5.72894096 epoch total loss 5.70108271\n",
      "Trained batch 1481 batch loss 6.19632816 epoch total loss 5.70141697\n",
      "Trained batch 1482 batch loss 4.69208145 epoch total loss 5.70073652\n",
      "Trained batch 1483 batch loss 4.36210918 epoch total loss 5.69983387\n",
      "Trained batch 1484 batch loss 5.19596481 epoch total loss 5.69949436\n",
      "Trained batch 1485 batch loss 4.86291695 epoch total loss 5.69893122\n",
      "Trained batch 1486 batch loss 4.90865183 epoch total loss 5.69839907\n",
      "Trained batch 1487 batch loss 5.15431404 epoch total loss 5.69803333\n",
      "Trained batch 1488 batch loss 4.28459 epoch total loss 5.697083\n",
      "Trained batch 1489 batch loss 5.06201887 epoch total loss 5.69665718\n",
      "Trained batch 1490 batch loss 5.20595551 epoch total loss 5.69632769\n",
      "Trained batch 1491 batch loss 5.76661539 epoch total loss 5.69637489\n",
      "Trained batch 1492 batch loss 6.07089424 epoch total loss 5.69662619\n",
      "Trained batch 1493 batch loss 6.10479164 epoch total loss 5.69689941\n",
      "Trained batch 1494 batch loss 5.7004118 epoch total loss 5.69690132\n",
      "Trained batch 1495 batch loss 5.83827114 epoch total loss 5.69699574\n",
      "Trained batch 1496 batch loss 6.15396452 epoch total loss 5.69730139\n",
      "Trained batch 1497 batch loss 5.69812965 epoch total loss 5.69730234\n",
      "Trained batch 1498 batch loss 6.62752724 epoch total loss 5.69792318\n",
      "Trained batch 1499 batch loss 5.76614094 epoch total loss 5.69796944\n",
      "Trained batch 1500 batch loss 5.96100521 epoch total loss 5.69814444\n",
      "Trained batch 1501 batch loss 5.52034092 epoch total loss 5.69802618\n",
      "Trained batch 1502 batch loss 5.58353901 epoch total loss 5.69795036\n",
      "Trained batch 1503 batch loss 5.75886631 epoch total loss 5.69799089\n",
      "Trained batch 1504 batch loss 5.95069265 epoch total loss 5.69815922\n",
      "Trained batch 1505 batch loss 5.9301753 epoch total loss 5.69831276\n",
      "Trained batch 1506 batch loss 5.76128292 epoch total loss 5.6983552\n",
      "Trained batch 1507 batch loss 5.93359 epoch total loss 5.69851112\n",
      "Trained batch 1508 batch loss 6.01502848 epoch total loss 5.69872093\n",
      "Trained batch 1509 batch loss 5.78520155 epoch total loss 5.69877815\n",
      "Trained batch 1510 batch loss 5.8368082 epoch total loss 5.69886971\n",
      "Trained batch 1511 batch loss 5.41143751 epoch total loss 5.69867897\n",
      "Trained batch 1512 batch loss 5.45330811 epoch total loss 5.69851685\n",
      "Trained batch 1513 batch loss 5.33802843 epoch total loss 5.69827843\n",
      "Trained batch 1514 batch loss 5.86053801 epoch total loss 5.69838524\n",
      "Trained batch 1515 batch loss 5.58594 epoch total loss 5.69831133\n",
      "Trained batch 1516 batch loss 5.87285089 epoch total loss 5.69842625\n",
      "Trained batch 1517 batch loss 5.47152853 epoch total loss 5.698277\n",
      "Trained batch 1518 batch loss 6.14813757 epoch total loss 5.69857359\n",
      "Trained batch 1519 batch loss 6.04119492 epoch total loss 5.69879913\n",
      "Trained batch 1520 batch loss 5.94417477 epoch total loss 5.6989603\n",
      "Trained batch 1521 batch loss 5.62941551 epoch total loss 5.698915\n",
      "Trained batch 1522 batch loss 5.9630003 epoch total loss 5.69908857\n",
      "Trained batch 1523 batch loss 5.74449921 epoch total loss 5.69911814\n",
      "Trained batch 1524 batch loss 5.97217131 epoch total loss 5.69929743\n",
      "Trained batch 1525 batch loss 5.68686438 epoch total loss 5.69928932\n",
      "Trained batch 1526 batch loss 6.05757332 epoch total loss 5.69952393\n",
      "Trained batch 1527 batch loss 5.93342 epoch total loss 5.69967747\n",
      "Trained batch 1528 batch loss 5.95531559 epoch total loss 5.69984436\n",
      "Trained batch 1529 batch loss 5.91996479 epoch total loss 5.69998837\n",
      "Trained batch 1530 batch loss 5.71665621 epoch total loss 5.69999933\n",
      "Trained batch 1531 batch loss 4.57876873 epoch total loss 5.69926739\n",
      "Trained batch 1532 batch loss 4.9486146 epoch total loss 5.6987772\n",
      "Trained batch 1533 batch loss 4.16100693 epoch total loss 5.69777393\n",
      "Trained batch 1534 batch loss 4.83601856 epoch total loss 5.69721222\n",
      "Trained batch 1535 batch loss 5.72180319 epoch total loss 5.69722795\n",
      "Trained batch 1536 batch loss 5.05724 epoch total loss 5.69681168\n",
      "Trained batch 1537 batch loss 5.67177105 epoch total loss 5.69679546\n",
      "Trained batch 1538 batch loss 5.37810802 epoch total loss 5.69658804\n",
      "Trained batch 1539 batch loss 5.37213612 epoch total loss 5.69637728\n",
      "Trained batch 1540 batch loss 5.92088127 epoch total loss 5.69652319\n",
      "Trained batch 1541 batch loss 5.46890497 epoch total loss 5.69637537\n",
      "Trained batch 1542 batch loss 5.24287605 epoch total loss 5.69608116\n",
      "Trained batch 1543 batch loss 5.35848808 epoch total loss 5.69586229\n",
      "Trained batch 1544 batch loss 4.98208189 epoch total loss 5.6954\n",
      "Trained batch 1545 batch loss 5.30661583 epoch total loss 5.69514894\n",
      "Trained batch 1546 batch loss 4.96667671 epoch total loss 5.69467783\n",
      "Trained batch 1547 batch loss 4.74569607 epoch total loss 5.69406462\n",
      "Trained batch 1548 batch loss 5.81833029 epoch total loss 5.69414473\n",
      "Trained batch 1549 batch loss 5.60217857 epoch total loss 5.6940856\n",
      "Trained batch 1550 batch loss 6.59171581 epoch total loss 5.69466496\n",
      "Trained batch 1551 batch loss 5.58447313 epoch total loss 5.69459391\n",
      "Trained batch 1552 batch loss 5.83024025 epoch total loss 5.69468117\n",
      "Trained batch 1553 batch loss 5.39797115 epoch total loss 5.69449043\n",
      "Trained batch 1554 batch loss 6.27090263 epoch total loss 5.69486141\n",
      "Trained batch 1555 batch loss 5.501894 epoch total loss 5.69473743\n",
      "Trained batch 1556 batch loss 6.55001163 epoch total loss 5.69528675\n",
      "Trained batch 1557 batch loss 5.49962854 epoch total loss 5.69516134\n",
      "Trained batch 1558 batch loss 3.38670444 epoch total loss 5.69368\n",
      "Trained batch 1559 batch loss 4.69944763 epoch total loss 5.6930418\n",
      "Trained batch 1560 batch loss 4.44738197 epoch total loss 5.6922431\n",
      "Trained batch 1561 batch loss 4.02862 epoch total loss 5.69117737\n",
      "Trained batch 1562 batch loss 5.76180363 epoch total loss 5.69122267\n",
      "Trained batch 1563 batch loss 6.8952446 epoch total loss 5.69199276\n",
      "Trained batch 1564 batch loss 7.4540782 epoch total loss 5.69311953\n",
      "Trained batch 1565 batch loss 6.513834 epoch total loss 5.69364405\n",
      "Trained batch 1566 batch loss 7.61900043 epoch total loss 5.69487333\n",
      "Trained batch 1567 batch loss 6.84991693 epoch total loss 5.69561052\n",
      "Trained batch 1568 batch loss 7.34374762 epoch total loss 5.69666147\n",
      "Trained batch 1569 batch loss 7.0608 epoch total loss 5.69753075\n",
      "Trained batch 1570 batch loss 6.406147 epoch total loss 5.69798231\n",
      "Trained batch 1571 batch loss 5.60644722 epoch total loss 5.69792414\n",
      "Trained batch 1572 batch loss 6.03907585 epoch total loss 5.6981411\n",
      "Trained batch 1573 batch loss 6.45557547 epoch total loss 5.6986227\n",
      "Trained batch 1574 batch loss 6.07065725 epoch total loss 5.69885874\n",
      "Trained batch 1575 batch loss 6.18830729 epoch total loss 5.69916964\n",
      "Trained batch 1576 batch loss 6.07747459 epoch total loss 5.69940948\n",
      "Trained batch 1577 batch loss 6.02378 epoch total loss 5.699615\n",
      "Trained batch 1578 batch loss 5.6369729 epoch total loss 5.69957542\n",
      "Trained batch 1579 batch loss 5.86541605 epoch total loss 5.69968033\n",
      "Trained batch 1580 batch loss 5.07923126 epoch total loss 5.69928741\n",
      "Trained batch 1581 batch loss 6.33886719 epoch total loss 5.69969177\n",
      "Trained batch 1582 batch loss 5.5975337 epoch total loss 5.6996274\n",
      "Trained batch 1583 batch loss 5.6068182 epoch total loss 5.69956875\n",
      "Trained batch 1584 batch loss 5.97433949 epoch total loss 5.69974232\n",
      "Trained batch 1585 batch loss 5.22737885 epoch total loss 5.69944429\n",
      "Trained batch 1586 batch loss 4.86041 epoch total loss 5.698915\n",
      "Trained batch 1587 batch loss 5.61910629 epoch total loss 5.69886494\n",
      "Trained batch 1588 batch loss 5.39200878 epoch total loss 5.69867134\n",
      "Trained batch 1589 batch loss 5.88914 epoch total loss 5.69879103\n",
      "Trained batch 1590 batch loss 5.92442226 epoch total loss 5.69893312\n",
      "Trained batch 1591 batch loss 5.53341341 epoch total loss 5.69882917\n",
      "Trained batch 1592 batch loss 6.65855312 epoch total loss 5.69943142\n",
      "Trained batch 1593 batch loss 6.66677237 epoch total loss 5.70003891\n",
      "Trained batch 1594 batch loss 7.63303 epoch total loss 5.70125151\n",
      "Trained batch 1595 batch loss 6.96799135 epoch total loss 5.70204544\n",
      "Trained batch 1596 batch loss 6.42789 epoch total loss 5.70250034\n",
      "Trained batch 1597 batch loss 5.20602703 epoch total loss 5.70218945\n",
      "Trained batch 1598 batch loss 5.56532097 epoch total loss 5.70210361\n",
      "Trained batch 1599 batch loss 5.72522497 epoch total loss 5.7021184\n",
      "Trained batch 1600 batch loss 5.83724737 epoch total loss 5.7022028\n",
      "Trained batch 1601 batch loss 6.29079247 epoch total loss 5.70257044\n",
      "Trained batch 1602 batch loss 5.94286966 epoch total loss 5.70272\n",
      "Trained batch 1603 batch loss 6.25441933 epoch total loss 5.70306492\n",
      "Trained batch 1604 batch loss 6.11333561 epoch total loss 5.7033205\n",
      "Trained batch 1605 batch loss 6.41142082 epoch total loss 5.70376158\n",
      "Trained batch 1606 batch loss 6.32655191 epoch total loss 5.70414877\n",
      "Trained batch 1607 batch loss 4.42207527 epoch total loss 5.70335102\n",
      "Trained batch 1608 batch loss 6.27451515 epoch total loss 5.70370626\n",
      "Trained batch 1609 batch loss 5.90678692 epoch total loss 5.70383263\n",
      "Trained batch 1610 batch loss 5.99676085 epoch total loss 5.70401478\n",
      "Trained batch 1611 batch loss 6.18069935 epoch total loss 5.70431089\n",
      "Trained batch 1612 batch loss 4.82299662 epoch total loss 5.70376396\n",
      "Trained batch 1613 batch loss 5.86065197 epoch total loss 5.70386124\n",
      "Trained batch 1614 batch loss 5.35843086 epoch total loss 5.70364714\n",
      "Trained batch 1615 batch loss 5.19711876 epoch total loss 5.70333338\n",
      "Trained batch 1616 batch loss 5.4176507 epoch total loss 5.70315695\n",
      "Trained batch 1617 batch loss 5.41950893 epoch total loss 5.70298195\n",
      "Trained batch 1618 batch loss 6.24238539 epoch total loss 5.70331526\n",
      "Trained batch 1619 batch loss 5.67875528 epoch total loss 5.7033\n",
      "Trained batch 1620 batch loss 5.33666849 epoch total loss 5.70307398\n",
      "Trained batch 1621 batch loss 5.29915953 epoch total loss 5.70282459\n",
      "Trained batch 1622 batch loss 5.09116268 epoch total loss 5.70244694\n",
      "Trained batch 1623 batch loss 5.27438736 epoch total loss 5.70218325\n",
      "Trained batch 1624 batch loss 5.76852512 epoch total loss 5.70222425\n",
      "Trained batch 1625 batch loss 6.13413429 epoch total loss 5.70249\n",
      "Trained batch 1626 batch loss 5.0081706 epoch total loss 5.70206261\n",
      "Trained batch 1627 batch loss 5.38629913 epoch total loss 5.70186853\n",
      "Trained batch 1628 batch loss 5.73830318 epoch total loss 5.70189095\n",
      "Trained batch 1629 batch loss 5.37310505 epoch total loss 5.70168924\n",
      "Trained batch 1630 batch loss 3.83375359 epoch total loss 5.7005434\n",
      "Trained batch 1631 batch loss 7.39741707 epoch total loss 5.70158386\n",
      "Trained batch 1632 batch loss 7.51492739 epoch total loss 5.70269489\n",
      "Trained batch 1633 batch loss 6.93389702 epoch total loss 5.7034483\n",
      "Trained batch 1634 batch loss 4.94445753 epoch total loss 5.70298386\n",
      "Trained batch 1635 batch loss 4.99685 epoch total loss 5.70255232\n",
      "Trained batch 1636 batch loss 5.78913879 epoch total loss 5.70260525\n",
      "Trained batch 1637 batch loss 5.16709757 epoch total loss 5.70227766\n",
      "Trained batch 1638 batch loss 5.70389843 epoch total loss 5.70227909\n",
      "Trained batch 1639 batch loss 5.02629328 epoch total loss 5.70186663\n",
      "Trained batch 1640 batch loss 5.82517862 epoch total loss 5.70194197\n",
      "Trained batch 1641 batch loss 4.31921101 epoch total loss 5.7010994\n",
      "Trained batch 1642 batch loss 6.02571774 epoch total loss 5.70129681\n",
      "Trained batch 1643 batch loss 5.97745419 epoch total loss 5.70146513\n",
      "Trained batch 1644 batch loss 6.41873 epoch total loss 5.70190144\n",
      "Trained batch 1645 batch loss 6.3166132 epoch total loss 5.7022748\n",
      "Trained batch 1646 batch loss 6.25060844 epoch total loss 5.70260811\n",
      "Trained batch 1647 batch loss 6.00841141 epoch total loss 5.70279408\n",
      "Trained batch 1648 batch loss 5.76016092 epoch total loss 5.70282888\n",
      "Trained batch 1649 batch loss 5.37732935 epoch total loss 5.702631\n",
      "Trained batch 1650 batch loss 5.64973879 epoch total loss 5.70259905\n",
      "Trained batch 1651 batch loss 5.88898849 epoch total loss 5.70271158\n",
      "Trained batch 1652 batch loss 5.85821152 epoch total loss 5.702806\n",
      "Trained batch 1653 batch loss 5.71411848 epoch total loss 5.70281267\n",
      "Trained batch 1654 batch loss 5.57354879 epoch total loss 5.70273399\n",
      "Trained batch 1655 batch loss 5.89755917 epoch total loss 5.70285177\n",
      "Trained batch 1656 batch loss 6.22915792 epoch total loss 5.70317\n",
      "Trained batch 1657 batch loss 6.27627134 epoch total loss 5.70351553\n",
      "Trained batch 1658 batch loss 6.02705574 epoch total loss 5.70371103\n",
      "Trained batch 1659 batch loss 6.15941715 epoch total loss 5.70398569\n",
      "Trained batch 1660 batch loss 6.18742657 epoch total loss 5.70427704\n",
      "Trained batch 1661 batch loss 6.10183096 epoch total loss 5.70451593\n",
      "Trained batch 1662 batch loss 6.35354614 epoch total loss 5.70490646\n",
      "Trained batch 1663 batch loss 6.52287436 epoch total loss 5.70539808\n",
      "Trained batch 1664 batch loss 6.72562027 epoch total loss 5.7060113\n",
      "Trained batch 1665 batch loss 5.78877735 epoch total loss 5.70606136\n",
      "Trained batch 1666 batch loss 6.07674026 epoch total loss 5.70628405\n",
      "Trained batch 1667 batch loss 5.99052191 epoch total loss 5.70645428\n",
      "Trained batch 1668 batch loss 6.19565058 epoch total loss 5.70674753\n",
      "Trained batch 1669 batch loss 5.82351303 epoch total loss 5.70681715\n",
      "Trained batch 1670 batch loss 6.37058449 epoch total loss 5.70721436\n",
      "Trained batch 1671 batch loss 5.91283607 epoch total loss 5.70733738\n",
      "Trained batch 1672 batch loss 5.96068144 epoch total loss 5.70748901\n",
      "Trained batch 1673 batch loss 6.16391039 epoch total loss 5.70776224\n",
      "Trained batch 1674 batch loss 5.25538921 epoch total loss 5.70749187\n",
      "Trained batch 1675 batch loss 6.04373837 epoch total loss 5.7076931\n",
      "Trained batch 1676 batch loss 5.911304 epoch total loss 5.70781422\n",
      "Trained batch 1677 batch loss 5.73176384 epoch total loss 5.70782852\n",
      "Trained batch 1678 batch loss 5.9177146 epoch total loss 5.70795345\n",
      "Trained batch 1679 batch loss 6.07529736 epoch total loss 5.70817232\n",
      "Trained batch 1680 batch loss 6.16763353 epoch total loss 5.70844603\n",
      "Trained batch 1681 batch loss 6.69000196 epoch total loss 5.70903\n",
      "Trained batch 1682 batch loss 5.7901516 epoch total loss 5.70907831\n",
      "Trained batch 1683 batch loss 7.2824254 epoch total loss 5.71001339\n",
      "Trained batch 1684 batch loss 7.84164429 epoch total loss 5.71127892\n",
      "Trained batch 1685 batch loss 6.00389 epoch total loss 5.71145296\n",
      "Trained batch 1686 batch loss 6.41260052 epoch total loss 5.71186876\n",
      "Trained batch 1687 batch loss 6.21674299 epoch total loss 5.71216822\n",
      "Trained batch 1688 batch loss 5.78802204 epoch total loss 5.71221304\n",
      "Trained batch 1689 batch loss 6.09334564 epoch total loss 5.71243906\n",
      "Trained batch 1690 batch loss 5.57595348 epoch total loss 5.71235847\n",
      "Trained batch 1691 batch loss 5.3860321 epoch total loss 5.71216536\n",
      "Trained batch 1692 batch loss 5.58564949 epoch total loss 5.71209049\n",
      "Trained batch 1693 batch loss 5.20320034 epoch total loss 5.71179\n",
      "Trained batch 1694 batch loss 5.37406397 epoch total loss 5.71159077\n",
      "Trained batch 1695 batch loss 5.60875416 epoch total loss 5.71152973\n",
      "Trained batch 1696 batch loss 5.69727802 epoch total loss 5.71152115\n",
      "Trained batch 1697 batch loss 5.31271744 epoch total loss 5.71128607\n",
      "Trained batch 1698 batch loss 5.75807858 epoch total loss 5.71131372\n",
      "Trained batch 1699 batch loss 5.6015892 epoch total loss 5.71124887\n",
      "Trained batch 1700 batch loss 5.97714281 epoch total loss 5.71140575\n",
      "Trained batch 1701 batch loss 5.72041225 epoch total loss 5.711411\n",
      "Trained batch 1702 batch loss 5.71753502 epoch total loss 5.71141481\n",
      "Trained batch 1703 batch loss 5.16033077 epoch total loss 5.71109104\n",
      "Trained batch 1704 batch loss 6.54072905 epoch total loss 5.71157837\n",
      "Trained batch 1705 batch loss 5.86456633 epoch total loss 5.71166754\n",
      "Trained batch 1706 batch loss 5.75595284 epoch total loss 5.71169376\n",
      "Trained batch 1707 batch loss 4.31791592 epoch total loss 5.71087742\n",
      "Trained batch 1708 batch loss 4.59715271 epoch total loss 5.71022511\n",
      "Trained batch 1709 batch loss 4.72251 epoch total loss 5.70964718\n",
      "Trained batch 1710 batch loss 5.65283823 epoch total loss 5.70961428\n",
      "Trained batch 1711 batch loss 5.01675 epoch total loss 5.70920944\n",
      "Trained batch 1712 batch loss 6.25742435 epoch total loss 5.70953\n",
      "Trained batch 1713 batch loss 6.20026159 epoch total loss 5.70981598\n",
      "Trained batch 1714 batch loss 6.65040445 epoch total loss 5.71036482\n",
      "Trained batch 1715 batch loss 4.81074715 epoch total loss 5.7098403\n",
      "Trained batch 1716 batch loss 5.48349857 epoch total loss 5.70970821\n",
      "Trained batch 1717 batch loss 5.76282883 epoch total loss 5.70973921\n",
      "Trained batch 1718 batch loss 4.70177698 epoch total loss 5.7091527\n",
      "Trained batch 1719 batch loss 5.32004452 epoch total loss 5.7089262\n",
      "Trained batch 1720 batch loss 6.50566387 epoch total loss 5.70938969\n",
      "Trained batch 1721 batch loss 6.64524412 epoch total loss 5.70993376\n",
      "Trained batch 1722 batch loss 6.20250702 epoch total loss 5.71021938\n",
      "Trained batch 1723 batch loss 4.70011473 epoch total loss 5.70963335\n",
      "Trained batch 1724 batch loss 4.97088385 epoch total loss 5.70920467\n",
      "Trained batch 1725 batch loss 5.41651964 epoch total loss 5.7090354\n",
      "Trained batch 1726 batch loss 6.07090378 epoch total loss 5.7092452\n",
      "Trained batch 1727 batch loss 5.48480844 epoch total loss 5.70911503\n",
      "Trained batch 1728 batch loss 5.56783581 epoch total loss 5.70903301\n",
      "Trained batch 1729 batch loss 5.68147898 epoch total loss 5.70901728\n",
      "Trained batch 1730 batch loss 5.41981697 epoch total loss 5.70885\n",
      "Trained batch 1731 batch loss 5.82582378 epoch total loss 5.70891762\n",
      "Trained batch 1732 batch loss 5.50784302 epoch total loss 5.70880175\n",
      "Trained batch 1733 batch loss 5.64572048 epoch total loss 5.70876503\n",
      "Trained batch 1734 batch loss 5.34322166 epoch total loss 5.70855427\n",
      "Trained batch 1735 batch loss 6.09116745 epoch total loss 5.70877457\n",
      "Trained batch 1736 batch loss 5.95336723 epoch total loss 5.70891523\n",
      "Trained batch 1737 batch loss 6.50128746 epoch total loss 5.70937109\n",
      "Trained batch 1738 batch loss 6.2516 epoch total loss 5.70968342\n",
      "Trained batch 1739 batch loss 6.35572815 epoch total loss 5.71005487\n",
      "Trained batch 1740 batch loss 6.43699312 epoch total loss 5.71047211\n",
      "Trained batch 1741 batch loss 4.85688257 epoch total loss 5.70998192\n",
      "Trained batch 1742 batch loss 4.3340292 epoch total loss 5.7091918\n",
      "Trained batch 1743 batch loss 5.2073431 epoch total loss 5.70890379\n",
      "Trained batch 1744 batch loss 6.35033226 epoch total loss 5.70927143\n",
      "Trained batch 1745 batch loss 6.93692207 epoch total loss 5.70997477\n",
      "Trained batch 1746 batch loss 6.16767025 epoch total loss 5.71023703\n",
      "Trained batch 1747 batch loss 5.6818819 epoch total loss 5.71022081\n",
      "Trained batch 1748 batch loss 5.87375832 epoch total loss 5.71031475\n",
      "Trained batch 1749 batch loss 6.1642704 epoch total loss 5.71057415\n",
      "Trained batch 1750 batch loss 5.43886614 epoch total loss 5.7104187\n",
      "Trained batch 1751 batch loss 5.64428806 epoch total loss 5.71038103\n",
      "Trained batch 1752 batch loss 4.89683914 epoch total loss 5.70991611\n",
      "Trained batch 1753 batch loss 5.04142094 epoch total loss 5.70953465\n",
      "Trained batch 1754 batch loss 5.9207592 epoch total loss 5.70965528\n",
      "Trained batch 1755 batch loss 5.92698765 epoch total loss 5.70977879\n",
      "Trained batch 1756 batch loss 5.83661 epoch total loss 5.70985126\n",
      "Trained batch 1757 batch loss 5.46151543 epoch total loss 5.70971\n",
      "Trained batch 1758 batch loss 5.69278336 epoch total loss 5.70970058\n",
      "Trained batch 1759 batch loss 5.80925131 epoch total loss 5.70975733\n",
      "Trained batch 1760 batch loss 5.89368916 epoch total loss 5.70986176\n",
      "Trained batch 1761 batch loss 5.91711807 epoch total loss 5.70997906\n",
      "Trained batch 1762 batch loss 5.8745451 epoch total loss 5.71007299\n",
      "Trained batch 1763 batch loss 5.53791428 epoch total loss 5.70997524\n",
      "Trained batch 1764 batch loss 5.71230602 epoch total loss 5.70997667\n",
      "Trained batch 1765 batch loss 5.57085419 epoch total loss 5.70989799\n",
      "Trained batch 1766 batch loss 5.5988822 epoch total loss 5.70983505\n",
      "Trained batch 1767 batch loss 5.80286884 epoch total loss 5.7098875\n",
      "Trained batch 1768 batch loss 5.10501 epoch total loss 5.70954561\n",
      "Trained batch 1769 batch loss 5.00159931 epoch total loss 5.70914555\n",
      "Trained batch 1770 batch loss 5.97569036 epoch total loss 5.70929623\n",
      "Trained batch 1771 batch loss 5.94803524 epoch total loss 5.70943117\n",
      "Trained batch 1772 batch loss 6.29617596 epoch total loss 5.7097621\n",
      "Trained batch 1773 batch loss 6.0918355 epoch total loss 5.70997763\n",
      "Trained batch 1774 batch loss 6.86755466 epoch total loss 5.71063\n",
      "Trained batch 1775 batch loss 6.65124607 epoch total loss 5.71115971\n",
      "Trained batch 1776 batch loss 7.43386841 epoch total loss 5.71212959\n",
      "Trained batch 1777 batch loss 6.37608814 epoch total loss 5.71250296\n",
      "Trained batch 1778 batch loss 6.59042454 epoch total loss 5.71299696\n",
      "Trained batch 1779 batch loss 6.6801424 epoch total loss 5.71354055\n",
      "Trained batch 1780 batch loss 6.44124222 epoch total loss 5.71394968\n",
      "Trained batch 1781 batch loss 6.98048115 epoch total loss 5.71466064\n",
      "Trained batch 1782 batch loss 6.13894129 epoch total loss 5.71489859\n",
      "Trained batch 1783 batch loss 6.26973152 epoch total loss 5.71520948\n",
      "Trained batch 1784 batch loss 5.86330843 epoch total loss 5.71529245\n",
      "Trained batch 1785 batch loss 5.75532532 epoch total loss 5.71531487\n",
      "Trained batch 1786 batch loss 5.23981333 epoch total loss 5.71504879\n",
      "Trained batch 1787 batch loss 5.71481705 epoch total loss 5.71504879\n",
      "Trained batch 1788 batch loss 5.58640766 epoch total loss 5.71497631\n",
      "Trained batch 1789 batch loss 5.25117874 epoch total loss 5.71471691\n",
      "Trained batch 1790 batch loss 4.60886 epoch total loss 5.71409893\n",
      "Trained batch 1791 batch loss 5.71942139 epoch total loss 5.71410227\n",
      "Trained batch 1792 batch loss 5.88720083 epoch total loss 5.71419859\n",
      "Trained batch 1793 batch loss 4.9218626 epoch total loss 5.71375656\n",
      "Trained batch 1794 batch loss 3.48301291 epoch total loss 5.71251345\n",
      "Trained batch 1795 batch loss 4.91191483 epoch total loss 5.7120676\n",
      "Trained batch 1796 batch loss 6.57153416 epoch total loss 5.71254587\n",
      "Trained batch 1797 batch loss 6.50294971 epoch total loss 5.71298552\n",
      "Trained batch 1798 batch loss 6.01113272 epoch total loss 5.71315145\n",
      "Trained batch 1799 batch loss 6.32755613 epoch total loss 5.71349239\n",
      "Trained batch 1800 batch loss 7.15092707 epoch total loss 5.71429157\n",
      "Trained batch 1801 batch loss 5.43926334 epoch total loss 5.71413898\n",
      "Trained batch 1802 batch loss 6.77590656 epoch total loss 5.71472836\n",
      "Trained batch 1803 batch loss 6.05334616 epoch total loss 5.71491623\n",
      "Trained batch 1804 batch loss 6.0077 epoch total loss 5.71507883\n",
      "Trained batch 1805 batch loss 6.26596928 epoch total loss 5.71538353\n",
      "Trained batch 1806 batch loss 6.23442 epoch total loss 5.71567106\n",
      "Trained batch 1807 batch loss 6.63116264 epoch total loss 5.71617746\n",
      "Trained batch 1808 batch loss 6.10135221 epoch total loss 5.71639061\n",
      "Trained batch 1809 batch loss 5.96837473 epoch total loss 5.71653032\n",
      "Trained batch 1810 batch loss 5.49860239 epoch total loss 5.71641\n",
      "Trained batch 1811 batch loss 6.19726372 epoch total loss 5.71667576\n",
      "Trained batch 1812 batch loss 5.49100542 epoch total loss 5.7165513\n",
      "Trained batch 1813 batch loss 6.08662224 epoch total loss 5.71675539\n",
      "Trained batch 1814 batch loss 6.36803436 epoch total loss 5.71711445\n",
      "Trained batch 1815 batch loss 5.6664896 epoch total loss 5.71708632\n",
      "Trained batch 1816 batch loss 6.20930767 epoch total loss 5.71735716\n",
      "Trained batch 1817 batch loss 6.27697754 epoch total loss 5.7176652\n",
      "Trained batch 1818 batch loss 5.79078865 epoch total loss 5.71770573\n",
      "Trained batch 1819 batch loss 6.55942822 epoch total loss 5.71816874\n",
      "Trained batch 1820 batch loss 4.73043585 epoch total loss 5.71762609\n",
      "Trained batch 1821 batch loss 4.18476295 epoch total loss 5.716784\n",
      "Trained batch 1822 batch loss 5.14042 epoch total loss 5.71646786\n",
      "Trained batch 1823 batch loss 5.61940718 epoch total loss 5.71641445\n",
      "Trained batch 1824 batch loss 6.15781736 epoch total loss 5.71665668\n",
      "Trained batch 1825 batch loss 5.26821041 epoch total loss 5.71641111\n",
      "Trained batch 1826 batch loss 5.86749268 epoch total loss 5.71649361\n",
      "Trained batch 1827 batch loss 5.7519989 epoch total loss 5.71651316\n",
      "Trained batch 1828 batch loss 6.09980774 epoch total loss 5.71672249\n",
      "Trained batch 1829 batch loss 6.04125595 epoch total loss 5.7169\n",
      "Trained batch 1830 batch loss 4.41373825 epoch total loss 5.71618795\n",
      "Trained batch 1831 batch loss 4.94959736 epoch total loss 5.71576929\n",
      "Trained batch 1832 batch loss 5.06873035 epoch total loss 5.71541595\n",
      "Trained batch 1833 batch loss 4.84857368 epoch total loss 5.71494293\n",
      "Trained batch 1834 batch loss 4.96282673 epoch total loss 5.71453285\n",
      "Trained batch 1835 batch loss 5.03820419 epoch total loss 5.71416426\n",
      "Trained batch 1836 batch loss 5.44482374 epoch total loss 5.71401739\n",
      "Trained batch 1837 batch loss 6.24948883 epoch total loss 5.71430826\n",
      "Trained batch 1838 batch loss 5.19122458 epoch total loss 5.71402407\n",
      "Trained batch 1839 batch loss 5.99625111 epoch total loss 5.71417713\n",
      "Trained batch 1840 batch loss 5.69645309 epoch total loss 5.71416759\n",
      "Trained batch 1841 batch loss 4.66426325 epoch total loss 5.7135973\n",
      "Trained batch 1842 batch loss 4.6513319 epoch total loss 5.71302032\n",
      "Trained batch 1843 batch loss 5.52012444 epoch total loss 5.7129159\n",
      "Trained batch 1844 batch loss 5.97519207 epoch total loss 5.71305847\n",
      "Trained batch 1845 batch loss 5.54768133 epoch total loss 5.71296883\n",
      "Trained batch 1846 batch loss 5.5749855 epoch total loss 5.71289444\n",
      "Trained batch 1847 batch loss 4.65905285 epoch total loss 5.71232367\n",
      "Trained batch 1848 batch loss 5.83491087 epoch total loss 5.71239042\n",
      "Trained batch 1849 batch loss 6.26218271 epoch total loss 5.71268749\n",
      "Trained batch 1850 batch loss 6.420784 epoch total loss 5.71307\n",
      "Trained batch 1851 batch loss 5.92585182 epoch total loss 5.71318483\n",
      "Trained batch 1852 batch loss 5.44547796 epoch total loss 5.71304035\n",
      "Trained batch 1853 batch loss 5.74312115 epoch total loss 5.71305656\n",
      "Trained batch 1854 batch loss 5.48325682 epoch total loss 5.71293259\n",
      "Trained batch 1855 batch loss 5.77309418 epoch total loss 5.71296549\n",
      "Trained batch 1856 batch loss 6.31764746 epoch total loss 5.71329117\n",
      "Trained batch 1857 batch loss 6.09195518 epoch total loss 5.71349478\n",
      "Trained batch 1858 batch loss 5.85489178 epoch total loss 5.71357059\n",
      "Trained batch 1859 batch loss 5.84368134 epoch total loss 5.71364069\n",
      "Trained batch 1860 batch loss 5.52305698 epoch total loss 5.71353865\n",
      "Trained batch 1861 batch loss 5.71919346 epoch total loss 5.71354151\n",
      "Trained batch 1862 batch loss 5.83466387 epoch total loss 5.71360636\n",
      "Trained batch 1863 batch loss 4.49714136 epoch total loss 5.71295357\n",
      "Trained batch 1864 batch loss 6.09535503 epoch total loss 5.71315908\n",
      "Trained batch 1865 batch loss 5.73546886 epoch total loss 5.71317053\n",
      "Trained batch 1866 batch loss 5.69276237 epoch total loss 5.71315956\n",
      "Trained batch 1867 batch loss 5.70192719 epoch total loss 5.71315384\n",
      "Trained batch 1868 batch loss 6.0311203 epoch total loss 5.71332407\n",
      "Trained batch 1869 batch loss 6.39733696 epoch total loss 5.71369028\n",
      "Trained batch 1870 batch loss 5.59865284 epoch total loss 5.71362829\n",
      "Trained batch 1871 batch loss 5.70987272 epoch total loss 5.71362638\n",
      "Trained batch 1872 batch loss 5.7050848 epoch total loss 5.71362209\n",
      "Trained batch 1873 batch loss 6.20557547 epoch total loss 5.71388483\n",
      "Trained batch 1874 batch loss 4.32609463 epoch total loss 5.7131443\n",
      "Trained batch 1875 batch loss 5.90387917 epoch total loss 5.71324635\n",
      "Trained batch 1876 batch loss 5.09857178 epoch total loss 5.71291876\n",
      "Trained batch 1877 batch loss 4.39577 epoch total loss 5.71221685\n",
      "Trained batch 1878 batch loss 5.43663597 epoch total loss 5.71207\n",
      "Trained batch 1879 batch loss 5.92534 epoch total loss 5.71218395\n",
      "Trained batch 1880 batch loss 5.21906567 epoch total loss 5.71192122\n",
      "Trained batch 1881 batch loss 5.71932268 epoch total loss 5.71192551\n",
      "Trained batch 1882 batch loss 5.05749655 epoch total loss 5.71157789\n",
      "Trained batch 1883 batch loss 5.8206377 epoch total loss 5.71163559\n",
      "Trained batch 1884 batch loss 5.74409485 epoch total loss 5.71165276\n",
      "Trained batch 1885 batch loss 5.3732686 epoch total loss 5.71147299\n",
      "Trained batch 1886 batch loss 5.96733189 epoch total loss 5.71160889\n",
      "Trained batch 1887 batch loss 5.82150269 epoch total loss 5.71166706\n",
      "Trained batch 1888 batch loss 6.17724466 epoch total loss 5.71191359\n",
      "Trained batch 1889 batch loss 5.77155828 epoch total loss 5.71194506\n",
      "Trained batch 1890 batch loss 5.97562647 epoch total loss 5.71208477\n",
      "Trained batch 1891 batch loss 5.03071404 epoch total loss 5.7117238\n",
      "Trained batch 1892 batch loss 5.70603132 epoch total loss 5.71172094\n",
      "Trained batch 1893 batch loss 5.49039268 epoch total loss 5.71160412\n",
      "Trained batch 1894 batch loss 5.17974043 epoch total loss 5.71132326\n",
      "Trained batch 1895 batch loss 5.41892242 epoch total loss 5.71116877\n",
      "Trained batch 1896 batch loss 5.60637808 epoch total loss 5.71111345\n",
      "Trained batch 1897 batch loss 5.88710928 epoch total loss 5.71120644\n",
      "Trained batch 1898 batch loss 5.88163948 epoch total loss 5.71129608\n",
      "Trained batch 1899 batch loss 5.3477006 epoch total loss 5.71110487\n",
      "Trained batch 1900 batch loss 5.61902857 epoch total loss 5.71105623\n",
      "Trained batch 1901 batch loss 5.95432 epoch total loss 5.71118402\n",
      "Trained batch 1902 batch loss 5.95996666 epoch total loss 5.71131468\n",
      "Trained batch 1903 batch loss 3.49777222 epoch total loss 5.71015167\n",
      "Trained batch 1904 batch loss 6.00053 epoch total loss 5.71030474\n",
      "Trained batch 1905 batch loss 4.18289614 epoch total loss 5.7095027\n",
      "Trained batch 1906 batch loss 5.79042387 epoch total loss 5.70954514\n",
      "Trained batch 1907 batch loss 6.1259923 epoch total loss 5.70976305\n",
      "Trained batch 1908 batch loss 4.94562292 epoch total loss 5.70936251\n",
      "Trained batch 1909 batch loss 5.82959843 epoch total loss 5.70942593\n",
      "Trained batch 1910 batch loss 5.56441164 epoch total loss 5.70935\n",
      "Trained batch 1911 batch loss 5.25335789 epoch total loss 5.70911121\n",
      "Trained batch 1912 batch loss 5.78891897 epoch total loss 5.7091527\n",
      "Trained batch 1913 batch loss 5.43349934 epoch total loss 5.70900869\n",
      "Trained batch 1914 batch loss 5.28443336 epoch total loss 5.70878696\n",
      "Trained batch 1915 batch loss 5.45363235 epoch total loss 5.70865393\n",
      "Trained batch 1916 batch loss 5.69024754 epoch total loss 5.70864439\n",
      "Trained batch 1917 batch loss 5.74872732 epoch total loss 5.70866537\n",
      "Trained batch 1918 batch loss 5.15176487 epoch total loss 5.70837498\n",
      "Trained batch 1919 batch loss 5.70401382 epoch total loss 5.70837259\n",
      "Trained batch 1920 batch loss 5.84083319 epoch total loss 5.70844173\n",
      "Trained batch 1921 batch loss 5.56565523 epoch total loss 5.70836735\n",
      "Trained batch 1922 batch loss 6.25229883 epoch total loss 5.70865\n",
      "Trained batch 1923 batch loss 6.33232498 epoch total loss 5.70897436\n",
      "Trained batch 1924 batch loss 6.21979523 epoch total loss 5.70923948\n",
      "Trained batch 1925 batch loss 5.79023743 epoch total loss 5.70928144\n",
      "Trained batch 1926 batch loss 5.73368931 epoch total loss 5.70929432\n",
      "Trained batch 1927 batch loss 5.71982 epoch total loss 5.70929956\n",
      "Trained batch 1928 batch loss 6.00907421 epoch total loss 5.70945501\n",
      "Trained batch 1929 batch loss 5.40057564 epoch total loss 5.7092948\n",
      "Trained batch 1930 batch loss 6.26433372 epoch total loss 5.70958233\n",
      "Trained batch 1931 batch loss 5.44675732 epoch total loss 5.70944595\n",
      "Trained batch 1932 batch loss 5.8044281 epoch total loss 5.70949554\n",
      "Trained batch 1933 batch loss 5.96038055 epoch total loss 5.70962477\n",
      "Trained batch 1934 batch loss 5.8028574 epoch total loss 5.70967293\n",
      "Trained batch 1935 batch loss 5.69397 epoch total loss 5.7096653\n",
      "Trained batch 1936 batch loss 5.48168802 epoch total loss 5.70954752\n",
      "Trained batch 1937 batch loss 5.3800621 epoch total loss 5.70937729\n",
      "Trained batch 1938 batch loss 5.63582325 epoch total loss 5.70933914\n",
      "Trained batch 1939 batch loss 5.29617167 epoch total loss 5.709126\n",
      "Trained batch 1940 batch loss 5.50051355 epoch total loss 5.70901871\n",
      "Trained batch 1941 batch loss 6.61842108 epoch total loss 5.70948696\n",
      "Trained batch 1942 batch loss 7.03163862 epoch total loss 5.71016741\n",
      "Trained batch 1943 batch loss 6.55192375 epoch total loss 5.71060085\n",
      "Trained batch 1944 batch loss 6.32328558 epoch total loss 5.71091604\n",
      "Trained batch 1945 batch loss 6.66149378 epoch total loss 5.71140432\n",
      "Trained batch 1946 batch loss 5.76741791 epoch total loss 5.71143341\n",
      "Trained batch 1947 batch loss 6.26659679 epoch total loss 5.71171856\n",
      "Trained batch 1948 batch loss 6.59179115 epoch total loss 5.71217\n",
      "Trained batch 1949 batch loss 6.43531418 epoch total loss 5.71254158\n",
      "Trained batch 1950 batch loss 6.41517496 epoch total loss 5.71290159\n",
      "Trained batch 1951 batch loss 5.83212709 epoch total loss 5.71296263\n",
      "Trained batch 1952 batch loss 5.86905193 epoch total loss 5.71304274\n",
      "Trained batch 1953 batch loss 5.48996544 epoch total loss 5.71292877\n",
      "Trained batch 1954 batch loss 5.66781187 epoch total loss 5.71290541\n",
      "Trained batch 1955 batch loss 6.10132313 epoch total loss 5.71310425\n",
      "Trained batch 1956 batch loss 5.97123 epoch total loss 5.71323681\n",
      "Trained batch 1957 batch loss 5.98625803 epoch total loss 5.71337605\n",
      "Trained batch 1958 batch loss 5.9507618 epoch total loss 5.71349764\n",
      "Trained batch 1959 batch loss 5.98053598 epoch total loss 5.71363401\n",
      "Trained batch 1960 batch loss 6.26591 epoch total loss 5.71391535\n",
      "Trained batch 1961 batch loss 5.40167856 epoch total loss 5.71375608\n",
      "Trained batch 1962 batch loss 5.50202084 epoch total loss 5.71364832\n",
      "Trained batch 1963 batch loss 6.01234102 epoch total loss 5.71380043\n",
      "Trained batch 1964 batch loss 5.65045929 epoch total loss 5.71376801\n",
      "Trained batch 1965 batch loss 5.83808 epoch total loss 5.71383142\n",
      "Trained batch 1966 batch loss 6.02203369 epoch total loss 5.7139883\n",
      "Trained batch 1967 batch loss 5.94454336 epoch total loss 5.71410561\n",
      "Trained batch 1968 batch loss 5.47954893 epoch total loss 5.7139864\n",
      "Trained batch 1969 batch loss 5.42537498 epoch total loss 5.71384\n",
      "Trained batch 1970 batch loss 5.68241358 epoch total loss 5.71382427\n",
      "Trained batch 1971 batch loss 5.73608589 epoch total loss 5.71383524\n",
      "Trained batch 1972 batch loss 5.24794674 epoch total loss 5.71359921\n",
      "Trained batch 1973 batch loss 5.44308 epoch total loss 5.71346235\n",
      "Trained batch 1974 batch loss 4.92529869 epoch total loss 5.71306324\n",
      "Trained batch 1975 batch loss 5.77618313 epoch total loss 5.71309519\n",
      "Trained batch 1976 batch loss 5.7418375 epoch total loss 5.71311\n",
      "Trained batch 1977 batch loss 5.89381599 epoch total loss 5.71320152\n",
      "Trained batch 1978 batch loss 4.94564629 epoch total loss 5.7128129\n",
      "Trained batch 1979 batch loss 5.55740261 epoch total loss 5.7127347\n",
      "Trained batch 1980 batch loss 5.95767975 epoch total loss 5.71285868\n",
      "Trained batch 1981 batch loss 5.84391975 epoch total loss 5.71292448\n",
      "Trained batch 1982 batch loss 6.12574291 epoch total loss 5.71313286\n",
      "Trained batch 1983 batch loss 5.59773064 epoch total loss 5.71307468\n",
      "Trained batch 1984 batch loss 6.16276598 epoch total loss 5.71330166\n",
      "Trained batch 1985 batch loss 4.38142586 epoch total loss 5.71263075\n",
      "Trained batch 1986 batch loss 5.5724287 epoch total loss 5.71256\n",
      "Trained batch 1987 batch loss 6.1576004 epoch total loss 5.71278381\n",
      "Trained batch 1988 batch loss 5.11652422 epoch total loss 5.71248388\n",
      "Trained batch 1989 batch loss 6.0478034 epoch total loss 5.71265268\n",
      "Trained batch 1990 batch loss 5.89269543 epoch total loss 5.71274281\n",
      "Trained batch 1991 batch loss 5.2601614 epoch total loss 5.71251535\n",
      "Trained batch 1992 batch loss 5.05491638 epoch total loss 5.71218538\n",
      "Trained batch 1993 batch loss 5.48132753 epoch total loss 5.71206951\n",
      "Trained batch 1994 batch loss 5.60396862 epoch total loss 5.71201515\n",
      "Trained batch 1995 batch loss 6.52754688 epoch total loss 5.7124238\n",
      "Trained batch 1996 batch loss 6.82728815 epoch total loss 5.71298218\n",
      "Trained batch 1997 batch loss 5.91629505 epoch total loss 5.71308374\n",
      "Trained batch 1998 batch loss 6.49723673 epoch total loss 5.71347618\n",
      "Trained batch 1999 batch loss 6.16013622 epoch total loss 5.7137\n",
      "Trained batch 2000 batch loss 6.88653088 epoch total loss 5.71428633\n",
      "Trained batch 2001 batch loss 6.85641575 epoch total loss 5.7148571\n",
      "Trained batch 2002 batch loss 5.89561129 epoch total loss 5.71494722\n",
      "Trained batch 2003 batch loss 5.36028433 epoch total loss 5.71477032\n",
      "Trained batch 2004 batch loss 6.30673265 epoch total loss 5.71506548\n",
      "Trained batch 2005 batch loss 5.95173 epoch total loss 5.71518373\n",
      "Trained batch 2006 batch loss 5.95029449 epoch total loss 5.71530104\n",
      "Trained batch 2007 batch loss 5.50694847 epoch total loss 5.71519709\n",
      "Trained batch 2008 batch loss 5.97054863 epoch total loss 5.7153244\n",
      "Trained batch 2009 batch loss 6.36522055 epoch total loss 5.7156477\n",
      "Trained batch 2010 batch loss 6.34297 epoch total loss 5.71595955\n",
      "Trained batch 2011 batch loss 6.6680336 epoch total loss 5.71643305\n",
      "Trained batch 2012 batch loss 6.53756523 epoch total loss 5.71684122\n",
      "Trained batch 2013 batch loss 5.97271347 epoch total loss 5.71696806\n",
      "Trained batch 2014 batch loss 6.03370333 epoch total loss 5.71712542\n",
      "Trained batch 2015 batch loss 7.7346983 epoch total loss 5.71812677\n",
      "Trained batch 2016 batch loss 7.24176693 epoch total loss 5.71888256\n",
      "Trained batch 2017 batch loss 5.8030448 epoch total loss 5.71892452\n",
      "Trained batch 2018 batch loss 6.33645248 epoch total loss 5.71923065\n",
      "Trained batch 2019 batch loss 7.33967 epoch total loss 5.72003317\n",
      "Trained batch 2020 batch loss 7.21593618 epoch total loss 5.7207737\n",
      "Trained batch 2021 batch loss 7.28977728 epoch total loss 5.72155\n",
      "Trained batch 2022 batch loss 6.66870117 epoch total loss 5.72201872\n",
      "Trained batch 2023 batch loss 6.07261658 epoch total loss 5.72219181\n",
      "Trained batch 2024 batch loss 6.13595819 epoch total loss 5.72239637\n",
      "Trained batch 2025 batch loss 5.18589973 epoch total loss 5.72213125\n",
      "Trained batch 2026 batch loss 6.07736826 epoch total loss 5.72230625\n",
      "Trained batch 2027 batch loss 5.49585 epoch total loss 5.72219467\n",
      "Trained batch 2028 batch loss 5.87180424 epoch total loss 5.72226858\n",
      "Trained batch 2029 batch loss 6.52656507 epoch total loss 5.72266483\n",
      "Trained batch 2030 batch loss 6.16162586 epoch total loss 5.72288132\n",
      "Trained batch 2031 batch loss 6.15755701 epoch total loss 5.72309542\n",
      "Trained batch 2032 batch loss 6.51704645 epoch total loss 5.72348595\n",
      "Trained batch 2033 batch loss 6.99993944 epoch total loss 5.72411346\n",
      "Trained batch 2034 batch loss 6.19764137 epoch total loss 5.72434616\n",
      "Trained batch 2035 batch loss 6.15526056 epoch total loss 5.72455788\n",
      "Trained batch 2036 batch loss 6.27379227 epoch total loss 5.72482777\n",
      "Trained batch 2037 batch loss 5.66082382 epoch total loss 5.7247963\n",
      "Trained batch 2038 batch loss 6.71024036 epoch total loss 5.72528\n",
      "Trained batch 2039 batch loss 6.69357538 epoch total loss 5.72575474\n",
      "Trained batch 2040 batch loss 6.81478786 epoch total loss 5.72628832\n",
      "Trained batch 2041 batch loss 6.41438866 epoch total loss 5.72662497\n",
      "Trained batch 2042 batch loss 5.90774584 epoch total loss 5.72671413\n",
      "Trained batch 2043 batch loss 6.04345036 epoch total loss 5.72686911\n",
      "Trained batch 2044 batch loss 5.55184555 epoch total loss 5.72678328\n",
      "Trained batch 2045 batch loss 6.53008699 epoch total loss 5.72717619\n",
      "Trained batch 2046 batch loss 6.86040878 epoch total loss 5.72773\n",
      "Trained batch 2047 batch loss 5.7292366 epoch total loss 5.72773075\n",
      "Trained batch 2048 batch loss 5.9441638 epoch total loss 5.72783661\n",
      "Trained batch 2049 batch loss 6.67960167 epoch total loss 5.72830105\n",
      "Trained batch 2050 batch loss 6.49105692 epoch total loss 5.72867346\n",
      "Trained batch 2051 batch loss 6.49711514 epoch total loss 5.72904778\n",
      "Trained batch 2052 batch loss 6.48268604 epoch total loss 5.72941494\n",
      "Trained batch 2053 batch loss 6.54890299 epoch total loss 5.72981405\n",
      "Trained batch 2054 batch loss 6.61067533 epoch total loss 5.73024273\n",
      "Trained batch 2055 batch loss 6.448174 epoch total loss 5.73059225\n",
      "Trained batch 2056 batch loss 6.4508152 epoch total loss 5.73094273\n",
      "Trained batch 2057 batch loss 6.41796494 epoch total loss 5.73127699\n",
      "Trained batch 2058 batch loss 6.41984367 epoch total loss 5.73161125\n",
      "Trained batch 2059 batch loss 6.04481506 epoch total loss 5.73176336\n",
      "Trained batch 2060 batch loss 6.68030787 epoch total loss 5.73222399\n",
      "Trained batch 2061 batch loss 6.1614418 epoch total loss 5.73243237\n",
      "Trained batch 2062 batch loss 6.62457323 epoch total loss 5.73286533\n",
      "Trained batch 2063 batch loss 6.40980053 epoch total loss 5.7331934\n",
      "Trained batch 2064 batch loss 6.36664915 epoch total loss 5.7335\n",
      "Trained batch 2065 batch loss 6.14170551 epoch total loss 5.73369789\n",
      "Trained batch 2066 batch loss 6.25150776 epoch total loss 5.73394871\n",
      "Trained batch 2067 batch loss 5.84994411 epoch total loss 5.7340045\n",
      "Trained batch 2068 batch loss 6.48959637 epoch total loss 5.73436975\n",
      "Trained batch 2069 batch loss 5.71391487 epoch total loss 5.73435974\n",
      "Trained batch 2070 batch loss 5.32063389 epoch total loss 5.73416\n",
      "Trained batch 2071 batch loss 5.92541313 epoch total loss 5.73425245\n",
      "Trained batch 2072 batch loss 5.6127882 epoch total loss 5.73419333\n",
      "Trained batch 2073 batch loss 5.40239143 epoch total loss 5.73403358\n",
      "Trained batch 2074 batch loss 5.86321735 epoch total loss 5.73409557\n",
      "Trained batch 2075 batch loss 5.72046852 epoch total loss 5.73408937\n",
      "Trained batch 2076 batch loss 5.79973555 epoch total loss 5.73412085\n",
      "Trained batch 2077 batch loss 5.39869642 epoch total loss 5.7339592\n",
      "Trained batch 2078 batch loss 5.36398315 epoch total loss 5.73378134\n",
      "Trained batch 2079 batch loss 4.02395439 epoch total loss 5.73295927\n",
      "Trained batch 2080 batch loss 5.07647419 epoch total loss 5.7326436\n",
      "Trained batch 2081 batch loss 5.18518353 epoch total loss 5.73238039\n",
      "Trained batch 2082 batch loss 6.13304472 epoch total loss 5.73257303\n",
      "Trained batch 2083 batch loss 5.83229733 epoch total loss 5.73262072\n",
      "Trained batch 2084 batch loss 5.00591946 epoch total loss 5.73227215\n",
      "Trained batch 2085 batch loss 6.10581207 epoch total loss 5.73245096\n",
      "Trained batch 2086 batch loss 6.35207796 epoch total loss 5.73274803\n",
      "Trained batch 2087 batch loss 6.61875725 epoch total loss 5.73317289\n",
      "Trained batch 2088 batch loss 6.62068462 epoch total loss 5.73359823\n",
      "Trained batch 2089 batch loss 5.59417629 epoch total loss 5.733531\n",
      "Trained batch 2090 batch loss 6.04805088 epoch total loss 5.73368168\n",
      "Trained batch 2091 batch loss 6.1520052 epoch total loss 5.73388195\n",
      "Trained batch 2092 batch loss 5.85676193 epoch total loss 5.7339406\n",
      "Trained batch 2093 batch loss 4.04571104 epoch total loss 5.73313379\n",
      "Trained batch 2094 batch loss 5.05720139 epoch total loss 5.73281145\n",
      "Trained batch 2095 batch loss 6.21968269 epoch total loss 5.73304367\n",
      "Trained batch 2096 batch loss 5.38650608 epoch total loss 5.73287868\n",
      "Trained batch 2097 batch loss 5.26304483 epoch total loss 5.73265409\n",
      "Trained batch 2098 batch loss 5.60809803 epoch total loss 5.73259497\n",
      "Trained batch 2099 batch loss 5.33680916 epoch total loss 5.73240662\n",
      "Trained batch 2100 batch loss 6.1865921 epoch total loss 5.73262262\n",
      "Trained batch 2101 batch loss 6.00288391 epoch total loss 5.73275137\n",
      "Trained batch 2102 batch loss 4.52739954 epoch total loss 5.73217773\n",
      "Trained batch 2103 batch loss 6.33875751 epoch total loss 5.73246622\n",
      "Trained batch 2104 batch loss 5.94509602 epoch total loss 5.73256779\n",
      "Trained batch 2105 batch loss 5.48642826 epoch total loss 5.73245049\n",
      "Trained batch 2106 batch loss 6.07704639 epoch total loss 5.73261452\n",
      "Trained batch 2107 batch loss 6.61881256 epoch total loss 5.73303509\n",
      "Trained batch 2108 batch loss 6.54631519 epoch total loss 5.73342085\n",
      "Trained batch 2109 batch loss 4.08102703 epoch total loss 5.73263741\n",
      "Trained batch 2110 batch loss 5.70240927 epoch total loss 5.73262262\n",
      "Trained batch 2111 batch loss 5.96941185 epoch total loss 5.73273516\n",
      "Trained batch 2112 batch loss 5.95562601 epoch total loss 5.73284101\n",
      "Trained batch 2113 batch loss 7.75213575 epoch total loss 5.7337966\n",
      "Trained batch 2114 batch loss 5.93704796 epoch total loss 5.73389292\n",
      "Trained batch 2115 batch loss 4.81473398 epoch total loss 5.73345804\n",
      "Trained batch 2116 batch loss 5.56778622 epoch total loss 5.73337936\n",
      "Trained batch 2117 batch loss 5.67524242 epoch total loss 5.73335171\n",
      "Trained batch 2118 batch loss 5.75758266 epoch total loss 5.73336363\n",
      "Trained batch 2119 batch loss 5.43937397 epoch total loss 5.73322487\n",
      "Trained batch 2120 batch loss 4.9732976 epoch total loss 5.73286629\n",
      "Trained batch 2121 batch loss 5.31581688 epoch total loss 5.73266935\n",
      "Trained batch 2122 batch loss 4.83464575 epoch total loss 5.7322464\n",
      "Trained batch 2123 batch loss 5.02341747 epoch total loss 5.73191261\n",
      "Trained batch 2124 batch loss 5.32692051 epoch total loss 5.73172188\n",
      "Trained batch 2125 batch loss 4.36463356 epoch total loss 5.73107862\n",
      "Trained batch 2126 batch loss 4.54473257 epoch total loss 5.73052073\n",
      "Trained batch 2127 batch loss 5.7025528 epoch total loss 5.73050737\n",
      "Trained batch 2128 batch loss 5.74103212 epoch total loss 5.73051214\n",
      "Trained batch 2129 batch loss 5.52679443 epoch total loss 5.7304163\n",
      "Trained batch 2130 batch loss 5.91570091 epoch total loss 5.73050356\n",
      "Trained batch 2131 batch loss 4.75914526 epoch total loss 5.7300477\n",
      "Trained batch 2132 batch loss 5.23453903 epoch total loss 5.72981501\n",
      "Trained batch 2133 batch loss 5.60544682 epoch total loss 5.72975683\n",
      "Trained batch 2134 batch loss 6.35619926 epoch total loss 5.73005056\n",
      "Trained batch 2135 batch loss 4.68761349 epoch total loss 5.72956228\n",
      "Trained batch 2136 batch loss 5.61475563 epoch total loss 5.72950888\n",
      "Trained batch 2137 batch loss 5.99588203 epoch total loss 5.72963333\n",
      "Trained batch 2138 batch loss 5.66497517 epoch total loss 5.72960329\n",
      "Trained batch 2139 batch loss 5.628829 epoch total loss 5.72955608\n",
      "Trained batch 2140 batch loss 6.36519909 epoch total loss 5.72985315\n",
      "Trained batch 2141 batch loss 4.38271332 epoch total loss 5.72922421\n",
      "Trained batch 2142 batch loss 6.08133411 epoch total loss 5.72938824\n",
      "Trained batch 2143 batch loss 5.13368702 epoch total loss 5.72911024\n",
      "Trained batch 2144 batch loss 5.53722334 epoch total loss 5.7290206\n",
      "Trained batch 2145 batch loss 5.41359234 epoch total loss 5.72887373\n",
      "Trained batch 2146 batch loss 6.07626963 epoch total loss 5.72903585\n",
      "Trained batch 2147 batch loss 5.3249712 epoch total loss 5.7288475\n",
      "Trained batch 2148 batch loss 5.91323519 epoch total loss 5.72893333\n",
      "Trained batch 2149 batch loss 5.95712185 epoch total loss 5.72903967\n",
      "Trained batch 2150 batch loss 5.90936232 epoch total loss 5.72912359\n",
      "Trained batch 2151 batch loss 6.40836048 epoch total loss 5.72943926\n",
      "Trained batch 2152 batch loss 5.03441143 epoch total loss 5.72911596\n",
      "Trained batch 2153 batch loss 5.36937141 epoch total loss 5.72894859\n",
      "Trained batch 2154 batch loss 5.34082937 epoch total loss 5.72876883\n",
      "Trained batch 2155 batch loss 5.62257385 epoch total loss 5.72871971\n",
      "Trained batch 2156 batch loss 5.17764139 epoch total loss 5.72846413\n",
      "Trained batch 2157 batch loss 6.13996601 epoch total loss 5.72865438\n",
      "Trained batch 2158 batch loss 6.52541256 epoch total loss 5.72902393\n",
      "Trained batch 2159 batch loss 6.30679512 epoch total loss 5.72929144\n",
      "Trained batch 2160 batch loss 4.32206774 epoch total loss 5.72864\n",
      "Trained batch 2161 batch loss 6.55382204 epoch total loss 5.72902155\n",
      "Trained batch 2162 batch loss 5.67790747 epoch total loss 5.72899818\n",
      "Trained batch 2163 batch loss 4.35676861 epoch total loss 5.72836351\n",
      "Trained batch 2164 batch loss 5.61610794 epoch total loss 5.72831154\n",
      "Trained batch 2165 batch loss 5.24756 epoch total loss 5.72809\n",
      "Trained batch 2166 batch loss 6.10971975 epoch total loss 5.72826576\n",
      "Trained batch 2167 batch loss 6.09809446 epoch total loss 5.72843647\n",
      "Trained batch 2168 batch loss 5.70035362 epoch total loss 5.72842312\n",
      "Trained batch 2169 batch loss 6.35020542 epoch total loss 5.72871\n",
      "Trained batch 2170 batch loss 6.24097252 epoch total loss 5.72894621\n",
      "Trained batch 2171 batch loss 5.92329597 epoch total loss 5.72903585\n",
      "Trained batch 2172 batch loss 6.40649462 epoch total loss 5.72934723\n",
      "Trained batch 2173 batch loss 5.63538933 epoch total loss 5.72930431\n",
      "Trained batch 2174 batch loss 5.73422 epoch total loss 5.7293067\n",
      "Trained batch 2175 batch loss 5.30500603 epoch total loss 5.72911167\n",
      "Trained batch 2176 batch loss 6.10696888 epoch total loss 5.72928524\n",
      "Trained batch 2177 batch loss 6.34117031 epoch total loss 5.7295661\n",
      "Trained batch 2178 batch loss 6.32593822 epoch total loss 5.72984028\n",
      "Trained batch 2179 batch loss 5.66238499 epoch total loss 5.72980881\n",
      "Trained batch 2180 batch loss 5.87132072 epoch total loss 5.72987366\n",
      "Trained batch 2181 batch loss 5.1761 epoch total loss 5.72962\n",
      "Trained batch 2182 batch loss 5.8285532 epoch total loss 5.7296648\n",
      "Trained batch 2183 batch loss 5.67459679 epoch total loss 5.72964\n",
      "Trained batch 2184 batch loss 4.56068563 epoch total loss 5.72910452\n",
      "Trained batch 2185 batch loss 4.97647953 epoch total loss 5.72876024\n",
      "Trained batch 2186 batch loss 4.88129807 epoch total loss 5.7283721\n",
      "Trained batch 2187 batch loss 5.12198353 epoch total loss 5.72809505\n",
      "Trained batch 2188 batch loss 4.59204674 epoch total loss 5.72757578\n",
      "Trained batch 2189 batch loss 4.91356754 epoch total loss 5.72720337\n",
      "Trained batch 2190 batch loss 5.70596886 epoch total loss 5.72719383\n",
      "Trained batch 2191 batch loss 5.46394539 epoch total loss 5.72707367\n",
      "Trained batch 2192 batch loss 5.52150965 epoch total loss 5.72697973\n",
      "Trained batch 2193 batch loss 6.02833 epoch total loss 5.72711754\n",
      "Trained batch 2194 batch loss 6.10215 epoch total loss 5.72728825\n",
      "Trained batch 2195 batch loss 5.8880806 epoch total loss 5.72736168\n",
      "Trained batch 2196 batch loss 5.29445839 epoch total loss 5.72716475\n",
      "Trained batch 2197 batch loss 5.73371315 epoch total loss 5.72716761\n",
      "Trained batch 2198 batch loss 6.00958538 epoch total loss 5.72729588\n",
      "Trained batch 2199 batch loss 5.58744478 epoch total loss 5.72723246\n",
      "Trained batch 2200 batch loss 5.94919348 epoch total loss 5.72733355\n",
      "Trained batch 2201 batch loss 6.50750875 epoch total loss 5.72768831\n",
      "Trained batch 2202 batch loss 5.51569271 epoch total loss 5.72759199\n",
      "Trained batch 2203 batch loss 5.77657795 epoch total loss 5.72761393\n",
      "Trained batch 2204 batch loss 6.64185143 epoch total loss 5.72802877\n",
      "Trained batch 2205 batch loss 6.4258008 epoch total loss 5.72834492\n",
      "Trained batch 2206 batch loss 5.33558178 epoch total loss 5.72816706\n",
      "Trained batch 2207 batch loss 5.75704336 epoch total loss 5.72818041\n",
      "Trained batch 2208 batch loss 6.97079134 epoch total loss 5.72874308\n",
      "Trained batch 2209 batch loss 6.13041544 epoch total loss 5.72892523\n",
      "Trained batch 2210 batch loss 4.27161837 epoch total loss 5.72826529\n",
      "Trained batch 2211 batch loss 4.51817322 epoch total loss 5.72771835\n",
      "Trained batch 2212 batch loss 5.21142769 epoch total loss 5.72748518\n",
      "Trained batch 2213 batch loss 6.42915154 epoch total loss 5.72780228\n",
      "Trained batch 2214 batch loss 5.56636524 epoch total loss 5.72772932\n",
      "Trained batch 2215 batch loss 5.71692562 epoch total loss 5.72772408\n",
      "Trained batch 2216 batch loss 5.07720184 epoch total loss 5.72743082\n",
      "Trained batch 2217 batch loss 5.80034 epoch total loss 5.72746372\n",
      "Trained batch 2218 batch loss 5.55182171 epoch total loss 5.72738457\n",
      "Trained batch 2219 batch loss 5.39662 epoch total loss 5.72723532\n",
      "Trained batch 2220 batch loss 6.07831573 epoch total loss 5.72739363\n",
      "Trained batch 2221 batch loss 6.58750534 epoch total loss 5.72778082\n",
      "Trained batch 2222 batch loss 6.0009222 epoch total loss 5.72790384\n",
      "Trained batch 2223 batch loss 5.97829342 epoch total loss 5.72801638\n",
      "Trained batch 2224 batch loss 5.46191406 epoch total loss 5.72789717\n",
      "Trained batch 2225 batch loss 5.91834927 epoch total loss 5.72798252\n",
      "Trained batch 2226 batch loss 6.31038237 epoch total loss 5.72824383\n",
      "Trained batch 2227 batch loss 5.38201618 epoch total loss 5.72808838\n",
      "Trained batch 2228 batch loss 5.40478039 epoch total loss 5.72794342\n",
      "Trained batch 2229 batch loss 5.44712 epoch total loss 5.72781706\n",
      "Trained batch 2230 batch loss 5.75855255 epoch total loss 5.72783136\n",
      "Trained batch 2231 batch loss 5.70963 epoch total loss 5.72782326\n",
      "Trained batch 2232 batch loss 5.11154079 epoch total loss 5.72754717\n",
      "Trained batch 2233 batch loss 5.55854416 epoch total loss 5.72747135\n",
      "Trained batch 2234 batch loss 5.66275454 epoch total loss 5.72744226\n",
      "Trained batch 2235 batch loss 4.84746218 epoch total loss 5.72704887\n",
      "Trained batch 2236 batch loss 4.61233854 epoch total loss 5.72655\n",
      "Trained batch 2237 batch loss 5.36164761 epoch total loss 5.72638702\n",
      "Trained batch 2238 batch loss 4.76294422 epoch total loss 5.72595644\n",
      "Trained batch 2239 batch loss 5.10267496 epoch total loss 5.72567797\n",
      "Trained batch 2240 batch loss 4.92255688 epoch total loss 5.72531939\n",
      "Trained batch 2241 batch loss 4.18265629 epoch total loss 5.72463131\n",
      "Trained batch 2242 batch loss 4.53902864 epoch total loss 5.7241025\n",
      "Trained batch 2243 batch loss 4.45816898 epoch total loss 5.72353792\n",
      "Trained batch 2244 batch loss 5.36258364 epoch total loss 5.72337675\n",
      "Trained batch 2245 batch loss 6.75813866 epoch total loss 5.72383785\n",
      "Trained batch 2246 batch loss 6.36328173 epoch total loss 5.72412252\n",
      "Trained batch 2247 batch loss 6.36794853 epoch total loss 5.7244091\n",
      "Trained batch 2248 batch loss 5.23989201 epoch total loss 5.72419357\n",
      "Trained batch 2249 batch loss 5.86935425 epoch total loss 5.72425795\n",
      "Trained batch 2250 batch loss 5.93339348 epoch total loss 5.72435093\n",
      "Trained batch 2251 batch loss 6.39436913 epoch total loss 5.72464895\n",
      "Trained batch 2252 batch loss 6.37202454 epoch total loss 5.72493649\n",
      "Trained batch 2253 batch loss 6.12950611 epoch total loss 5.72511625\n",
      "Trained batch 2254 batch loss 6.48451805 epoch total loss 5.7254529\n",
      "Trained batch 2255 batch loss 5.67753077 epoch total loss 5.72543192\n",
      "Trained batch 2256 batch loss 6.38165569 epoch total loss 5.72572279\n",
      "Trained batch 2257 batch loss 5.39147139 epoch total loss 5.72557449\n",
      "Trained batch 2258 batch loss 5.99127531 epoch total loss 5.72569227\n",
      "Trained batch 2259 batch loss 5.29209232 epoch total loss 5.7255\n",
      "Trained batch 2260 batch loss 5.16624928 epoch total loss 5.72525263\n",
      "Trained batch 2261 batch loss 5.36058044 epoch total loss 5.72509146\n",
      "Trained batch 2262 batch loss 5.47519112 epoch total loss 5.72498131\n",
      "Trained batch 2263 batch loss 5.65604591 epoch total loss 5.72495079\n",
      "Trained batch 2264 batch loss 5.36503506 epoch total loss 5.724792\n",
      "Trained batch 2265 batch loss 5.18933105 epoch total loss 5.72455549\n",
      "Trained batch 2266 batch loss 5.41749287 epoch total loss 5.72442\n",
      "Trained batch 2267 batch loss 4.44538879 epoch total loss 5.72385597\n",
      "Trained batch 2268 batch loss 6.02163792 epoch total loss 5.7239871\n",
      "Trained batch 2269 batch loss 5.55105686 epoch total loss 5.72391081\n",
      "Trained batch 2270 batch loss 5.08967876 epoch total loss 5.72363138\n",
      "Trained batch 2271 batch loss 5.05616474 epoch total loss 5.72333765\n",
      "Trained batch 2272 batch loss 5.60140848 epoch total loss 5.72328424\n",
      "Trained batch 2273 batch loss 5.16446877 epoch total loss 5.7230382\n",
      "Trained batch 2274 batch loss 6.3142643 epoch total loss 5.72329807\n",
      "Trained batch 2275 batch loss 5.97234917 epoch total loss 5.72340775\n",
      "Trained batch 2276 batch loss 6.03095722 epoch total loss 5.72354317\n",
      "Trained batch 2277 batch loss 6.01117897 epoch total loss 5.72366905\n",
      "Trained batch 2278 batch loss 5.78826761 epoch total loss 5.72369766\n",
      "Trained batch 2279 batch loss 5.62309599 epoch total loss 5.72365332\n",
      "Trained batch 2280 batch loss 6.58643532 epoch total loss 5.72403193\n",
      "Trained batch 2281 batch loss 6.23023605 epoch total loss 5.72425413\n",
      "Trained batch 2282 batch loss 4.93676472 epoch total loss 5.7239089\n",
      "Trained batch 2283 batch loss 4.4759407 epoch total loss 5.72336197\n",
      "Trained batch 2284 batch loss 4.47145653 epoch total loss 5.72281408\n",
      "Trained batch 2285 batch loss 5.51522064 epoch total loss 5.72272348\n",
      "Trained batch 2286 batch loss 5.21520805 epoch total loss 5.72250128\n",
      "Trained batch 2287 batch loss 5.79463 epoch total loss 5.72253275\n",
      "Trained batch 2288 batch loss 5.61156416 epoch total loss 5.72248411\n",
      "Trained batch 2289 batch loss 5.80625582 epoch total loss 5.72252083\n",
      "Trained batch 2290 batch loss 6.16364336 epoch total loss 5.72271395\n",
      "Trained batch 2291 batch loss 6.32954693 epoch total loss 5.72297859\n",
      "Trained batch 2292 batch loss 6.70104122 epoch total loss 5.72340536\n",
      "Trained batch 2293 batch loss 5.96792078 epoch total loss 5.7235117\n",
      "Trained batch 2294 batch loss 6.3887434 epoch total loss 5.72380161\n",
      "Trained batch 2295 batch loss 7.19403458 epoch total loss 5.72444248\n",
      "Trained batch 2296 batch loss 5.29263496 epoch total loss 5.72425461\n",
      "Trained batch 2297 batch loss 5.6781764 epoch total loss 5.72423458\n",
      "Trained batch 2298 batch loss 5.22218752 epoch total loss 5.72401619\n",
      "Trained batch 2299 batch loss 6.31535816 epoch total loss 5.7242732\n",
      "Trained batch 2300 batch loss 5.80783081 epoch total loss 5.72430944\n",
      "Trained batch 2301 batch loss 5.85469 epoch total loss 5.72436619\n",
      "Trained batch 2302 batch loss 5.36466122 epoch total loss 5.72421\n",
      "Trained batch 2303 batch loss 5.81125259 epoch total loss 5.72424746\n",
      "Trained batch 2304 batch loss 5.33698416 epoch total loss 5.72407961\n",
      "Trained batch 2305 batch loss 6.50384378 epoch total loss 5.72441769\n",
      "Trained batch 2306 batch loss 5.71230745 epoch total loss 5.72441244\n",
      "Trained batch 2307 batch loss 5.8797121 epoch total loss 5.72447968\n",
      "Trained batch 2308 batch loss 5.90486145 epoch total loss 5.72455835\n",
      "Trained batch 2309 batch loss 5.69505882 epoch total loss 5.72454548\n",
      "Trained batch 2310 batch loss 5.34305334 epoch total loss 5.72438\n",
      "Trained batch 2311 batch loss 6.24489689 epoch total loss 5.72460556\n",
      "Trained batch 2312 batch loss 6.21180534 epoch total loss 5.72481632\n",
      "Trained batch 2313 batch loss 4.92440176 epoch total loss 5.72447062\n",
      "Trained batch 2314 batch loss 5.89093161 epoch total loss 5.72454214\n",
      "Trained batch 2315 batch loss 5.88195229 epoch total loss 5.72461033\n",
      "Trained batch 2316 batch loss 5.8994627 epoch total loss 5.72468567\n",
      "Trained batch 2317 batch loss 5.67588711 epoch total loss 5.72466469\n",
      "Trained batch 2318 batch loss 5.59694624 epoch total loss 5.72460938\n",
      "Trained batch 2319 batch loss 5.94112253 epoch total loss 5.72470284\n",
      "Trained batch 2320 batch loss 6.00574207 epoch total loss 5.72482395\n",
      "Trained batch 2321 batch loss 4.75109673 epoch total loss 5.72440434\n",
      "Trained batch 2322 batch loss 5.37285852 epoch total loss 5.72425318\n",
      "Trained batch 2323 batch loss 5.63397646 epoch total loss 5.72421408\n",
      "Trained batch 2324 batch loss 7.59497356 epoch total loss 5.72501898\n",
      "Trained batch 2325 batch loss 7.57318878 epoch total loss 5.72581387\n",
      "Trained batch 2326 batch loss 6.50114489 epoch total loss 5.72614717\n",
      "Trained batch 2327 batch loss 6.282094 epoch total loss 5.72638607\n",
      "Trained batch 2328 batch loss 5.79691744 epoch total loss 5.72641659\n",
      "Trained batch 2329 batch loss 5.75023842 epoch total loss 5.7264266\n",
      "Trained batch 2330 batch loss 6.36340714 epoch total loss 5.7267\n",
      "Trained batch 2331 batch loss 5.38737154 epoch total loss 5.72655439\n",
      "Trained batch 2332 batch loss 5.65947342 epoch total loss 5.72652578\n",
      "Trained batch 2333 batch loss 5.54889393 epoch total loss 5.72644949\n",
      "Trained batch 2334 batch loss 5.85453749 epoch total loss 5.72650433\n",
      "Trained batch 2335 batch loss 5.61917639 epoch total loss 5.72645855\n",
      "Trained batch 2336 batch loss 5.12765169 epoch total loss 5.72620201\n",
      "Trained batch 2337 batch loss 4.65269279 epoch total loss 5.72574282\n",
      "Trained batch 2338 batch loss 5.94829941 epoch total loss 5.72583771\n",
      "Trained batch 2339 batch loss 6.11725092 epoch total loss 5.72600508\n",
      "Trained batch 2340 batch loss 6.0688858 epoch total loss 5.72615194\n",
      "Trained batch 2341 batch loss 5.89609528 epoch total loss 5.72622442\n",
      "Trained batch 2342 batch loss 6.2169137 epoch total loss 5.72643423\n",
      "Trained batch 2343 batch loss 6.50347948 epoch total loss 5.72676611\n",
      "Trained batch 2344 batch loss 6.46921206 epoch total loss 5.72708225\n",
      "Trained batch 2345 batch loss 6.30580759 epoch total loss 5.72732925\n",
      "Trained batch 2346 batch loss 6.51966763 epoch total loss 5.72766685\n",
      "Trained batch 2347 batch loss 5.95597458 epoch total loss 5.72776413\n",
      "Trained batch 2348 batch loss 5.30970287 epoch total loss 5.72758627\n",
      "Trained batch 2349 batch loss 4.84416771 epoch total loss 5.72720957\n",
      "Trained batch 2350 batch loss 5.07793522 epoch total loss 5.72693348\n",
      "Trained batch 2351 batch loss 4.71804142 epoch total loss 5.72650433\n",
      "Trained batch 2352 batch loss 5.12481403 epoch total loss 5.72624874\n",
      "Trained batch 2353 batch loss 5.72574329 epoch total loss 5.72624826\n",
      "Trained batch 2354 batch loss 6.14650106 epoch total loss 5.72642708\n",
      "Trained batch 2355 batch loss 4.57825851 epoch total loss 5.72593927\n",
      "Trained batch 2356 batch loss 5.14438486 epoch total loss 5.72569227\n",
      "Trained batch 2357 batch loss 4.94612598 epoch total loss 5.72536182\n",
      "Trained batch 2358 batch loss 5.32890129 epoch total loss 5.7251935\n",
      "Trained batch 2359 batch loss 4.43914938 epoch total loss 5.72464848\n",
      "Trained batch 2360 batch loss 4.61042595 epoch total loss 5.72417641\n",
      "Trained batch 2361 batch loss 4.68310165 epoch total loss 5.72373533\n",
      "Trained batch 2362 batch loss 5.93461418 epoch total loss 5.7238245\n",
      "Trained batch 2363 batch loss 4.53868532 epoch total loss 5.72332335\n",
      "Trained batch 2364 batch loss 5.33404589 epoch total loss 5.72315836\n",
      "Trained batch 2365 batch loss 5.49747705 epoch total loss 5.72306299\n",
      "Trained batch 2366 batch loss 4.72295284 epoch total loss 5.72264\n",
      "Trained batch 2367 batch loss 4.89061069 epoch total loss 5.72228861\n",
      "Trained batch 2368 batch loss 4.50277424 epoch total loss 5.72177362\n",
      "Trained batch 2369 batch loss 4.19213104 epoch total loss 5.72112799\n",
      "Trained batch 2370 batch loss 4.52050686 epoch total loss 5.72062159\n",
      "Trained batch 2371 batch loss 5.70253086 epoch total loss 5.72061396\n",
      "Trained batch 2372 batch loss 6.51468897 epoch total loss 5.7209487\n",
      "Trained batch 2373 batch loss 6.72851 epoch total loss 5.72137308\n",
      "Trained batch 2374 batch loss 5.89832926 epoch total loss 5.72144747\n",
      "Trained batch 2375 batch loss 6.56621933 epoch total loss 5.72180367\n",
      "Trained batch 2376 batch loss 5.68655109 epoch total loss 5.72178841\n",
      "Trained batch 2377 batch loss 5.93287373 epoch total loss 5.7218771\n",
      "Trained batch 2378 batch loss 5.40771532 epoch total loss 5.72174549\n",
      "Trained batch 2379 batch loss 6.61227846 epoch total loss 5.72212\n",
      "Trained batch 2380 batch loss 6.06423 epoch total loss 5.72226334\n",
      "Trained batch 2381 batch loss 6.64360237 epoch total loss 5.72265053\n",
      "Trained batch 2382 batch loss 5.11617851 epoch total loss 5.7223959\n",
      "Trained batch 2383 batch loss 5.06444311 epoch total loss 5.72212\n",
      "Trained batch 2384 batch loss 5.22000885 epoch total loss 5.72190905\n",
      "Trained batch 2385 batch loss 5.20457554 epoch total loss 5.72169209\n",
      "Trained batch 2386 batch loss 4.17441845 epoch total loss 5.72104359\n",
      "Trained batch 2387 batch loss 6.32514095 epoch total loss 5.72129679\n",
      "Trained batch 2388 batch loss 6.63047886 epoch total loss 5.72167778\n",
      "Trained batch 2389 batch loss 6.06700754 epoch total loss 5.72182226\n",
      "Trained batch 2390 batch loss 6.23685503 epoch total loss 5.72203827\n",
      "Trained batch 2391 batch loss 6.24721 epoch total loss 5.72225761\n",
      "Trained batch 2392 batch loss 7.38756323 epoch total loss 5.7229538\n",
      "Trained batch 2393 batch loss 6.2996068 epoch total loss 5.72319508\n",
      "Trained batch 2394 batch loss 6.2337923 epoch total loss 5.72340822\n",
      "Trained batch 2395 batch loss 5.68919611 epoch total loss 5.72339392\n",
      "Trained batch 2396 batch loss 6.07415867 epoch total loss 5.72354031\n",
      "Trained batch 2397 batch loss 5.05513334 epoch total loss 5.72326136\n",
      "Trained batch 2398 batch loss 6.07729959 epoch total loss 5.7234087\n",
      "Trained batch 2399 batch loss 5.77079439 epoch total loss 5.72342825\n",
      "Trained batch 2400 batch loss 6.02783585 epoch total loss 5.72355556\n",
      "Trained batch 2401 batch loss 6.18167782 epoch total loss 5.7237463\n",
      "Trained batch 2402 batch loss 6.24448 epoch total loss 5.72396278\n",
      "Trained batch 2403 batch loss 5.73840618 epoch total loss 5.72396898\n",
      "Trained batch 2404 batch loss 5.86322975 epoch total loss 5.72402668\n",
      "Trained batch 2405 batch loss 5.80930328 epoch total loss 5.72406244\n",
      "Trained batch 2406 batch loss 5.72561932 epoch total loss 5.72406292\n",
      "Trained batch 2407 batch loss 5.69842529 epoch total loss 5.72405243\n",
      "Trained batch 2408 batch loss 6.03457069 epoch total loss 5.72418118\n",
      "Trained batch 2409 batch loss 6.29857159 epoch total loss 5.72441959\n",
      "Trained batch 2410 batch loss 6.56145859 epoch total loss 5.72476721\n",
      "Trained batch 2411 batch loss 4.31450176 epoch total loss 5.72418213\n",
      "Trained batch 2412 batch loss 4.27102 epoch total loss 5.72358\n",
      "Trained batch 2413 batch loss 4.27395725 epoch total loss 5.72297907\n",
      "Trained batch 2414 batch loss 3.92873812 epoch total loss 5.72223616\n",
      "Trained batch 2415 batch loss 4.36765957 epoch total loss 5.72167492\n",
      "Trained batch 2416 batch loss 4.61197329 epoch total loss 5.72121572\n",
      "Trained batch 2417 batch loss 4.5031414 epoch total loss 5.72071171\n",
      "Trained batch 2418 batch loss 6.10322762 epoch total loss 5.72087\n",
      "Trained batch 2419 batch loss 4.86603355 epoch total loss 5.72051668\n",
      "Trained batch 2420 batch loss 5.89140224 epoch total loss 5.72058725\n",
      "Trained batch 2421 batch loss 6.11957645 epoch total loss 5.72075176\n",
      "Trained batch 2422 batch loss 6.02356482 epoch total loss 5.72087669\n",
      "Trained batch 2423 batch loss 6.43674088 epoch total loss 5.72117233\n",
      "Trained batch 2424 batch loss 5.24036264 epoch total loss 5.72097397\n",
      "Trained batch 2425 batch loss 5.99339581 epoch total loss 5.72108603\n",
      "Trained batch 2426 batch loss 6.00962543 epoch total loss 5.72120523\n",
      "Trained batch 2427 batch loss 6.15196896 epoch total loss 5.72138262\n",
      "Trained batch 2428 batch loss 6.33031797 epoch total loss 5.72163343\n",
      "Trained batch 2429 batch loss 6.39470387 epoch total loss 5.72191048\n",
      "Trained batch 2430 batch loss 6.45776272 epoch total loss 5.72221327\n",
      "Trained batch 2431 batch loss 5.4154892 epoch total loss 5.72208691\n",
      "Trained batch 2432 batch loss 5.32962036 epoch total loss 5.72192574\n",
      "Trained batch 2433 batch loss 6.34301472 epoch total loss 5.72218084\n",
      "Trained batch 2434 batch loss 6.04208565 epoch total loss 5.72231245\n",
      "Trained batch 2435 batch loss 5.65779 epoch total loss 5.72228622\n",
      "Trained batch 2436 batch loss 3.82162619 epoch total loss 5.72150564\n",
      "Trained batch 2437 batch loss 5.60419941 epoch total loss 5.72145748\n",
      "Trained batch 2438 batch loss 5.80851364 epoch total loss 5.72149324\n",
      "Trained batch 2439 batch loss 5.32058334 epoch total loss 5.72132874\n",
      "Trained batch 2440 batch loss 5.82003975 epoch total loss 5.72136974\n",
      "Trained batch 2441 batch loss 5.88018036 epoch total loss 5.72143459\n",
      "Trained batch 2442 batch loss 5.83563614 epoch total loss 5.72148132\n",
      "Trained batch 2443 batch loss 5.25944 epoch total loss 5.7212925\n",
      "Trained batch 2444 batch loss 4.98715973 epoch total loss 5.72099209\n",
      "Trained batch 2445 batch loss 5.71084404 epoch total loss 5.7209878\n",
      "Trained batch 2446 batch loss 5.57768822 epoch total loss 5.72092962\n",
      "Trained batch 2447 batch loss 4.08939409 epoch total loss 5.720263\n",
      "Trained batch 2448 batch loss 5.77216864 epoch total loss 5.72028446\n",
      "Trained batch 2449 batch loss 6.12211132 epoch total loss 5.72044849\n",
      "Trained batch 2450 batch loss 5.92190266 epoch total loss 5.72053051\n",
      "Trained batch 2451 batch loss 6.28204536 epoch total loss 5.72076\n",
      "Trained batch 2452 batch loss 5.93600368 epoch total loss 5.72084713\n",
      "Trained batch 2453 batch loss 5.96841049 epoch total loss 5.72094822\n",
      "Trained batch 2454 batch loss 6.04427099 epoch total loss 5.72108\n",
      "Trained batch 2455 batch loss 5.59980488 epoch total loss 5.72103071\n",
      "Trained batch 2456 batch loss 5.78920269 epoch total loss 5.72105837\n",
      "Trained batch 2457 batch loss 6.08040714 epoch total loss 5.72120428\n",
      "Trained batch 2458 batch loss 5.36664295 epoch total loss 5.72106\n",
      "Trained batch 2459 batch loss 4.77013826 epoch total loss 5.72067356\n",
      "Trained batch 2460 batch loss 4.13497639 epoch total loss 5.72002888\n",
      "Trained batch 2461 batch loss 5.06395817 epoch total loss 5.71976185\n",
      "Trained batch 2462 batch loss 5.11747 epoch total loss 5.71951723\n",
      "Trained batch 2463 batch loss 5.31728649 epoch total loss 5.71935368\n",
      "Trained batch 2464 batch loss 6.22602034 epoch total loss 5.71955919\n",
      "Trained batch 2465 batch loss 5.77226162 epoch total loss 5.71958065\n",
      "Trained batch 2466 batch loss 5.25108957 epoch total loss 5.71939087\n",
      "Trained batch 2467 batch loss 5.11430073 epoch total loss 5.7191453\n",
      "Trained batch 2468 batch loss 6.05216026 epoch total loss 5.71928024\n",
      "Trained batch 2469 batch loss 4.83183479 epoch total loss 5.71892071\n",
      "Trained batch 2470 batch loss 4.81940746 epoch total loss 5.71855688\n",
      "Trained batch 2471 batch loss 3.8211813 epoch total loss 5.7177887\n",
      "Trained batch 2472 batch loss 5.03928518 epoch total loss 5.71751451\n",
      "Trained batch 2473 batch loss 6.03240252 epoch total loss 5.71764135\n",
      "Trained batch 2474 batch loss 4.25927162 epoch total loss 5.71705198\n",
      "Trained batch 2475 batch loss 5.0706892 epoch total loss 5.71679068\n",
      "Trained batch 2476 batch loss 6.67953253 epoch total loss 5.7171793\n",
      "Trained batch 2477 batch loss 6.67043781 epoch total loss 5.71756458\n",
      "Trained batch 2478 batch loss 5.67790699 epoch total loss 5.71754837\n",
      "Trained batch 2479 batch loss 6.37368155 epoch total loss 5.71781301\n",
      "Trained batch 2480 batch loss 4.95968437 epoch total loss 5.71750784\n",
      "Trained batch 2481 batch loss 5.92540026 epoch total loss 5.71759176\n",
      "Trained batch 2482 batch loss 5.42002773 epoch total loss 5.7174716\n",
      "Trained batch 2483 batch loss 5.25579 epoch total loss 5.71728563\n",
      "Trained batch 2484 batch loss 5.21850681 epoch total loss 5.71708488\n",
      "Trained batch 2485 batch loss 4.3758688 epoch total loss 5.71654558\n",
      "Trained batch 2486 batch loss 5.2907238 epoch total loss 5.7163744\n",
      "Trained batch 2487 batch loss 5.37640429 epoch total loss 5.71623755\n",
      "Trained batch 2488 batch loss 5.8116436 epoch total loss 5.71627569\n",
      "Trained batch 2489 batch loss 4.40440941 epoch total loss 5.71574831\n",
      "Trained batch 2490 batch loss 5.32179642 epoch total loss 5.71559048\n",
      "Trained batch 2491 batch loss 4.25145054 epoch total loss 5.71500254\n",
      "Trained batch 2492 batch loss 5.35725975 epoch total loss 5.71485901\n",
      "Trained batch 2493 batch loss 4.5113678 epoch total loss 5.71437645\n",
      "Trained batch 2494 batch loss 5.67083549 epoch total loss 5.71435881\n",
      "Trained batch 2495 batch loss 5.96268082 epoch total loss 5.71445847\n",
      "Trained batch 2496 batch loss 5.58066845 epoch total loss 5.71440506\n",
      "Trained batch 2497 batch loss 5.91869926 epoch total loss 5.71448708\n",
      "Trained batch 2498 batch loss 5.72952366 epoch total loss 5.71449327\n",
      "Trained batch 2499 batch loss 5.72570324 epoch total loss 5.71449757\n",
      "Trained batch 2500 batch loss 5.16133595 epoch total loss 5.71427631\n",
      "Trained batch 2501 batch loss 5.98445129 epoch total loss 5.71438408\n",
      "Trained batch 2502 batch loss 5.48928738 epoch total loss 5.71429396\n",
      "Trained batch 2503 batch loss 5.66385221 epoch total loss 5.71427393\n",
      "Trained batch 2504 batch loss 5.56456804 epoch total loss 5.71421432\n",
      "Trained batch 2505 batch loss 4.5938983 epoch total loss 5.71376705\n",
      "Trained batch 2506 batch loss 4.84832859 epoch total loss 5.71342182\n",
      "Trained batch 2507 batch loss 3.61340451 epoch total loss 5.71258402\n",
      "Trained batch 2508 batch loss 5.49955082 epoch total loss 5.71249914\n",
      "Trained batch 2509 batch loss 5.42442608 epoch total loss 5.7123847\n",
      "Trained batch 2510 batch loss 5.46939182 epoch total loss 5.7122879\n",
      "Trained batch 2511 batch loss 5.18653965 epoch total loss 5.71207857\n",
      "Trained batch 2512 batch loss 5.52669954 epoch total loss 5.71200466\n",
      "Trained batch 2513 batch loss 4.87863111 epoch total loss 5.71167326\n",
      "Trained batch 2514 batch loss 5.23154306 epoch total loss 5.71148205\n",
      "Trained batch 2515 batch loss 5.17520428 epoch total loss 5.7112689\n",
      "Trained batch 2516 batch loss 3.89331198 epoch total loss 5.71054602\n",
      "Trained batch 2517 batch loss 4.20593643 epoch total loss 5.70994854\n",
      "Trained batch 2518 batch loss 5.75624847 epoch total loss 5.70996666\n",
      "Trained batch 2519 batch loss 5.15780878 epoch total loss 5.70974779\n",
      "Trained batch 2520 batch loss 5.56917667 epoch total loss 5.709692\n",
      "Trained batch 2521 batch loss 5.04108334 epoch total loss 5.70942688\n",
      "Trained batch 2522 batch loss 5.294837 epoch total loss 5.70926237\n",
      "Trained batch 2523 batch loss 5.27157211 epoch total loss 5.7090888\n",
      "Trained batch 2524 batch loss 5.31627703 epoch total loss 5.70893335\n",
      "Trained batch 2525 batch loss 5.56775093 epoch total loss 5.70887709\n",
      "Trained batch 2526 batch loss 4.46871948 epoch total loss 5.70838642\n",
      "Trained batch 2527 batch loss 5.2931509 epoch total loss 5.70822191\n",
      "Trained batch 2528 batch loss 4.60309267 epoch total loss 5.70778513\n",
      "Trained batch 2529 batch loss 4.6444273 epoch total loss 5.70736456\n",
      "Trained batch 2530 batch loss 4.56519 epoch total loss 5.70691299\n",
      "Trained batch 2531 batch loss 4.13176346 epoch total loss 5.70629072\n",
      "Trained batch 2532 batch loss 4.3312254 epoch total loss 5.7057476\n",
      "Trained batch 2533 batch loss 4.8218956 epoch total loss 5.70539904\n",
      "Trained batch 2534 batch loss 5.68901062 epoch total loss 5.70539236\n",
      "Trained batch 2535 batch loss 5.80305529 epoch total loss 5.70543098\n",
      "Trained batch 2536 batch loss 5.64866638 epoch total loss 5.70540857\n",
      "Trained batch 2537 batch loss 5.18902063 epoch total loss 5.70520496\n",
      "Trained batch 2538 batch loss 5.13307714 epoch total loss 5.70497942\n",
      "Trained batch 2539 batch loss 4.86418724 epoch total loss 5.70464849\n",
      "Trained batch 2540 batch loss 5.87797642 epoch total loss 5.70471668\n",
      "Trained batch 2541 batch loss 5.95633936 epoch total loss 5.70481586\n",
      "Trained batch 2542 batch loss 5.65974 epoch total loss 5.70479822\n",
      "Trained batch 2543 batch loss 5.79617786 epoch total loss 5.70483398\n",
      "Trained batch 2544 batch loss 6.03823137 epoch total loss 5.70496511\n",
      "Trained batch 2545 batch loss 5.5200758 epoch total loss 5.70489264\n",
      "Trained batch 2546 batch loss 6.0402441 epoch total loss 5.70502424\n",
      "Trained batch 2547 batch loss 5.54841709 epoch total loss 5.70496273\n",
      "Trained batch 2548 batch loss 5.54154205 epoch total loss 5.70489883\n",
      "Trained batch 2549 batch loss 5.87129498 epoch total loss 5.70496416\n",
      "Trained batch 2550 batch loss 5.68731308 epoch total loss 5.70495701\n",
      "Trained batch 2551 batch loss 5.65252399 epoch total loss 5.7049365\n",
      "Trained batch 2552 batch loss 4.93795443 epoch total loss 5.70463562\n",
      "Trained batch 2553 batch loss 5.24612808 epoch total loss 5.70445633\n",
      "Trained batch 2554 batch loss 5.60334206 epoch total loss 5.70441675\n",
      "Trained batch 2555 batch loss 5.85886669 epoch total loss 5.70447683\n",
      "Trained batch 2556 batch loss 5.39699554 epoch total loss 5.70435667\n",
      "Trained batch 2557 batch loss 5.73391914 epoch total loss 5.70436859\n",
      "Trained batch 2558 batch loss 6.05540657 epoch total loss 5.70450592\n",
      "Trained batch 2559 batch loss 6.14108753 epoch total loss 5.70467615\n",
      "Trained batch 2560 batch loss 5.24966145 epoch total loss 5.70449877\n",
      "Trained batch 2561 batch loss 6.18534231 epoch total loss 5.70468664\n",
      "Trained batch 2562 batch loss 5.02431679 epoch total loss 5.70442104\n",
      "Trained batch 2563 batch loss 5.23760319 epoch total loss 5.70423889\n",
      "Trained batch 2564 batch loss 6.06287766 epoch total loss 5.7043786\n",
      "Trained batch 2565 batch loss 5.85812378 epoch total loss 5.70443869\n",
      "Trained batch 2566 batch loss 5.8500967 epoch total loss 5.70449495\n",
      "Trained batch 2567 batch loss 6.1239233 epoch total loss 5.70465851\n",
      "Trained batch 2568 batch loss 5.60395288 epoch total loss 5.70461893\n",
      "Trained batch 2569 batch loss 6.23283768 epoch total loss 5.70482445\n",
      "Trained batch 2570 batch loss 5.90119028 epoch total loss 5.70490122\n",
      "Trained batch 2571 batch loss 6.24890232 epoch total loss 5.70511246\n",
      "Trained batch 2572 batch loss 5.72886229 epoch total loss 5.70512199\n",
      "Trained batch 2573 batch loss 5.93151522 epoch total loss 5.70520973\n",
      "Trained batch 2574 batch loss 5.14790154 epoch total loss 5.70499325\n",
      "Trained batch 2575 batch loss 5.4475174 epoch total loss 5.70489311\n",
      "Trained batch 2576 batch loss 5.6418581 epoch total loss 5.70486832\n",
      "Trained batch 2577 batch loss 6.16647816 epoch total loss 5.70504761\n",
      "Trained batch 2578 batch loss 6.0129571 epoch total loss 5.70516682\n",
      "Trained batch 2579 batch loss 6.76701641 epoch total loss 5.70557833\n",
      "Trained batch 2580 batch loss 6.81836796 epoch total loss 5.70601\n",
      "Trained batch 2581 batch loss 5.43143082 epoch total loss 5.70590353\n",
      "Trained batch 2582 batch loss 5.76286125 epoch total loss 5.70592546\n",
      "Trained batch 2583 batch loss 5.7117672 epoch total loss 5.70592785\n",
      "Trained batch 2584 batch loss 6.30340862 epoch total loss 5.70615911\n",
      "Trained batch 2585 batch loss 6.59189034 epoch total loss 5.70650148\n",
      "Trained batch 2586 batch loss 6.07475567 epoch total loss 5.70664406\n",
      "Trained batch 2587 batch loss 6.24305916 epoch total loss 5.70685148\n",
      "Trained batch 2588 batch loss 5.52555132 epoch total loss 5.70678139\n",
      "Trained batch 2589 batch loss 5.52648878 epoch total loss 5.70671177\n",
      "Trained batch 2590 batch loss 6.25235271 epoch total loss 5.70692205\n",
      "Trained batch 2591 batch loss 6.22685862 epoch total loss 5.7071228\n",
      "Trained batch 2592 batch loss 6.1019659 epoch total loss 5.70727491\n",
      "Trained batch 2593 batch loss 5.67159033 epoch total loss 5.70726156\n",
      "Trained batch 2594 batch loss 5.86880684 epoch total loss 5.70732355\n",
      "Trained batch 2595 batch loss 5.80243969 epoch total loss 5.70736074\n",
      "Trained batch 2596 batch loss 5.6505394 epoch total loss 5.70733881\n",
      "Trained batch 2597 batch loss 5.72326 epoch total loss 5.70734501\n",
      "Trained batch 2598 batch loss 5.81340408 epoch total loss 5.70738554\n",
      "Trained batch 2599 batch loss 6.27031136 epoch total loss 5.7076025\n",
      "Trained batch 2600 batch loss 5.83916283 epoch total loss 5.70765305\n",
      "Trained batch 2601 batch loss 4.95197678 epoch total loss 5.70736217\n",
      "Trained batch 2602 batch loss 5.77996063 epoch total loss 5.70739031\n",
      "Trained batch 2603 batch loss 5.36974049 epoch total loss 5.70726109\n",
      "Trained batch 2604 batch loss 4.32987881 epoch total loss 5.7067318\n",
      "Trained batch 2605 batch loss 4.31484222 epoch total loss 5.70619774\n",
      "Trained batch 2606 batch loss 4.46529388 epoch total loss 5.70572138\n",
      "Trained batch 2607 batch loss 4.29924774 epoch total loss 5.7051816\n",
      "Trained batch 2608 batch loss 4.24327278 epoch total loss 5.70462084\n",
      "Trained batch 2609 batch loss 4.75267887 epoch total loss 5.70425606\n",
      "Trained batch 2610 batch loss 4.43011951 epoch total loss 5.70376778\n",
      "Trained batch 2611 batch loss 4.16572666 epoch total loss 5.70317888\n",
      "Trained batch 2612 batch loss 4.70776081 epoch total loss 5.70279789\n",
      "Trained batch 2613 batch loss 3.80368209 epoch total loss 5.70207119\n",
      "Trained batch 2614 batch loss 3.97157383 epoch total loss 5.70140934\n",
      "Trained batch 2615 batch loss 4.63335228 epoch total loss 5.70100069\n",
      "Trained batch 2616 batch loss 3.75985432 epoch total loss 5.70025873\n",
      "Trained batch 2617 batch loss 4.28344631 epoch total loss 5.69971752\n",
      "Trained batch 2618 batch loss 4.39756393 epoch total loss 5.6992197\n",
      "Trained batch 2619 batch loss 4.58259201 epoch total loss 5.69879389\n",
      "Trained batch 2620 batch loss 4.64821148 epoch total loss 5.69839287\n",
      "Trained batch 2621 batch loss 3.71118689 epoch total loss 5.6976347\n",
      "Trained batch 2622 batch loss 4.32489872 epoch total loss 5.69711113\n",
      "Trained batch 2623 batch loss 3.95480585 epoch total loss 5.6964469\n",
      "Trained batch 2624 batch loss 5.59192371 epoch total loss 5.69640684\n",
      "Trained batch 2625 batch loss 6.32097816 epoch total loss 5.69664526\n",
      "Trained batch 2626 batch loss 6.10671711 epoch total loss 5.69680119\n",
      "Trained batch 2627 batch loss 5.23986244 epoch total loss 5.69662714\n",
      "Trained batch 2628 batch loss 5.48195553 epoch total loss 5.6965456\n",
      "Trained batch 2629 batch loss 6.20880127 epoch total loss 5.69674063\n",
      "Trained batch 2630 batch loss 5.80643463 epoch total loss 5.69678259\n",
      "Trained batch 2631 batch loss 6.35389423 epoch total loss 5.69703197\n",
      "Trained batch 2632 batch loss 5.58223152 epoch total loss 5.69698858\n",
      "Trained batch 2633 batch loss 6.01514959 epoch total loss 5.6971097\n",
      "Trained batch 2634 batch loss 5.52848959 epoch total loss 5.69704533\n",
      "Trained batch 2635 batch loss 6.5714817 epoch total loss 5.6973772\n",
      "Trained batch 2636 batch loss 5.84313488 epoch total loss 5.69743252\n",
      "Trained batch 2637 batch loss 6.00527573 epoch total loss 5.69754887\n",
      "Trained batch 2638 batch loss 6.18336868 epoch total loss 5.6977334\n",
      "Trained batch 2639 batch loss 5.99824333 epoch total loss 5.69784689\n",
      "Trained batch 2640 batch loss 6.6236639 epoch total loss 5.69819784\n",
      "Trained batch 2641 batch loss 6.34813309 epoch total loss 5.69844389\n",
      "Trained batch 2642 batch loss 6.70702 epoch total loss 5.69882536\n",
      "Trained batch 2643 batch loss 7.05623722 epoch total loss 5.69933939\n",
      "Trained batch 2644 batch loss 7.05707026 epoch total loss 5.69985247\n",
      "Trained batch 2645 batch loss 6.81823349 epoch total loss 5.70027542\n",
      "Trained batch 2646 batch loss 6.79587126 epoch total loss 5.70068932\n",
      "Trained batch 2647 batch loss 7.06033707 epoch total loss 5.70120335\n",
      "Trained batch 2648 batch loss 6.71297407 epoch total loss 5.70158529\n",
      "Trained batch 2649 batch loss 7.68352604 epoch total loss 5.70233345\n",
      "Trained batch 2650 batch loss 6.88813114 epoch total loss 5.70278072\n",
      "Trained batch 2651 batch loss 7.07340717 epoch total loss 5.70329762\n",
      "Trained batch 2652 batch loss 3.94777822 epoch total loss 5.70263577\n",
      "Trained batch 2653 batch loss 5.11304235 epoch total loss 5.70241404\n",
      "Trained batch 2654 batch loss 4.63612556 epoch total loss 5.70201206\n",
      "Trained batch 2655 batch loss 4.8028264 epoch total loss 5.70167303\n",
      "Trained batch 2656 batch loss 5.47106409 epoch total loss 5.70158625\n",
      "Trained batch 2657 batch loss 5.35932064 epoch total loss 5.7014575\n",
      "Trained batch 2658 batch loss 5.32374239 epoch total loss 5.7013154\n",
      "Trained batch 2659 batch loss 6.47803497 epoch total loss 5.7016077\n",
      "Trained batch 2660 batch loss 6.48125029 epoch total loss 5.70190096\n",
      "Trained batch 2661 batch loss 5.15390587 epoch total loss 5.70169544\n",
      "Trained batch 2662 batch loss 5.89034557 epoch total loss 5.70176601\n",
      "Trained batch 2663 batch loss 4.02677727 epoch total loss 5.70113707\n",
      "Trained batch 2664 batch loss 4.53919029 epoch total loss 5.70070076\n",
      "Trained batch 2665 batch loss 4.49836445 epoch total loss 5.70024967\n",
      "Trained batch 2666 batch loss 4.79295969 epoch total loss 5.69990921\n",
      "Trained batch 2667 batch loss 4.92215729 epoch total loss 5.69961739\n",
      "Trained batch 2668 batch loss 5.42111111 epoch total loss 5.69951296\n",
      "Trained batch 2669 batch loss 6.27718973 epoch total loss 5.69972944\n",
      "Trained batch 2670 batch loss 6.04716301 epoch total loss 5.69985962\n",
      "Trained batch 2671 batch loss 6.01121521 epoch total loss 5.69997597\n",
      "Trained batch 2672 batch loss 5.77283096 epoch total loss 5.70000315\n",
      "Trained batch 2673 batch loss 5.38945246 epoch total loss 5.6998868\n",
      "Trained batch 2674 batch loss 6.51804256 epoch total loss 5.70019293\n",
      "Trained batch 2675 batch loss 5.80019045 epoch total loss 5.70023\n",
      "Trained batch 2676 batch loss 5.90458918 epoch total loss 5.70030642\n",
      "Trained batch 2677 batch loss 5.7709074 epoch total loss 5.70033264\n",
      "Trained batch 2678 batch loss 5.86353588 epoch total loss 5.7003932\n",
      "Trained batch 2679 batch loss 5.42379761 epoch total loss 5.70029\n",
      "Trained batch 2680 batch loss 6.26008606 epoch total loss 5.70049906\n",
      "Trained batch 2681 batch loss 5.93578148 epoch total loss 5.70058632\n",
      "Trained batch 2682 batch loss 6.12118244 epoch total loss 5.7007432\n",
      "Trained batch 2683 batch loss 5.70272589 epoch total loss 5.70074415\n",
      "Trained batch 2684 batch loss 5.70260286 epoch total loss 5.70074463\n",
      "Trained batch 2685 batch loss 5.5535059 epoch total loss 5.70069\n",
      "Trained batch 2686 batch loss 5.64040375 epoch total loss 5.70066738\n",
      "Trained batch 2687 batch loss 6.26601267 epoch total loss 5.70087767\n",
      "Trained batch 2688 batch loss 5.94728 epoch total loss 5.7009697\n",
      "Trained batch 2689 batch loss 5.3636322 epoch total loss 5.70084381\n",
      "Trained batch 2690 batch loss 5.96623945 epoch total loss 5.70094252\n",
      "Trained batch 2691 batch loss 5.44422531 epoch total loss 5.70084715\n",
      "Trained batch 2692 batch loss 5.48476505 epoch total loss 5.70076656\n",
      "Trained batch 2693 batch loss 5.87690973 epoch total loss 5.70083189\n",
      "Trained batch 2694 batch loss 5.68279552 epoch total loss 5.70082521\n",
      "Trained batch 2695 batch loss 5.88839912 epoch total loss 5.70089483\n",
      "Trained batch 2696 batch loss 5.19216251 epoch total loss 5.70070648\n",
      "Trained batch 2697 batch loss 6.09174919 epoch total loss 5.70085144\n",
      "Trained batch 2698 batch loss 5.83579 epoch total loss 5.70090151\n",
      "Trained batch 2699 batch loss 5.28854179 epoch total loss 5.70074844\n",
      "Trained batch 2700 batch loss 5.46022797 epoch total loss 5.70065928\n",
      "Trained batch 2701 batch loss 5.30618477 epoch total loss 5.70051336\n",
      "Trained batch 2702 batch loss 5.892097 epoch total loss 5.70058441\n",
      "Trained batch 2703 batch loss 5.80901957 epoch total loss 5.70062447\n",
      "Trained batch 2704 batch loss 5.71385 epoch total loss 5.70062923\n",
      "Trained batch 2705 batch loss 5.70696163 epoch total loss 5.70063162\n",
      "Trained batch 2706 batch loss 5.01890373 epoch total loss 5.70038\n",
      "Trained batch 2707 batch loss 5.89830542 epoch total loss 5.7004528\n",
      "Trained batch 2708 batch loss 5.91709614 epoch total loss 5.70053291\n",
      "Trained batch 2709 batch loss 6.30673218 epoch total loss 5.70075655\n",
      "Trained batch 2710 batch loss 6.23175192 epoch total loss 5.70095253\n",
      "Trained batch 2711 batch loss 5.84752941 epoch total loss 5.70100641\n",
      "Trained batch 2712 batch loss 6.55981922 epoch total loss 5.70132303\n",
      "Trained batch 2713 batch loss 6.05510044 epoch total loss 5.70145321\n",
      "Trained batch 2714 batch loss 5.45674467 epoch total loss 5.70136309\n",
      "Trained batch 2715 batch loss 6.38669395 epoch total loss 5.70161581\n",
      "Trained batch 2716 batch loss 6.05331898 epoch total loss 5.70174551\n",
      "Trained batch 2717 batch loss 5.99476862 epoch total loss 5.70185328\n",
      "Trained batch 2718 batch loss 5.15561819 epoch total loss 5.70165205\n",
      "Trained batch 2719 batch loss 5.80480385 epoch total loss 5.70169\n",
      "Trained batch 2720 batch loss 5.59016562 epoch total loss 5.70164919\n",
      "Trained batch 2721 batch loss 5.86802101 epoch total loss 5.70171\n",
      "Trained batch 2722 batch loss 4.7258358 epoch total loss 5.70135164\n",
      "Trained batch 2723 batch loss 5.72906971 epoch total loss 5.70136213\n",
      "Trained batch 2724 batch loss 5.68894386 epoch total loss 5.70135736\n",
      "Trained batch 2725 batch loss 5.77779 epoch total loss 5.70138502\n",
      "Trained batch 2726 batch loss 5.80706406 epoch total loss 5.70142365\n",
      "Trained batch 2727 batch loss 5.99356556 epoch total loss 5.70153093\n",
      "Trained batch 2728 batch loss 6.19378757 epoch total loss 5.70171118\n",
      "Trained batch 2729 batch loss 5.7279129 epoch total loss 5.70172\n",
      "Trained batch 2730 batch loss 6.16184044 epoch total loss 5.70188904\n",
      "Trained batch 2731 batch loss 5.52751732 epoch total loss 5.70182514\n",
      "Trained batch 2732 batch loss 5.50499487 epoch total loss 5.70175314\n",
      "Trained batch 2733 batch loss 5.83097267 epoch total loss 5.70180035\n",
      "Trained batch 2734 batch loss 5.95001364 epoch total loss 5.70189142\n",
      "Trained batch 2735 batch loss 5.70880699 epoch total loss 5.70189381\n",
      "Trained batch 2736 batch loss 5.50079441 epoch total loss 5.70182037\n",
      "Trained batch 2737 batch loss 5.84093857 epoch total loss 5.7018714\n",
      "Trained batch 2738 batch loss 4.97471142 epoch total loss 5.7016058\n",
      "Trained batch 2739 batch loss 4.69524193 epoch total loss 5.70123816\n",
      "Trained batch 2740 batch loss 4.72637177 epoch total loss 5.70088243\n",
      "Trained batch 2741 batch loss 5.44492865 epoch total loss 5.70078945\n",
      "Trained batch 2742 batch loss 5.019032 epoch total loss 5.70054054\n",
      "Trained batch 2743 batch loss 5.91907597 epoch total loss 5.70062\n",
      "Trained batch 2744 batch loss 6.05812454 epoch total loss 5.70075035\n",
      "Trained batch 2745 batch loss 6.02688503 epoch total loss 5.70086956\n",
      "Trained batch 2746 batch loss 5.37548542 epoch total loss 5.70075083\n",
      "Trained batch 2747 batch loss 5.56973 epoch total loss 5.70070314\n",
      "Trained batch 2748 batch loss 5.94367456 epoch total loss 5.70079136\n",
      "Trained batch 2749 batch loss 6.01513577 epoch total loss 5.70090532\n",
      "Trained batch 2750 batch loss 5.58341408 epoch total loss 5.70086241\n",
      "Trained batch 2751 batch loss 6.2778368 epoch total loss 5.70107269\n",
      "Trained batch 2752 batch loss 5.57119656 epoch total loss 5.70102549\n",
      "Trained batch 2753 batch loss 7.00822306 epoch total loss 5.7015\n",
      "Trained batch 2754 batch loss 5.68754959 epoch total loss 5.70149469\n",
      "Trained batch 2755 batch loss 5.85816574 epoch total loss 5.70155191\n",
      "Trained batch 2756 batch loss 6.28601551 epoch total loss 5.70176411\n",
      "Trained batch 2757 batch loss 5.99775171 epoch total loss 5.7018714\n",
      "Trained batch 2758 batch loss 6.55078 epoch total loss 5.70217943\n",
      "Trained batch 2759 batch loss 6.10918045 epoch total loss 5.70232677\n",
      "Trained batch 2760 batch loss 5.85919619 epoch total loss 5.70238352\n",
      "Trained batch 2761 batch loss 5.56660414 epoch total loss 5.7023344\n",
      "Trained batch 2762 batch loss 5.30279636 epoch total loss 5.70219\n",
      "Trained batch 2763 batch loss 5.87253904 epoch total loss 5.70225143\n",
      "Trained batch 2764 batch loss 5.04836226 epoch total loss 5.70201492\n",
      "Trained batch 2765 batch loss 5.32381964 epoch total loss 5.70187807\n",
      "Trained batch 2766 batch loss 5.94109344 epoch total loss 5.70196486\n",
      "Trained batch 2767 batch loss 5.2482338 epoch total loss 5.70180082\n",
      "Trained batch 2768 batch loss 5.85818052 epoch total loss 5.70185757\n",
      "Trained batch 2769 batch loss 5.86537504 epoch total loss 5.70191622\n",
      "Trained batch 2770 batch loss 5.6254797 epoch total loss 5.70188856\n",
      "Trained batch 2771 batch loss 3.49725676 epoch total loss 5.70109272\n",
      "Trained batch 2772 batch loss 3.40214205 epoch total loss 5.7002635\n",
      "Trained batch 2773 batch loss 3.84816837 epoch total loss 5.69959593\n",
      "Trained batch 2774 batch loss 3.9146347 epoch total loss 5.69895267\n",
      "Trained batch 2775 batch loss 4.46985149 epoch total loss 5.69850969\n",
      "Trained batch 2776 batch loss 4.48344564 epoch total loss 5.69807196\n",
      "Trained batch 2777 batch loss 5.42378426 epoch total loss 5.69797325\n",
      "Trained batch 2778 batch loss 5.9201026 epoch total loss 5.69805288\n",
      "Trained batch 2779 batch loss 5.29213667 epoch total loss 5.69790697\n",
      "Trained batch 2780 batch loss 6.1657424 epoch total loss 5.69807529\n",
      "Trained batch 2781 batch loss 6.23894548 epoch total loss 5.69827\n",
      "Trained batch 2782 batch loss 6.12108946 epoch total loss 5.69842196\n",
      "Trained batch 2783 batch loss 5.84664154 epoch total loss 5.69847536\n",
      "Trained batch 2784 batch loss 5.87164783 epoch total loss 5.69853735\n",
      "Trained batch 2785 batch loss 5.27548027 epoch total loss 5.69838572\n",
      "Trained batch 2786 batch loss 5.86842155 epoch total loss 5.69844675\n",
      "Trained batch 2787 batch loss 5.35750294 epoch total loss 5.6983242\n",
      "Trained batch 2788 batch loss 5.91727924 epoch total loss 5.6984024\n",
      "Trained batch 2789 batch loss 5.52749062 epoch total loss 5.69834137\n",
      "Trained batch 2790 batch loss 6.21414614 epoch total loss 5.69852591\n",
      "Trained batch 2791 batch loss 6.46644592 epoch total loss 5.69880152\n",
      "Trained batch 2792 batch loss 6.64847851 epoch total loss 5.6991415\n",
      "Trained batch 2793 batch loss 6.24904728 epoch total loss 5.69933844\n",
      "Trained batch 2794 batch loss 6.20167685 epoch total loss 5.6995182\n",
      "Trained batch 2795 batch loss 5.36677742 epoch total loss 5.69939947\n",
      "Trained batch 2796 batch loss 7.12195253 epoch total loss 5.69990826\n",
      "Trained batch 2797 batch loss 6.88421822 epoch total loss 5.70033169\n",
      "Trained batch 2798 batch loss 7.18743753 epoch total loss 5.70086288\n",
      "Trained batch 2799 batch loss 6.73222542 epoch total loss 5.70123148\n",
      "Trained batch 2800 batch loss 6.79991531 epoch total loss 5.70162392\n",
      "Trained batch 2801 batch loss 6.87956905 epoch total loss 5.70204449\n",
      "Trained batch 2802 batch loss 6.53131533 epoch total loss 5.7023406\n",
      "Trained batch 2803 batch loss 7.11043739 epoch total loss 5.70284271\n",
      "Trained batch 2804 batch loss 6.5440197 epoch total loss 5.70314264\n",
      "Trained batch 2805 batch loss 6.01776886 epoch total loss 5.7032547\n",
      "Trained batch 2806 batch loss 6.24515629 epoch total loss 5.70344782\n",
      "Trained batch 2807 batch loss 6.3734951 epoch total loss 5.70368671\n",
      "Trained batch 2808 batch loss 6.61381483 epoch total loss 5.70401096\n",
      "Trained batch 2809 batch loss 6.23142147 epoch total loss 5.70419836\n",
      "Trained batch 2810 batch loss 5.4418 epoch total loss 5.7041049\n",
      "Trained batch 2811 batch loss 6.34441757 epoch total loss 5.70433283\n",
      "Trained batch 2812 batch loss 6.48380756 epoch total loss 5.70461\n",
      "Trained batch 2813 batch loss 5.92385769 epoch total loss 5.70468807\n",
      "Trained batch 2814 batch loss 6.63092661 epoch total loss 5.70501709\n",
      "Trained batch 2815 batch loss 6.30545044 epoch total loss 5.70523\n",
      "Trained batch 2816 batch loss 7.15961 epoch total loss 5.70574665\n",
      "Trained batch 2817 batch loss 6.33362865 epoch total loss 5.70597\n",
      "Trained batch 2818 batch loss 5.63631582 epoch total loss 5.70594501\n",
      "Trained batch 2819 batch loss 6.15691185 epoch total loss 5.70610523\n",
      "Trained batch 2820 batch loss 4.84524441 epoch total loss 5.7058\n",
      "Trained batch 2821 batch loss 6.06225872 epoch total loss 5.70592642\n",
      "Trained batch 2822 batch loss 5.86976147 epoch total loss 5.70598459\n",
      "Trained batch 2823 batch loss 6.60489082 epoch total loss 5.70630312\n",
      "Trained batch 2824 batch loss 6.00311089 epoch total loss 5.70640802\n",
      "Trained batch 2825 batch loss 5.35160065 epoch total loss 5.70628262\n",
      "Trained batch 2826 batch loss 5.31038904 epoch total loss 5.70614243\n",
      "Trained batch 2827 batch loss 4.34962702 epoch total loss 5.70566273\n",
      "Trained batch 2828 batch loss 5.61610603 epoch total loss 5.70563078\n",
      "Trained batch 2829 batch loss 5.04421139 epoch total loss 5.70539713\n",
      "Trained batch 2830 batch loss 5.78558683 epoch total loss 5.70542526\n",
      "Trained batch 2831 batch loss 5.22604179 epoch total loss 5.70525599\n",
      "Trained batch 2832 batch loss 6.63525486 epoch total loss 5.70558453\n",
      "Trained batch 2833 batch loss 6.0276804 epoch total loss 5.70569801\n",
      "Trained batch 2834 batch loss 5.05487 epoch total loss 5.70546818\n",
      "Trained batch 2835 batch loss 5.85802889 epoch total loss 5.70552206\n",
      "Trained batch 2836 batch loss 5.7031517 epoch total loss 5.70552111\n",
      "Trained batch 2837 batch loss 6.50917244 epoch total loss 5.70580435\n",
      "Trained batch 2838 batch loss 5.69710255 epoch total loss 5.70580149\n",
      "Trained batch 2839 batch loss 5.15282631 epoch total loss 5.70560646\n",
      "Trained batch 2840 batch loss 4.37339878 epoch total loss 5.70513725\n",
      "Trained batch 2841 batch loss 5.3536253 epoch total loss 5.70501328\n",
      "Trained batch 2842 batch loss 5.31264257 epoch total loss 5.70487547\n",
      "Trained batch 2843 batch loss 5.13203144 epoch total loss 5.70467377\n",
      "Trained batch 2844 batch loss 5.46254921 epoch total loss 5.70458889\n",
      "Trained batch 2845 batch loss 5.38935089 epoch total loss 5.70447826\n",
      "Trained batch 2846 batch loss 5.52366924 epoch total loss 5.70441437\n",
      "Trained batch 2847 batch loss 6.6094923 epoch total loss 5.70473242\n",
      "Trained batch 2848 batch loss 6.0925684 epoch total loss 5.70486879\n",
      "Trained batch 2849 batch loss 6.77674484 epoch total loss 5.70524454\n",
      "Trained batch 2850 batch loss 6.4727993 epoch total loss 5.70551395\n",
      "Trained batch 2851 batch loss 6.25311708 epoch total loss 5.70570612\n",
      "Trained batch 2852 batch loss 6.65319347 epoch total loss 5.70603848\n",
      "Trained batch 2853 batch loss 6.19106436 epoch total loss 5.70620823\n",
      "Trained batch 2854 batch loss 6.61951923 epoch total loss 5.70652819\n",
      "Trained batch 2855 batch loss 6.1833415 epoch total loss 5.70669556\n",
      "Trained batch 2856 batch loss 6.48749447 epoch total loss 5.70696878\n",
      "Trained batch 2857 batch loss 5.00596905 epoch total loss 5.70672321\n",
      "Trained batch 2858 batch loss 6.10819721 epoch total loss 5.70686388\n",
      "Trained batch 2859 batch loss 5.08021402 epoch total loss 5.70664454\n",
      "Trained batch 2860 batch loss 6.78240776 epoch total loss 5.70702076\n",
      "Trained batch 2861 batch loss 5.52139521 epoch total loss 5.70695591\n",
      "Trained batch 2862 batch loss 6.43545771 epoch total loss 5.70721054\n",
      "Trained batch 2863 batch loss 7.69018841 epoch total loss 5.70790291\n",
      "Trained batch 2864 batch loss 6.37294483 epoch total loss 5.70813513\n",
      "Trained batch 2865 batch loss 5.45040798 epoch total loss 5.70804548\n",
      "Trained batch 2866 batch loss 5.7693758 epoch total loss 5.70806694\n",
      "Trained batch 2867 batch loss 5.37648153 epoch total loss 5.70795107\n",
      "Trained batch 2868 batch loss 5.27339172 epoch total loss 5.7078\n",
      "Trained batch 2869 batch loss 5.48668671 epoch total loss 5.70772266\n",
      "Trained batch 2870 batch loss 5.50842333 epoch total loss 5.70765305\n",
      "Trained batch 2871 batch loss 3.76690364 epoch total loss 5.70697737\n",
      "Trained batch 2872 batch loss 5.41411209 epoch total loss 5.70687532\n",
      "Trained batch 2873 batch loss 5.35284376 epoch total loss 5.7067523\n",
      "Trained batch 2874 batch loss 5.88958549 epoch total loss 5.70681572\n",
      "Trained batch 2875 batch loss 5.23952198 epoch total loss 5.70665359\n",
      "Trained batch 2876 batch loss 6.09247351 epoch total loss 5.70678759\n",
      "Trained batch 2877 batch loss 5.65471745 epoch total loss 5.70676899\n",
      "Trained batch 2878 batch loss 4.51346493 epoch total loss 5.70635462\n",
      "Trained batch 2879 batch loss 5.07897472 epoch total loss 5.70613623\n",
      "Trained batch 2880 batch loss 5.38470173 epoch total loss 5.70602465\n",
      "Trained batch 2881 batch loss 5.27162743 epoch total loss 5.70587397\n",
      "Trained batch 2882 batch loss 5.27684212 epoch total loss 5.70572519\n",
      "Trained batch 2883 batch loss 5.60629463 epoch total loss 5.70569038\n",
      "Trained batch 2884 batch loss 5.65098763 epoch total loss 5.70567131\n",
      "Trained batch 2885 batch loss 5.75034332 epoch total loss 5.70568657\n",
      "Trained batch 2886 batch loss 5.65069294 epoch total loss 5.7056675\n",
      "Trained batch 2887 batch loss 5.78286886 epoch total loss 5.70569468\n",
      "Trained batch 2888 batch loss 5.49357033 epoch total loss 5.70562124\n",
      "Trained batch 2889 batch loss 5.16226053 epoch total loss 5.70543289\n",
      "Trained batch 2890 batch loss 5.34784508 epoch total loss 5.70530939\n",
      "Trained batch 2891 batch loss 5.58628702 epoch total loss 5.70526791\n",
      "Trained batch 2892 batch loss 5.01334715 epoch total loss 5.70502901\n",
      "Trained batch 2893 batch loss 5.29092169 epoch total loss 5.70488548\n",
      "Trained batch 2894 batch loss 5.33685541 epoch total loss 5.70475817\n",
      "Trained batch 2895 batch loss 4.55112 epoch total loss 5.70435953\n",
      "Trained batch 2896 batch loss 5.19364262 epoch total loss 5.7041831\n",
      "Trained batch 2897 batch loss 5.84458637 epoch total loss 5.70423126\n",
      "Trained batch 2898 batch loss 5.82982635 epoch total loss 5.70427465\n",
      "Trained batch 2899 batch loss 6.43414593 epoch total loss 5.70452642\n",
      "Trained batch 2900 batch loss 6.53767824 epoch total loss 5.70481348\n",
      "Trained batch 2901 batch loss 6.30836439 epoch total loss 5.70502138\n",
      "Trained batch 2902 batch loss 6.52741814 epoch total loss 5.7053051\n",
      "Trained batch 2903 batch loss 6.58167791 epoch total loss 5.70560694\n",
      "Trained batch 2904 batch loss 6.0074234 epoch total loss 5.70571089\n",
      "Trained batch 2905 batch loss 6.85814762 epoch total loss 5.70610762\n",
      "Trained batch 2906 batch loss 6.76566 epoch total loss 5.70647192\n",
      "Trained batch 2907 batch loss 6.77937698 epoch total loss 5.70684099\n",
      "Trained batch 2908 batch loss 6.66834068 epoch total loss 5.70717144\n",
      "Trained batch 2909 batch loss 6.37788582 epoch total loss 5.70740175\n",
      "Trained batch 2910 batch loss 6.82179785 epoch total loss 5.70778513\n",
      "Trained batch 2911 batch loss 6.9927249 epoch total loss 5.7082262\n",
      "Trained batch 2912 batch loss 6.55460072 epoch total loss 5.70851707\n",
      "Trained batch 2913 batch loss 6.50563049 epoch total loss 5.70879078\n",
      "Trained batch 2914 batch loss 6.5948 epoch total loss 5.709095\n",
      "Trained batch 2915 batch loss 6.11999702 epoch total loss 5.70923567\n",
      "Trained batch 2916 batch loss 6.17684889 epoch total loss 5.70939636\n",
      "Trained batch 2917 batch loss 6.305 epoch total loss 5.70960045\n",
      "Trained batch 2918 batch loss 6.66779184 epoch total loss 5.70992899\n",
      "Trained batch 2919 batch loss 5.86741161 epoch total loss 5.70998287\n",
      "Trained batch 2920 batch loss 6.5846138 epoch total loss 5.71028185\n",
      "Trained batch 2921 batch loss 6.57385826 epoch total loss 5.71057796\n",
      "Trained batch 2922 batch loss 6.47965527 epoch total loss 5.71084118\n",
      "Trained batch 2923 batch loss 6.95361137 epoch total loss 5.71126604\n",
      "Trained batch 2924 batch loss 6.39797306 epoch total loss 5.71150112\n",
      "Trained batch 2925 batch loss 5.8067317 epoch total loss 5.71153402\n",
      "Trained batch 2926 batch loss 5.84187222 epoch total loss 5.71157837\n",
      "Trained batch 2927 batch loss 5.89762926 epoch total loss 5.71164227\n",
      "Trained batch 2928 batch loss 5.79455566 epoch total loss 5.7116704\n",
      "Trained batch 2929 batch loss 5.93254089 epoch total loss 5.71174574\n",
      "Trained batch 2930 batch loss 5.11412334 epoch total loss 5.71154165\n",
      "Trained batch 2931 batch loss 5.00547171 epoch total loss 5.71130085\n",
      "Trained batch 2932 batch loss 5.62806702 epoch total loss 5.71127272\n",
      "Trained batch 2933 batch loss 5.01253033 epoch total loss 5.71103382\n",
      "Trained batch 2934 batch loss 6.70776367 epoch total loss 5.71137333\n",
      "Trained batch 2935 batch loss 5.58482838 epoch total loss 5.71133\n",
      "Trained batch 2936 batch loss 6.33370256 epoch total loss 5.71154213\n",
      "Trained batch 2937 batch loss 6.82038689 epoch total loss 5.71192\n",
      "Trained batch 2938 batch loss 6.00836277 epoch total loss 5.7120204\n",
      "Trained batch 2939 batch loss 6.55728579 epoch total loss 5.71230793\n",
      "Trained batch 2940 batch loss 6.17082262 epoch total loss 5.71246338\n",
      "Trained batch 2941 batch loss 6.40594196 epoch total loss 5.71269941\n",
      "Trained batch 2942 batch loss 6.81190109 epoch total loss 5.71307325\n",
      "Trained batch 2943 batch loss 6.45154476 epoch total loss 5.71332407\n",
      "Trained batch 2944 batch loss 6.48611259 epoch total loss 5.71358633\n",
      "Trained batch 2945 batch loss 6.5074935 epoch total loss 5.71385622\n",
      "Trained batch 2946 batch loss 6.54949141 epoch total loss 5.71413946\n",
      "Trained batch 2947 batch loss 6.51058674 epoch total loss 5.71441\n",
      "Trained batch 2948 batch loss 6.66312504 epoch total loss 5.71473169\n",
      "Trained batch 2949 batch loss 6.60694408 epoch total loss 5.71503448\n",
      "Trained batch 2950 batch loss 6.38067722 epoch total loss 5.71526\n",
      "Trained batch 2951 batch loss 5.16254902 epoch total loss 5.71507263\n",
      "Trained batch 2952 batch loss 6.22847176 epoch total loss 5.71524668\n",
      "Trained batch 2953 batch loss 6.66307926 epoch total loss 5.71556711\n",
      "Trained batch 2954 batch loss 5.69957638 epoch total loss 5.71556187\n",
      "Trained batch 2955 batch loss 5.10724115 epoch total loss 5.71535587\n",
      "Trained batch 2956 batch loss 4.17202187 epoch total loss 5.71483374\n",
      "Trained batch 2957 batch loss 6.26126766 epoch total loss 5.71501875\n",
      "Trained batch 2958 batch loss 6.36373901 epoch total loss 5.71523809\n",
      "Trained batch 2959 batch loss 5.615798 epoch total loss 5.71520424\n",
      "Trained batch 2960 batch loss 6.58600569 epoch total loss 5.71549845\n",
      "Trained batch 2961 batch loss 7.50080109 epoch total loss 5.71610117\n",
      "Trained batch 2962 batch loss 6.12860394 epoch total loss 5.71624041\n",
      "Trained batch 2963 batch loss 4.46172523 epoch total loss 5.7158165\n",
      "Trained batch 2964 batch loss 4.94215441 epoch total loss 5.71555519\n",
      "Trained batch 2965 batch loss 5.16489935 epoch total loss 5.71536922\n",
      "Trained batch 2966 batch loss 5.68514347 epoch total loss 5.71535921\n",
      "Trained batch 2967 batch loss 4.03763199 epoch total loss 5.71479368\n",
      "Trained batch 2968 batch loss 6.56673431 epoch total loss 5.71508074\n",
      "Trained batch 2969 batch loss 4.64419317 epoch total loss 5.71472025\n",
      "Trained batch 2970 batch loss 6.72098351 epoch total loss 5.7150588\n",
      "Trained batch 2971 batch loss 6.14292526 epoch total loss 5.71520281\n",
      "Trained batch 2972 batch loss 6.25768471 epoch total loss 5.71538544\n",
      "Trained batch 2973 batch loss 6.17424059 epoch total loss 5.71553946\n",
      "Trained batch 2974 batch loss 6.31324196 epoch total loss 5.71574\n",
      "Trained batch 2975 batch loss 4.43208694 epoch total loss 5.71530867\n",
      "Trained batch 2976 batch loss 5.3423996 epoch total loss 5.71518326\n",
      "Trained batch 2977 batch loss 6.00052452 epoch total loss 5.71527863\n",
      "Trained batch 2978 batch loss 4.72570896 epoch total loss 5.71494675\n",
      "Trained batch 2979 batch loss 4.96608543 epoch total loss 5.71469545\n",
      "Trained batch 2980 batch loss 4.10355 epoch total loss 5.71415472\n",
      "Trained batch 2981 batch loss 4.9681344 epoch total loss 5.71390486\n",
      "Trained batch 2982 batch loss 5.66862345 epoch total loss 5.7138896\n",
      "Trained batch 2983 batch loss 5.38060856 epoch total loss 5.71377802\n",
      "Trained batch 2984 batch loss 4.45688391 epoch total loss 5.71335649\n",
      "Trained batch 2985 batch loss 5.02266502 epoch total loss 5.71312571\n",
      "Trained batch 2986 batch loss 5.03593922 epoch total loss 5.71289825\n",
      "Trained batch 2987 batch loss 4.75107098 epoch total loss 5.71257687\n",
      "Trained batch 2988 batch loss 5.50026798 epoch total loss 5.71250582\n",
      "Trained batch 2989 batch loss 4.49554729 epoch total loss 5.7120986\n",
      "Trained batch 2990 batch loss 4.68259048 epoch total loss 5.71175385\n",
      "Trained batch 2991 batch loss 4.79617453 epoch total loss 5.71144819\n",
      "Trained batch 2992 batch loss 5.04455566 epoch total loss 5.71122551\n",
      "Trained batch 2993 batch loss 5.7982769 epoch total loss 5.7112546\n",
      "Trained batch 2994 batch loss 5.47312164 epoch total loss 5.71117496\n",
      "Trained batch 2995 batch loss 4.98881578 epoch total loss 5.71093369\n",
      "Trained batch 2996 batch loss 4.53351068 epoch total loss 5.71054029\n",
      "Trained batch 2997 batch loss 5.40278578 epoch total loss 5.71043777\n",
      "Trained batch 2998 batch loss 6.08011913 epoch total loss 5.7105608\n",
      "Trained batch 2999 batch loss 6.29009342 epoch total loss 5.71075439\n",
      "Trained batch 3000 batch loss 4.91824865 epoch total loss 5.71049\n",
      "Trained batch 3001 batch loss 5.92728806 epoch total loss 5.71056271\n",
      "Trained batch 3002 batch loss 5.3987565 epoch total loss 5.71045876\n",
      "Trained batch 3003 batch loss 4.38710403 epoch total loss 5.71001768\n",
      "Trained batch 3004 batch loss 5.24142838 epoch total loss 5.70986223\n",
      "Trained batch 3005 batch loss 3.84414959 epoch total loss 5.70924091\n",
      "Trained batch 3006 batch loss 4.31950235 epoch total loss 5.70877886\n",
      "Trained batch 3007 batch loss 5.95978785 epoch total loss 5.7088623\n",
      "Trained batch 3008 batch loss 4.92081451 epoch total loss 5.7086\n",
      "Trained batch 3009 batch loss 5.82687664 epoch total loss 5.70863914\n",
      "Trained batch 3010 batch loss 5.87471533 epoch total loss 5.70869446\n",
      "Trained batch 3011 batch loss 5.84744692 epoch total loss 5.70874071\n",
      "Trained batch 3012 batch loss 5.77885246 epoch total loss 5.70876408\n",
      "Trained batch 3013 batch loss 6.12131 epoch total loss 5.70890093\n",
      "Trained batch 3014 batch loss 5.89355659 epoch total loss 5.70896244\n",
      "Trained batch 3015 batch loss 5.29253578 epoch total loss 5.70882416\n",
      "Trained batch 3016 batch loss 5.7930851 epoch total loss 5.70885229\n",
      "Trained batch 3017 batch loss 5.86484766 epoch total loss 5.70890427\n",
      "Trained batch 3018 batch loss 5.94674301 epoch total loss 5.70898294\n",
      "Trained batch 3019 batch loss 5.59154224 epoch total loss 5.70894432\n",
      "Trained batch 3020 batch loss 5.78542614 epoch total loss 5.70896959\n",
      "Trained batch 3021 batch loss 5.26835632 epoch total loss 5.7088232\n",
      "Trained batch 3022 batch loss 5.84409237 epoch total loss 5.70886803\n",
      "Trained batch 3023 batch loss 5.74420929 epoch total loss 5.70887947\n",
      "Trained batch 3024 batch loss 5.74116182 epoch total loss 5.70889\n",
      "Trained batch 3025 batch loss 5.72537613 epoch total loss 5.70889521\n",
      "Trained batch 3026 batch loss 5.95081902 epoch total loss 5.70897532\n",
      "Trained batch 3027 batch loss 5.1418581 epoch total loss 5.70878839\n",
      "Trained batch 3028 batch loss 5.94488811 epoch total loss 5.70886612\n",
      "Trained batch 3029 batch loss 6.00038338 epoch total loss 5.70896244\n",
      "Trained batch 3030 batch loss 5.84852886 epoch total loss 5.70900822\n",
      "Trained batch 3031 batch loss 5.64208841 epoch total loss 5.70898628\n",
      "Trained batch 3032 batch loss 5.63361931 epoch total loss 5.70896101\n",
      "Trained batch 3033 batch loss 6.11489 epoch total loss 5.709095\n",
      "Trained batch 3034 batch loss 5.56194 epoch total loss 5.70904684\n",
      "Trained batch 3035 batch loss 5.80713749 epoch total loss 5.70907879\n",
      "Trained batch 3036 batch loss 5.78581047 epoch total loss 5.70910406\n",
      "Trained batch 3037 batch loss 5.10452 epoch total loss 5.70890522\n",
      "Trained batch 3038 batch loss 6.11687946 epoch total loss 5.70903969\n",
      "Trained batch 3039 batch loss 5.3341465 epoch total loss 5.70891619\n",
      "Trained batch 3040 batch loss 5.41205835 epoch total loss 5.70881844\n",
      "Trained batch 3041 batch loss 5.4772048 epoch total loss 5.70874214\n",
      "Trained batch 3042 batch loss 5.34328127 epoch total loss 5.70862246\n",
      "Trained batch 3043 batch loss 5.45885181 epoch total loss 5.70854044\n",
      "Trained batch 3044 batch loss 5.4000783 epoch total loss 5.70843887\n",
      "Trained batch 3045 batch loss 5.59498024 epoch total loss 5.70840216\n",
      "Trained batch 3046 batch loss 5.43726254 epoch total loss 5.70831299\n",
      "Trained batch 3047 batch loss 5.67808771 epoch total loss 5.70830297\n",
      "Trained batch 3048 batch loss 5.42100382 epoch total loss 5.70820904\n",
      "Trained batch 3049 batch loss 5.07184124 epoch total loss 5.70800066\n",
      "Trained batch 3050 batch loss 5.45041037 epoch total loss 5.70791626\n",
      "Trained batch 3051 batch loss 5.34321165 epoch total loss 5.70779705\n",
      "Trained batch 3052 batch loss 5.98305941 epoch total loss 5.7078867\n",
      "Trained batch 3053 batch loss 5.77087307 epoch total loss 5.70790768\n",
      "Trained batch 3054 batch loss 5.16587734 epoch total loss 5.70773029\n",
      "Trained batch 3055 batch loss 5.61918211 epoch total loss 5.70770121\n",
      "Trained batch 3056 batch loss 5.33925819 epoch total loss 5.70758104\n",
      "Trained batch 3057 batch loss 6.2097683 epoch total loss 5.70774508\n",
      "Trained batch 3058 batch loss 5.60486507 epoch total loss 5.7077117\n",
      "Trained batch 3059 batch loss 5.89040375 epoch total loss 5.7077713\n",
      "Trained batch 3060 batch loss 6.04904175 epoch total loss 5.70788288\n",
      "Trained batch 3061 batch loss 5.79603815 epoch total loss 5.70791197\n",
      "Trained batch 3062 batch loss 5.06007671 epoch total loss 5.70770025\n",
      "Trained batch 3063 batch loss 5.70183659 epoch total loss 5.70769835\n",
      "Trained batch 3064 batch loss 6.32774925 epoch total loss 5.70790052\n",
      "Trained batch 3065 batch loss 5.56001711 epoch total loss 5.70785284\n",
      "Trained batch 3066 batch loss 5.4149251 epoch total loss 5.707757\n",
      "Trained batch 3067 batch loss 5.0822649 epoch total loss 5.70755291\n",
      "Trained batch 3068 batch loss 4.8386755 epoch total loss 5.70726919\n",
      "Trained batch 3069 batch loss 5.24388695 epoch total loss 5.70711851\n",
      "Trained batch 3070 batch loss 4.57084703 epoch total loss 5.70674801\n",
      "Trained batch 3071 batch loss 4.66075659 epoch total loss 5.70640755\n",
      "Trained batch 3072 batch loss 4.65340948 epoch total loss 5.7060647\n",
      "Trained batch 3073 batch loss 4.41369295 epoch total loss 5.70564461\n",
      "Trained batch 3074 batch loss 4.67842 epoch total loss 5.70531\n",
      "Trained batch 3075 batch loss 4.52322102 epoch total loss 5.70492554\n",
      "Trained batch 3076 batch loss 4.52408218 epoch total loss 5.70454168\n",
      "Trained batch 3077 batch loss 5.18687153 epoch total loss 5.70437336\n",
      "Trained batch 3078 batch loss 6.17121077 epoch total loss 5.70452547\n",
      "Trained batch 3079 batch loss 6.55028868 epoch total loss 5.7048\n",
      "Trained batch 3080 batch loss 6.06506491 epoch total loss 5.70491695\n",
      "Trained batch 3081 batch loss 6.22036362 epoch total loss 5.70508432\n",
      "Trained batch 3082 batch loss 6.69804955 epoch total loss 5.70540619\n",
      "Trained batch 3083 batch loss 6.52035475 epoch total loss 5.70567036\n",
      "Trained batch 3084 batch loss 5.32403755 epoch total loss 5.70554686\n",
      "Trained batch 3085 batch loss 5.84460068 epoch total loss 5.70559168\n",
      "Trained batch 3086 batch loss 5.59059095 epoch total loss 5.70555401\n",
      "Trained batch 3087 batch loss 5.10062265 epoch total loss 5.70535851\n",
      "Trained batch 3088 batch loss 6.06502 epoch total loss 5.70547485\n",
      "Trained batch 3089 batch loss 5.61904669 epoch total loss 5.70544672\n",
      "Trained batch 3090 batch loss 5.96021175 epoch total loss 5.70552921\n",
      "Trained batch 3091 batch loss 5.41026115 epoch total loss 5.70543385\n",
      "Trained batch 3092 batch loss 5.43621063 epoch total loss 5.70534658\n",
      "Trained batch 3093 batch loss 5.59649038 epoch total loss 5.7053113\n",
      "Trained batch 3094 batch loss 5.94150639 epoch total loss 5.70538759\n",
      "Trained batch 3095 batch loss 6.02539062 epoch total loss 5.70549107\n",
      "Trained batch 3096 batch loss 5.78579235 epoch total loss 5.70551634\n",
      "Trained batch 3097 batch loss 5.7954607 epoch total loss 5.70554543\n",
      "Trained batch 3098 batch loss 5.54861832 epoch total loss 5.70549488\n",
      "Trained batch 3099 batch loss 5.90403938 epoch total loss 5.70555878\n",
      "Trained batch 3100 batch loss 5.59797144 epoch total loss 5.70552397\n",
      "Trained batch 3101 batch loss 5.14878273 epoch total loss 5.70534468\n",
      "Trained batch 3102 batch loss 5.59662151 epoch total loss 5.70530939\n",
      "Trained batch 3103 batch loss 5.40888596 epoch total loss 5.70521355\n",
      "Trained batch 3104 batch loss 5.66908503 epoch total loss 5.7052021\n",
      "Trained batch 3105 batch loss 5.50784588 epoch total loss 5.70513868\n",
      "Trained batch 3106 batch loss 5.65171528 epoch total loss 5.70512152\n",
      "Trained batch 3107 batch loss 5.74627781 epoch total loss 5.70513487\n",
      "Trained batch 3108 batch loss 5.33167362 epoch total loss 5.70501471\n",
      "Trained batch 3109 batch loss 5.46119785 epoch total loss 5.70493603\n",
      "Trained batch 3110 batch loss 5.206007 epoch total loss 5.70477533\n",
      "Trained batch 3111 batch loss 5.1609745 epoch total loss 5.70460033\n",
      "Trained batch 3112 batch loss 5.53112221 epoch total loss 5.70454454\n",
      "Trained batch 3113 batch loss 6.04543209 epoch total loss 5.70465422\n",
      "Trained batch 3114 batch loss 5.87792778 epoch total loss 5.70470953\n",
      "Trained batch 3115 batch loss 5.47106409 epoch total loss 5.70463419\n",
      "Trained batch 3116 batch loss 5.40229 epoch total loss 5.70453739\n",
      "Trained batch 3117 batch loss 5.08729506 epoch total loss 5.7043395\n",
      "Trained batch 3118 batch loss 5.37376499 epoch total loss 5.70423317\n",
      "Trained batch 3119 batch loss 5.01281738 epoch total loss 5.70401192\n",
      "Trained batch 3120 batch loss 5.12213802 epoch total loss 5.70382547\n",
      "Trained batch 3121 batch loss 5.43924809 epoch total loss 5.7037406\n",
      "Trained batch 3122 batch loss 5.2624712 epoch total loss 5.70359898\n",
      "Trained batch 3123 batch loss 5.33612537 epoch total loss 5.70348167\n",
      "Trained batch 3124 batch loss 6.14451885 epoch total loss 5.70362282\n",
      "Trained batch 3125 batch loss 4.55366325 epoch total loss 5.70325422\n",
      "Trained batch 3126 batch loss 4.42374182 epoch total loss 5.7028451\n",
      "Trained batch 3127 batch loss 6.28221512 epoch total loss 5.70303\n",
      "Trained batch 3128 batch loss 5.56887817 epoch total loss 5.70298719\n",
      "Trained batch 3129 batch loss 6.08001423 epoch total loss 5.70310736\n",
      "Trained batch 3130 batch loss 5.8496027 epoch total loss 5.70315456\n",
      "Trained batch 3131 batch loss 5.70438957 epoch total loss 5.70315504\n",
      "Trained batch 3132 batch loss 5.19031715 epoch total loss 5.70299101\n",
      "Trained batch 3133 batch loss 6.08574 epoch total loss 5.70311308\n",
      "Trained batch 3134 batch loss 6.44789648 epoch total loss 5.70335054\n",
      "Trained batch 3135 batch loss 5.76396513 epoch total loss 5.70336962\n",
      "Trained batch 3136 batch loss 6.26625729 epoch total loss 5.70354891\n",
      "Trained batch 3137 batch loss 5.61757708 epoch total loss 5.70352173\n",
      "Trained batch 3138 batch loss 5.67339611 epoch total loss 5.70351219\n",
      "Trained batch 3139 batch loss 5.72478 epoch total loss 5.70351887\n",
      "Trained batch 3140 batch loss 5.69549179 epoch total loss 5.70351648\n",
      "Trained batch 3141 batch loss 5.95540333 epoch total loss 5.70359612\n",
      "Trained batch 3142 batch loss 5.36803913 epoch total loss 5.7034893\n",
      "Trained batch 3143 batch loss 6.22084522 epoch total loss 5.70365381\n",
      "Trained batch 3144 batch loss 5.92815924 epoch total loss 5.70372534\n",
      "Trained batch 3145 batch loss 5.76596546 epoch total loss 5.70374489\n",
      "Trained batch 3146 batch loss 5.67726898 epoch total loss 5.70373631\n",
      "Trained batch 3147 batch loss 6.00037289 epoch total loss 5.70383072\n",
      "Trained batch 3148 batch loss 6.05712509 epoch total loss 5.70394278\n",
      "Trained batch 3149 batch loss 5.70059681 epoch total loss 5.70394182\n",
      "Trained batch 3150 batch loss 5.7093935 epoch total loss 5.70394325\n",
      "Trained batch 3151 batch loss 6.46330452 epoch total loss 5.70418453\n",
      "Trained batch 3152 batch loss 6.24047947 epoch total loss 5.70435429\n",
      "Trained batch 3153 batch loss 5.25432 epoch total loss 5.70421171\n",
      "Trained batch 3154 batch loss 5.58445 epoch total loss 5.70417356\n",
      "Trained batch 3155 batch loss 5.88662148 epoch total loss 5.70423126\n",
      "Trained batch 3156 batch loss 6.07793617 epoch total loss 5.70434952\n",
      "Trained batch 3157 batch loss 5.77903938 epoch total loss 5.70437336\n",
      "Trained batch 3158 batch loss 5.4040184 epoch total loss 5.70427847\n",
      "Trained batch 3159 batch loss 5.86089849 epoch total loss 5.70432806\n",
      "Trained batch 3160 batch loss 5.58569908 epoch total loss 5.70429087\n",
      "Trained batch 3161 batch loss 5.04522514 epoch total loss 5.70408201\n",
      "Trained batch 3162 batch loss 4.7448864 epoch total loss 5.70377874\n",
      "Trained batch 3163 batch loss 4.91638231 epoch total loss 5.70352936\n",
      "Trained batch 3164 batch loss 5.2196188 epoch total loss 5.70337629\n",
      "Trained batch 3165 batch loss 4.06757212 epoch total loss 5.70286\n",
      "Trained batch 3166 batch loss 4.35980701 epoch total loss 5.70243549\n",
      "Trained batch 3167 batch loss 4.48703194 epoch total loss 5.70205116\n",
      "Trained batch 3168 batch loss 5.05856 epoch total loss 5.70184803\n",
      "Trained batch 3169 batch loss 4.96662331 epoch total loss 5.70161629\n",
      "Trained batch 3170 batch loss 4.81990051 epoch total loss 5.70133829\n",
      "Trained batch 3171 batch loss 4.79286385 epoch total loss 5.70105171\n",
      "Trained batch 3172 batch loss 4.721632 epoch total loss 5.70074272\n",
      "Trained batch 3173 batch loss 5.03714418 epoch total loss 5.70053339\n",
      "Trained batch 3174 batch loss 5.25362873 epoch total loss 5.70039272\n",
      "Trained batch 3175 batch loss 5.13366318 epoch total loss 5.70021391\n",
      "Trained batch 3176 batch loss 5.83644962 epoch total loss 5.70025682\n",
      "Trained batch 3177 batch loss 5.16683722 epoch total loss 5.7000885\n",
      "Trained batch 3178 batch loss 4.94624805 epoch total loss 5.69985104\n",
      "Trained batch 3179 batch loss 5.21386051 epoch total loss 5.69969797\n",
      "Trained batch 3180 batch loss 6.20141363 epoch total loss 5.6998558\n",
      "Trained batch 3181 batch loss 6.12989092 epoch total loss 5.69999123\n",
      "Trained batch 3182 batch loss 6.25577831 epoch total loss 5.70016575\n",
      "Trained batch 3183 batch loss 5.52551365 epoch total loss 5.70011091\n",
      "Trained batch 3184 batch loss 5.77723122 epoch total loss 5.70013523\n",
      "Trained batch 3185 batch loss 6.68312645 epoch total loss 5.70044374\n",
      "Trained batch 3186 batch loss 6.34650803 epoch total loss 5.7006464\n",
      "Trained batch 3187 batch loss 7.17371273 epoch total loss 5.70110893\n",
      "Trained batch 3188 batch loss 6.10000134 epoch total loss 5.70123386\n",
      "Trained batch 3189 batch loss 6.29173613 epoch total loss 5.70141888\n",
      "Trained batch 3190 batch loss 6.15864086 epoch total loss 5.70156193\n",
      "Trained batch 3191 batch loss 5.87508869 epoch total loss 5.70161629\n",
      "Trained batch 3192 batch loss 6.15882397 epoch total loss 5.70175934\n",
      "Trained batch 3193 batch loss 6.41433907 epoch total loss 5.7019825\n",
      "Trained batch 3194 batch loss 6.27561951 epoch total loss 5.70216179\n",
      "Trained batch 3195 batch loss 5.71770382 epoch total loss 5.70216656\n",
      "Trained batch 3196 batch loss 5.78623199 epoch total loss 5.70219326\n",
      "Trained batch 3197 batch loss 7.03902721 epoch total loss 5.70261145\n",
      "Trained batch 3198 batch loss 6.25118637 epoch total loss 5.70278311\n",
      "Trained batch 3199 batch loss 5.47873449 epoch total loss 5.70271301\n",
      "Trained batch 3200 batch loss 6.06601524 epoch total loss 5.7028265\n",
      "Trained batch 3201 batch loss 6.30334139 epoch total loss 5.7030139\n",
      "Trained batch 3202 batch loss 5.592659 epoch total loss 5.70297909\n",
      "Trained batch 3203 batch loss 5.79184246 epoch total loss 5.70300674\n",
      "Trained batch 3204 batch loss 6.50892782 epoch total loss 5.70325851\n",
      "Trained batch 3205 batch loss 7.04836607 epoch total loss 5.70367813\n",
      "Trained batch 3206 batch loss 6.36877584 epoch total loss 5.70388603\n",
      "Trained batch 3207 batch loss 5.99829483 epoch total loss 5.70397758\n",
      "Trained batch 3208 batch loss 6.49367 epoch total loss 5.70422411\n",
      "Trained batch 3209 batch loss 5.99630547 epoch total loss 5.70431471\n",
      "Trained batch 3210 batch loss 5.47538805 epoch total loss 5.70424318\n",
      "Trained batch 3211 batch loss 6.35983086 epoch total loss 5.70444727\n",
      "Trained batch 3212 batch loss 6.56493664 epoch total loss 5.70471525\n",
      "Trained batch 3213 batch loss 6.21187592 epoch total loss 5.70487261\n",
      "Trained batch 3214 batch loss 6.97217655 epoch total loss 5.70526695\n",
      "Trained batch 3215 batch loss 6.85062122 epoch total loss 5.70562363\n",
      "Trained batch 3216 batch loss 6.9213171 epoch total loss 5.70600176\n",
      "Trained batch 3217 batch loss 5.90068531 epoch total loss 5.70606232\n",
      "Trained batch 3218 batch loss 5.8196764 epoch total loss 5.7060976\n",
      "Trained batch 3219 batch loss 6.73642254 epoch total loss 5.70641804\n",
      "Trained batch 3220 batch loss 5.64200401 epoch total loss 5.70639801\n",
      "Trained batch 3221 batch loss 5.96908569 epoch total loss 5.70647955\n",
      "Trained batch 3222 batch loss 6.84910917 epoch total loss 5.70683432\n",
      "Trained batch 3223 batch loss 5.82235432 epoch total loss 5.70687\n",
      "Trained batch 3224 batch loss 5.70910263 epoch total loss 5.70687056\n",
      "Trained batch 3225 batch loss 5.26472569 epoch total loss 5.7067337\n",
      "Trained batch 3226 batch loss 5.83981895 epoch total loss 5.70677519\n",
      "Trained batch 3227 batch loss 6.28002596 epoch total loss 5.70695257\n",
      "Trained batch 3228 batch loss 6.17726326 epoch total loss 5.70709848\n",
      "Trained batch 3229 batch loss 5.89540958 epoch total loss 5.70715666\n",
      "Trained batch 3230 batch loss 5.92545557 epoch total loss 5.70722437\n",
      "Trained batch 3231 batch loss 5.96172047 epoch total loss 5.70730257\n",
      "Trained batch 3232 batch loss 5.93196583 epoch total loss 5.70737219\n",
      "Trained batch 3233 batch loss 6.81958532 epoch total loss 5.70771646\n",
      "Trained batch 3234 batch loss 5.65308285 epoch total loss 5.7076993\n",
      "Trained batch 3235 batch loss 6.34727859 epoch total loss 5.70789719\n",
      "Trained batch 3236 batch loss 5.64612675 epoch total loss 5.70787811\n",
      "Trained batch 3237 batch loss 5.37445068 epoch total loss 5.70777512\n",
      "Trained batch 3238 batch loss 5.92615604 epoch total loss 5.70784235\n",
      "Trained batch 3239 batch loss 6.38522291 epoch total loss 5.70805168\n",
      "Trained batch 3240 batch loss 6.72852421 epoch total loss 5.70836639\n",
      "Trained batch 3241 batch loss 6.0967226 epoch total loss 5.70848656\n",
      "Trained batch 3242 batch loss 5.76497412 epoch total loss 5.7085042\n",
      "Trained batch 3243 batch loss 5.85206223 epoch total loss 5.70854855\n",
      "Trained batch 3244 batch loss 4.36475849 epoch total loss 5.70813417\n",
      "Trained batch 3245 batch loss 5.75577641 epoch total loss 5.70814896\n",
      "Trained batch 3246 batch loss 5.90325975 epoch total loss 5.70820856\n",
      "Trained batch 3247 batch loss 5.61443138 epoch total loss 5.70818\n",
      "Trained batch 3248 batch loss 5.9075222 epoch total loss 5.70824194\n",
      "Trained batch 3249 batch loss 5.79526281 epoch total loss 5.70826864\n",
      "Trained batch 3250 batch loss 5.71359062 epoch total loss 5.7082696\n",
      "Trained batch 3251 batch loss 6.1566124 epoch total loss 5.7084074\n",
      "Trained batch 3252 batch loss 6.2524929 epoch total loss 5.70857477\n",
      "Trained batch 3253 batch loss 6.10469913 epoch total loss 5.70869684\n",
      "Trained batch 3254 batch loss 5.23334789 epoch total loss 5.70855045\n",
      "Trained batch 3255 batch loss 5.36065102 epoch total loss 5.70844364\n",
      "Trained batch 3256 batch loss 5.99771404 epoch total loss 5.70853281\n",
      "Trained batch 3257 batch loss 6.57036161 epoch total loss 5.70879745\n",
      "Trained batch 3258 batch loss 6.6509 epoch total loss 5.70908642\n",
      "Trained batch 3259 batch loss 6.17608309 epoch total loss 5.70922947\n",
      "Trained batch 3260 batch loss 5.80781 epoch total loss 5.70926\n",
      "Trained batch 3261 batch loss 4.86375427 epoch total loss 5.70900059\n",
      "Trained batch 3262 batch loss 5.92531729 epoch total loss 5.70906687\n",
      "Trained batch 3263 batch loss 5.65116501 epoch total loss 5.70904922\n",
      "Trained batch 3264 batch loss 5.89567661 epoch total loss 5.70910645\n",
      "Trained batch 3265 batch loss 4.72892475 epoch total loss 5.70880604\n",
      "Trained batch 3266 batch loss 5.34003973 epoch total loss 5.70869303\n",
      "Trained batch 3267 batch loss 6.07112598 epoch total loss 5.70880365\n",
      "Trained batch 3268 batch loss 5.97627783 epoch total loss 5.70888567\n",
      "Trained batch 3269 batch loss 6.64071035 epoch total loss 5.70917082\n",
      "Trained batch 3270 batch loss 6.34706402 epoch total loss 5.70936584\n",
      "Trained batch 3271 batch loss 6.17964029 epoch total loss 5.70951\n",
      "Trained batch 3272 batch loss 5.79448557 epoch total loss 5.70953608\n",
      "Trained batch 3273 batch loss 7.02165651 epoch total loss 5.70993662\n",
      "Trained batch 3274 batch loss 5.78743553 epoch total loss 5.70996046\n",
      "Trained batch 3275 batch loss 5.71705246 epoch total loss 5.70996237\n",
      "Trained batch 3276 batch loss 5.68841267 epoch total loss 5.70995569\n",
      "Trained batch 3277 batch loss 6.08609962 epoch total loss 5.71007\n",
      "Trained batch 3278 batch loss 5.92375 epoch total loss 5.71013546\n",
      "Trained batch 3279 batch loss 5.67388535 epoch total loss 5.71012449\n",
      "Trained batch 3280 batch loss 6.16805649 epoch total loss 5.71026421\n",
      "Trained batch 3281 batch loss 6.18062878 epoch total loss 5.71040726\n",
      "Trained batch 3282 batch loss 6.13607883 epoch total loss 5.71053696\n",
      "Trained batch 3283 batch loss 6.1963625 epoch total loss 5.71068525\n",
      "Trained batch 3284 batch loss 5.32046032 epoch total loss 5.71056652\n",
      "Trained batch 3285 batch loss 5.71261501 epoch total loss 5.710567\n",
      "Trained batch 3286 batch loss 5.07185173 epoch total loss 5.71037292\n",
      "Trained batch 3287 batch loss 5.7813139 epoch total loss 5.71039438\n",
      "Trained batch 3288 batch loss 5.02610588 epoch total loss 5.710186\n",
      "Trained batch 3289 batch loss 6.46013451 epoch total loss 5.71041441\n",
      "Trained batch 3290 batch loss 5.74749184 epoch total loss 5.71042585\n",
      "Trained batch 3291 batch loss 5.9742074 epoch total loss 5.71050596\n",
      "Trained batch 3292 batch loss 6.30115271 epoch total loss 5.71068525\n",
      "Trained batch 3293 batch loss 5.87642813 epoch total loss 5.7107358\n",
      "Trained batch 3294 batch loss 6.12755871 epoch total loss 5.71086216\n",
      "Trained batch 3295 batch loss 4.58651638 epoch total loss 5.71052074\n",
      "Trained batch 3296 batch loss 5.1826005 epoch total loss 5.71036053\n",
      "Trained batch 3297 batch loss 6.34025478 epoch total loss 5.71055126\n",
      "Trained batch 3298 batch loss 5.65698338 epoch total loss 5.71053457\n",
      "Trained batch 3299 batch loss 5.71009207 epoch total loss 5.71053505\n",
      "Trained batch 3300 batch loss 5.79702759 epoch total loss 5.71056128\n",
      "Trained batch 3301 batch loss 5.99572849 epoch total loss 5.71064758\n",
      "Trained batch 3302 batch loss 5.76098204 epoch total loss 5.71066284\n",
      "Trained batch 3303 batch loss 5.5647893 epoch total loss 5.71061897\n",
      "Trained batch 3304 batch loss 5.95978355 epoch total loss 5.71069384\n",
      "Trained batch 3305 batch loss 5.48691416 epoch total loss 5.71062613\n",
      "Trained batch 3306 batch loss 5.38089657 epoch total loss 5.71052647\n",
      "Trained batch 3307 batch loss 4.81964588 epoch total loss 5.71025705\n",
      "Trained batch 3308 batch loss 4.45532274 epoch total loss 5.70987749\n",
      "Trained batch 3309 batch loss 4.5134 epoch total loss 5.70951605\n",
      "Trained batch 3310 batch loss 4.97108412 epoch total loss 5.70929289\n",
      "Trained batch 3311 batch loss 4.62212229 epoch total loss 5.70896482\n",
      "Trained batch 3312 batch loss 4.78375673 epoch total loss 5.7086854\n",
      "Trained batch 3313 batch loss 5.84938335 epoch total loss 5.70872784\n",
      "Trained batch 3314 batch loss 6.11663103 epoch total loss 5.70885134\n",
      "Trained batch 3315 batch loss 5.57673025 epoch total loss 5.70881128\n",
      "Trained batch 3316 batch loss 5.05977821 epoch total loss 5.70861578\n",
      "Trained batch 3317 batch loss 6.15189838 epoch total loss 5.70874929\n",
      "Trained batch 3318 batch loss 6.12184906 epoch total loss 5.70887375\n",
      "Trained batch 3319 batch loss 6.10197783 epoch total loss 5.708992\n",
      "Trained batch 3320 batch loss 4.65694284 epoch total loss 5.70867491\n",
      "Trained batch 3321 batch loss 4.71577549 epoch total loss 5.70837545\n",
      "Trained batch 3322 batch loss 6.61545849 epoch total loss 5.70864868\n",
      "Trained batch 3323 batch loss 5.45221615 epoch total loss 5.70857191\n",
      "Trained batch 3324 batch loss 7.57703686 epoch total loss 5.70913363\n",
      "Trained batch 3325 batch loss 5.97671795 epoch total loss 5.70921421\n",
      "Trained batch 3326 batch loss 6.17671 epoch total loss 5.7093544\n",
      "Trained batch 3327 batch loss 5.97868586 epoch total loss 5.70943546\n",
      "Trained batch 3328 batch loss 5.2926116 epoch total loss 5.70931\n",
      "Trained batch 3329 batch loss 5.69926834 epoch total loss 5.70930719\n",
      "Trained batch 3330 batch loss 6.82807446 epoch total loss 5.70964289\n",
      "Trained batch 3331 batch loss 5.38403034 epoch total loss 5.70954561\n",
      "Trained batch 3332 batch loss 5.73703671 epoch total loss 5.70955372\n",
      "Trained batch 3333 batch loss 5.10458755 epoch total loss 5.70937252\n",
      "Trained batch 3334 batch loss 4.99334049 epoch total loss 5.70915794\n",
      "Trained batch 3335 batch loss 5.81740332 epoch total loss 5.70919037\n",
      "Trained batch 3336 batch loss 5.71011066 epoch total loss 5.70919085\n",
      "Trained batch 3337 batch loss 5.86628056 epoch total loss 5.70923853\n",
      "Trained batch 3338 batch loss 5.00828743 epoch total loss 5.70902824\n",
      "Trained batch 3339 batch loss 4.97523975 epoch total loss 5.70880842\n",
      "Trained batch 3340 batch loss 4.41035 epoch total loss 5.70841932\n",
      "Trained batch 3341 batch loss 5.02952194 epoch total loss 5.70821619\n",
      "Trained batch 3342 batch loss 4.85633659 epoch total loss 5.70796108\n",
      "Trained batch 3343 batch loss 4.90795517 epoch total loss 5.70772171\n",
      "Trained batch 3344 batch loss 5.47929764 epoch total loss 5.70765305\n",
      "Trained batch 3345 batch loss 5.16700268 epoch total loss 5.70749187\n",
      "Trained batch 3346 batch loss 5.27786064 epoch total loss 5.70736361\n",
      "Trained batch 3347 batch loss 5.46342087 epoch total loss 5.70729\n",
      "Trained batch 3348 batch loss 5.0589447 epoch total loss 5.70709658\n",
      "Trained batch 3349 batch loss 5.35100365 epoch total loss 5.70699024\n",
      "Trained batch 3350 batch loss 5.09345198 epoch total loss 5.70680714\n",
      "Trained batch 3351 batch loss 4.72789717 epoch total loss 5.70651531\n",
      "Trained batch 3352 batch loss 5.54310799 epoch total loss 5.70646667\n",
      "Trained batch 3353 batch loss 5.00840139 epoch total loss 5.7062583\n",
      "Trained batch 3354 batch loss 4.26079464 epoch total loss 5.70582771\n",
      "Trained batch 3355 batch loss 4.38320112 epoch total loss 5.70543337\n",
      "Trained batch 3356 batch loss 5.19128799 epoch total loss 5.7052803\n",
      "Trained batch 3357 batch loss 6.07990742 epoch total loss 5.70539188\n",
      "Trained batch 3358 batch loss 5.54120636 epoch total loss 5.70534277\n",
      "Trained batch 3359 batch loss 5.00038433 epoch total loss 5.70513296\n",
      "Trained batch 3360 batch loss 4.95740223 epoch total loss 5.70491028\n",
      "Trained batch 3361 batch loss 4.58908367 epoch total loss 5.7045784\n",
      "Trained batch 3362 batch loss 3.61292148 epoch total loss 5.70395613\n",
      "Trained batch 3363 batch loss 5.74807453 epoch total loss 5.70396948\n",
      "Trained batch 3364 batch loss 5.09214687 epoch total loss 5.70378733\n",
      "Trained batch 3365 batch loss 5.24883556 epoch total loss 5.70365191\n",
      "Trained batch 3366 batch loss 5.12782574 epoch total loss 5.70348072\n",
      "Trained batch 3367 batch loss 4.96735477 epoch total loss 5.70326185\n",
      "Trained batch 3368 batch loss 5.77702951 epoch total loss 5.70328379\n",
      "Trained batch 3369 batch loss 5.76797295 epoch total loss 5.70330286\n",
      "Trained batch 3370 batch loss 5.76116657 epoch total loss 5.7033205\n",
      "Trained batch 3371 batch loss 5.99537182 epoch total loss 5.70340729\n",
      "Trained batch 3372 batch loss 5.77213097 epoch total loss 5.70342731\n",
      "Trained batch 3373 batch loss 5.18621063 epoch total loss 5.70327377\n",
      "Trained batch 3374 batch loss 5.52560902 epoch total loss 5.70322132\n",
      "Trained batch 3375 batch loss 6.47389889 epoch total loss 5.70344973\n",
      "Trained batch 3376 batch loss 6.12029171 epoch total loss 5.70357323\n",
      "Trained batch 3377 batch loss 6.07170677 epoch total loss 5.70368242\n",
      "Trained batch 3378 batch loss 5.90328455 epoch total loss 5.70374155\n",
      "Trained batch 3379 batch loss 6.04568 epoch total loss 5.70384216\n",
      "Trained batch 3380 batch loss 5.82065964 epoch total loss 5.70387697\n",
      "Trained batch 3381 batch loss 5.62266636 epoch total loss 5.70385265\n",
      "Trained batch 3382 batch loss 6.36722851 epoch total loss 5.70404911\n",
      "Trained batch 3383 batch loss 5.71444798 epoch total loss 5.70405197\n",
      "Trained batch 3384 batch loss 6.1679697 epoch total loss 5.7041893\n",
      "Trained batch 3385 batch loss 5.69077206 epoch total loss 5.70418549\n",
      "Trained batch 3386 batch loss 5.59786654 epoch total loss 5.70415401\n",
      "Trained batch 3387 batch loss 5.42909336 epoch total loss 5.70407295\n",
      "Trained batch 3388 batch loss 5.95768261 epoch total loss 5.70414782\n",
      "Trained batch 3389 batch loss 6.14419365 epoch total loss 5.70427752\n",
      "Trained batch 3390 batch loss 5.59151554 epoch total loss 5.70424461\n",
      "Trained batch 3391 batch loss 5.15138435 epoch total loss 5.70408154\n",
      "Trained batch 3392 batch loss 6.05181503 epoch total loss 5.70418453\n",
      "Trained batch 3393 batch loss 5.4485445 epoch total loss 5.70410919\n",
      "Trained batch 3394 batch loss 5.48342323 epoch total loss 5.70404482\n",
      "Trained batch 3395 batch loss 4.5097971 epoch total loss 5.70369291\n",
      "Trained batch 3396 batch loss 4.52126789 epoch total loss 5.70334482\n",
      "Trained batch 3397 batch loss 5.40886688 epoch total loss 5.70325804\n",
      "Trained batch 3398 batch loss 5.41049623 epoch total loss 5.70317173\n",
      "Trained batch 3399 batch loss 5.2392869 epoch total loss 5.70303535\n",
      "Trained batch 3400 batch loss 5.39074135 epoch total loss 5.70294333\n",
      "Trained batch 3401 batch loss 5.4596467 epoch total loss 5.7028718\n",
      "Trained batch 3402 batch loss 4.97488928 epoch total loss 5.7026577\n",
      "Trained batch 3403 batch loss 4.95567894 epoch total loss 5.70243788\n",
      "Trained batch 3404 batch loss 4.93234062 epoch total loss 5.70221138\n",
      "Trained batch 3405 batch loss 4.7804594 epoch total loss 5.70194101\n",
      "Trained batch 3406 batch loss 5.73313618 epoch total loss 5.70195\n",
      "Trained batch 3407 batch loss 5.29629183 epoch total loss 5.70183134\n",
      "Trained batch 3408 batch loss 4.6977191 epoch total loss 5.70153618\n",
      "Trained batch 3409 batch loss 5.33470345 epoch total loss 5.70142841\n",
      "Trained batch 3410 batch loss 5.45638371 epoch total loss 5.70135689\n",
      "Trained batch 3411 batch loss 6.03535748 epoch total loss 5.70145464\n",
      "Trained batch 3412 batch loss 5.89757633 epoch total loss 5.70151234\n",
      "Trained batch 3413 batch loss 5.78857 epoch total loss 5.70153809\n",
      "Trained batch 3414 batch loss 5.41904974 epoch total loss 5.70145559\n",
      "Trained batch 3415 batch loss 5.74411392 epoch total loss 5.70146799\n",
      "Trained batch 3416 batch loss 5.87705517 epoch total loss 5.70151949\n",
      "Trained batch 3417 batch loss 6.79458284 epoch total loss 5.70183945\n",
      "Trained batch 3418 batch loss 5.8172946 epoch total loss 5.70187283\n",
      "Trained batch 3419 batch loss 6.12231 epoch total loss 5.70199633\n",
      "Trained batch 3420 batch loss 5.81513309 epoch total loss 5.70202923\n",
      "Trained batch 3421 batch loss 6.68646669 epoch total loss 5.70231676\n",
      "Trained batch 3422 batch loss 6.99134254 epoch total loss 5.70269346\n",
      "Trained batch 3423 batch loss 5.76109171 epoch total loss 5.70271063\n",
      "Trained batch 3424 batch loss 6.1072216 epoch total loss 5.70282888\n",
      "Trained batch 3425 batch loss 5.83385181 epoch total loss 5.70286703\n",
      "Trained batch 3426 batch loss 5.32107782 epoch total loss 5.70275545\n",
      "Trained batch 3427 batch loss 5.71379423 epoch total loss 5.70275831\n",
      "Trained batch 3428 batch loss 4.80532789 epoch total loss 5.70249653\n",
      "Trained batch 3429 batch loss 5.74002552 epoch total loss 5.7025075\n",
      "Trained batch 3430 batch loss 5.156147 epoch total loss 5.70234823\n",
      "Trained batch 3431 batch loss 5.62928581 epoch total loss 5.70232677\n",
      "Trained batch 3432 batch loss 5.23159695 epoch total loss 5.70219\n",
      "Trained batch 3433 batch loss 5.5148468 epoch total loss 5.70213556\n",
      "Trained batch 3434 batch loss 5.84227848 epoch total loss 5.70217609\n",
      "Trained batch 3435 batch loss 5.87845802 epoch total loss 5.70222759\n",
      "Trained batch 3436 batch loss 5.5217905 epoch total loss 5.70217514\n",
      "Trained batch 3437 batch loss 5.90276146 epoch total loss 5.70223331\n",
      "Trained batch 3438 batch loss 5.25489807 epoch total loss 5.70210361\n",
      "Trained batch 3439 batch loss 6.14367723 epoch total loss 5.70223236\n",
      "Trained batch 3440 batch loss 5.82660389 epoch total loss 5.70226812\n",
      "Trained batch 3441 batch loss 4.94175196 epoch total loss 5.70204735\n",
      "Trained batch 3442 batch loss 5.02945328 epoch total loss 5.70185184\n",
      "Trained batch 3443 batch loss 5.34781361 epoch total loss 5.70174885\n",
      "Trained batch 3444 batch loss 4.62197781 epoch total loss 5.70143509\n",
      "Trained batch 3445 batch loss 5.33414602 epoch total loss 5.70132828\n",
      "Trained batch 3446 batch loss 5.27958393 epoch total loss 5.70120573\n",
      "Trained batch 3447 batch loss 4.92797279 epoch total loss 5.70098162\n",
      "Trained batch 3448 batch loss 5.46793652 epoch total loss 5.70091391\n",
      "Trained batch 3449 batch loss 5.31188 epoch total loss 5.70080137\n",
      "Trained batch 3450 batch loss 5.00553465 epoch total loss 5.7006\n",
      "Trained batch 3451 batch loss 5.09419727 epoch total loss 5.70042419\n",
      "Trained batch 3452 batch loss 4.72031736 epoch total loss 5.70014048\n",
      "Trained batch 3453 batch loss 4.90994406 epoch total loss 5.69991159\n",
      "Trained batch 3454 batch loss 4.84416246 epoch total loss 5.69966364\n",
      "Trained batch 3455 batch loss 5.04076052 epoch total loss 5.6994729\n",
      "Trained batch 3456 batch loss 4.90485764 epoch total loss 5.69924307\n",
      "Trained batch 3457 batch loss 4.91591358 epoch total loss 5.69901657\n",
      "Trained batch 3458 batch loss 4.8652997 epoch total loss 5.69877529\n",
      "Trained batch 3459 batch loss 4.95132923 epoch total loss 5.69855928\n",
      "Trained batch 3460 batch loss 4.8316412 epoch total loss 5.69830894\n",
      "Trained batch 3461 batch loss 4.91111755 epoch total loss 5.69808102\n",
      "Trained batch 3462 batch loss 5.72701931 epoch total loss 5.69808912\n",
      "Trained batch 3463 batch loss 4.74245882 epoch total loss 5.69781303\n",
      "Trained batch 3464 batch loss 5.37652111 epoch total loss 5.69772053\n",
      "Trained batch 3465 batch loss 5.1501112 epoch total loss 5.69756269\n",
      "Trained batch 3466 batch loss 5.9900136 epoch total loss 5.69764709\n",
      "Trained batch 3467 batch loss 5.27142096 epoch total loss 5.69752407\n",
      "Trained batch 3468 batch loss 5.04985142 epoch total loss 5.69733763\n",
      "Trained batch 3469 batch loss 4.9191246 epoch total loss 5.69711351\n",
      "Trained batch 3470 batch loss 4.96482 epoch total loss 5.69690275\n",
      "Trained batch 3471 batch loss 4.17195892 epoch total loss 5.69646311\n",
      "Trained batch 3472 batch loss 4.77875423 epoch total loss 5.69619894\n",
      "Trained batch 3473 batch loss 5.15549612 epoch total loss 5.69604349\n",
      "Trained batch 3474 batch loss 5.38036108 epoch total loss 5.69595289\n",
      "Trained batch 3475 batch loss 5.13449192 epoch total loss 5.69579124\n",
      "Trained batch 3476 batch loss 4.67140675 epoch total loss 5.69549704\n",
      "Trained batch 3477 batch loss 4.94308376 epoch total loss 5.69528055\n",
      "Trained batch 3478 batch loss 5.34154844 epoch total loss 5.69517899\n",
      "Trained batch 3479 batch loss 4.68767643 epoch total loss 5.69488907\n",
      "Trained batch 3480 batch loss 4.68103123 epoch total loss 5.6945982\n",
      "Trained batch 3481 batch loss 4.49624777 epoch total loss 5.69425392\n",
      "Trained batch 3482 batch loss 4.36484432 epoch total loss 5.69387197\n",
      "Trained batch 3483 batch loss 4.6697073 epoch total loss 5.69357824\n",
      "Trained batch 3484 batch loss 4.85357571 epoch total loss 5.69333696\n",
      "Trained batch 3485 batch loss 4.10206604 epoch total loss 5.69288\n",
      "Trained batch 3486 batch loss 4.73854446 epoch total loss 5.69260645\n",
      "Trained batch 3487 batch loss 5.30881166 epoch total loss 5.6924963\n",
      "Trained batch 3488 batch loss 5.94001055 epoch total loss 5.69256687\n",
      "Trained batch 3489 batch loss 5.83629513 epoch total loss 5.69260836\n",
      "Trained batch 3490 batch loss 5.76033115 epoch total loss 5.69262743\n",
      "Trained batch 3491 batch loss 5.74402332 epoch total loss 5.69264221\n",
      "Trained batch 3492 batch loss 5.32384109 epoch total loss 5.69253683\n",
      "Trained batch 3493 batch loss 5.66922951 epoch total loss 5.69253\n",
      "Trained batch 3494 batch loss 6.05346298 epoch total loss 5.69263315\n",
      "Trained batch 3495 batch loss 6.4230957 epoch total loss 5.69284248\n",
      "Trained batch 3496 batch loss 5.57555246 epoch total loss 5.6928091\n",
      "Trained batch 3497 batch loss 5.81129265 epoch total loss 5.69284296\n",
      "Trained batch 3498 batch loss 5.9057107 epoch total loss 5.692904\n",
      "Trained batch 3499 batch loss 6.48448753 epoch total loss 5.69313\n",
      "Trained batch 3500 batch loss 6.11373806 epoch total loss 5.69325\n",
      "Trained batch 3501 batch loss 5.7706995 epoch total loss 5.69327211\n",
      "Trained batch 3502 batch loss 5.91096354 epoch total loss 5.6933341\n",
      "Trained batch 3503 batch loss 5.17808247 epoch total loss 5.69318724\n",
      "Trained batch 3504 batch loss 5.15617 epoch total loss 5.6930337\n",
      "Trained batch 3505 batch loss 5.38397598 epoch total loss 5.69294596\n",
      "Trained batch 3506 batch loss 6.15137577 epoch total loss 5.69307709\n",
      "Trained batch 3507 batch loss 5.28499651 epoch total loss 5.69296074\n",
      "Trained batch 3508 batch loss 5.55236626 epoch total loss 5.69292068\n",
      "Trained batch 3509 batch loss 5.39613438 epoch total loss 5.69283628\n",
      "Trained batch 3510 batch loss 6.11308861 epoch total loss 5.69295597\n",
      "Trained batch 3511 batch loss 5.56143093 epoch total loss 5.6929183\n",
      "Trained batch 3512 batch loss 5.66119194 epoch total loss 5.69290924\n",
      "Trained batch 3513 batch loss 5.77023554 epoch total loss 5.69293118\n",
      "Trained batch 3514 batch loss 5.7980361 epoch total loss 5.69296122\n",
      "Trained batch 3515 batch loss 5.2788949 epoch total loss 5.69284391\n",
      "Trained batch 3516 batch loss 5.96739626 epoch total loss 5.69292164\n",
      "Trained batch 3517 batch loss 5.79408693 epoch total loss 5.69295073\n",
      "Trained batch 3518 batch loss 5.66121149 epoch total loss 5.69294167\n",
      "Trained batch 3519 batch loss 5.75945234 epoch total loss 5.69296074\n",
      "Trained batch 3520 batch loss 5.4961586 epoch total loss 5.69290495\n",
      "Trained batch 3521 batch loss 6.68326855 epoch total loss 5.69318628\n",
      "Trained batch 3522 batch loss 6.18186188 epoch total loss 5.69332504\n",
      "Trained batch 3523 batch loss 5.44587612 epoch total loss 5.69325447\n",
      "Trained batch 3524 batch loss 5.94227552 epoch total loss 5.69332504\n",
      "Trained batch 3525 batch loss 6.24103689 epoch total loss 5.69348\n",
      "Trained batch 3526 batch loss 5.70461464 epoch total loss 5.69348335\n",
      "Trained batch 3527 batch loss 5.28731346 epoch total loss 5.69336843\n",
      "Trained batch 3528 batch loss 6.0811739 epoch total loss 5.69347858\n",
      "Trained batch 3529 batch loss 6.08185434 epoch total loss 5.69358873\n",
      "Trained batch 3530 batch loss 6.20104504 epoch total loss 5.69373226\n",
      "Trained batch 3531 batch loss 5.71520901 epoch total loss 5.69373846\n",
      "Trained batch 3532 batch loss 5.99777269 epoch total loss 5.69382429\n",
      "Trained batch 3533 batch loss 4.78926182 epoch total loss 5.69356823\n",
      "Trained batch 3534 batch loss 5.69544125 epoch total loss 5.69356871\n",
      "Trained batch 3535 batch loss 5.99620342 epoch total loss 5.69365454\n",
      "Trained batch 3536 batch loss 5.63385487 epoch total loss 5.69363785\n",
      "Trained batch 3537 batch loss 6.18257 epoch total loss 5.69377565\n",
      "Trained batch 3538 batch loss 5.94221878 epoch total loss 5.69384575\n",
      "Trained batch 3539 batch loss 6.12092447 epoch total loss 5.69396639\n",
      "Trained batch 3540 batch loss 5.74624634 epoch total loss 5.69398117\n",
      "Trained batch 3541 batch loss 6.4951849 epoch total loss 5.69420767\n",
      "Trained batch 3542 batch loss 6.02779675 epoch total loss 5.69430161\n",
      "Trained batch 3543 batch loss 6.27450085 epoch total loss 5.69446564\n",
      "Trained batch 3544 batch loss 5.68611717 epoch total loss 5.69446325\n",
      "Trained batch 3545 batch loss 6.04286289 epoch total loss 5.69456148\n",
      "Trained batch 3546 batch loss 6.26196671 epoch total loss 5.6947217\n",
      "Trained batch 3547 batch loss 5.52368069 epoch total loss 5.69467306\n",
      "Trained batch 3548 batch loss 6.45659256 epoch total loss 5.69488811\n",
      "Trained batch 3549 batch loss 6.09845161 epoch total loss 5.6950016\n",
      "Trained batch 3550 batch loss 6.49760723 epoch total loss 5.69522762\n",
      "Trained batch 3551 batch loss 6.06090307 epoch total loss 5.69533062\n",
      "Trained batch 3552 batch loss 5.50220299 epoch total loss 5.69527626\n",
      "Trained batch 3553 batch loss 5.94238329 epoch total loss 5.69534588\n",
      "Trained batch 3554 batch loss 6.1952157 epoch total loss 5.69548655\n",
      "Trained batch 3555 batch loss 6.16816711 epoch total loss 5.69561958\n",
      "Trained batch 3556 batch loss 5.23213196 epoch total loss 5.69548941\n",
      "Trained batch 3557 batch loss 5.75966311 epoch total loss 5.69550753\n",
      "Trained batch 3558 batch loss 4.77404976 epoch total loss 5.69524813\n",
      "Trained batch 3559 batch loss 5.35872746 epoch total loss 5.69515371\n",
      "Trained batch 3560 batch loss 5.25618076 epoch total loss 5.69503069\n",
      "Trained batch 3561 batch loss 5.4917717 epoch total loss 5.69497347\n",
      "Trained batch 3562 batch loss 5.27394199 epoch total loss 5.69485521\n",
      "Trained batch 3563 batch loss 5.53600883 epoch total loss 5.69481039\n",
      "Trained batch 3564 batch loss 5.73502731 epoch total loss 5.69482136\n",
      "Trained batch 3565 batch loss 5.25201893 epoch total loss 5.69469738\n",
      "Trained batch 3566 batch loss 5.45697403 epoch total loss 5.69463062\n",
      "Trained batch 3567 batch loss 5.20551443 epoch total loss 5.69449329\n",
      "Trained batch 3568 batch loss 6.00059128 epoch total loss 5.69457912\n",
      "Trained batch 3569 batch loss 5.17952299 epoch total loss 5.69443464\n",
      "Trained batch 3570 batch loss 5.60106754 epoch total loss 5.69440889\n",
      "Trained batch 3571 batch loss 5.28007364 epoch total loss 5.69429255\n",
      "Trained batch 3572 batch loss 5.73740911 epoch total loss 5.69430494\n",
      "Trained batch 3573 batch loss 5.84987497 epoch total loss 5.69434834\n",
      "Trained batch 3574 batch loss 6.00162172 epoch total loss 5.69443417\n",
      "Trained batch 3575 batch loss 6.67098713 epoch total loss 5.69470787\n",
      "Trained batch 3576 batch loss 6.29447556 epoch total loss 5.69487572\n",
      "Trained batch 3577 batch loss 6.18336821 epoch total loss 5.69501209\n",
      "Trained batch 3578 batch loss 5.64971972 epoch total loss 5.69499969\n",
      "Trained batch 3579 batch loss 5.76055813 epoch total loss 5.69501781\n",
      "Trained batch 3580 batch loss 5.21757412 epoch total loss 5.6948843\n",
      "Trained batch 3581 batch loss 6.21395302 epoch total loss 5.69502926\n",
      "Trained batch 3582 batch loss 7.6290741 epoch total loss 5.69556952\n",
      "Trained batch 3583 batch loss 6.38151312 epoch total loss 5.69576073\n",
      "Trained batch 3584 batch loss 6.9959321 epoch total loss 5.6961236\n",
      "Trained batch 3585 batch loss 5.44440079 epoch total loss 5.6960535\n",
      "Trained batch 3586 batch loss 5.89112234 epoch total loss 5.69610786\n",
      "Trained batch 3587 batch loss 5.09531784 epoch total loss 5.69594049\n",
      "Trained batch 3588 batch loss 4.97735262 epoch total loss 5.69573975\n",
      "Trained batch 3589 batch loss 5.54022789 epoch total loss 5.69569683\n",
      "Trained batch 3590 batch loss 5.40748739 epoch total loss 5.69561672\n",
      "Trained batch 3591 batch loss 6.50793695 epoch total loss 5.69584274\n",
      "Trained batch 3592 batch loss 5.4868021 epoch total loss 5.69578457\n",
      "Trained batch 3593 batch loss 5.79124928 epoch total loss 5.69581079\n",
      "Trained batch 3594 batch loss 5.17038345 epoch total loss 5.69566488\n",
      "Trained batch 3595 batch loss 5.07467318 epoch total loss 5.69549179\n",
      "Trained batch 3596 batch loss 5.25131941 epoch total loss 5.69536829\n",
      "Trained batch 3597 batch loss 5.57255363 epoch total loss 5.69533443\n",
      "Trained batch 3598 batch loss 6.19872093 epoch total loss 5.69547415\n",
      "Trained batch 3599 batch loss 5.65668154 epoch total loss 5.69546318\n",
      "Trained batch 3600 batch loss 5.63472462 epoch total loss 5.69544649\n",
      "Trained batch 3601 batch loss 5.87482452 epoch total loss 5.69549656\n",
      "Trained batch 3602 batch loss 5.46854639 epoch total loss 5.69543362\n",
      "Trained batch 3603 batch loss 5.21850729 epoch total loss 5.69530106\n",
      "Trained batch 3604 batch loss 5.39475679 epoch total loss 5.69521761\n",
      "Trained batch 3605 batch loss 5.40242767 epoch total loss 5.69513655\n",
      "Trained batch 3606 batch loss 5.38293791 epoch total loss 5.69505\n",
      "Trained batch 3607 batch loss 5.14925289 epoch total loss 5.69489813\n",
      "Trained batch 3608 batch loss 5.53697395 epoch total loss 5.69485474\n",
      "Trained batch 3609 batch loss 5.61330509 epoch total loss 5.69483185\n",
      "Trained batch 3610 batch loss 5.47096729 epoch total loss 5.69477\n",
      "Trained batch 3611 batch loss 5.71653032 epoch total loss 5.69477606\n",
      "Trained batch 3612 batch loss 5.5571 epoch total loss 5.69473791\n",
      "Trained batch 3613 batch loss 5.74991512 epoch total loss 5.69475317\n",
      "Trained batch 3614 batch loss 5.624403 epoch total loss 5.69473362\n",
      "Trained batch 3615 batch loss 5.79976606 epoch total loss 5.69476271\n",
      "Trained batch 3616 batch loss 5.41753197 epoch total loss 5.69468594\n",
      "Trained batch 3617 batch loss 5.51041889 epoch total loss 5.69463491\n",
      "Trained batch 3618 batch loss 5.5858984 epoch total loss 5.69460487\n",
      "Trained batch 3619 batch loss 5.3425827 epoch total loss 5.69450712\n",
      "Trained batch 3620 batch loss 5.87906504 epoch total loss 5.69455814\n",
      "Trained batch 3621 batch loss 5.33238935 epoch total loss 5.69445801\n",
      "Trained batch 3622 batch loss 4.62858295 epoch total loss 5.6941638\n",
      "Trained batch 3623 batch loss 5.41871166 epoch total loss 5.69408751\n",
      "Trained batch 3624 batch loss 5.96838617 epoch total loss 5.69416332\n",
      "Trained batch 3625 batch loss 5.79941654 epoch total loss 5.69419241\n",
      "Trained batch 3626 batch loss 5.78213167 epoch total loss 5.69421625\n",
      "Trained batch 3627 batch loss 5.42472267 epoch total loss 5.69414186\n",
      "Trained batch 3628 batch loss 5.3100462 epoch total loss 5.69403601\n",
      "Trained batch 3629 batch loss 6.24100113 epoch total loss 5.69418669\n",
      "Trained batch 3630 batch loss 5.41008949 epoch total loss 5.69410849\n",
      "Trained batch 3631 batch loss 5.520576 epoch total loss 5.6940608\n",
      "Trained batch 3632 batch loss 6.52573633 epoch total loss 5.69428968\n",
      "Trained batch 3633 batch loss 6.80136 epoch total loss 5.69459438\n",
      "Trained batch 3634 batch loss 5.67955112 epoch total loss 5.69459\n",
      "Trained batch 3635 batch loss 5.73193455 epoch total loss 5.69460058\n",
      "Trained batch 3636 batch loss 5.3589797 epoch total loss 5.69450855\n",
      "Trained batch 3637 batch loss 5.28768253 epoch total loss 5.6943965\n",
      "Trained batch 3638 batch loss 4.92924213 epoch total loss 5.69418621\n",
      "Trained batch 3639 batch loss 5.52761698 epoch total loss 5.69414043\n",
      "Trained batch 3640 batch loss 6.28352213 epoch total loss 5.69430208\n",
      "Trained batch 3641 batch loss 5.55978823 epoch total loss 5.69426537\n",
      "Trained batch 3642 batch loss 5.72331285 epoch total loss 5.69427299\n",
      "Trained batch 3643 batch loss 5.18324471 epoch total loss 5.6941328\n",
      "Trained batch 3644 batch loss 4.63536263 epoch total loss 5.69384241\n",
      "Trained batch 3645 batch loss 5.84617853 epoch total loss 5.6938839\n",
      "Trained batch 3646 batch loss 5.11237335 epoch total loss 5.69372463\n",
      "Trained batch 3647 batch loss 4.92498684 epoch total loss 5.69351435\n",
      "Trained batch 3648 batch loss 5.00778866 epoch total loss 5.693326\n",
      "Trained batch 3649 batch loss 5.44096279 epoch total loss 5.69325733\n",
      "Trained batch 3650 batch loss 6.3700943 epoch total loss 5.69344234\n",
      "Trained batch 3651 batch loss 6.16471195 epoch total loss 5.69357109\n",
      "Trained batch 3652 batch loss 5.42742348 epoch total loss 5.69349861\n",
      "Trained batch 3653 batch loss 6.37228298 epoch total loss 5.69368458\n",
      "Trained batch 3654 batch loss 5.63899422 epoch total loss 5.69366932\n",
      "Trained batch 3655 batch loss 5.82562733 epoch total loss 5.69370556\n",
      "Trained batch 3656 batch loss 5.59470844 epoch total loss 5.69367838\n",
      "Trained batch 3657 batch loss 5.21391 epoch total loss 5.69354725\n",
      "Trained batch 3658 batch loss 5.54387617 epoch total loss 5.69350624\n",
      "Trained batch 3659 batch loss 5.10853577 epoch total loss 5.6933465\n",
      "Trained batch 3660 batch loss 5.23600197 epoch total loss 5.69322157\n",
      "Trained batch 3661 batch loss 4.96243811 epoch total loss 5.69302225\n",
      "Trained batch 3662 batch loss 5.09095669 epoch total loss 5.69285822\n",
      "Trained batch 3663 batch loss 4.83861208 epoch total loss 5.69262457\n",
      "Trained batch 3664 batch loss 4.9831233 epoch total loss 5.69243097\n",
      "Trained batch 3665 batch loss 5.18464613 epoch total loss 5.69229269\n",
      "Trained batch 3666 batch loss 4.96472359 epoch total loss 5.69209385\n",
      "Trained batch 3667 batch loss 4.9612546 epoch total loss 5.69189453\n",
      "Trained batch 3668 batch loss 5.04233265 epoch total loss 5.69171762\n",
      "Trained batch 3669 batch loss 5.62246227 epoch total loss 5.69169903\n",
      "Trained batch 3670 batch loss 5.29785585 epoch total loss 5.69159222\n",
      "Trained batch 3671 batch loss 5.50988197 epoch total loss 5.69154263\n",
      "Trained batch 3672 batch loss 5.71750546 epoch total loss 5.6915493\n",
      "Trained batch 3673 batch loss 6.01725769 epoch total loss 5.69163799\n",
      "Trained batch 3674 batch loss 5.96224785 epoch total loss 5.6917119\n",
      "Trained batch 3675 batch loss 6.17774 epoch total loss 5.69184399\n",
      "Trained batch 3676 batch loss 6.52649403 epoch total loss 5.69207144\n",
      "Trained batch 3677 batch loss 6.20076513 epoch total loss 5.69220972\n",
      "Trained batch 3678 batch loss 6.11021805 epoch total loss 5.69232321\n",
      "Trained batch 3679 batch loss 6.26911068 epoch total loss 5.69248\n",
      "Trained batch 3680 batch loss 6.20964527 epoch total loss 5.69262075\n",
      "Trained batch 3681 batch loss 6.15302086 epoch total loss 5.69274569\n",
      "Trained batch 3682 batch loss 6.14075565 epoch total loss 5.69286728\n",
      "Trained batch 3683 batch loss 5.86225367 epoch total loss 5.69291306\n",
      "Trained batch 3684 batch loss 6.13211679 epoch total loss 5.69303226\n",
      "Trained batch 3685 batch loss 6.06138086 epoch total loss 5.69313192\n",
      "Trained batch 3686 batch loss 5.9625721 epoch total loss 5.69320536\n",
      "Trained batch 3687 batch loss 5.85457039 epoch total loss 5.69324923\n",
      "Trained batch 3688 batch loss 5.70655 epoch total loss 5.69325304\n",
      "Trained batch 3689 batch loss 6.28677654 epoch total loss 5.69341373\n",
      "Trained batch 3690 batch loss 5.76421213 epoch total loss 5.69343281\n",
      "Trained batch 3691 batch loss 5.33813047 epoch total loss 5.69333649\n",
      "Trained batch 3692 batch loss 6.2885046 epoch total loss 5.69349813\n",
      "Trained batch 3693 batch loss 5.82495499 epoch total loss 5.69353342\n",
      "Trained batch 3694 batch loss 5.95994473 epoch total loss 5.69360542\n",
      "Trained batch 3695 batch loss 6.03225279 epoch total loss 5.69369698\n",
      "Trained batch 3696 batch loss 5.53138542 epoch total loss 5.69365311\n",
      "Trained batch 3697 batch loss 5.97816277 epoch total loss 5.69373035\n",
      "Trained batch 3698 batch loss 5.99032593 epoch total loss 5.69381046\n",
      "Trained batch 3699 batch loss 5.67399216 epoch total loss 5.69380522\n",
      "Trained batch 3700 batch loss 5.86133957 epoch total loss 5.69385052\n",
      "Trained batch 3701 batch loss 5.58127546 epoch total loss 5.69382\n",
      "Trained batch 3702 batch loss 5.58304405 epoch total loss 5.69379044\n",
      "Trained batch 3703 batch loss 5.75253582 epoch total loss 5.69380617\n",
      "Trained batch 3704 batch loss 5.68300295 epoch total loss 5.69380331\n",
      "Trained batch 3705 batch loss 5.86026096 epoch total loss 5.69384813\n",
      "Trained batch 3706 batch loss 5.96524668 epoch total loss 5.69392109\n",
      "Trained batch 3707 batch loss 5.45409346 epoch total loss 5.69385624\n",
      "Trained batch 3708 batch loss 5.93134403 epoch total loss 5.69392\n",
      "Trained batch 3709 batch loss 5.44250679 epoch total loss 5.6938529\n",
      "Trained batch 3710 batch loss 5.45504189 epoch total loss 5.69378853\n",
      "Trained batch 3711 batch loss 5.91354418 epoch total loss 5.69384766\n",
      "Trained batch 3712 batch loss 5.39160824 epoch total loss 5.69376659\n",
      "Trained batch 3713 batch loss 5.6422348 epoch total loss 5.69375277\n",
      "Trained batch 3714 batch loss 5.84444809 epoch total loss 5.6937933\n",
      "Trained batch 3715 batch loss 5.58791733 epoch total loss 5.69376469\n",
      "Trained batch 3716 batch loss 6.23198128 epoch total loss 5.69390965\n",
      "Trained batch 3717 batch loss 5.56842422 epoch total loss 5.69387579\n",
      "Trained batch 3718 batch loss 6.06335592 epoch total loss 5.69397497\n",
      "Trained batch 3719 batch loss 5.47720337 epoch total loss 5.6939168\n",
      "Trained batch 3720 batch loss 5.73547649 epoch total loss 5.69392776\n",
      "Trained batch 3721 batch loss 6.07217932 epoch total loss 5.69403\n",
      "Trained batch 3722 batch loss 6.81658268 epoch total loss 5.69433117\n",
      "Trained batch 3723 batch loss 6.53000355 epoch total loss 5.69455528\n",
      "Trained batch 3724 batch loss 6.16857243 epoch total loss 5.6946826\n",
      "Trained batch 3725 batch loss 6.37700081 epoch total loss 5.6948657\n",
      "Trained batch 3726 batch loss 6.03952026 epoch total loss 5.69495821\n",
      "Trained batch 3727 batch loss 6.41928482 epoch total loss 5.69515276\n",
      "Trained batch 3728 batch loss 6.30215359 epoch total loss 5.69531584\n",
      "Trained batch 3729 batch loss 6.07642508 epoch total loss 5.69541788\n",
      "Trained batch 3730 batch loss 6.09726334 epoch total loss 5.69552565\n",
      "Trained batch 3731 batch loss 6.23019314 epoch total loss 5.69566917\n",
      "Trained batch 3732 batch loss 6.02516413 epoch total loss 5.69575739\n",
      "Trained batch 3733 batch loss 5.80450106 epoch total loss 5.69578648\n",
      "Trained batch 3734 batch loss 5.58590126 epoch total loss 5.69575691\n",
      "Trained batch 3735 batch loss 5.25329256 epoch total loss 5.69563866\n",
      "Trained batch 3736 batch loss 5.5352335 epoch total loss 5.69559574\n",
      "Trained batch 3737 batch loss 5.41172791 epoch total loss 5.69552\n",
      "Trained batch 3738 batch loss 5.04433823 epoch total loss 5.69534588\n",
      "Trained batch 3739 batch loss 5.60705376 epoch total loss 5.69532251\n",
      "Trained batch 3740 batch loss 5.28171062 epoch total loss 5.69521189\n",
      "Trained batch 3741 batch loss 5.0983777 epoch total loss 5.69505215\n",
      "Trained batch 3742 batch loss 6.25551796 epoch total loss 5.69520187\n",
      "Trained batch 3743 batch loss 6.11734676 epoch total loss 5.69531441\n",
      "Trained batch 3744 batch loss 5.4446311 epoch total loss 5.69524765\n",
      "Trained batch 3745 batch loss 5.68332577 epoch total loss 5.69524479\n",
      "Trained batch 3746 batch loss 5.71491051 epoch total loss 5.69525\n",
      "Trained batch 3747 batch loss 4.90781593 epoch total loss 5.69503975\n",
      "Trained batch 3748 batch loss 5.86965942 epoch total loss 5.69508648\n",
      "Trained batch 3749 batch loss 5.65292931 epoch total loss 5.69507504\n",
      "Trained batch 3750 batch loss 5.69457436 epoch total loss 5.69507504\n",
      "Trained batch 3751 batch loss 6.38856697 epoch total loss 5.69526\n",
      "Trained batch 3752 batch loss 5.74912548 epoch total loss 5.69527435\n",
      "Trained batch 3753 batch loss 6.03518534 epoch total loss 5.69536495\n",
      "Trained batch 3754 batch loss 5.00345421 epoch total loss 5.69518089\n",
      "Trained batch 3755 batch loss 5.62805557 epoch total loss 5.69516325\n",
      "Trained batch 3756 batch loss 5.31659555 epoch total loss 5.69506216\n",
      "Trained batch 3757 batch loss 5.86836433 epoch total loss 5.69510889\n",
      "Trained batch 3758 batch loss 5.17908859 epoch total loss 5.69497156\n",
      "Trained batch 3759 batch loss 5.78434896 epoch total loss 5.6949954\n",
      "Trained batch 3760 batch loss 5.7691617 epoch total loss 5.69501543\n",
      "Trained batch 3761 batch loss 5.19131136 epoch total loss 5.69488144\n",
      "Trained batch 3762 batch loss 6.06669188 epoch total loss 5.69498\n",
      "Trained batch 3763 batch loss 5.78924 epoch total loss 5.69500542\n",
      "Trained batch 3764 batch loss 4.27250195 epoch total loss 5.69462776\n",
      "Trained batch 3765 batch loss 4.31261539 epoch total loss 5.6942606\n",
      "Trained batch 3766 batch loss 3.95833206 epoch total loss 5.6937995\n",
      "Trained batch 3767 batch loss 4.45131826 epoch total loss 5.69347\n",
      "Trained batch 3768 batch loss 4.14706659 epoch total loss 5.69305944\n",
      "Trained batch 3769 batch loss 4.95788479 epoch total loss 5.69286394\n",
      "Trained batch 3770 batch loss 5.87270832 epoch total loss 5.69291162\n",
      "Trained batch 3771 batch loss 6.18917751 epoch total loss 5.69304323\n",
      "Trained batch 3772 batch loss 4.78416395 epoch total loss 5.69280243\n",
      "Trained batch 3773 batch loss 3.80528498 epoch total loss 5.69230175\n",
      "Trained batch 3774 batch loss 5.00868797 epoch total loss 5.69212055\n",
      "Trained batch 3775 batch loss 6.08572674 epoch total loss 5.69222498\n",
      "Trained batch 3776 batch loss 6.17253065 epoch total loss 5.69235182\n",
      "Trained batch 3777 batch loss 6.69811201 epoch total loss 5.69261789\n",
      "Trained batch 3778 batch loss 5.53937626 epoch total loss 5.69257736\n",
      "Trained batch 3779 batch loss 5.84308147 epoch total loss 5.69261742\n",
      "Trained batch 3780 batch loss 5.13425636 epoch total loss 5.6924696\n",
      "Trained batch 3781 batch loss 6.02228451 epoch total loss 5.69255686\n",
      "Trained batch 3782 batch loss 6.39016581 epoch total loss 5.69274139\n",
      "Trained batch 3783 batch loss 5.42035866 epoch total loss 5.69266891\n",
      "Trained batch 3784 batch loss 5.96279907 epoch total loss 5.69274044\n",
      "Trained batch 3785 batch loss 5.52077579 epoch total loss 5.69269514\n",
      "Trained batch 3786 batch loss 6.43001699 epoch total loss 5.69288969\n",
      "Trained batch 3787 batch loss 5.63969707 epoch total loss 5.69287634\n",
      "Trained batch 3788 batch loss 4.96041 epoch total loss 5.69268274\n",
      "Trained batch 3789 batch loss 5.32870388 epoch total loss 5.6925869\n",
      "Trained batch 3790 batch loss 5.66926908 epoch total loss 5.6925807\n",
      "Trained batch 3791 batch loss 5.78275681 epoch total loss 5.69260454\n",
      "Trained batch 3792 batch loss 5.60299873 epoch total loss 5.69258118\n",
      "Trained batch 3793 batch loss 5.71972656 epoch total loss 5.69258833\n",
      "Trained batch 3794 batch loss 5.31985044 epoch total loss 5.69249058\n",
      "Trained batch 3795 batch loss 6.25236797 epoch total loss 5.69263792\n",
      "Trained batch 3796 batch loss 5.50824 epoch total loss 5.69258928\n",
      "Trained batch 3797 batch loss 5.56987381 epoch total loss 5.69255686\n",
      "Trained batch 3798 batch loss 5.96253967 epoch total loss 5.69262791\n",
      "Trained batch 3799 batch loss 6.04682541 epoch total loss 5.69272137\n",
      "Trained batch 3800 batch loss 5.66062641 epoch total loss 5.69271278\n",
      "Trained batch 3801 batch loss 4.90292549 epoch total loss 5.69250488\n",
      "Trained batch 3802 batch loss 4.90290546 epoch total loss 5.69229698\n",
      "Trained batch 3803 batch loss 5.33369589 epoch total loss 5.69220257\n",
      "Trained batch 3804 batch loss 5.15520048 epoch total loss 5.69206142\n",
      "Trained batch 3805 batch loss 5.27148294 epoch total loss 5.6919508\n",
      "Trained batch 3806 batch loss 5.24946404 epoch total loss 5.69183493\n",
      "Trained batch 3807 batch loss 4.95538568 epoch total loss 5.69164133\n",
      "Trained batch 3808 batch loss 4.31631565 epoch total loss 5.69128\n",
      "Trained batch 3809 batch loss 5.20708 epoch total loss 5.69115305\n",
      "Trained batch 3810 batch loss 5.62788773 epoch total loss 5.69113588\n",
      "Trained batch 3811 batch loss 5.88853264 epoch total loss 5.69118786\n",
      "Trained batch 3812 batch loss 4.82291126 epoch total loss 5.69096\n",
      "Trained batch 3813 batch loss 5.54740906 epoch total loss 5.69092226\n",
      "Trained batch 3814 batch loss 6.12400866 epoch total loss 5.69103527\n",
      "Trained batch 3815 batch loss 5.4021759 epoch total loss 5.69096\n",
      "Trained batch 3816 batch loss 4.56079435 epoch total loss 5.69066381\n",
      "Trained batch 3817 batch loss 5.73650932 epoch total loss 5.69067574\n",
      "Trained batch 3818 batch loss 5.08458138 epoch total loss 5.69051647\n",
      "Trained batch 3819 batch loss 5.83362913 epoch total loss 5.69055414\n",
      "Trained batch 3820 batch loss 6.95621347 epoch total loss 5.69088554\n",
      "Trained batch 3821 batch loss 4.69436646 epoch total loss 5.69062519\n",
      "Trained batch 3822 batch loss 6.50560856 epoch total loss 5.69083834\n",
      "Trained batch 3823 batch loss 6.29767466 epoch total loss 5.69099712\n",
      "Trained batch 3824 batch loss 5.71769905 epoch total loss 5.6910038\n",
      "Trained batch 3825 batch loss 5.2782979 epoch total loss 5.69089556\n",
      "Trained batch 3826 batch loss 5.83101273 epoch total loss 5.6909318\n",
      "Trained batch 3827 batch loss 6.32271862 epoch total loss 5.69109678\n",
      "Trained batch 3828 batch loss 6.09913731 epoch total loss 5.69120359\n",
      "Trained batch 3829 batch loss 6.05470562 epoch total loss 5.69129848\n",
      "Trained batch 3830 batch loss 5.36698532 epoch total loss 5.69121408\n",
      "Trained batch 3831 batch loss 5.97072411 epoch total loss 5.69128704\n",
      "Trained batch 3832 batch loss 6.47676468 epoch total loss 5.69149208\n",
      "Trained batch 3833 batch loss 5.60485172 epoch total loss 5.69146967\n",
      "Trained batch 3834 batch loss 5.6705265 epoch total loss 5.69146395\n",
      "Trained batch 3835 batch loss 5.62698603 epoch total loss 5.69144678\n",
      "Trained batch 3836 batch loss 5.63352919 epoch total loss 5.69143152\n",
      "Trained batch 3837 batch loss 5.8374424 epoch total loss 5.69146967\n",
      "Trained batch 3838 batch loss 5.71000481 epoch total loss 5.69147491\n",
      "Trained batch 3839 batch loss 5.82045317 epoch total loss 5.69150829\n",
      "Trained batch 3840 batch loss 5.88286114 epoch total loss 5.69155836\n",
      "Trained batch 3841 batch loss 5.74271107 epoch total loss 5.69157171\n",
      "Trained batch 3842 batch loss 6.01173 epoch total loss 5.69165468\n",
      "Trained batch 3843 batch loss 6.18500519 epoch total loss 5.69178343\n",
      "Trained batch 3844 batch loss 5.86801386 epoch total loss 5.6918292\n",
      "Trained batch 3845 batch loss 5.93254471 epoch total loss 5.69189119\n",
      "Trained batch 3846 batch loss 6.3085227 epoch total loss 5.69205189\n",
      "Trained batch 3847 batch loss 6.40227127 epoch total loss 5.69223642\n",
      "Trained batch 3848 batch loss 6.01779079 epoch total loss 5.69232082\n",
      "Trained batch 3849 batch loss 5.71721315 epoch total loss 5.69232702\n",
      "Trained batch 3850 batch loss 6.02061749 epoch total loss 5.69241285\n",
      "Trained batch 3851 batch loss 5.25967789 epoch total loss 5.69230032\n",
      "Trained batch 3852 batch loss 5.59296322 epoch total loss 5.69227505\n",
      "Trained batch 3853 batch loss 6.1397419 epoch total loss 5.6923914\n",
      "Trained batch 3854 batch loss 5.77619 epoch total loss 5.69241285\n",
      "Trained batch 3855 batch loss 6.12109423 epoch total loss 5.69252396\n",
      "Trained batch 3856 batch loss 5.87634563 epoch total loss 5.69257164\n",
      "Trained batch 3857 batch loss 5.85077095 epoch total loss 5.69261312\n",
      "Trained batch 3858 batch loss 6.34012222 epoch total loss 5.69278049\n",
      "Trained batch 3859 batch loss 5.84124708 epoch total loss 5.69281912\n",
      "Trained batch 3860 batch loss 5.24266243 epoch total loss 5.69270277\n",
      "Trained batch 3861 batch loss 5.83259821 epoch total loss 5.69273853\n",
      "Trained batch 3862 batch loss 6.05009508 epoch total loss 5.69283152\n",
      "Trained batch 3863 batch loss 6.00525284 epoch total loss 5.69291258\n",
      "Trained batch 3864 batch loss 5.10638237 epoch total loss 5.69276047\n",
      "Trained batch 3865 batch loss 5.3190217 epoch total loss 5.69266367\n",
      "Trained batch 3866 batch loss 5.73606396 epoch total loss 5.69267464\n",
      "Trained batch 3867 batch loss 5.8002677 epoch total loss 5.69270277\n",
      "Trained batch 3868 batch loss 5.19550133 epoch total loss 5.69257402\n",
      "Trained batch 3869 batch loss 5.55643749 epoch total loss 5.69253922\n",
      "Trained batch 3870 batch loss 5.98927927 epoch total loss 5.69261599\n",
      "Trained batch 3871 batch loss 5.59751415 epoch total loss 5.69259167\n",
      "Trained batch 3872 batch loss 6.46380615 epoch total loss 5.69279051\n",
      "Trained batch 3873 batch loss 6.11133862 epoch total loss 5.69289827\n",
      "Trained batch 3874 batch loss 5.71727848 epoch total loss 5.69290447\n",
      "Trained batch 3875 batch loss 6.01312447 epoch total loss 5.69298744\n",
      "Trained batch 3876 batch loss 5.8745327 epoch total loss 5.69303417\n",
      "Trained batch 3877 batch loss 5.79116249 epoch total loss 5.69305944\n",
      "Trained batch 3878 batch loss 6.09353638 epoch total loss 5.69316292\n",
      "Trained batch 3879 batch loss 6.08106852 epoch total loss 5.69326305\n",
      "Trained batch 3880 batch loss 5.89129066 epoch total loss 5.69331408\n",
      "Trained batch 3881 batch loss 5.81615973 epoch total loss 5.69334602\n",
      "Trained batch 3882 batch loss 5.59427929 epoch total loss 5.69332027\n",
      "Trained batch 3883 batch loss 5.77387333 epoch total loss 5.69334078\n",
      "Trained batch 3884 batch loss 5.97316647 epoch total loss 5.69341278\n",
      "Trained batch 3885 batch loss 6.15550947 epoch total loss 5.69353199\n",
      "Trained batch 3886 batch loss 6.69980812 epoch total loss 5.69379044\n",
      "Trained batch 3887 batch loss 5.61020851 epoch total loss 5.69376898\n",
      "Trained batch 3888 batch loss 5.29347038 epoch total loss 5.69366598\n",
      "Trained batch 3889 batch loss 5.84187746 epoch total loss 5.69370413\n",
      "Trained batch 3890 batch loss 5.40938807 epoch total loss 5.69363117\n",
      "Trained batch 3891 batch loss 5.77855825 epoch total loss 5.69365311\n",
      "Trained batch 3892 batch loss 6.17824364 epoch total loss 5.69377756\n",
      "Trained batch 3893 batch loss 5.80754852 epoch total loss 5.69380617\n",
      "Trained batch 3894 batch loss 5.67109299 epoch total loss 5.69380093\n",
      "Trained batch 3895 batch loss 5.46693707 epoch total loss 5.69374228\n",
      "Trained batch 3896 batch loss 5.66421127 epoch total loss 5.69373465\n",
      "Trained batch 3897 batch loss 5.706357 epoch total loss 5.69373846\n",
      "Trained batch 3898 batch loss 5.82931042 epoch total loss 5.69377327\n",
      "Trained batch 3899 batch loss 5.58197975 epoch total loss 5.69374466\n",
      "Trained batch 3900 batch loss 6.04819679 epoch total loss 5.69383574\n",
      "Trained batch 3901 batch loss 5.6194973 epoch total loss 5.69381666\n",
      "Trained batch 3902 batch loss 5.25787592 epoch total loss 5.69370461\n",
      "Trained batch 3903 batch loss 5.46660805 epoch total loss 5.69364643\n",
      "Trained batch 3904 batch loss 5.90705585 epoch total loss 5.69370127\n",
      "Trained batch 3905 batch loss 5.64395332 epoch total loss 5.69368839\n",
      "Trained batch 3906 batch loss 5.85060215 epoch total loss 5.69372892\n",
      "Trained batch 3907 batch loss 5.70380592 epoch total loss 5.69373131\n",
      "Trained batch 3908 batch loss 6.05074692 epoch total loss 5.69382286\n",
      "Trained batch 3909 batch loss 6.01782417 epoch total loss 5.69390535\n",
      "Trained batch 3910 batch loss 6.18798065 epoch total loss 5.69403172\n",
      "Trained batch 3911 batch loss 5.97713375 epoch total loss 5.69410419\n",
      "Trained batch 3912 batch loss 5.79064274 epoch total loss 5.69412851\n",
      "Trained batch 3913 batch loss 5.81439161 epoch total loss 5.69415951\n",
      "Trained batch 3914 batch loss 5.37151146 epoch total loss 5.69407701\n",
      "Trained batch 3915 batch loss 5.62743 epoch total loss 5.69406\n",
      "Trained batch 3916 batch loss 5.97743607 epoch total loss 5.69413185\n",
      "Trained batch 3917 batch loss 4.86077785 epoch total loss 5.69391918\n",
      "Trained batch 3918 batch loss 5.79254532 epoch total loss 5.69394445\n",
      "Trained batch 3919 batch loss 5.97387505 epoch total loss 5.69401646\n",
      "Trained batch 3920 batch loss 5.366539 epoch total loss 5.69393301\n",
      "Trained batch 3921 batch loss 5.36558104 epoch total loss 5.69384909\n",
      "Trained batch 3922 batch loss 5.45321655 epoch total loss 5.69378757\n",
      "Trained batch 3923 batch loss 5.35547256 epoch total loss 5.69370127\n",
      "Trained batch 3924 batch loss 6.03682137 epoch total loss 5.69378901\n",
      "Trained batch 3925 batch loss 5.42453 epoch total loss 5.69372034\n",
      "Trained batch 3926 batch loss 5.31354237 epoch total loss 5.69362354\n",
      "Trained batch 3927 batch loss 5.67225266 epoch total loss 5.69361782\n",
      "Trained batch 3928 batch loss 5.74643183 epoch total loss 5.69363117\n",
      "Trained batch 3929 batch loss 5.31319857 epoch total loss 5.69353437\n",
      "Trained batch 3930 batch loss 5.2422142 epoch total loss 5.69341946\n",
      "Trained batch 3931 batch loss 5.43121862 epoch total loss 5.6933527\n",
      "Trained batch 3932 batch loss 5.85949135 epoch total loss 5.69339514\n",
      "Trained batch 3933 batch loss 5.12020445 epoch total loss 5.6932497\n",
      "Trained batch 3934 batch loss 5.73778152 epoch total loss 5.69326115\n",
      "Trained batch 3935 batch loss 5.58114052 epoch total loss 5.69323301\n",
      "Trained batch 3936 batch loss 5.9410634 epoch total loss 5.69329596\n",
      "Trained batch 3937 batch loss 5.12743092 epoch total loss 5.69315195\n",
      "Trained batch 3938 batch loss 5.40728 epoch total loss 5.69307947\n",
      "Trained batch 3939 batch loss 5.64791775 epoch total loss 5.6930685\n",
      "Trained batch 3940 batch loss 5.6004324 epoch total loss 5.69304466\n",
      "Trained batch 3941 batch loss 5.87666416 epoch total loss 5.69309139\n",
      "Trained batch 3942 batch loss 5.44144058 epoch total loss 5.6930275\n",
      "Trained batch 3943 batch loss 5.32583761 epoch total loss 5.69293451\n",
      "Trained batch 3944 batch loss 4.64182711 epoch total loss 5.69266796\n",
      "Trained batch 3945 batch loss 5.41599131 epoch total loss 5.69259787\n",
      "Trained batch 3946 batch loss 4.98179293 epoch total loss 5.6924181\n",
      "Trained batch 3947 batch loss 4.33054733 epoch total loss 5.69207287\n",
      "Trained batch 3948 batch loss 6.19919491 epoch total loss 5.69220114\n",
      "Trained batch 3949 batch loss 5.64856672 epoch total loss 5.69219\n",
      "Trained batch 3950 batch loss 5.42513275 epoch total loss 5.69212294\n",
      "Trained batch 3951 batch loss 5.86227751 epoch total loss 5.69216537\n",
      "Trained batch 3952 batch loss 5.24594116 epoch total loss 5.69205284\n",
      "Trained batch 3953 batch loss 5.34492064 epoch total loss 5.6919651\n",
      "Trained batch 3954 batch loss 5.55898428 epoch total loss 5.69193125\n",
      "Trained batch 3955 batch loss 5.51643276 epoch total loss 5.6918869\n",
      "Trained batch 3956 batch loss 6.09900665 epoch total loss 5.69199\n",
      "Trained batch 3957 batch loss 5.83592415 epoch total loss 5.69202614\n",
      "Trained batch 3958 batch loss 5.46738052 epoch total loss 5.69196939\n",
      "Trained batch 3959 batch loss 5.70420504 epoch total loss 5.69197273\n",
      "Trained batch 3960 batch loss 6.08316135 epoch total loss 5.69207144\n",
      "Trained batch 3961 batch loss 5.91204166 epoch total loss 5.69212723\n",
      "Trained batch 3962 batch loss 5.32029343 epoch total loss 5.69203329\n",
      "Trained batch 3963 batch loss 5.1718092 epoch total loss 5.69190216\n",
      "Trained batch 3964 batch loss 5.68822575 epoch total loss 5.69190073\n",
      "Trained batch 3965 batch loss 5.80490303 epoch total loss 5.69192934\n",
      "Trained batch 3966 batch loss 5.88456106 epoch total loss 5.69197798\n",
      "Trained batch 3967 batch loss 6.02570724 epoch total loss 5.6920619\n",
      "Trained batch 3968 batch loss 6.16768122 epoch total loss 5.69218206\n",
      "Trained batch 3969 batch loss 6.05763245 epoch total loss 5.69227409\n",
      "Trained batch 3970 batch loss 5.5744524 epoch total loss 5.69224453\n",
      "Trained batch 3971 batch loss 5.62378263 epoch total loss 5.69222736\n",
      "Trained batch 3972 batch loss 5.50698662 epoch total loss 5.69218063\n",
      "Trained batch 3973 batch loss 5.2179327 epoch total loss 5.69206142\n",
      "Trained batch 3974 batch loss 5.28234 epoch total loss 5.6919589\n",
      "Trained batch 3975 batch loss 4.95881939 epoch total loss 5.69177437\n",
      "Trained batch 3976 batch loss 6.08251476 epoch total loss 5.6918726\n",
      "Trained batch 3977 batch loss 5.96462345 epoch total loss 5.69194126\n",
      "Trained batch 3978 batch loss 5.86302757 epoch total loss 5.69198418\n",
      "Trained batch 3979 batch loss 6.20501328 epoch total loss 5.69211292\n",
      "Trained batch 3980 batch loss 5.77164173 epoch total loss 5.69213295\n",
      "Trained batch 3981 batch loss 6.02633095 epoch total loss 5.69221687\n",
      "Trained batch 3982 batch loss 6.09131241 epoch total loss 5.69231701\n",
      "Trained batch 3983 batch loss 5.13457775 epoch total loss 5.6921773\n",
      "Trained batch 3984 batch loss 4.7523 epoch total loss 5.69194126\n",
      "Trained batch 3985 batch loss 5.91764402 epoch total loss 5.691998\n",
      "Trained batch 3986 batch loss 5.78460073 epoch total loss 5.69202137\n",
      "Trained batch 3987 batch loss 7.14654541 epoch total loss 5.69238615\n",
      "Trained batch 3988 batch loss 6.01436806 epoch total loss 5.69246674\n",
      "Trained batch 3989 batch loss 6.53363514 epoch total loss 5.6926775\n",
      "Trained batch 3990 batch loss 4.58101654 epoch total loss 5.69239855\n",
      "Trained batch 3991 batch loss 6.49038696 epoch total loss 5.69259834\n",
      "Trained batch 3992 batch loss 5.80124092 epoch total loss 5.69262552\n",
      "Trained batch 3993 batch loss 6.05224323 epoch total loss 5.69271564\n",
      "Trained batch 3994 batch loss 5.4210434 epoch total loss 5.69264793\n",
      "Trained batch 3995 batch loss 6.5747714 epoch total loss 5.69286871\n",
      "Trained batch 3996 batch loss 6.31238413 epoch total loss 5.69302368\n",
      "Trained batch 3997 batch loss 6.2980957 epoch total loss 5.69317532\n",
      "Trained batch 3998 batch loss 5.98553133 epoch total loss 5.69324827\n",
      "Trained batch 3999 batch loss 6.5965538 epoch total loss 5.69347429\n",
      "Trained batch 4000 batch loss 6.31643963 epoch total loss 5.69362974\n",
      "Trained batch 4001 batch loss 5.91610479 epoch total loss 5.69368553\n",
      "Trained batch 4002 batch loss 6.23579597 epoch total loss 5.69382095\n",
      "Trained batch 4003 batch loss 5.98218155 epoch total loss 5.69389296\n",
      "Trained batch 4004 batch loss 5.23177862 epoch total loss 5.69377804\n",
      "Trained batch 4005 batch loss 4.76202488 epoch total loss 5.69354534\n",
      "Trained batch 4006 batch loss 4.41584826 epoch total loss 5.69322634\n",
      "Trained batch 4007 batch loss 5.883008 epoch total loss 5.69327354\n",
      "Trained batch 4008 batch loss 5.04070044 epoch total loss 5.69311094\n",
      "Trained batch 4009 batch loss 5.27262878 epoch total loss 5.69300604\n",
      "Trained batch 4010 batch loss 5.76720238 epoch total loss 5.69302464\n",
      "Trained batch 4011 batch loss 4.5804925 epoch total loss 5.69274712\n",
      "Trained batch 4012 batch loss 6.48138809 epoch total loss 5.69294357\n",
      "Trained batch 4013 batch loss 6.21597958 epoch total loss 5.69307423\n",
      "Trained batch 4014 batch loss 6.80705643 epoch total loss 5.69335175\n",
      "Trained batch 4015 batch loss 6.31938171 epoch total loss 5.69350767\n",
      "Trained batch 4016 batch loss 5.31405735 epoch total loss 5.69341326\n",
      "Trained batch 4017 batch loss 5.9091177 epoch total loss 5.69346666\n",
      "Trained batch 4018 batch loss 4.9174161 epoch total loss 5.69327402\n",
      "Trained batch 4019 batch loss 4.97234917 epoch total loss 5.69309473\n",
      "Trained batch 4020 batch loss 4.77701235 epoch total loss 5.6928668\n",
      "Trained batch 4021 batch loss 5.16566277 epoch total loss 5.69273567\n",
      "Trained batch 4022 batch loss 4.50904179 epoch total loss 5.69244146\n",
      "Trained batch 4023 batch loss 3.92266703 epoch total loss 5.69200134\n",
      "Trained batch 4024 batch loss 4.44590235 epoch total loss 5.69169188\n",
      "Trained batch 4025 batch loss 5.20468569 epoch total loss 5.69157076\n",
      "Trained batch 4026 batch loss 5.34764862 epoch total loss 5.6914854\n",
      "Trained batch 4027 batch loss 4.69670582 epoch total loss 5.6912384\n",
      "Trained batch 4028 batch loss 5.81155396 epoch total loss 5.69126844\n",
      "Trained batch 4029 batch loss 4.74618101 epoch total loss 5.69103384\n",
      "Trained batch 4030 batch loss 6.30765438 epoch total loss 5.69118738\n",
      "Trained batch 4031 batch loss 5.17085791 epoch total loss 5.69105768\n",
      "Trained batch 4032 batch loss 4.97146 epoch total loss 5.69087934\n",
      "Trained batch 4033 batch loss 5.40210247 epoch total loss 5.69080782\n",
      "Trained batch 4034 batch loss 5.26666546 epoch total loss 5.69070292\n",
      "Trained batch 4035 batch loss 6.10727119 epoch total loss 5.69080591\n",
      "Trained batch 4036 batch loss 6.18900347 epoch total loss 5.69092941\n",
      "Trained batch 4037 batch loss 6.14480972 epoch total loss 5.69104195\n",
      "Trained batch 4038 batch loss 5.65531349 epoch total loss 5.69103336\n",
      "Trained batch 4039 batch loss 5.30763674 epoch total loss 5.69093847\n",
      "Trained batch 4040 batch loss 5.72705555 epoch total loss 5.69094753\n",
      "Trained batch 4041 batch loss 5.66965151 epoch total loss 5.69094229\n",
      "Trained batch 4042 batch loss 5.16678238 epoch total loss 5.69081259\n",
      "Trained batch 4043 batch loss 5.94896603 epoch total loss 5.69087648\n",
      "Trained batch 4044 batch loss 6.42524052 epoch total loss 5.69105816\n",
      "Trained batch 4045 batch loss 5.92250109 epoch total loss 5.6911149\n",
      "Trained batch 4046 batch loss 5.19624519 epoch total loss 5.69099236\n",
      "Trained batch 4047 batch loss 4.41831636 epoch total loss 5.69067812\n",
      "Trained batch 4048 batch loss 3.66485214 epoch total loss 5.69017744\n",
      "Trained batch 4049 batch loss 4.78591681 epoch total loss 5.6899538\n",
      "Trained batch 4050 batch loss 4.54488277 epoch total loss 5.68967104\n",
      "Trained batch 4051 batch loss 4.59947395 epoch total loss 5.6894021\n",
      "Trained batch 4052 batch loss 4.50317478 epoch total loss 5.68910933\n",
      "Trained batch 4053 batch loss 5.31837845 epoch total loss 5.68901777\n",
      "Trained batch 4054 batch loss 4.85980797 epoch total loss 5.68881321\n",
      "Trained batch 4055 batch loss 4.79515 epoch total loss 5.68859291\n",
      "Trained batch 4056 batch loss 4.66050863 epoch total loss 5.68833923\n",
      "Trained batch 4057 batch loss 4.60138273 epoch total loss 5.68807125\n",
      "Trained batch 4058 batch loss 4.55648136 epoch total loss 5.68779278\n",
      "Trained batch 4059 batch loss 4.3705864 epoch total loss 5.68746805\n",
      "Trained batch 4060 batch loss 4.93744564 epoch total loss 5.68728352\n",
      "Trained batch 4061 batch loss 4.79353 epoch total loss 5.68706322\n",
      "Trained batch 4062 batch loss 3.90289354 epoch total loss 5.68662405\n",
      "Trained batch 4063 batch loss 4.57406759 epoch total loss 5.68635035\n",
      "Trained batch 4064 batch loss 5.06371117 epoch total loss 5.68619728\n",
      "Trained batch 4065 batch loss 4.40773296 epoch total loss 5.68588257\n",
      "Trained batch 4066 batch loss 4.09883 epoch total loss 5.68549252\n",
      "Trained batch 4067 batch loss 4.28390026 epoch total loss 5.68514776\n",
      "Trained batch 4068 batch loss 4.61485767 epoch total loss 5.68488503\n",
      "Trained batch 4069 batch loss 4.67598486 epoch total loss 5.68463659\n",
      "Trained batch 4070 batch loss 6.27791309 epoch total loss 5.68478251\n",
      "Trained batch 4071 batch loss 5.99297142 epoch total loss 5.68485785\n",
      "Trained batch 4072 batch loss 5.45637608 epoch total loss 5.68480206\n",
      "Trained batch 4073 batch loss 5.06868219 epoch total loss 5.68465042\n",
      "Trained batch 4074 batch loss 5.49999428 epoch total loss 5.68460512\n",
      "Trained batch 4075 batch loss 4.93383884 epoch total loss 5.68442106\n",
      "Trained batch 4076 batch loss 4.8232336 epoch total loss 5.68420935\n",
      "Trained batch 4077 batch loss 6.18291187 epoch total loss 5.68433189\n",
      "Trained batch 4078 batch loss 5.43733263 epoch total loss 5.68427134\n",
      "Trained batch 4079 batch loss 5.04450893 epoch total loss 5.68411493\n",
      "Trained batch 4080 batch loss 5.94702721 epoch total loss 5.68417931\n",
      "Trained batch 4081 batch loss 6.39029121 epoch total loss 5.6843524\n",
      "Trained batch 4082 batch loss 6.08056402 epoch total loss 5.6844492\n",
      "Trained batch 4083 batch loss 4.76286316 epoch total loss 5.68422365\n",
      "Trained batch 4084 batch loss 5.09539461 epoch total loss 5.68407965\n",
      "Trained batch 4085 batch loss 4.14649868 epoch total loss 5.68370342\n",
      "Trained batch 4086 batch loss 4.38584518 epoch total loss 5.68338585\n",
      "Trained batch 4087 batch loss 4.46748257 epoch total loss 5.6830883\n",
      "Trained batch 4088 batch loss 4.42293262 epoch total loss 5.68278027\n",
      "Trained batch 4089 batch loss 5.82155466 epoch total loss 5.68281412\n",
      "Trained batch 4090 batch loss 4.42377424 epoch total loss 5.68250656\n",
      "Trained batch 4091 batch loss 4.89573479 epoch total loss 5.6823144\n",
      "Trained batch 4092 batch loss 4.986022 epoch total loss 5.68214417\n",
      "Trained batch 4093 batch loss 4.93213606 epoch total loss 5.68196058\n",
      "Trained batch 4094 batch loss 4.87860632 epoch total loss 5.6817646\n",
      "Trained batch 4095 batch loss 5.61258602 epoch total loss 5.68174791\n",
      "Trained batch 4096 batch loss 5.33826113 epoch total loss 5.68166399\n",
      "Trained batch 4097 batch loss 5.94983578 epoch total loss 5.68172932\n",
      "Trained batch 4098 batch loss 5.57862234 epoch total loss 5.68170404\n",
      "Trained batch 4099 batch loss 6.3630991 epoch total loss 5.68187046\n",
      "Trained batch 4100 batch loss 5.35885382 epoch total loss 5.68179178\n",
      "Trained batch 4101 batch loss 6.174119 epoch total loss 5.68191147\n",
      "Trained batch 4102 batch loss 5.57165432 epoch total loss 5.68188477\n",
      "Trained batch 4103 batch loss 5.65861368 epoch total loss 5.68187904\n",
      "Trained batch 4104 batch loss 4.65758896 epoch total loss 5.68162966\n",
      "Trained batch 4105 batch loss 5.27433205 epoch total loss 5.68153048\n",
      "Trained batch 4106 batch loss 5.6525445 epoch total loss 5.68152332\n",
      "Trained batch 4107 batch loss 4.6152153 epoch total loss 5.68126345\n",
      "Trained batch 4108 batch loss 4.76198578 epoch total loss 5.68104\n",
      "Trained batch 4109 batch loss 5.34277439 epoch total loss 5.68095779\n",
      "Trained batch 4110 batch loss 5.67856026 epoch total loss 5.68095684\n",
      "Trained batch 4111 batch loss 5.720438 epoch total loss 5.68096638\n",
      "Trained batch 4112 batch loss 6.14957809 epoch total loss 5.68108082\n",
      "Trained batch 4113 batch loss 4.78145504 epoch total loss 5.68086195\n",
      "Trained batch 4114 batch loss 5.70848751 epoch total loss 5.68086863\n",
      "Trained batch 4115 batch loss 5.99352694 epoch total loss 5.68094492\n",
      "Trained batch 4116 batch loss 5.64642096 epoch total loss 5.68093634\n",
      "Trained batch 4117 batch loss 5.46504211 epoch total loss 5.68088388\n",
      "Trained batch 4118 batch loss 5.77227163 epoch total loss 5.68090582\n",
      "Trained batch 4119 batch loss 5.79544067 epoch total loss 5.68093348\n",
      "Trained batch 4120 batch loss 5.61794949 epoch total loss 5.68091822\n",
      "Trained batch 4121 batch loss 5.66403 epoch total loss 5.68091393\n",
      "Trained batch 4122 batch loss 4.13113117 epoch total loss 5.68053818\n",
      "Trained batch 4123 batch loss 4.72588396 epoch total loss 5.68030643\n",
      "Trained batch 4124 batch loss 4.53634167 epoch total loss 5.68002939\n",
      "Trained batch 4125 batch loss 4.97799492 epoch total loss 5.67985916\n",
      "Trained batch 4126 batch loss 5.090415 epoch total loss 5.67971659\n",
      "Trained batch 4127 batch loss 5.72662926 epoch total loss 5.67972755\n",
      "Trained batch 4128 batch loss 5.802104 epoch total loss 5.6797576\n",
      "Trained batch 4129 batch loss 4.78620815 epoch total loss 5.67954111\n",
      "Trained batch 4130 batch loss 4.65824652 epoch total loss 5.67929411\n",
      "Trained batch 4131 batch loss 5.17690802 epoch total loss 5.67917252\n",
      "Trained batch 4132 batch loss 4.57639408 epoch total loss 5.67890549\n",
      "Trained batch 4133 batch loss 4.38413334 epoch total loss 5.67859268\n",
      "Trained batch 4134 batch loss 5.22421026 epoch total loss 5.67848253\n",
      "Trained batch 4135 batch loss 4.66353273 epoch total loss 5.67823744\n",
      "Trained batch 4136 batch loss 6.22849 epoch total loss 5.67837048\n",
      "Trained batch 4137 batch loss 6.42076349 epoch total loss 5.67855\n",
      "Trained batch 4138 batch loss 6.6459074 epoch total loss 5.67878342\n",
      "Trained batch 4139 batch loss 5.76218367 epoch total loss 5.67880344\n",
      "Trained batch 4140 batch loss 5.96632099 epoch total loss 5.67887306\n",
      "Trained batch 4141 batch loss 6.07804489 epoch total loss 5.67897\n",
      "Trained batch 4142 batch loss 5.89975929 epoch total loss 5.67902327\n",
      "Trained batch 4143 batch loss 6.18521643 epoch total loss 5.67914534\n",
      "Trained batch 4144 batch loss 6.1296072 epoch total loss 5.67925406\n",
      "Trained batch 4145 batch loss 6.37304974 epoch total loss 5.67942142\n",
      "Trained batch 4146 batch loss 6.05449295 epoch total loss 5.67951155\n",
      "Trained batch 4147 batch loss 6.03957796 epoch total loss 5.67959833\n",
      "Trained batch 4148 batch loss 5.88799906 epoch total loss 5.67964888\n",
      "Trained batch 4149 batch loss 5.69832039 epoch total loss 5.67965364\n",
      "Trained batch 4150 batch loss 5.17063332 epoch total loss 5.67953062\n",
      "Trained batch 4151 batch loss 6.62051249 epoch total loss 5.6797576\n",
      "Trained batch 4152 batch loss 6.28081131 epoch total loss 5.67990255\n",
      "Trained batch 4153 batch loss 5.78190041 epoch total loss 5.67992687\n",
      "Trained batch 4154 batch loss 5.74482536 epoch total loss 5.67994213\n",
      "Trained batch 4155 batch loss 5.88662243 epoch total loss 5.6799922\n",
      "Trained batch 4156 batch loss 6.03829432 epoch total loss 5.68007851\n",
      "Trained batch 4157 batch loss 5.84337807 epoch total loss 5.68011808\n",
      "Trained batch 4158 batch loss 5.91949368 epoch total loss 5.68017578\n",
      "Trained batch 4159 batch loss 5.9646616 epoch total loss 5.68024397\n",
      "Trained batch 4160 batch loss 6.09365559 epoch total loss 5.68034363\n",
      "Trained batch 4161 batch loss 5.87737179 epoch total loss 5.68039083\n",
      "Trained batch 4162 batch loss 5.30118418 epoch total loss 5.68029928\n",
      "Trained batch 4163 batch loss 5.83492708 epoch total loss 5.68033648\n",
      "Trained batch 4164 batch loss 5.69781113 epoch total loss 5.68034029\n",
      "Trained batch 4165 batch loss 5.81590843 epoch total loss 5.68037319\n",
      "Trained batch 4166 batch loss 5.17091656 epoch total loss 5.68025112\n",
      "Trained batch 4167 batch loss 5.37064505 epoch total loss 5.68017673\n",
      "Trained batch 4168 batch loss 6.286901 epoch total loss 5.68032265\n",
      "Trained batch 4169 batch loss 5.28784418 epoch total loss 5.68022823\n",
      "Trained batch 4170 batch loss 5.57042217 epoch total loss 5.68020201\n",
      "Trained batch 4171 batch loss 5.60982513 epoch total loss 5.68018484\n",
      "Trained batch 4172 batch loss 5.8910079 epoch total loss 5.68023539\n",
      "Trained batch 4173 batch loss 6.83233404 epoch total loss 5.68051147\n",
      "Trained batch 4174 batch loss 6.81355286 epoch total loss 5.68078279\n",
      "Trained batch 4175 batch loss 5.0605 epoch total loss 5.6806345\n",
      "Trained batch 4176 batch loss 4.61447954 epoch total loss 5.68037939\n",
      "Trained batch 4177 batch loss 6.56806421 epoch total loss 5.68059158\n",
      "Trained batch 4178 batch loss 5.8354373 epoch total loss 5.68062878\n",
      "Trained batch 4179 batch loss 6.33498144 epoch total loss 5.68078566\n",
      "Trained batch 4180 batch loss 5.55373859 epoch total loss 5.68075562\n",
      "Trained batch 4181 batch loss 4.51260424 epoch total loss 5.68047619\n",
      "Trained batch 4182 batch loss 5.74743271 epoch total loss 5.6804924\n",
      "Trained batch 4183 batch loss 5.24610138 epoch total loss 5.68038845\n",
      "Trained batch 4184 batch loss 5.7337532 epoch total loss 5.68040133\n",
      "Trained batch 4185 batch loss 5.72312069 epoch total loss 5.68041134\n",
      "Trained batch 4186 batch loss 5.97128057 epoch total loss 5.68048048\n",
      "Trained batch 4187 batch loss 5.42331886 epoch total loss 5.68041945\n",
      "Trained batch 4188 batch loss 5.76662827 epoch total loss 5.68044043\n",
      "Trained batch 4189 batch loss 5.0815506 epoch total loss 5.68029737\n",
      "Trained batch 4190 batch loss 6.35341 epoch total loss 5.68045807\n",
      "Trained batch 4191 batch loss 5.17118 epoch total loss 5.68033648\n",
      "Trained batch 4192 batch loss 5.76141882 epoch total loss 5.68035603\n",
      "Trained batch 4193 batch loss 5.64052486 epoch total loss 5.68034649\n",
      "Trained batch 4194 batch loss 5.98969746 epoch total loss 5.6804204\n",
      "Trained batch 4195 batch loss 5.49556255 epoch total loss 5.68037653\n",
      "Trained batch 4196 batch loss 5.92748117 epoch total loss 5.68043566\n",
      "Trained batch 4197 batch loss 5.42276192 epoch total loss 5.68037415\n",
      "Trained batch 4198 batch loss 5.18286133 epoch total loss 5.68025541\n",
      "Trained batch 4199 batch loss 5.26863384 epoch total loss 5.68015766\n",
      "Trained batch 4200 batch loss 5.40695906 epoch total loss 5.68009233\n",
      "Trained batch 4201 batch loss 5.75410271 epoch total loss 5.68011\n",
      "Trained batch 4202 batch loss 5.55635071 epoch total loss 5.68008089\n",
      "Trained batch 4203 batch loss 5.68749809 epoch total loss 5.68008232\n",
      "Trained batch 4204 batch loss 5.4177084 epoch total loss 5.68002033\n",
      "Trained batch 4205 batch loss 5.22919655 epoch total loss 5.67991257\n",
      "Trained batch 4206 batch loss 5.7602849 epoch total loss 5.67993164\n",
      "Trained batch 4207 batch loss 6.05738068 epoch total loss 5.68002129\n",
      "Trained batch 4208 batch loss 5.70572472 epoch total loss 5.68002748\n",
      "Trained batch 4209 batch loss 4.89520264 epoch total loss 5.67984056\n",
      "Trained batch 4210 batch loss 5.62769699 epoch total loss 5.67982817\n",
      "Trained batch 4211 batch loss 6.19994926 epoch total loss 5.67995119\n",
      "Trained batch 4212 batch loss 5.66329718 epoch total loss 5.67994785\n",
      "Trained batch 4213 batch loss 6.21127796 epoch total loss 5.68007374\n",
      "Trained batch 4214 batch loss 5.92569399 epoch total loss 5.68013191\n",
      "Trained batch 4215 batch loss 5.44476557 epoch total loss 5.68007612\n",
      "Trained batch 4216 batch loss 5.84426689 epoch total loss 5.68011522\n",
      "Trained batch 4217 batch loss 5.87462044 epoch total loss 5.68016148\n",
      "Trained batch 4218 batch loss 5.71946192 epoch total loss 5.68017054\n",
      "Trained batch 4219 batch loss 5.86297703 epoch total loss 5.68021393\n",
      "Trained batch 4220 batch loss 6.21352577 epoch total loss 5.68034029\n",
      "Trained batch 4221 batch loss 5.99169159 epoch total loss 5.6804142\n",
      "Trained batch 4222 batch loss 5.53145266 epoch total loss 5.68037844\n",
      "Trained batch 4223 batch loss 5.68401146 epoch total loss 5.68037939\n",
      "Trained batch 4224 batch loss 4.86118031 epoch total loss 5.68018532\n",
      "Trained batch 4225 batch loss 5.19332123 epoch total loss 5.6800704\n",
      "Trained batch 4226 batch loss 5.69044781 epoch total loss 5.68007278\n",
      "Trained batch 4227 batch loss 6.26467705 epoch total loss 5.68021154\n",
      "Trained batch 4228 batch loss 6.16671467 epoch total loss 5.68032646\n",
      "Trained batch 4229 batch loss 6.23559046 epoch total loss 5.68045807\n",
      "Trained batch 4230 batch loss 5.32031727 epoch total loss 5.68037271\n",
      "Trained batch 4231 batch loss 6.00011301 epoch total loss 5.68044806\n",
      "Trained batch 4232 batch loss 4.08722878 epoch total loss 5.68007183\n",
      "Trained batch 4233 batch loss 4.73512602 epoch total loss 5.67984867\n",
      "Trained batch 4234 batch loss 5.46999693 epoch total loss 5.67979908\n",
      "Trained batch 4235 batch loss 5.63611078 epoch total loss 5.67978907\n",
      "Trained batch 4236 batch loss 5.78009796 epoch total loss 5.67981243\n",
      "Trained batch 4237 batch loss 5.3103075 epoch total loss 5.67972517\n",
      "Trained batch 4238 batch loss 5.68680429 epoch total loss 5.67972708\n",
      "Trained batch 4239 batch loss 5.22783422 epoch total loss 5.67962074\n",
      "Trained batch 4240 batch loss 5.87085295 epoch total loss 5.67966604\n",
      "Trained batch 4241 batch loss 5.73855686 epoch total loss 5.67968\n",
      "Trained batch 4242 batch loss 5.1668396 epoch total loss 5.67955875\n",
      "Trained batch 4243 batch loss 5.86660862 epoch total loss 5.67960262\n",
      "Trained batch 4244 batch loss 6.42772198 epoch total loss 5.67977905\n",
      "Trained batch 4245 batch loss 5.91629171 epoch total loss 5.67983484\n",
      "Trained batch 4246 batch loss 6.4603796 epoch total loss 5.6800189\n",
      "Trained batch 4247 batch loss 5.76632 epoch total loss 5.68003893\n",
      "Trained batch 4248 batch loss 6.00590515 epoch total loss 5.6801157\n",
      "Trained batch 4249 batch loss 6.39054 epoch total loss 5.68028259\n",
      "Trained batch 4250 batch loss 5.65666056 epoch total loss 5.68027735\n",
      "Trained batch 4251 batch loss 6.17965221 epoch total loss 5.68039465\n",
      "Trained batch 4252 batch loss 6.02799082 epoch total loss 5.68047619\n",
      "Trained batch 4253 batch loss 6.2903862 epoch total loss 5.68061972\n",
      "Trained batch 4254 batch loss 5.57857561 epoch total loss 5.68059587\n",
      "Trained batch 4255 batch loss 5.89249897 epoch total loss 5.68064547\n",
      "Trained batch 4256 batch loss 6.33669853 epoch total loss 5.68079948\n",
      "Trained batch 4257 batch loss 5.80267525 epoch total loss 5.68082809\n",
      "Trained batch 4258 batch loss 6.28937817 epoch total loss 5.68097115\n",
      "Trained batch 4259 batch loss 6.20467 epoch total loss 5.68109417\n",
      "Trained batch 4260 batch loss 6.72387791 epoch total loss 5.68133879\n",
      "Trained batch 4261 batch loss 5.48565578 epoch total loss 5.68129301\n",
      "Trained batch 4262 batch loss 6.33734751 epoch total loss 5.68144703\n",
      "Trained batch 4263 batch loss 5.29655695 epoch total loss 5.68135691\n",
      "Trained batch 4264 batch loss 5.22980499 epoch total loss 5.68125105\n",
      "Trained batch 4265 batch loss 6.22639894 epoch total loss 5.68137932\n",
      "Trained batch 4266 batch loss 6.6115427 epoch total loss 5.68159723\n",
      "Trained batch 4267 batch loss 5.13225698 epoch total loss 5.68146849\n",
      "Trained batch 4268 batch loss 6.49688435 epoch total loss 5.68165922\n",
      "Trained batch 4269 batch loss 6.57955837 epoch total loss 5.68187\n",
      "Trained batch 4270 batch loss 6.59015703 epoch total loss 5.68208265\n",
      "Trained batch 4271 batch loss 6.37540054 epoch total loss 5.68224478\n",
      "Trained batch 4272 batch loss 5.81690359 epoch total loss 5.68227625\n",
      "Trained batch 4273 batch loss 4.95861149 epoch total loss 5.68210697\n",
      "Trained batch 4274 batch loss 6.60021257 epoch total loss 5.68232155\n",
      "Trained batch 4275 batch loss 6.14411736 epoch total loss 5.68243\n",
      "Trained batch 4276 batch loss 6.11036491 epoch total loss 5.68253\n",
      "Trained batch 4277 batch loss 6.27283335 epoch total loss 5.68266821\n",
      "Trained batch 4278 batch loss 6.99592876 epoch total loss 5.68297529\n",
      "Trained batch 4279 batch loss 5.83322954 epoch total loss 5.68301058\n",
      "Trained batch 4280 batch loss 6.80192566 epoch total loss 5.68327188\n",
      "Trained batch 4281 batch loss 6.26653 epoch total loss 5.68340826\n",
      "Trained batch 4282 batch loss 6.44375134 epoch total loss 5.68358564\n",
      "Trained batch 4283 batch loss 5.81844378 epoch total loss 5.68361712\n",
      "Trained batch 4284 batch loss 3.69544744 epoch total loss 5.68315268\n",
      "Trained batch 4285 batch loss 5.44632435 epoch total loss 5.68309784\n",
      "Trained batch 4286 batch loss 5.31192303 epoch total loss 5.68301153\n",
      "Trained batch 4287 batch loss 4.70888 epoch total loss 5.68278408\n",
      "Trained batch 4288 batch loss 4.30988789 epoch total loss 5.68246412\n",
      "Trained batch 4289 batch loss 5.40696859 epoch total loss 5.68239975\n",
      "Trained batch 4290 batch loss 5.68276501 epoch total loss 5.6824\n",
      "Trained batch 4291 batch loss 5.78953362 epoch total loss 5.68242502\n",
      "Trained batch 4292 batch loss 5.67134619 epoch total loss 5.68242264\n",
      "Trained batch 4293 batch loss 5.79655695 epoch total loss 5.68244886\n",
      "Trained batch 4294 batch loss 5.85872173 epoch total loss 5.68249035\n",
      "Trained batch 4295 batch loss 5.07006073 epoch total loss 5.68234777\n",
      "Trained batch 4296 batch loss 5.65376806 epoch total loss 5.6823411\n",
      "Trained batch 4297 batch loss 5.44314098 epoch total loss 5.68228579\n",
      "Trained batch 4298 batch loss 5.02940845 epoch total loss 5.68213367\n",
      "Trained batch 4299 batch loss 5.33003378 epoch total loss 5.68205166\n",
      "Trained batch 4300 batch loss 5.77091217 epoch total loss 5.68207264\n",
      "Trained batch 4301 batch loss 6.20501709 epoch total loss 5.68219423\n",
      "Trained batch 4302 batch loss 5.09477234 epoch total loss 5.68205786\n",
      "Trained batch 4303 batch loss 5.49643564 epoch total loss 5.68201447\n",
      "Trained batch 4304 batch loss 5.55116606 epoch total loss 5.68198395\n",
      "Trained batch 4305 batch loss 5.59043121 epoch total loss 5.68196297\n",
      "Trained batch 4306 batch loss 5.86460304 epoch total loss 5.68200541\n",
      "Trained batch 4307 batch loss 4.76439762 epoch total loss 5.68179226\n",
      "Trained batch 4308 batch loss 5.60946178 epoch total loss 5.68177509\n",
      "Trained batch 4309 batch loss 5.12781334 epoch total loss 5.68164635\n",
      "Trained batch 4310 batch loss 5.41851521 epoch total loss 5.68158531\n",
      "Trained batch 4311 batch loss 5.19182873 epoch total loss 5.68147182\n",
      "Trained batch 4312 batch loss 5.59366894 epoch total loss 5.68145132\n",
      "Trained batch 4313 batch loss 4.73155594 epoch total loss 5.68123102\n",
      "Trained batch 4314 batch loss 5.16894341 epoch total loss 5.68111229\n",
      "Trained batch 4315 batch loss 4.80128574 epoch total loss 5.6809082\n",
      "Trained batch 4316 batch loss 5.42459393 epoch total loss 5.6808486\n",
      "Trained batch 4317 batch loss 6.17266178 epoch total loss 5.68096256\n",
      "Trained batch 4318 batch loss 4.64925957 epoch total loss 5.68072319\n",
      "Trained batch 4319 batch loss 5.64143848 epoch total loss 5.68071413\n",
      "Trained batch 4320 batch loss 4.23799181 epoch total loss 5.68038034\n",
      "Trained batch 4321 batch loss 4.9818306 epoch total loss 5.6802187\n",
      "Trained batch 4322 batch loss 5.86061 epoch total loss 5.68026066\n",
      "Trained batch 4323 batch loss 5.44811821 epoch total loss 5.68020678\n",
      "Trained batch 4324 batch loss 5.01831341 epoch total loss 5.68005323\n",
      "Trained batch 4325 batch loss 4.46580219 epoch total loss 5.67977238\n",
      "Trained batch 4326 batch loss 4.44456482 epoch total loss 5.67948723\n",
      "Trained batch 4327 batch loss 6.51679802 epoch total loss 5.67968082\n",
      "Trained batch 4328 batch loss 6.39057922 epoch total loss 5.67984486\n",
      "Trained batch 4329 batch loss 5.47840881 epoch total loss 5.6797986\n",
      "Trained batch 4330 batch loss 6.36790609 epoch total loss 5.67995739\n",
      "Trained batch 4331 batch loss 5.84188175 epoch total loss 5.67999458\n",
      "Trained batch 4332 batch loss 5.9458456 epoch total loss 5.68005562\n",
      "Trained batch 4333 batch loss 5.64973927 epoch total loss 5.68004894\n",
      "Trained batch 4334 batch loss 5.92778873 epoch total loss 5.68010616\n",
      "Trained batch 4335 batch loss 4.87323427 epoch total loss 5.67992\n",
      "Trained batch 4336 batch loss 5.2125597 epoch total loss 5.67981243\n",
      "Trained batch 4337 batch loss 6.34941864 epoch total loss 5.67996693\n",
      "Trained batch 4338 batch loss 6.19382095 epoch total loss 5.68008518\n",
      "Trained batch 4339 batch loss 6.35770702 epoch total loss 5.68024111\n",
      "Trained batch 4340 batch loss 6.42062426 epoch total loss 5.68041182\n",
      "Trained batch 4341 batch loss 6.14668798 epoch total loss 5.6805191\n",
      "Trained batch 4342 batch loss 6.12639141 epoch total loss 5.68062162\n",
      "Trained batch 4343 batch loss 6.27141237 epoch total loss 5.680758\n",
      "Trained batch 4344 batch loss 6.13233662 epoch total loss 5.68086195\n",
      "Trained batch 4345 batch loss 6.35653305 epoch total loss 5.6810174\n",
      "Trained batch 4346 batch loss 5.92881536 epoch total loss 5.68107462\n",
      "Trained batch 4347 batch loss 6.24860382 epoch total loss 5.68120527\n",
      "Trained batch 4348 batch loss 6.30395031 epoch total loss 5.6813488\n",
      "Trained batch 4349 batch loss 6.06971407 epoch total loss 5.68143797\n",
      "Trained batch 4350 batch loss 5.29674816 epoch total loss 5.68134975\n",
      "Trained batch 4351 batch loss 6.06924725 epoch total loss 5.68143845\n",
      "Trained batch 4352 batch loss 4.31138229 epoch total loss 5.68112373\n",
      "Trained batch 4353 batch loss 4.14457226 epoch total loss 5.6807704\n",
      "Trained batch 4354 batch loss 5.89285851 epoch total loss 5.68081903\n",
      "Trained batch 4355 batch loss 6.43004894 epoch total loss 5.68099117\n",
      "Trained batch 4356 batch loss 5.78080177 epoch total loss 5.68101406\n",
      "Trained batch 4357 batch loss 6.06285858 epoch total loss 5.6811018\n",
      "Trained batch 4358 batch loss 6.3374505 epoch total loss 5.68125248\n",
      "Trained batch 4359 batch loss 6.33405 epoch total loss 5.68140221\n",
      "Trained batch 4360 batch loss 6.10934401 epoch total loss 5.68150043\n",
      "Trained batch 4361 batch loss 5.46118975 epoch total loss 5.68145\n",
      "Trained batch 4362 batch loss 5.47699356 epoch total loss 5.68140268\n",
      "Trained batch 4363 batch loss 5.43322468 epoch total loss 5.68134594\n",
      "Trained batch 4364 batch loss 5.58769751 epoch total loss 5.68132448\n",
      "Trained batch 4365 batch loss 5.55629349 epoch total loss 5.68129587\n",
      "Trained batch 4366 batch loss 5.6249094 epoch total loss 5.681283\n",
      "Trained batch 4367 batch loss 5.34310436 epoch total loss 5.68120575\n",
      "Trained batch 4368 batch loss 6.75477886 epoch total loss 5.68145132\n",
      "Trained batch 4369 batch loss 5.33560658 epoch total loss 5.68137217\n",
      "Trained batch 4370 batch loss 5.73033905 epoch total loss 5.68138361\n",
      "Trained batch 4371 batch loss 5.90288162 epoch total loss 5.68143415\n",
      "Trained batch 4372 batch loss 5.71854687 epoch total loss 5.68144274\n",
      "Trained batch 4373 batch loss 5.83744717 epoch total loss 5.6814785\n",
      "Trained batch 4374 batch loss 5.70466614 epoch total loss 5.68148375\n",
      "Trained batch 4375 batch loss 6.03794575 epoch total loss 5.68156528\n",
      "Trained batch 4376 batch loss 6.04441071 epoch total loss 5.68164825\n",
      "Trained batch 4377 batch loss 5.93688059 epoch total loss 5.68170643\n",
      "Trained batch 4378 batch loss 5.52250242 epoch total loss 5.68167067\n",
      "Trained batch 4379 batch loss 6.19595718 epoch total loss 5.68178797\n",
      "Trained batch 4380 batch loss 5.93006659 epoch total loss 5.68184423\n",
      "Trained batch 4381 batch loss 6.08751869 epoch total loss 5.68193722\n",
      "Trained batch 4382 batch loss 5.76792097 epoch total loss 5.68195677\n",
      "Trained batch 4383 batch loss 6.52438354 epoch total loss 5.68214846\n",
      "Trained batch 4384 batch loss 6.16561794 epoch total loss 5.68225908\n",
      "Trained batch 4385 batch loss 6.31442165 epoch total loss 5.68240309\n",
      "Trained batch 4386 batch loss 5.09997463 epoch total loss 5.68227053\n",
      "Trained batch 4387 batch loss 5.68311405 epoch total loss 5.68227053\n",
      "Trained batch 4388 batch loss 4.87563086 epoch total loss 5.68208647\n",
      "Trained batch 4389 batch loss 5.64493656 epoch total loss 5.68207788\n",
      "Trained batch 4390 batch loss 6.10539913 epoch total loss 5.68217468\n",
      "Trained batch 4391 batch loss 5.81427479 epoch total loss 5.68220472\n",
      "Trained batch 4392 batch loss 5.69360638 epoch total loss 5.68220711\n",
      "Trained batch 4393 batch loss 4.93921661 epoch total loss 5.68203831\n",
      "Trained batch 4394 batch loss 5.61867237 epoch total loss 5.682024\n",
      "Trained batch 4395 batch loss 5.82641602 epoch total loss 5.68205643\n",
      "Trained batch 4396 batch loss 5.46462727 epoch total loss 5.68200731\n",
      "Trained batch 4397 batch loss 4.99742413 epoch total loss 5.68185139\n",
      "Trained batch 4398 batch loss 5.9334383 epoch total loss 5.68190908\n",
      "Trained batch 4399 batch loss 5.62772274 epoch total loss 5.68189621\n",
      "Trained batch 4400 batch loss 5.53933764 epoch total loss 5.68186378\n",
      "Trained batch 4401 batch loss 5.34870338 epoch total loss 5.68178844\n",
      "Trained batch 4402 batch loss 5.39959574 epoch total loss 5.68172455\n",
      "Trained batch 4403 batch loss 6.31837 epoch total loss 5.68186903\n",
      "Trained batch 4404 batch loss 5.22482443 epoch total loss 5.68176508\n",
      "Trained batch 4405 batch loss 5.15232754 epoch total loss 5.68164492\n",
      "Trained batch 4406 batch loss 4.3444376 epoch total loss 5.68134117\n",
      "Trained batch 4407 batch loss 4.24415398 epoch total loss 5.68101549\n",
      "Trained batch 4408 batch loss 5.45153713 epoch total loss 5.68096304\n",
      "Trained batch 4409 batch loss 5.29030752 epoch total loss 5.68087482\n",
      "Trained batch 4410 batch loss 5.17620182 epoch total loss 5.68076038\n",
      "Trained batch 4411 batch loss 5.05140877 epoch total loss 5.68061733\n",
      "Trained batch 4412 batch loss 4.48970127 epoch total loss 5.68034744\n",
      "Trained batch 4413 batch loss 4.92853117 epoch total loss 5.68017721\n",
      "Trained batch 4414 batch loss 4.04389715 epoch total loss 5.67980623\n",
      "Trained batch 4415 batch loss 4.12348747 epoch total loss 5.67945337\n",
      "Trained batch 4416 batch loss 4.80830288 epoch total loss 5.67925644\n",
      "Trained batch 4417 batch loss 4.4994154 epoch total loss 5.67898941\n",
      "Trained batch 4418 batch loss 6.21767616 epoch total loss 5.679111\n",
      "Trained batch 4419 batch loss 6.65462303 epoch total loss 5.67933178\n",
      "Trained batch 4420 batch loss 4.21405602 epoch total loss 5.67900038\n",
      "Trained batch 4421 batch loss 4.66641903 epoch total loss 5.67877102\n",
      "Trained batch 4422 batch loss 4.40294552 epoch total loss 5.67848253\n",
      "Trained batch 4423 batch loss 4.40879059 epoch total loss 5.67819548\n",
      "Trained batch 4424 batch loss 4.23573303 epoch total loss 5.67786932\n",
      "Trained batch 4425 batch loss 4.76958132 epoch total loss 5.67766428\n",
      "Trained batch 4426 batch loss 4.01260805 epoch total loss 5.67728758\n",
      "Trained batch 4427 batch loss 5.08967209 epoch total loss 5.67715502\n",
      "Trained batch 4428 batch loss 4.84056473 epoch total loss 5.67696619\n",
      "Trained batch 4429 batch loss 5.43346119 epoch total loss 5.67691088\n",
      "Trained batch 4430 batch loss 5.53170156 epoch total loss 5.67687798\n",
      "Trained batch 4431 batch loss 5.22255135 epoch total loss 5.67677546\n",
      "Trained batch 4432 batch loss 5.20107651 epoch total loss 5.67666817\n",
      "Trained batch 4433 batch loss 5.2449789 epoch total loss 5.67657089\n",
      "Trained batch 4434 batch loss 5.2727375 epoch total loss 5.67648\n",
      "Trained batch 4435 batch loss 5.21380043 epoch total loss 5.67637539\n",
      "Trained batch 4436 batch loss 5.20793152 epoch total loss 5.67626953\n",
      "Trained batch 4437 batch loss 5.05570602 epoch total loss 5.67613\n",
      "Trained batch 4438 batch loss 4.37215042 epoch total loss 5.67583609\n",
      "Trained batch 4439 batch loss 4.54659081 epoch total loss 5.67558193\n",
      "Trained batch 4440 batch loss 4.41086912 epoch total loss 5.67529678\n",
      "Trained batch 4441 batch loss 4.3200264 epoch total loss 5.67499161\n",
      "Trained batch 4442 batch loss 4.29110146 epoch total loss 5.67468\n",
      "Trained batch 4443 batch loss 4.25203896 epoch total loss 5.67436\n",
      "Trained batch 4444 batch loss 4.33774662 epoch total loss 5.67405939\n",
      "Trained batch 4445 batch loss 3.95462084 epoch total loss 5.67367268\n",
      "Trained batch 4446 batch loss 4.54774475 epoch total loss 5.673419\n",
      "Trained batch 4447 batch loss 4.41386271 epoch total loss 5.67313576\n",
      "Trained batch 4448 batch loss 4.54609203 epoch total loss 5.67288256\n",
      "Trained batch 4449 batch loss 4.31103373 epoch total loss 5.67257643\n",
      "Trained batch 4450 batch loss 4.31480122 epoch total loss 5.67227125\n",
      "Trained batch 4451 batch loss 4.1505971 epoch total loss 5.67192936\n",
      "Trained batch 4452 batch loss 4.21108341 epoch total loss 5.6716013\n",
      "Trained batch 4453 batch loss 3.99535036 epoch total loss 5.67122507\n",
      "Trained batch 4454 batch loss 4.38969564 epoch total loss 5.67093754\n",
      "Trained batch 4455 batch loss 4.42282104 epoch total loss 5.67065716\n",
      "Trained batch 4456 batch loss 4.71861649 epoch total loss 5.67044353\n",
      "Trained batch 4457 batch loss 4.613451 epoch total loss 5.67020607\n",
      "Trained batch 4458 batch loss 4.50706768 epoch total loss 5.66994572\n",
      "Trained batch 4459 batch loss 4.65297318 epoch total loss 5.66971731\n",
      "Trained batch 4460 batch loss 4.19342518 epoch total loss 5.66938639\n",
      "Trained batch 4461 batch loss 4.33891106 epoch total loss 5.66908836\n",
      "Trained batch 4462 batch loss 4.32946444 epoch total loss 5.66878796\n",
      "Trained batch 4463 batch loss 3.62568307 epoch total loss 5.66833\n",
      "Trained batch 4464 batch loss 3.96336031 epoch total loss 5.66794825\n",
      "Trained batch 4465 batch loss 3.99910736 epoch total loss 5.66757441\n",
      "Trained batch 4466 batch loss 3.97119045 epoch total loss 5.66719484\n",
      "Trained batch 4467 batch loss 4.67150307 epoch total loss 5.66697168\n",
      "Trained batch 4468 batch loss 3.52733469 epoch total loss 5.66649294\n",
      "Trained batch 4469 batch loss 4.79574776 epoch total loss 5.66629791\n",
      "Trained batch 4470 batch loss 5.11639833 epoch total loss 5.66617489\n",
      "Trained batch 4471 batch loss 6.50235176 epoch total loss 5.66636181\n",
      "Trained batch 4472 batch loss 6.5492425 epoch total loss 5.66655922\n",
      "Trained batch 4473 batch loss 6.33046055 epoch total loss 5.66670752\n",
      "Trained batch 4474 batch loss 6.13739443 epoch total loss 5.6668129\n",
      "Trained batch 4475 batch loss 5.67966461 epoch total loss 5.66681576\n",
      "Trained batch 4476 batch loss 5.1085968 epoch total loss 5.6666913\n",
      "Trained batch 4477 batch loss 6.26054955 epoch total loss 5.66682339\n",
      "Trained batch 4478 batch loss 5.91496658 epoch total loss 5.6668787\n",
      "Trained batch 4479 batch loss 5.66516972 epoch total loss 5.6668787\n",
      "Trained batch 4480 batch loss 5.62936068 epoch total loss 5.66687\n",
      "Trained batch 4481 batch loss 5.85696411 epoch total loss 5.66691256\n",
      "Trained batch 4482 batch loss 5.67325974 epoch total loss 5.66691399\n",
      "Trained batch 4483 batch loss 6.090909 epoch total loss 5.66700888\n",
      "Trained batch 4484 batch loss 5.73185635 epoch total loss 5.66702366\n",
      "Trained batch 4485 batch loss 5.02857828 epoch total loss 5.66688156\n",
      "Trained batch 4486 batch loss 5.39631033 epoch total loss 5.666821\n",
      "Trained batch 4487 batch loss 5.76030827 epoch total loss 5.66684198\n",
      "Trained batch 4488 batch loss 5.61934 epoch total loss 5.66683102\n",
      "Trained batch 4489 batch loss 5.55499649 epoch total loss 5.66680622\n",
      "Trained batch 4490 batch loss 6.26558208 epoch total loss 5.66693974\n",
      "Trained batch 4491 batch loss 5.23220301 epoch total loss 5.66684294\n",
      "Trained batch 4492 batch loss 5.27611256 epoch total loss 5.66675568\n",
      "Trained batch 4493 batch loss 4.90278244 epoch total loss 5.66658545\n",
      "Trained batch 4494 batch loss 5.31931782 epoch total loss 5.6665082\n",
      "Trained batch 4495 batch loss 5.72683 epoch total loss 5.66652155\n",
      "Trained batch 4496 batch loss 5.55041361 epoch total loss 5.6664958\n",
      "Trained batch 4497 batch loss 5.74335909 epoch total loss 5.66651297\n",
      "Trained batch 4498 batch loss 6.15243673 epoch total loss 5.66662073\n",
      "Trained batch 4499 batch loss 5.77949333 epoch total loss 5.666646\n",
      "Trained batch 4500 batch loss 4.4634 epoch total loss 5.6663785\n",
      "Trained batch 4501 batch loss 4.97797585 epoch total loss 5.66622543\n",
      "Trained batch 4502 batch loss 5.05182552 epoch total loss 5.66608953\n",
      "Trained batch 4503 batch loss 5.97439241 epoch total loss 5.66615772\n",
      "Trained batch 4504 batch loss 4.46302891 epoch total loss 5.66589069\n",
      "Trained batch 4505 batch loss 5.40079165 epoch total loss 5.66583157\n",
      "Trained batch 4506 batch loss 5.54682779 epoch total loss 5.66580534\n",
      "Trained batch 4507 batch loss 5.36516 epoch total loss 5.66573858\n",
      "Trained batch 4508 batch loss 6.14905548 epoch total loss 5.66584587\n",
      "Trained batch 4509 batch loss 5.93215418 epoch total loss 5.66590452\n",
      "Trained batch 4510 batch loss 6.21240854 epoch total loss 5.66602612\n",
      "Trained batch 4511 batch loss 5.50771904 epoch total loss 5.66599083\n",
      "Trained batch 4512 batch loss 3.6906631 epoch total loss 5.66555309\n",
      "Trained batch 4513 batch loss 5.50816774 epoch total loss 5.66551828\n",
      "Trained batch 4514 batch loss 6.12196398 epoch total loss 5.66561937\n",
      "Trained batch 4515 batch loss 5.52447176 epoch total loss 5.66558838\n",
      "Trained batch 4516 batch loss 5.77261448 epoch total loss 5.66561222\n",
      "Trained batch 4517 batch loss 5.49357462 epoch total loss 5.66557407\n",
      "Trained batch 4518 batch loss 5.51121092 epoch total loss 5.66554\n",
      "Trained batch 4519 batch loss 6.17910242 epoch total loss 5.66565371\n",
      "Trained batch 4520 batch loss 4.50749969 epoch total loss 5.66539764\n",
      "Trained batch 4521 batch loss 5.72270632 epoch total loss 5.66541052\n",
      "Trained batch 4522 batch loss 5.72856045 epoch total loss 5.66542435\n",
      "Trained batch 4523 batch loss 5.52520132 epoch total loss 5.66539335\n",
      "Trained batch 4524 batch loss 5.80137444 epoch total loss 5.66542339\n",
      "Trained batch 4525 batch loss 5.71638107 epoch total loss 5.66543484\n",
      "Trained batch 4526 batch loss 5.30676222 epoch total loss 5.66535521\n",
      "Trained batch 4527 batch loss 5.79293203 epoch total loss 5.66538334\n",
      "Trained batch 4528 batch loss 5.88615417 epoch total loss 5.66543245\n",
      "Trained batch 4529 batch loss 5.69790554 epoch total loss 5.66543961\n",
      "Trained batch 4530 batch loss 5.96946764 epoch total loss 5.66550636\n",
      "Trained batch 4531 batch loss 6.01576424 epoch total loss 5.66558361\n",
      "Trained batch 4532 batch loss 5.34329748 epoch total loss 5.66551256\n",
      "Trained batch 4533 batch loss 6.31209469 epoch total loss 5.66565561\n",
      "Trained batch 4534 batch loss 6.30886745 epoch total loss 5.66579723\n",
      "Trained batch 4535 batch loss 5.8459549 epoch total loss 5.66583681\n",
      "Trained batch 4536 batch loss 5.90574598 epoch total loss 5.66588974\n",
      "Trained batch 4537 batch loss 5.88209248 epoch total loss 5.6659379\n",
      "Trained batch 4538 batch loss 6.04429674 epoch total loss 5.66602135\n",
      "Trained batch 4539 batch loss 6.06917286 epoch total loss 5.66611\n",
      "Trained batch 4540 batch loss 5.35123396 epoch total loss 5.66604042\n",
      "Trained batch 4541 batch loss 5.97681522 epoch total loss 5.66610909\n",
      "Trained batch 4542 batch loss 4.76196623 epoch total loss 5.66591\n",
      "Trained batch 4543 batch loss 5.7340641 epoch total loss 5.66592503\n",
      "Trained batch 4544 batch loss 5.84282303 epoch total loss 5.66596413\n",
      "Trained batch 4545 batch loss 5.76466227 epoch total loss 5.66598606\n",
      "Trained batch 4546 batch loss 5.37045383 epoch total loss 5.66592121\n",
      "Trained batch 4547 batch loss 5.38091135 epoch total loss 5.66585827\n",
      "Trained batch 4548 batch loss 4.74377155 epoch total loss 5.66565561\n",
      "Trained batch 4549 batch loss 6.49096537 epoch total loss 5.66583681\n",
      "Trained batch 4550 batch loss 5.30931 epoch total loss 5.66575861\n",
      "Trained batch 4551 batch loss 5.93436384 epoch total loss 5.66581726\n",
      "Trained batch 4552 batch loss 4.71373272 epoch total loss 5.66560793\n",
      "Trained batch 4553 batch loss 5.93770123 epoch total loss 5.66566753\n",
      "Trained batch 4554 batch loss 5.119699 epoch total loss 5.66554785\n",
      "Trained batch 4555 batch loss 5.83500242 epoch total loss 5.66558504\n",
      "Trained batch 4556 batch loss 4.22864866 epoch total loss 5.66527\n",
      "Trained batch 4557 batch loss 5.66260099 epoch total loss 5.6652689\n",
      "Trained batch 4558 batch loss 5.43451 epoch total loss 5.66521835\n",
      "Trained batch 4559 batch loss 6.65475464 epoch total loss 5.66543531\n",
      "Trained batch 4560 batch loss 5.32598877 epoch total loss 5.66536093\n",
      "Trained batch 4561 batch loss 6.00620365 epoch total loss 5.66543531\n",
      "Trained batch 4562 batch loss 5.56647301 epoch total loss 5.66541386\n",
      "Trained batch 4563 batch loss 6.17642546 epoch total loss 5.66552544\n",
      "Trained batch 4564 batch loss 6.3893466 epoch total loss 5.66568375\n",
      "Trained batch 4565 batch loss 5.95790339 epoch total loss 5.66574764\n",
      "Trained batch 4566 batch loss 5.56063175 epoch total loss 5.66572475\n",
      "Trained batch 4567 batch loss 5.27065086 epoch total loss 5.66563845\n",
      "Trained batch 4568 batch loss 4.47922134 epoch total loss 5.66537857\n",
      "Trained batch 4569 batch loss 4.46566916 epoch total loss 5.66511583\n",
      "Trained batch 4570 batch loss 6.37957096 epoch total loss 5.66527176\n",
      "Trained batch 4571 batch loss 5.48115253 epoch total loss 5.6652317\n",
      "Trained batch 4572 batch loss 5.70471573 epoch total loss 5.66524029\n",
      "Trained batch 4573 batch loss 5.66962814 epoch total loss 5.66524124\n",
      "Trained batch 4574 batch loss 5.27734709 epoch total loss 5.66515636\n",
      "Trained batch 4575 batch loss 5.32261944 epoch total loss 5.6650815\n",
      "Trained batch 4576 batch loss 5.41967201 epoch total loss 5.6650281\n",
      "Trained batch 4577 batch loss 5.74248123 epoch total loss 5.66504478\n",
      "Trained batch 4578 batch loss 5.20667 epoch total loss 5.66494465\n",
      "Trained batch 4579 batch loss 5.19645643 epoch total loss 5.66484261\n",
      "Trained batch 4580 batch loss 5.97023439 epoch total loss 5.66490936\n",
      "Trained batch 4581 batch loss 5.72331047 epoch total loss 5.66492224\n",
      "Trained batch 4582 batch loss 5.3560853 epoch total loss 5.66485453\n",
      "Trained batch 4583 batch loss 4.91580248 epoch total loss 5.66469097\n",
      "Trained batch 4584 batch loss 6.02735615 epoch total loss 5.66477\n",
      "Trained batch 4585 batch loss 5.68268299 epoch total loss 5.66477442\n",
      "Trained batch 4586 batch loss 5.24543858 epoch total loss 5.66468287\n",
      "Trained batch 4587 batch loss 5.56047297 epoch total loss 5.66466045\n",
      "Trained batch 4588 batch loss 5.88057137 epoch total loss 5.66470766\n",
      "Trained batch 4589 batch loss 5.17230701 epoch total loss 5.6646\n",
      "Trained batch 4590 batch loss 4.5472374 epoch total loss 5.66435671\n",
      "Trained batch 4591 batch loss 5.95656 epoch total loss 5.66442\n",
      "Trained batch 4592 batch loss 3.7017405 epoch total loss 5.66399288\n",
      "Trained batch 4593 batch loss 5.75752687 epoch total loss 5.66401339\n",
      "Trained batch 4594 batch loss 4.44192839 epoch total loss 5.66374683\n",
      "Trained batch 4595 batch loss 5.26636124 epoch total loss 5.66366053\n",
      "Trained batch 4596 batch loss 4.83203793 epoch total loss 5.66347933\n",
      "Trained batch 4597 batch loss 5.24280548 epoch total loss 5.66338778\n",
      "Trained batch 4598 batch loss 6.08940887 epoch total loss 5.66348076\n",
      "Trained batch 4599 batch loss 5.9988184 epoch total loss 5.66355324\n",
      "Trained batch 4600 batch loss 6.60364914 epoch total loss 5.6637578\n",
      "Trained batch 4601 batch loss 5.78143787 epoch total loss 5.66378307\n",
      "Trained batch 4602 batch loss 4.90994358 epoch total loss 5.66361952\n",
      "Trained batch 4603 batch loss 6.12336254 epoch total loss 5.66371918\n",
      "Trained batch 4604 batch loss 5.61355114 epoch total loss 5.66370821\n",
      "Trained batch 4605 batch loss 5.7165184 epoch total loss 5.66371965\n",
      "Trained batch 4606 batch loss 6.81477165 epoch total loss 5.66396952\n",
      "Trained batch 4607 batch loss 5.47167683 epoch total loss 5.66392756\n",
      "Trained batch 4608 batch loss 5.36023235 epoch total loss 5.66386175\n",
      "Trained batch 4609 batch loss 6.27911568 epoch total loss 5.66399527\n",
      "Trained batch 4610 batch loss 6.9494648 epoch total loss 5.66427374\n",
      "Trained batch 4611 batch loss 6.82882261 epoch total loss 5.66452646\n",
      "Trained batch 4612 batch loss 6.75664711 epoch total loss 5.66476297\n",
      "Trained batch 4613 batch loss 5.11717415 epoch total loss 5.66464424\n",
      "Trained batch 4614 batch loss 5.81007576 epoch total loss 5.66467571\n",
      "Trained batch 4615 batch loss 5.26042366 epoch total loss 5.66458797\n",
      "Trained batch 4616 batch loss 5.69341803 epoch total loss 5.66459417\n",
      "Trained batch 4617 batch loss 5.45951748 epoch total loss 5.66455\n",
      "Trained batch 4618 batch loss 5.73702097 epoch total loss 5.66456556\n",
      "Trained batch 4619 batch loss 5.97959471 epoch total loss 5.66463375\n",
      "Trained batch 4620 batch loss 5.57203436 epoch total loss 5.66461372\n",
      "Trained batch 4621 batch loss 5.74829292 epoch total loss 5.66463184\n",
      "Trained batch 4622 batch loss 4.7305727 epoch total loss 5.66442966\n",
      "Trained batch 4623 batch loss 5.06447 epoch total loss 5.6643\n",
      "Trained batch 4624 batch loss 6.1739645 epoch total loss 5.66441\n",
      "Trained batch 4625 batch loss 5.23063278 epoch total loss 5.66431618\n",
      "Trained batch 4626 batch loss 5.94865704 epoch total loss 5.66437769\n",
      "Trained batch 4627 batch loss 6.08172655 epoch total loss 5.66446829\n",
      "Trained batch 4628 batch loss 5.76263094 epoch total loss 5.66448927\n",
      "Trained batch 4629 batch loss 5.50834942 epoch total loss 5.66445541\n",
      "Trained batch 4630 batch loss 5.88465452 epoch total loss 5.6645031\n",
      "Trained batch 4631 batch loss 5.8859272 epoch total loss 5.66455078\n",
      "Trained batch 4632 batch loss 5.89067888 epoch total loss 5.6646\n",
      "Trained batch 4633 batch loss 5.65832424 epoch total loss 5.66459846\n",
      "Trained batch 4634 batch loss 5.6875639 epoch total loss 5.66460323\n",
      "Trained batch 4635 batch loss 4.78644 epoch total loss 5.66441393\n",
      "Trained batch 4636 batch loss 4.56081295 epoch total loss 5.66417599\n",
      "Trained batch 4637 batch loss 4.42592144 epoch total loss 5.66390896\n",
      "Trained batch 4638 batch loss 4.2783823 epoch total loss 5.66361046\n",
      "Trained batch 4639 batch loss 5.67248535 epoch total loss 5.66361189\n",
      "Trained batch 4640 batch loss 4.13647175 epoch total loss 5.66328287\n",
      "Trained batch 4641 batch loss 4.24886036 epoch total loss 5.66297817\n",
      "Trained batch 4642 batch loss 5.35589504 epoch total loss 5.66291189\n",
      "Trained batch 4643 batch loss 4.66190434 epoch total loss 5.66269636\n",
      "Trained batch 4644 batch loss 4.83161545 epoch total loss 5.66251755\n",
      "Trained batch 4645 batch loss 5.23095322 epoch total loss 5.66242409\n",
      "Trained batch 4646 batch loss 5.04891396 epoch total loss 5.66229248\n",
      "Trained batch 4647 batch loss 5.33394766 epoch total loss 5.66222143\n",
      "Trained batch 4648 batch loss 5.78801537 epoch total loss 5.66224861\n",
      "Trained batch 4649 batch loss 4.93741322 epoch total loss 5.66209269\n",
      "Trained batch 4650 batch loss 5.16429281 epoch total loss 5.6619854\n",
      "Trained batch 4651 batch loss 5.79589748 epoch total loss 5.66201401\n",
      "Trained batch 4652 batch loss 5.36810923 epoch total loss 5.66195059\n",
      "Trained batch 4653 batch loss 6.18644047 epoch total loss 5.66206312\n",
      "Trained batch 4654 batch loss 5.44381046 epoch total loss 5.66201639\n",
      "Trained batch 4655 batch loss 5.23450518 epoch total loss 5.66192436\n",
      "Trained batch 4656 batch loss 5.27946234 epoch total loss 5.66184235\n",
      "Trained batch 4657 batch loss 4.52315474 epoch total loss 5.66159773\n",
      "Trained batch 4658 batch loss 5.72788429 epoch total loss 5.66161203\n",
      "Trained batch 4659 batch loss 5.30361557 epoch total loss 5.66153526\n",
      "Trained batch 4660 batch loss 5.90784931 epoch total loss 5.66158819\n",
      "Trained batch 4661 batch loss 6.37746859 epoch total loss 5.66174126\n",
      "Trained batch 4662 batch loss 5.54675436 epoch total loss 5.66171694\n",
      "Trained batch 4663 batch loss 5.2992568 epoch total loss 5.66163921\n",
      "Trained batch 4664 batch loss 4.72441673 epoch total loss 5.66143799\n",
      "Trained batch 4665 batch loss 4.93890953 epoch total loss 5.66128349\n",
      "Trained batch 4666 batch loss 5.82115078 epoch total loss 5.66131735\n",
      "Trained batch 4667 batch loss 5.19877148 epoch total loss 5.66121817\n",
      "Trained batch 4668 batch loss 5.45583105 epoch total loss 5.6611743\n",
      "Trained batch 4669 batch loss 5.91393089 epoch total loss 5.66122818\n",
      "Trained batch 4670 batch loss 6.31449 epoch total loss 5.66136837\n",
      "Trained batch 4671 batch loss 6.4777689 epoch total loss 5.66154337\n",
      "Trained batch 4672 batch loss 6.4010582 epoch total loss 5.6617012\n",
      "Trained batch 4673 batch loss 5.80199623 epoch total loss 5.66173172\n",
      "Trained batch 4674 batch loss 6.28956509 epoch total loss 5.66186571\n",
      "Trained batch 4675 batch loss 6.56489038 epoch total loss 5.66205883\n",
      "Trained batch 4676 batch loss 6.50402308 epoch total loss 5.66223907\n",
      "Trained batch 4677 batch loss 4.85362387 epoch total loss 5.66206598\n",
      "Trained batch 4678 batch loss 4.8881855 epoch total loss 5.66190052\n",
      "Trained batch 4679 batch loss 4.95390129 epoch total loss 5.66174936\n",
      "Trained batch 4680 batch loss 5.22727633 epoch total loss 5.66165638\n",
      "Trained batch 4681 batch loss 5.71472931 epoch total loss 5.66166735\n",
      "Trained batch 4682 batch loss 5.94034958 epoch total loss 5.66172695\n",
      "Trained batch 4683 batch loss 6.04083347 epoch total loss 5.66180801\n",
      "Trained batch 4684 batch loss 6.3737278 epoch total loss 5.66195965\n",
      "Trained batch 4685 batch loss 4.44039965 epoch total loss 5.66169882\n",
      "Trained batch 4686 batch loss 5.4379015 epoch total loss 5.66165066\n",
      "Trained batch 4687 batch loss 5.68857765 epoch total loss 5.66165686\n",
      "Trained batch 4688 batch loss 5.35503197 epoch total loss 5.66159153\n",
      "Trained batch 4689 batch loss 6.46488667 epoch total loss 5.66176271\n",
      "Trained batch 4690 batch loss 5.10370922 epoch total loss 5.66164398\n",
      "Trained batch 4691 batch loss 6.71038389 epoch total loss 5.66186762\n",
      "Trained batch 4692 batch loss 5.95869493 epoch total loss 5.66193056\n",
      "Trained batch 4693 batch loss 4.71773 epoch total loss 5.66172934\n",
      "Trained batch 4694 batch loss 6.53603268 epoch total loss 5.6619153\n",
      "Trained batch 4695 batch loss 6.56487942 epoch total loss 5.66210794\n",
      "Trained batch 4696 batch loss 6.65217352 epoch total loss 5.66231871\n",
      "Trained batch 4697 batch loss 5.91398335 epoch total loss 5.66237211\n",
      "Trained batch 4698 batch loss 5.80049229 epoch total loss 5.66240168\n",
      "Trained batch 4699 batch loss 5.58955765 epoch total loss 5.66238642\n",
      "Trained batch 4700 batch loss 4.9784956 epoch total loss 5.66224051\n",
      "Trained batch 4701 batch loss 4.99189472 epoch total loss 5.66209793\n",
      "Trained batch 4702 batch loss 4.5203805 epoch total loss 5.66185522\n",
      "Trained batch 4703 batch loss 4.534935 epoch total loss 5.66161537\n",
      "Trained batch 4704 batch loss 4.307868 epoch total loss 5.66132784\n",
      "Trained batch 4705 batch loss 4.43139553 epoch total loss 5.66106653\n",
      "Trained batch 4706 batch loss 4.87735081 epoch total loss 5.6609\n",
      "Trained batch 4707 batch loss 4.49979734 epoch total loss 5.66065311\n",
      "Trained batch 4708 batch loss 4.29801702 epoch total loss 5.66036415\n",
      "Trained batch 4709 batch loss 4.41634 epoch total loss 5.6601\n",
      "Trained batch 4710 batch loss 4.5688324 epoch total loss 5.65986824\n",
      "Trained batch 4711 batch loss 4.46849632 epoch total loss 5.65961504\n",
      "Trained batch 4712 batch loss 4.45543098 epoch total loss 5.65935946\n",
      "Trained batch 4713 batch loss 4.52463627 epoch total loss 5.65911913\n",
      "Trained batch 4714 batch loss 4.5298748 epoch total loss 5.65887928\n",
      "Trained batch 4715 batch loss 4.17486191 epoch total loss 5.65856457\n",
      "Trained batch 4716 batch loss 4.54030704 epoch total loss 5.65832758\n",
      "Trained batch 4717 batch loss 5.13054037 epoch total loss 5.658216\n",
      "Trained batch 4718 batch loss 5.02553129 epoch total loss 5.65808201\n",
      "Trained batch 4719 batch loss 4.16886854 epoch total loss 5.65776587\n",
      "Trained batch 4720 batch loss 4.19192505 epoch total loss 5.65745544\n",
      "Trained batch 4721 batch loss 4.65416098 epoch total loss 5.65724277\n",
      "Trained batch 4722 batch loss 4.45848656 epoch total loss 5.6569891\n",
      "Trained batch 4723 batch loss 4.52538109 epoch total loss 5.65674973\n",
      "Trained batch 4724 batch loss 4.27855873 epoch total loss 5.6564579\n",
      "Trained batch 4725 batch loss 5.13587046 epoch total loss 5.65634775\n",
      "Trained batch 4726 batch loss 4.52460766 epoch total loss 5.65610886\n",
      "Trained batch 4727 batch loss 4.8680954 epoch total loss 5.65594196\n",
      "Trained batch 4728 batch loss 5.42638874 epoch total loss 5.65589285\n",
      "Trained batch 4729 batch loss 5.57854652 epoch total loss 5.65587664\n",
      "Trained batch 4730 batch loss 6.09964371 epoch total loss 5.65597057\n",
      "Trained batch 4731 batch loss 6.24791431 epoch total loss 5.6560955\n",
      "Trained batch 4732 batch loss 6.35614967 epoch total loss 5.65624332\n",
      "Trained batch 4733 batch loss 5.4625926 epoch total loss 5.65620232\n",
      "Trained batch 4734 batch loss 6.24990511 epoch total loss 5.6563282\n",
      "Trained batch 4735 batch loss 5.90770721 epoch total loss 5.65638113\n",
      "Trained batch 4736 batch loss 6.32107115 epoch total loss 5.65652132\n",
      "Trained batch 4737 batch loss 5.52545261 epoch total loss 5.65649366\n",
      "Trained batch 4738 batch loss 5.38056 epoch total loss 5.65643549\n",
      "Trained batch 4739 batch loss 5.68739033 epoch total loss 5.65644217\n",
      "Trained batch 4740 batch loss 4.92126179 epoch total loss 5.65628719\n",
      "Trained batch 4741 batch loss 6.26920891 epoch total loss 5.65641642\n",
      "Trained batch 4742 batch loss 5.28902674 epoch total loss 5.65633917\n",
      "Trained batch 4743 batch loss 5.50159883 epoch total loss 5.65630627\n",
      "Trained batch 4744 batch loss 6.06097 epoch total loss 5.65639162\n",
      "Trained batch 4745 batch loss 5.92515659 epoch total loss 5.65644836\n",
      "Trained batch 4746 batch loss 6.45363808 epoch total loss 5.65661621\n",
      "Trained batch 4747 batch loss 6.96017742 epoch total loss 5.65689087\n",
      "Trained batch 4748 batch loss 6.31587648 epoch total loss 5.65703\n",
      "Trained batch 4749 batch loss 6.13721752 epoch total loss 5.65713072\n",
      "Trained batch 4750 batch loss 6.99202251 epoch total loss 5.65741205\n",
      "Trained batch 4751 batch loss 6.64179516 epoch total loss 5.65761948\n",
      "Trained batch 4752 batch loss 6.26036644 epoch total loss 5.65774632\n",
      "Trained batch 4753 batch loss 7.08755493 epoch total loss 5.6580472\n",
      "Trained batch 4754 batch loss 6.58074856 epoch total loss 5.6582408\n",
      "Trained batch 4755 batch loss 5.75989389 epoch total loss 5.65826225\n",
      "Trained batch 4756 batch loss 6.31910706 epoch total loss 5.65840101\n",
      "Trained batch 4757 batch loss 6.77518 epoch total loss 5.65863609\n",
      "Trained batch 4758 batch loss 6.80173206 epoch total loss 5.65887594\n",
      "Trained batch 4759 batch loss 6.48714542 epoch total loss 5.65905\n",
      "Trained batch 4760 batch loss 6.10364151 epoch total loss 5.65914297\n",
      "Trained batch 4761 batch loss 5.26709175 epoch total loss 5.65906096\n",
      "Trained batch 4762 batch loss 4.28216076 epoch total loss 5.65877151\n",
      "Trained batch 4763 batch loss 5.03995228 epoch total loss 5.65864134\n",
      "Trained batch 4764 batch loss 5.26180744 epoch total loss 5.65855837\n",
      "Trained batch 4765 batch loss 5.35179901 epoch total loss 5.65849352\n",
      "Trained batch 4766 batch loss 5.00277901 epoch total loss 5.65835619\n",
      "Trained batch 4767 batch loss 5.77842569 epoch total loss 5.65838146\n",
      "Trained batch 4768 batch loss 5.74270821 epoch total loss 5.65839911\n",
      "Trained batch 4769 batch loss 6.8668828 epoch total loss 5.65865231\n",
      "Trained batch 4770 batch loss 5.11911535 epoch total loss 5.6585393\n",
      "Trained batch 4771 batch loss 6.29787922 epoch total loss 5.65867329\n",
      "Trained batch 4772 batch loss 6.39029312 epoch total loss 5.65882683\n",
      "Trained batch 4773 batch loss 5.0642767 epoch total loss 5.65870237\n",
      "Trained batch 4774 batch loss 5.49092484 epoch total loss 5.65866709\n",
      "Trained batch 4775 batch loss 4.72858286 epoch total loss 5.65847206\n",
      "Trained batch 4776 batch loss 4.80451441 epoch total loss 5.65829372\n",
      "Trained batch 4777 batch loss 4.65944958 epoch total loss 5.65808439\n",
      "Trained batch 4778 batch loss 7.13830757 epoch total loss 5.65839434\n",
      "Trained batch 4779 batch loss 6.25551271 epoch total loss 5.65851927\n",
      "Trained batch 4780 batch loss 6.70990944 epoch total loss 5.65873909\n",
      "Trained batch 4781 batch loss 6.72978783 epoch total loss 5.6589632\n",
      "Trained batch 4782 batch loss 6.27584743 epoch total loss 5.65909243\n",
      "Trained batch 4783 batch loss 6.71740913 epoch total loss 5.6593132\n",
      "Trained batch 4784 batch loss 5.18061352 epoch total loss 5.65921307\n",
      "Trained batch 4785 batch loss 6.5282135 epoch total loss 5.65939474\n",
      "Trained batch 4786 batch loss 6.44053841 epoch total loss 5.65955782\n",
      "Trained batch 4787 batch loss 6.45709753 epoch total loss 5.65972471\n",
      "Trained batch 4788 batch loss 5.06871605 epoch total loss 5.65960121\n",
      "Trained batch 4789 batch loss 5.87674952 epoch total loss 5.65964651\n",
      "Trained batch 4790 batch loss 5.44770241 epoch total loss 5.65960217\n",
      "Trained batch 4791 batch loss 5.93966 epoch total loss 5.65966034\n",
      "Trained batch 4792 batch loss 5.81458282 epoch total loss 5.65969276\n",
      "Trained batch 4793 batch loss 5.35893679 epoch total loss 5.6596303\n",
      "Trained batch 4794 batch loss 5.25386286 epoch total loss 5.65954542\n",
      "Trained batch 4795 batch loss 6.17174339 epoch total loss 5.65965223\n",
      "Trained batch 4796 batch loss 6.80689335 epoch total loss 5.65989161\n",
      "Trained batch 4797 batch loss 5.92963219 epoch total loss 5.65994787\n",
      "Trained batch 4798 batch loss 6.41980076 epoch total loss 5.66010618\n",
      "Trained batch 4799 batch loss 6.28960323 epoch total loss 5.66023731\n",
      "Trained batch 4800 batch loss 5.97122383 epoch total loss 5.66030169\n",
      "Trained batch 4801 batch loss 6.11472702 epoch total loss 5.66039658\n",
      "Trained batch 4802 batch loss 5.79055548 epoch total loss 5.66042376\n",
      "Trained batch 4803 batch loss 6.24377441 epoch total loss 5.66054535\n",
      "Trained batch 4804 batch loss 6.53228474 epoch total loss 5.66072702\n",
      "Trained batch 4805 batch loss 4.86103153 epoch total loss 5.66056061\n",
      "Trained batch 4806 batch loss 5.19079208 epoch total loss 5.66046286\n",
      "Trained batch 4807 batch loss 5.44131565 epoch total loss 5.66041756\n",
      "Trained batch 4808 batch loss 5.90211344 epoch total loss 5.66046762\n",
      "Trained batch 4809 batch loss 5.51566505 epoch total loss 5.66043758\n",
      "Trained batch 4810 batch loss 5.5440464 epoch total loss 5.66041374\n",
      "Trained batch 4811 batch loss 5.70417213 epoch total loss 5.6604228\n",
      "Trained batch 4812 batch loss 5.99075699 epoch total loss 5.66049147\n",
      "Trained batch 4813 batch loss 6.25484848 epoch total loss 5.66061497\n",
      "Trained batch 4814 batch loss 6.22660637 epoch total loss 5.66073227\n",
      "Trained batch 4815 batch loss 6.36036587 epoch total loss 5.6608777\n",
      "Trained batch 4816 batch loss 6.56721878 epoch total loss 5.66106606\n",
      "Trained batch 4817 batch loss 6.45886564 epoch total loss 5.66123152\n",
      "Trained batch 4818 batch loss 6.78364944 epoch total loss 5.66146421\n",
      "Trained batch 4819 batch loss 6.61180067 epoch total loss 5.66166162\n",
      "Trained batch 4820 batch loss 6.56080246 epoch total loss 5.66184807\n",
      "Trained batch 4821 batch loss 6.33043575 epoch total loss 5.66198683\n",
      "Trained batch 4822 batch loss 6.35111332 epoch total loss 5.6621294\n",
      "Trained batch 4823 batch loss 6.17109585 epoch total loss 5.66223526\n",
      "Trained batch 4824 batch loss 5.98323298 epoch total loss 5.66230154\n",
      "Trained batch 4825 batch loss 5.69096756 epoch total loss 5.66230774\n",
      "Trained batch 4826 batch loss 5.93494892 epoch total loss 5.66236448\n",
      "Trained batch 4827 batch loss 5.41455078 epoch total loss 5.66231298\n",
      "Trained batch 4828 batch loss 4.76764 epoch total loss 5.66212749\n",
      "Trained batch 4829 batch loss 5.0074873 epoch total loss 5.66199207\n",
      "Trained batch 4830 batch loss 5.13738632 epoch total loss 5.66188335\n",
      "Trained batch 4831 batch loss 5.75651932 epoch total loss 5.6619029\n",
      "Trained batch 4832 batch loss 6.57327 epoch total loss 5.66209173\n",
      "Trained batch 4833 batch loss 6.50817966 epoch total loss 5.66226673\n",
      "Trained batch 4834 batch loss 6.11284447 epoch total loss 5.66235971\n",
      "Trained batch 4835 batch loss 4.86587334 epoch total loss 5.66219521\n",
      "Trained batch 4836 batch loss 5.63434029 epoch total loss 5.66218948\n",
      "Trained batch 4837 batch loss 5.30318356 epoch total loss 5.6621151\n",
      "Trained batch 4838 batch loss 6.22731876 epoch total loss 5.66223192\n",
      "Trained batch 4839 batch loss 5.90364361 epoch total loss 5.66228151\n",
      "Trained batch 4840 batch loss 6.2493372 epoch total loss 5.66240311\n",
      "Trained batch 4841 batch loss 5.80770397 epoch total loss 5.66243315\n",
      "Trained batch 4842 batch loss 5.94889545 epoch total loss 5.66249275\n",
      "Trained batch 4843 batch loss 5.17032242 epoch total loss 5.66239071\n",
      "Trained batch 4844 batch loss 6.00259542 epoch total loss 5.6624608\n",
      "Trained batch 4845 batch loss 4.60047436 epoch total loss 5.66224146\n",
      "Trained batch 4846 batch loss 4.37901258 epoch total loss 5.66197681\n",
      "Trained batch 4847 batch loss 4.37201786 epoch total loss 5.66171026\n",
      "Trained batch 4848 batch loss 4.31971931 epoch total loss 5.6614337\n",
      "Trained batch 4849 batch loss 4.79691267 epoch total loss 5.66125536\n",
      "Trained batch 4850 batch loss 4.70230913 epoch total loss 5.66105795\n",
      "Trained batch 4851 batch loss 4.37068748 epoch total loss 5.66079187\n",
      "Trained batch 4852 batch loss 5.1792326 epoch total loss 5.66069269\n",
      "Trained batch 4853 batch loss 4.63605118 epoch total loss 5.66048193\n",
      "Trained batch 4854 batch loss 4.88816929 epoch total loss 5.66032267\n",
      "Trained batch 4855 batch loss 5.5994215 epoch total loss 5.66031027\n",
      "Trained batch 4856 batch loss 5.13976908 epoch total loss 5.66020346\n",
      "Trained batch 4857 batch loss 5.05996513 epoch total loss 5.66008\n",
      "Trained batch 4858 batch loss 4.35022879 epoch total loss 5.65981\n",
      "Trained batch 4859 batch loss 5.39442253 epoch total loss 5.65975571\n",
      "Trained batch 4860 batch loss 4.89114285 epoch total loss 5.6595974\n",
      "Trained batch 4861 batch loss 4.78294 epoch total loss 5.65941715\n",
      "Trained batch 4862 batch loss 4.89108324 epoch total loss 5.65925884\n",
      "Trained batch 4863 batch loss 4.94850254 epoch total loss 5.65911293\n",
      "Trained batch 4864 batch loss 4.63884258 epoch total loss 5.65890312\n",
      "Trained batch 4865 batch loss 5.61452675 epoch total loss 5.65889406\n",
      "Trained batch 4866 batch loss 4.95611811 epoch total loss 5.65874958\n",
      "Trained batch 4867 batch loss 6.51807261 epoch total loss 5.65892649\n",
      "Trained batch 4868 batch loss 5.183321 epoch total loss 5.65882874\n",
      "Trained batch 4869 batch loss 4.63942337 epoch total loss 5.65861893\n",
      "Trained batch 4870 batch loss 7.16599751 epoch total loss 5.65892839\n",
      "Trained batch 4871 batch loss 6.06168079 epoch total loss 5.65901136\n",
      "Trained batch 4872 batch loss 6.4067 epoch total loss 5.65916491\n",
      "Trained batch 4873 batch loss 6.01789856 epoch total loss 5.65923834\n",
      "Trained batch 4874 batch loss 6.55392 epoch total loss 5.65942192\n",
      "Trained batch 4875 batch loss 6.11400318 epoch total loss 5.65951538\n",
      "Trained batch 4876 batch loss 6.42733145 epoch total loss 5.65967274\n",
      "Trained batch 4877 batch loss 6.16283131 epoch total loss 5.65977573\n",
      "Trained batch 4878 batch loss 5.65307093 epoch total loss 5.6597743\n",
      "Trained batch 4879 batch loss 4.21355295 epoch total loss 5.65947771\n",
      "Trained batch 4880 batch loss 6.10043669 epoch total loss 5.65956783\n",
      "Trained batch 4881 batch loss 6.05264568 epoch total loss 5.65964842\n",
      "Trained batch 4882 batch loss 6.42331409 epoch total loss 5.65980482\n",
      "Trained batch 4883 batch loss 5.78714895 epoch total loss 5.65983105\n",
      "Trained batch 4884 batch loss 5.98592377 epoch total loss 5.6598978\n",
      "Trained batch 4885 batch loss 5.44019556 epoch total loss 5.65985298\n",
      "Trained batch 4886 batch loss 5.88677645 epoch total loss 5.65989923\n",
      "Trained batch 4887 batch loss 5.85271454 epoch total loss 5.65993881\n",
      "Trained batch 4888 batch loss 5.50717926 epoch total loss 5.65990782\n",
      "Trained batch 4889 batch loss 5.78637028 epoch total loss 5.65993357\n",
      "Trained batch 4890 batch loss 5.73244381 epoch total loss 5.65994835\n",
      "Trained batch 4891 batch loss 5.81318331 epoch total loss 5.65998\n",
      "Trained batch 4892 batch loss 6.22998238 epoch total loss 5.66009617\n",
      "Trained batch 4893 batch loss 6.49352646 epoch total loss 5.66026688\n",
      "Trained batch 4894 batch loss 5.89501 epoch total loss 5.66031456\n",
      "Trained batch 4895 batch loss 6.17529249 epoch total loss 5.66042\n",
      "Trained batch 4896 batch loss 6.82259655 epoch total loss 5.66065741\n",
      "Trained batch 4897 batch loss 5.01087952 epoch total loss 5.66052485\n",
      "Trained batch 4898 batch loss 5.77892447 epoch total loss 5.66054916\n",
      "Trained batch 4899 batch loss 5.76861572 epoch total loss 5.6605711\n",
      "Trained batch 4900 batch loss 6.08230639 epoch total loss 5.66065741\n",
      "Trained batch 4901 batch loss 6.35203266 epoch total loss 5.66079807\n",
      "Trained batch 4902 batch loss 6.19682026 epoch total loss 5.66090775\n",
      "Trained batch 4903 batch loss 6.11398029 epoch total loss 5.661\n",
      "Trained batch 4904 batch loss 5.59431 epoch total loss 5.66098642\n",
      "Trained batch 4905 batch loss 5.00473595 epoch total loss 5.66085243\n",
      "Trained batch 4906 batch loss 5.19285583 epoch total loss 5.66075706\n",
      "Trained batch 4907 batch loss 6.11558342 epoch total loss 5.66084957\n",
      "Trained batch 4908 batch loss 5.94924355 epoch total loss 5.66090822\n",
      "Trained batch 4909 batch loss 5.71310234 epoch total loss 5.66091919\n",
      "Trained batch 4910 batch loss 5.18756294 epoch total loss 5.66082239\n",
      "Trained batch 4911 batch loss 5.77199364 epoch total loss 5.66084528\n",
      "Trained batch 4912 batch loss 4.93017912 epoch total loss 5.66069603\n",
      "Trained batch 4913 batch loss 5.43329334 epoch total loss 5.66065\n",
      "Trained batch 4914 batch loss 5.35362387 epoch total loss 5.66058731\n",
      "Trained batch 4915 batch loss 5.62993193 epoch total loss 5.66058159\n",
      "Trained batch 4916 batch loss 5.4442215 epoch total loss 5.66053724\n",
      "Trained batch 4917 batch loss 6.99701214 epoch total loss 5.66080904\n",
      "Trained batch 4918 batch loss 5.97817039 epoch total loss 5.66087341\n",
      "Trained batch 4919 batch loss 5.82844448 epoch total loss 5.66090727\n",
      "Trained batch 4920 batch loss 4.33306074 epoch total loss 5.66063786\n",
      "Trained batch 4921 batch loss 4.3749342 epoch total loss 5.66037655\n",
      "Trained batch 4922 batch loss 4.67172 epoch total loss 5.6601758\n",
      "Trained batch 4923 batch loss 5.80372667 epoch total loss 5.66020489\n",
      "Trained batch 4924 batch loss 5.68392 epoch total loss 5.66020966\n",
      "Trained batch 4925 batch loss 6.52113771 epoch total loss 5.66038465\n",
      "Trained batch 4926 batch loss 5.20139551 epoch total loss 5.66029167\n",
      "Trained batch 4927 batch loss 6.17825794 epoch total loss 5.66039658\n",
      "Trained batch 4928 batch loss 6.32600212 epoch total loss 5.66053152\n",
      "Trained batch 4929 batch loss 5.51820087 epoch total loss 5.66050243\n",
      "Trained batch 4930 batch loss 5.64434576 epoch total loss 5.66049957\n",
      "Trained batch 4931 batch loss 6.10814571 epoch total loss 5.66059\n",
      "Trained batch 4932 batch loss 6.1360569 epoch total loss 5.66068649\n",
      "Trained batch 4933 batch loss 6.60767269 epoch total loss 5.66087866\n",
      "Trained batch 4934 batch loss 5.77485943 epoch total loss 5.66090155\n",
      "Trained batch 4935 batch loss 5.04974 epoch total loss 5.66077757\n",
      "Trained batch 4936 batch loss 5.40785789 epoch total loss 5.66072655\n",
      "Trained batch 4937 batch loss 5.95414305 epoch total loss 5.66078615\n",
      "Trained batch 4938 batch loss 5.64723778 epoch total loss 5.66078329\n",
      "Trained batch 4939 batch loss 6.08089542 epoch total loss 5.66086817\n",
      "Trained batch 4940 batch loss 5.88191509 epoch total loss 5.66091299\n",
      "Trained batch 4941 batch loss 4.17608356 epoch total loss 5.66061258\n",
      "Trained batch 4942 batch loss 5.74183 epoch total loss 5.6606288\n",
      "Trained batch 4943 batch loss 5.79327488 epoch total loss 5.6606555\n",
      "Trained batch 4944 batch loss 5.70759583 epoch total loss 5.66066504\n",
      "Trained batch 4945 batch loss 5.64881372 epoch total loss 5.66066265\n",
      "Trained batch 4946 batch loss 5.44810677 epoch total loss 5.66061926\n",
      "Trained batch 4947 batch loss 5.56548166 epoch total loss 5.6606\n",
      "Trained batch 4948 batch loss 7.14703083 epoch total loss 5.66090059\n",
      "Trained batch 4949 batch loss 5.34041119 epoch total loss 5.66083574\n",
      "Trained batch 4950 batch loss 5.33311176 epoch total loss 5.66077\n",
      "Trained batch 4951 batch loss 5.75597 epoch total loss 5.66078901\n",
      "Trained batch 4952 batch loss 5.91508 epoch total loss 5.66084051\n",
      "Trained batch 4953 batch loss 5.61557961 epoch total loss 5.66083145\n",
      "Trained batch 4954 batch loss 5.89867783 epoch total loss 5.66087914\n",
      "Trained batch 4955 batch loss 5.71375275 epoch total loss 5.66088963\n",
      "Trained batch 4956 batch loss 5.71271229 epoch total loss 5.6609\n",
      "Trained batch 4957 batch loss 5.48519516 epoch total loss 5.66086483\n",
      "Trained batch 4958 batch loss 6.02671242 epoch total loss 5.66093874\n",
      "Trained batch 4959 batch loss 5.73532677 epoch total loss 5.66095352\n",
      "Trained batch 4960 batch loss 5.07178783 epoch total loss 5.66083479\n",
      "Trained batch 4961 batch loss 6.06973743 epoch total loss 5.66091728\n",
      "Trained batch 4962 batch loss 5.6879344 epoch total loss 5.66092253\n",
      "Trained batch 4963 batch loss 6.25203133 epoch total loss 5.66104174\n",
      "Trained batch 4964 batch loss 5.01580286 epoch total loss 5.66091156\n",
      "Trained batch 4965 batch loss 5.04929 epoch total loss 5.66078854\n",
      "Trained batch 4966 batch loss 4.97243 epoch total loss 5.66065\n",
      "Trained batch 4967 batch loss 4.08060646 epoch total loss 5.66033173\n",
      "Trained batch 4968 batch loss 5.2208147 epoch total loss 5.66024303\n",
      "Trained batch 4969 batch loss 4.18402815 epoch total loss 5.65994596\n",
      "Trained batch 4970 batch loss 5.98380089 epoch total loss 5.66001129\n",
      "Trained batch 4971 batch loss 5.10758114 epoch total loss 5.6599\n",
      "Trained batch 4972 batch loss 5.69650459 epoch total loss 5.65990782\n",
      "Trained batch 4973 batch loss 5.08952332 epoch total loss 5.6597929\n",
      "Trained batch 4974 batch loss 5.06306362 epoch total loss 5.65967274\n",
      "Trained batch 4975 batch loss 5.9661994 epoch total loss 5.65973473\n",
      "Trained batch 4976 batch loss 5.9882 epoch total loss 5.65980053\n",
      "Trained batch 4977 batch loss 5.04521799 epoch total loss 5.65967703\n",
      "Trained batch 4978 batch loss 5.20357847 epoch total loss 5.65958548\n",
      "Trained batch 4979 batch loss 4.28689194 epoch total loss 5.65931\n",
      "Trained batch 4980 batch loss 4.42618084 epoch total loss 5.65906191\n",
      "Trained batch 4981 batch loss 4.86749935 epoch total loss 5.65890312\n",
      "Trained batch 4982 batch loss 5.44350719 epoch total loss 5.65885973\n",
      "Trained batch 4983 batch loss 4.66175413 epoch total loss 5.65866\n",
      "Trained batch 4984 batch loss 5.1104 epoch total loss 5.65855\n",
      "Trained batch 4985 batch loss 4.78056717 epoch total loss 5.65837383\n",
      "Trained batch 4986 batch loss 4.66925573 epoch total loss 5.65817547\n",
      "Trained batch 4987 batch loss 4.34537649 epoch total loss 5.65791273\n",
      "Trained batch 4988 batch loss 4.36048079 epoch total loss 5.65765238\n",
      "Trained batch 4989 batch loss 4.2570467 epoch total loss 5.657372\n",
      "Trained batch 4990 batch loss 4.55870056 epoch total loss 5.6571517\n",
      "Trained batch 4991 batch loss 4.47868061 epoch total loss 5.65691566\n",
      "Trained batch 4992 batch loss 4.85773611 epoch total loss 5.65675545\n",
      "Trained batch 4993 batch loss 4.58005953 epoch total loss 5.65654\n",
      "Trained batch 4994 batch loss 4.92725611 epoch total loss 5.656394\n",
      "Trained batch 4995 batch loss 4.87363 epoch total loss 5.65623713\n",
      "Trained batch 4996 batch loss 4.98533916 epoch total loss 5.65610266\n",
      "Trained batch 4997 batch loss 4.38095856 epoch total loss 5.65584755\n",
      "Trained batch 4998 batch loss 4.95000935 epoch total loss 5.65570593\n",
      "Trained batch 4999 batch loss 5.35837221 epoch total loss 5.65564632\n",
      "Trained batch 5000 batch loss 6.22557926 epoch total loss 5.65576029\n",
      "Trained batch 5001 batch loss 6.08859301 epoch total loss 5.6558466\n",
      "Trained batch 5002 batch loss 6.33706522 epoch total loss 5.65598297\n",
      "Trained batch 5003 batch loss 6.015553 epoch total loss 5.65605497\n",
      "Trained batch 5004 batch loss 6.43141603 epoch total loss 5.65621\n",
      "Trained batch 5005 batch loss 5.82763863 epoch total loss 5.65624428\n",
      "Trained batch 5006 batch loss 6.56354332 epoch total loss 5.65642548\n",
      "Trained batch 5007 batch loss 7.16094303 epoch total loss 5.65672588\n",
      "Trained batch 5008 batch loss 6.71217 epoch total loss 5.65693665\n",
      "Trained batch 5009 batch loss 5.82714176 epoch total loss 5.6569705\n",
      "Trained batch 5010 batch loss 5.76181126 epoch total loss 5.65699148\n",
      "Trained batch 5011 batch loss 5.63805199 epoch total loss 5.65698767\n",
      "Trained batch 5012 batch loss 5.51327324 epoch total loss 5.65695906\n",
      "Trained batch 5013 batch loss 5.18052673 epoch total loss 5.65686417\n",
      "Trained batch 5014 batch loss 5.62941 epoch total loss 5.65685844\n",
      "Trained batch 5015 batch loss 5.36282253 epoch total loss 5.6568\n",
      "Trained batch 5016 batch loss 6.45634127 epoch total loss 5.65695953\n",
      "Trained batch 5017 batch loss 6.18227863 epoch total loss 5.65706396\n",
      "Trained batch 5018 batch loss 6.81279707 epoch total loss 5.65729427\n",
      "Trained batch 5019 batch loss 6.12647343 epoch total loss 5.65738773\n",
      "Trained batch 5020 batch loss 6.56094933 epoch total loss 5.65756798\n",
      "Trained batch 5021 batch loss 6.49216 epoch total loss 5.65773392\n",
      "Trained batch 5022 batch loss 6.51981068 epoch total loss 5.65790558\n",
      "Trained batch 5023 batch loss 6.99307442 epoch total loss 5.65817118\n",
      "Trained batch 5024 batch loss 6.18955326 epoch total loss 5.65827703\n",
      "Trained batch 5025 batch loss 6.23419571 epoch total loss 5.65839148\n",
      "Trained batch 5026 batch loss 6.41516542 epoch total loss 5.65854216\n",
      "Trained batch 5027 batch loss 5.71005297 epoch total loss 5.65855265\n",
      "Trained batch 5028 batch loss 4.02125788 epoch total loss 5.65822744\n",
      "Trained batch 5029 batch loss 6.3514061 epoch total loss 5.65836525\n",
      "Trained batch 5030 batch loss 6.25991631 epoch total loss 5.65848446\n",
      "Trained batch 5031 batch loss 6.38686 epoch total loss 5.65862942\n",
      "Trained batch 5032 batch loss 4.67438 epoch total loss 5.65843391\n",
      "Trained batch 5033 batch loss 5.89835358 epoch total loss 5.6584816\n",
      "Trained batch 5034 batch loss 4.56040335 epoch total loss 5.65826321\n",
      "Trained batch 5035 batch loss 4.49810934 epoch total loss 5.65803289\n",
      "Trained batch 5036 batch loss 4.54015541 epoch total loss 5.65781116\n",
      "Trained batch 5037 batch loss 4.58152103 epoch total loss 5.65759754\n",
      "Trained batch 5038 batch loss 4.77538109 epoch total loss 5.65742254\n",
      "Trained batch 5039 batch loss 3.87378073 epoch total loss 5.65706825\n",
      "Trained batch 5040 batch loss 4.30764 epoch total loss 5.65680075\n",
      "Trained batch 5041 batch loss 4.44024134 epoch total loss 5.65655899\n",
      "Trained batch 5042 batch loss 3.98562813 epoch total loss 5.65622807\n",
      "Trained batch 5043 batch loss 4.27302885 epoch total loss 5.65595388\n",
      "Trained batch 5044 batch loss 4.12243462 epoch total loss 5.65564966\n",
      "Trained batch 5045 batch loss 4.34011459 epoch total loss 5.65538883\n",
      "Trained batch 5046 batch loss 4.39081955 epoch total loss 5.65513849\n",
      "Trained batch 5047 batch loss 5.97190285 epoch total loss 5.65520144\n",
      "Trained batch 5048 batch loss 5.24349833 epoch total loss 5.65512\n",
      "Trained batch 5049 batch loss 6.45681381 epoch total loss 5.65527868\n",
      "Trained batch 5050 batch loss 6.12930918 epoch total loss 5.65537262\n",
      "Trained batch 5051 batch loss 5.83724213 epoch total loss 5.65540838\n",
      "Trained batch 5052 batch loss 5.37590933 epoch total loss 5.65535307\n",
      "Trained batch 5053 batch loss 6.19979668 epoch total loss 5.65546083\n",
      "Trained batch 5054 batch loss 5.80021763 epoch total loss 5.65548944\n",
      "Trained batch 5055 batch loss 6.27482033 epoch total loss 5.65561199\n",
      "Trained batch 5056 batch loss 6.22613716 epoch total loss 5.655725\n",
      "Trained batch 5057 batch loss 6.27858353 epoch total loss 5.6558485\n",
      "Trained batch 5058 batch loss 6.08761692 epoch total loss 5.65593386\n",
      "Trained batch 5059 batch loss 6.07877 epoch total loss 5.6560173\n",
      "Trained batch 5060 batch loss 6.04254055 epoch total loss 5.6560936\n",
      "Trained batch 5061 batch loss 6.58382225 epoch total loss 5.65627718\n",
      "Trained batch 5062 batch loss 6.1970396 epoch total loss 5.65638399\n",
      "Trained batch 5063 batch loss 5.77456188 epoch total loss 5.65640736\n",
      "Trained batch 5064 batch loss 5.34706259 epoch total loss 5.65634632\n",
      "Trained batch 5065 batch loss 5.92229843 epoch total loss 5.65639877\n",
      "Trained batch 5066 batch loss 5.20028925 epoch total loss 5.65630913\n",
      "Trained batch 5067 batch loss 5.83899307 epoch total loss 5.65634537\n",
      "Trained batch 5068 batch loss 5.93429232 epoch total loss 5.65639973\n",
      "Trained batch 5069 batch loss 6.09007 epoch total loss 5.65648556\n",
      "Trained batch 5070 batch loss 6.21955919 epoch total loss 5.65659618\n",
      "Trained batch 5071 batch loss 5.0094347 epoch total loss 5.65646887\n",
      "Trained batch 5072 batch loss 6.06946564 epoch total loss 5.65655041\n",
      "Trained batch 5073 batch loss 6.49949932 epoch total loss 5.65671682\n",
      "Trained batch 5074 batch loss 6.54830551 epoch total loss 5.6568923\n",
      "Trained batch 5075 batch loss 5.95575571 epoch total loss 5.65695143\n",
      "Trained batch 5076 batch loss 5.57622385 epoch total loss 5.65693521\n",
      "Trained batch 5077 batch loss 5.67746067 epoch total loss 5.65693951\n",
      "Trained batch 5078 batch loss 5.87744904 epoch total loss 5.6569829\n",
      "Trained batch 5079 batch loss 5.82953262 epoch total loss 5.65701675\n",
      "Trained batch 5080 batch loss 5.66790962 epoch total loss 5.65701914\n",
      "Trained batch 5081 batch loss 5.78636026 epoch total loss 5.65704441\n",
      "Trained batch 5082 batch loss 5.92844582 epoch total loss 5.65709782\n",
      "Trained batch 5083 batch loss 6.29805374 epoch total loss 5.65722418\n",
      "Trained batch 5084 batch loss 5.49071074 epoch total loss 5.65719128\n",
      "Trained batch 5085 batch loss 5.91462135 epoch total loss 5.65724182\n",
      "Trained batch 5086 batch loss 6.44558764 epoch total loss 5.65739679\n",
      "Trained batch 5087 batch loss 7.52431679 epoch total loss 5.65776348\n",
      "Trained batch 5088 batch loss 7.22627783 epoch total loss 5.65807199\n",
      "Trained batch 5089 batch loss 5.92322254 epoch total loss 5.65812397\n",
      "Trained batch 5090 batch loss 6.79633331 epoch total loss 5.65834761\n",
      "Trained batch 5091 batch loss 6.16744518 epoch total loss 5.65844774\n",
      "Trained batch 5092 batch loss 5.80379772 epoch total loss 5.65847683\n",
      "Trained batch 5093 batch loss 6.02309 epoch total loss 5.65854836\n",
      "Trained batch 5094 batch loss 5.56508636 epoch total loss 5.65852976\n",
      "Trained batch 5095 batch loss 5.7156086 epoch total loss 5.65854073\n",
      "Trained batch 5096 batch loss 6.30426359 epoch total loss 5.65866756\n",
      "Trained batch 5097 batch loss 5.80253601 epoch total loss 5.6586957\n",
      "Trained batch 5098 batch loss 6.13610315 epoch total loss 5.65878963\n",
      "Trained batch 5099 batch loss 6.39759493 epoch total loss 5.65893459\n",
      "Trained batch 5100 batch loss 5.79199743 epoch total loss 5.65896082\n",
      "Trained batch 5101 batch loss 6.1060276 epoch total loss 5.65904856\n",
      "Trained batch 5102 batch loss 4.39489365 epoch total loss 5.6588006\n",
      "Trained batch 5103 batch loss 5.3234458 epoch total loss 5.65873528\n",
      "Trained batch 5104 batch loss 5.61559248 epoch total loss 5.65872669\n",
      "Trained batch 5105 batch loss 5.55434608 epoch total loss 5.65870619\n",
      "Trained batch 5106 batch loss 6.89226246 epoch total loss 5.65894794\n",
      "Trained batch 5107 batch loss 6.00383377 epoch total loss 5.65901566\n",
      "Trained batch 5108 batch loss 6.17168856 epoch total loss 5.65911579\n",
      "Trained batch 5109 batch loss 6.30889702 epoch total loss 5.65924311\n",
      "Trained batch 5110 batch loss 6.67232513 epoch total loss 5.65944099\n",
      "Trained batch 5111 batch loss 6.74789858 epoch total loss 5.65965414\n",
      "Trained batch 5112 batch loss 6.69792843 epoch total loss 5.65985727\n",
      "Trained batch 5113 batch loss 5.69078207 epoch total loss 5.65986347\n",
      "Trained batch 5114 batch loss 6.38062906 epoch total loss 5.66000414\n",
      "Trained batch 5115 batch loss 6.07886028 epoch total loss 5.66008615\n",
      "Trained batch 5116 batch loss 5.93069935 epoch total loss 5.66013908\n",
      "Trained batch 5117 batch loss 7.11819553 epoch total loss 5.66042423\n",
      "Trained batch 5118 batch loss 6.62724733 epoch total loss 5.66061306\n",
      "Trained batch 5119 batch loss 6.38517952 epoch total loss 5.66075468\n",
      "Trained batch 5120 batch loss 6.37070656 epoch total loss 5.66089344\n",
      "Trained batch 5121 batch loss 5.98153687 epoch total loss 5.66095591\n",
      "Trained batch 5122 batch loss 5.77205086 epoch total loss 5.66097784\n",
      "Trained batch 5123 batch loss 5.66801548 epoch total loss 5.66097879\n",
      "Trained batch 5124 batch loss 6.56695366 epoch total loss 5.6611557\n",
      "Trained batch 5125 batch loss 5.34032726 epoch total loss 5.66109276\n",
      "Trained batch 5126 batch loss 5.92597342 epoch total loss 5.66114473\n",
      "Trained batch 5127 batch loss 6.06643295 epoch total loss 5.66122389\n",
      "Trained batch 5128 batch loss 5.85398293 epoch total loss 5.66126108\n",
      "Trained batch 5129 batch loss 6.29969883 epoch total loss 5.66138554\n",
      "Trained batch 5130 batch loss 6.2366333 epoch total loss 5.66149759\n",
      "Trained batch 5131 batch loss 4.72690296 epoch total loss 5.66131544\n",
      "Trained batch 5132 batch loss 5.70132256 epoch total loss 5.66132307\n",
      "Trained batch 5133 batch loss 4.94176197 epoch total loss 5.66118288\n",
      "Trained batch 5134 batch loss 5.33400965 epoch total loss 5.66111898\n",
      "Trained batch 5135 batch loss 5.5676403 epoch total loss 5.66110086\n",
      "Trained batch 5136 batch loss 5.41045666 epoch total loss 5.66105223\n",
      "Trained batch 5137 batch loss 5.64680386 epoch total loss 5.66104937\n",
      "Trained batch 5138 batch loss 4.87912369 epoch total loss 5.66089725\n",
      "Trained batch 5139 batch loss 5.21329927 epoch total loss 5.66081\n",
      "Trained batch 5140 batch loss 5.56392479 epoch total loss 5.6607914\n",
      "Trained batch 5141 batch loss 5.7559967 epoch total loss 5.66080952\n",
      "Trained batch 5142 batch loss 5.78715897 epoch total loss 5.66083431\n",
      "Trained batch 5143 batch loss 5.7330327 epoch total loss 5.66084814\n",
      "Trained batch 5144 batch loss 5.69896698 epoch total loss 5.66085577\n",
      "Trained batch 5145 batch loss 4.52510738 epoch total loss 5.66063499\n",
      "Trained batch 5146 batch loss 4.50083923 epoch total loss 5.66040945\n",
      "Trained batch 5147 batch loss 5.1666975 epoch total loss 5.66031313\n",
      "Trained batch 5148 batch loss 3.86790895 epoch total loss 5.65996504\n",
      "Trained batch 5149 batch loss 4.26867533 epoch total loss 5.65969515\n",
      "Trained batch 5150 batch loss 4.26317072 epoch total loss 5.65942383\n",
      "Trained batch 5151 batch loss 4.67151976 epoch total loss 5.65923214\n",
      "Trained batch 5152 batch loss 4.09515572 epoch total loss 5.65892887\n",
      "Trained batch 5153 batch loss 4.20606327 epoch total loss 5.65864706\n",
      "Trained batch 5154 batch loss 3.99482393 epoch total loss 5.65832424\n",
      "Trained batch 5155 batch loss 4.93370533 epoch total loss 5.65818357\n",
      "Trained batch 5156 batch loss 5.32189417 epoch total loss 5.65811825\n",
      "Trained batch 5157 batch loss 6.31265879 epoch total loss 5.65824509\n",
      "Trained batch 5158 batch loss 5.89325666 epoch total loss 5.65829039\n",
      "Trained batch 5159 batch loss 4.60341167 epoch total loss 5.6580863\n",
      "Trained batch 5160 batch loss 4.99888515 epoch total loss 5.65795803\n",
      "Trained batch 5161 batch loss 5.45509863 epoch total loss 5.65791893\n",
      "Trained batch 5162 batch loss 5.69309616 epoch total loss 5.65792561\n",
      "Trained batch 5163 batch loss 5.69453907 epoch total loss 5.65793324\n",
      "Trained batch 5164 batch loss 5.08527279 epoch total loss 5.65782213\n",
      "Trained batch 5165 batch loss 6.53633118 epoch total loss 5.65799236\n",
      "Trained batch 5166 batch loss 6.24041176 epoch total loss 5.65810537\n",
      "Trained batch 5167 batch loss 6.91654 epoch total loss 5.65834856\n",
      "Trained batch 5168 batch loss 6.33628 epoch total loss 5.65847969\n",
      "Trained batch 5169 batch loss 6.42985439 epoch total loss 5.65862894\n",
      "Trained batch 5170 batch loss 5.65138817 epoch total loss 5.65862799\n",
      "Trained batch 5171 batch loss 5.79541588 epoch total loss 5.65865421\n",
      "Trained batch 5172 batch loss 5.58784342 epoch total loss 5.65864038\n",
      "Trained batch 5173 batch loss 5.79819 epoch total loss 5.65866756\n",
      "Trained batch 5174 batch loss 6.17440796 epoch total loss 5.65876722\n",
      "Trained batch 5175 batch loss 4.76030064 epoch total loss 5.65859318\n",
      "Trained batch 5176 batch loss 5.99771929 epoch total loss 5.65865898\n",
      "Trained batch 5177 batch loss 5.57719374 epoch total loss 5.65864325\n",
      "Trained batch 5178 batch loss 6.62243652 epoch total loss 5.65882969\n",
      "Trained batch 5179 batch loss 6.51078 epoch total loss 5.6589942\n",
      "Trained batch 5180 batch loss 5.83599281 epoch total loss 5.65902853\n",
      "Trained batch 5181 batch loss 3.99665403 epoch total loss 5.65870762\n",
      "Trained batch 5182 batch loss 5.9750433 epoch total loss 5.65876865\n",
      "Trained batch 5183 batch loss 5.27594662 epoch total loss 5.65869474\n",
      "Trained batch 5184 batch loss 5.69897127 epoch total loss 5.65870237\n",
      "Trained batch 5185 batch loss 6.50680256 epoch total loss 5.65886593\n",
      "Trained batch 5186 batch loss 4.42543364 epoch total loss 5.65862799\n",
      "Trained batch 5187 batch loss 6.23950672 epoch total loss 5.65874\n",
      "Trained batch 5188 batch loss 5.60658932 epoch total loss 5.65873\n",
      "Trained batch 5189 batch loss 5.93341351 epoch total loss 5.65878296\n",
      "Trained batch 5190 batch loss 6.85048485 epoch total loss 5.65901279\n",
      "Trained batch 5191 batch loss 6.45937634 epoch total loss 5.65916681\n",
      "Trained batch 5192 batch loss 6.76068354 epoch total loss 5.65937853\n",
      "Trained batch 5193 batch loss 5.89271688 epoch total loss 5.65942383\n",
      "Trained batch 5194 batch loss 6.25157928 epoch total loss 5.65953779\n",
      "Trained batch 5195 batch loss 5.68374634 epoch total loss 5.65954208\n",
      "Trained batch 5196 batch loss 5.9790926 epoch total loss 5.6596036\n",
      "Trained batch 5197 batch loss 5.34193516 epoch total loss 5.65954256\n",
      "Trained batch 5198 batch loss 5.84584427 epoch total loss 5.65957832\n",
      "Trained batch 5199 batch loss 5.19561863 epoch total loss 5.65948915\n",
      "Trained batch 5200 batch loss 6.69455242 epoch total loss 5.65968847\n",
      "Trained batch 5201 batch loss 5.62875605 epoch total loss 5.65968227\n",
      "Trained batch 5202 batch loss 4.98691273 epoch total loss 5.65955305\n",
      "Trained batch 5203 batch loss 5.12715816 epoch total loss 5.65945053\n",
      "Trained batch 5204 batch loss 5.3502965 epoch total loss 5.65939093\n",
      "Trained batch 5205 batch loss 5.11342621 epoch total loss 5.65928602\n",
      "Trained batch 5206 batch loss 5.58910894 epoch total loss 5.65927267\n",
      "Trained batch 5207 batch loss 5.65619373 epoch total loss 5.65927219\n",
      "Trained batch 5208 batch loss 6.28679752 epoch total loss 5.65939283\n",
      "Trained batch 5209 batch loss 5.50533772 epoch total loss 5.65936327\n",
      "Trained batch 5210 batch loss 5.95189619 epoch total loss 5.65941906\n",
      "Trained batch 5211 batch loss 5.44547081 epoch total loss 5.65937805\n",
      "Trained batch 5212 batch loss 6.1167779 epoch total loss 5.65946579\n",
      "Trained batch 5213 batch loss 5.53944874 epoch total loss 5.6594429\n",
      "Trained batch 5214 batch loss 5.65026426 epoch total loss 5.65944099\n",
      "Trained batch 5215 batch loss 5.72253609 epoch total loss 5.65945339\n",
      "Trained batch 5216 batch loss 5.2805481 epoch total loss 5.65938091\n",
      "Trained batch 5217 batch loss 5.38005686 epoch total loss 5.65932751\n",
      "Trained batch 5218 batch loss 6.06431246 epoch total loss 5.65940523\n",
      "Trained batch 5219 batch loss 5.72661829 epoch total loss 5.65941811\n",
      "Trained batch 5220 batch loss 5.34456301 epoch total loss 5.65935755\n",
      "Trained batch 5221 batch loss 5.62093067 epoch total loss 5.65935\n",
      "Trained batch 5222 batch loss 6.0063076 epoch total loss 5.65941668\n",
      "Trained batch 5223 batch loss 5.42398262 epoch total loss 5.65937138\n",
      "Trained batch 5224 batch loss 6.07806778 epoch total loss 5.65945148\n",
      "Trained batch 5225 batch loss 5.40169239 epoch total loss 5.65940237\n",
      "Trained batch 5226 batch loss 5.96877813 epoch total loss 5.6594615\n",
      "Trained batch 5227 batch loss 5.6539278 epoch total loss 5.65946054\n",
      "Trained batch 5228 batch loss 6.2157383 epoch total loss 5.65956688\n",
      "Trained batch 5229 batch loss 5.98007774 epoch total loss 5.65962791\n",
      "Trained batch 5230 batch loss 4.97047186 epoch total loss 5.65949631\n",
      "Trained batch 5231 batch loss 4.57791805 epoch total loss 5.65929\n",
      "Trained batch 5232 batch loss 4.57476091 epoch total loss 5.65908241\n",
      "Trained batch 5233 batch loss 4.3150053 epoch total loss 5.6588254\n",
      "Trained batch 5234 batch loss 4.29568529 epoch total loss 5.65856457\n",
      "Trained batch 5235 batch loss 5.00930119 epoch total loss 5.65844059\n",
      "Trained batch 5236 batch loss 5.57283115 epoch total loss 5.65842438\n",
      "Trained batch 5237 batch loss 5.17209864 epoch total loss 5.65833139\n",
      "Trained batch 5238 batch loss 4.38821363 epoch total loss 5.65808916\n",
      "Trained batch 5239 batch loss 4.27610445 epoch total loss 5.65782499\n",
      "Trained batch 5240 batch loss 5.08401442 epoch total loss 5.6577158\n",
      "Trained batch 5241 batch loss 4.11430883 epoch total loss 5.65742111\n",
      "Trained batch 5242 batch loss 4.21728897 epoch total loss 5.65714645\n",
      "Trained batch 5243 batch loss 4.11786175 epoch total loss 5.65685272\n",
      "Trained batch 5244 batch loss 4.49627 epoch total loss 5.65663147\n",
      "Trained batch 5245 batch loss 4.54163313 epoch total loss 5.6564188\n",
      "Trained batch 5246 batch loss 5.16220284 epoch total loss 5.65632439\n",
      "Trained batch 5247 batch loss 5.83971882 epoch total loss 5.65635967\n",
      "Trained batch 5248 batch loss 5.24276447 epoch total loss 5.65628052\n",
      "Trained batch 5249 batch loss 5.3281889 epoch total loss 5.65621805\n",
      "Trained batch 5250 batch loss 6.18282795 epoch total loss 5.65631866\n",
      "Trained batch 5251 batch loss 5.65702391 epoch total loss 5.65631866\n",
      "Trained batch 5252 batch loss 5.8378315 epoch total loss 5.656353\n",
      "Trained batch 5253 batch loss 5.97162294 epoch total loss 5.65641308\n",
      "Trained batch 5254 batch loss 5.51194048 epoch total loss 5.65638542\n",
      "Trained batch 5255 batch loss 6.60025787 epoch total loss 5.65656471\n",
      "Trained batch 5256 batch loss 5.76340961 epoch total loss 5.65658522\n",
      "Trained batch 5257 batch loss 4.88794899 epoch total loss 5.6564393\n",
      "Trained batch 5258 batch loss 4.51000166 epoch total loss 5.65622091\n",
      "Trained batch 5259 batch loss 4.46876431 epoch total loss 5.65599537\n",
      "Trained batch 5260 batch loss 4.78190136 epoch total loss 5.65582895\n",
      "Trained batch 5261 batch loss 5.73301554 epoch total loss 5.65584326\n",
      "Trained batch 5262 batch loss 6.53622723 epoch total loss 5.6560111\n",
      "Trained batch 5263 batch loss 5.55439091 epoch total loss 5.65599155\n",
      "Trained batch 5264 batch loss 5.14195156 epoch total loss 5.65589428\n",
      "Trained batch 5265 batch loss 6.75965 epoch total loss 5.65610361\n",
      "Trained batch 5266 batch loss 6.18891907 epoch total loss 5.65620518\n",
      "Trained batch 5267 batch loss 6.4362545 epoch total loss 5.656353\n",
      "Trained batch 5268 batch loss 5.80199 epoch total loss 5.65638065\n",
      "Trained batch 5269 batch loss 6.1793251 epoch total loss 5.65648031\n",
      "Trained batch 5270 batch loss 6.06181145 epoch total loss 5.65655708\n",
      "Trained batch 5271 batch loss 5.60534763 epoch total loss 5.65654755\n",
      "Trained batch 5272 batch loss 6.06585836 epoch total loss 5.65662527\n",
      "Trained batch 5273 batch loss 5.35567 epoch total loss 5.65656805\n",
      "Trained batch 5274 batch loss 5.49721622 epoch total loss 5.65653801\n",
      "Trained batch 5275 batch loss 5.61342859 epoch total loss 5.65653\n",
      "Trained batch 5276 batch loss 5.75585222 epoch total loss 5.65654898\n",
      "Trained batch 5277 batch loss 6.06134129 epoch total loss 5.65662527\n",
      "Trained batch 5278 batch loss 5.41411114 epoch total loss 5.65657949\n",
      "Trained batch 5279 batch loss 5.65476322 epoch total loss 5.65657902\n",
      "Trained batch 5280 batch loss 6.00026798 epoch total loss 5.65664387\n",
      "Trained batch 5281 batch loss 6.04573345 epoch total loss 5.6567173\n",
      "Trained batch 5282 batch loss 5.91564369 epoch total loss 5.65676641\n",
      "Trained batch 5283 batch loss 5.35970211 epoch total loss 5.65671\n",
      "Trained batch 5284 batch loss 6.19003534 epoch total loss 5.65681124\n",
      "Trained batch 5285 batch loss 5.91957378 epoch total loss 5.65686083\n",
      "Trained batch 5286 batch loss 5.03656101 epoch total loss 5.65674353\n",
      "Trained batch 5287 batch loss 4.34986305 epoch total loss 5.65649652\n",
      "Trained batch 5288 batch loss 4.7362833 epoch total loss 5.65632248\n",
      "Trained batch 5289 batch loss 5.27518749 epoch total loss 5.65625048\n",
      "Trained batch 5290 batch loss 5.62251663 epoch total loss 5.65624428\n",
      "Trained batch 5291 batch loss 5.99561262 epoch total loss 5.65630817\n",
      "Trained batch 5292 batch loss 5.36381912 epoch total loss 5.65625286\n",
      "Trained batch 5293 batch loss 6.04203749 epoch total loss 5.65632582\n",
      "Trained batch 5294 batch loss 5.5793705 epoch total loss 5.65631151\n",
      "Trained batch 5295 batch loss 6.17853355 epoch total loss 5.65641\n",
      "Trained batch 5296 batch loss 6.0963726 epoch total loss 5.65649319\n",
      "Trained batch 5297 batch loss 5.71177101 epoch total loss 5.6565032\n",
      "Trained batch 5298 batch loss 6.12096214 epoch total loss 5.65659094\n",
      "Trained batch 5299 batch loss 6.36200714 epoch total loss 5.65672398\n",
      "Trained batch 5300 batch loss 6.38442516 epoch total loss 5.65686131\n",
      "Trained batch 5301 batch loss 5.98835945 epoch total loss 5.65692377\n",
      "Trained batch 5302 batch loss 5.84724092 epoch total loss 5.65696\n",
      "Trained batch 5303 batch loss 5.80269051 epoch total loss 5.65698719\n",
      "Trained batch 5304 batch loss 5.86853218 epoch total loss 5.65702724\n",
      "Trained batch 5305 batch loss 5.9595623 epoch total loss 5.65708447\n",
      "Trained batch 5306 batch loss 5.25620174 epoch total loss 5.65700865\n",
      "Trained batch 5307 batch loss 5.77435398 epoch total loss 5.65703058\n",
      "Trained batch 5308 batch loss 5.92154408 epoch total loss 5.65708065\n",
      "Trained batch 5309 batch loss 5.4502697 epoch total loss 5.65704155\n",
      "Trained batch 5310 batch loss 5.78123665 epoch total loss 5.65706491\n",
      "Trained batch 5311 batch loss 5.33311081 epoch total loss 5.65700436\n",
      "Trained batch 5312 batch loss 6.09848499 epoch total loss 5.65708733\n",
      "Trained batch 5313 batch loss 6.36490059 epoch total loss 5.65722036\n",
      "Trained batch 5314 batch loss 5.44969416 epoch total loss 5.65718126\n",
      "Trained batch 5315 batch loss 5.7966218 epoch total loss 5.65720749\n",
      "Trained batch 5316 batch loss 5.40881777 epoch total loss 5.65716076\n",
      "Trained batch 5317 batch loss 6.17041826 epoch total loss 5.65725708\n",
      "Trained batch 5318 batch loss 6.26957417 epoch total loss 5.65737247\n",
      "Trained batch 5319 batch loss 5.49108124 epoch total loss 5.657341\n",
      "Trained batch 5320 batch loss 5.11061764 epoch total loss 5.65723848\n",
      "Trained batch 5321 batch loss 5.10870266 epoch total loss 5.65713549\n",
      "Trained batch 5322 batch loss 5.06755543 epoch total loss 5.65702486\n",
      "Trained batch 5323 batch loss 5.52324104 epoch total loss 5.65699959\n",
      "Trained batch 5324 batch loss 5.59420824 epoch total loss 5.65698767\n",
      "Trained batch 5325 batch loss 6.02176523 epoch total loss 5.65705633\n",
      "Trained batch 5326 batch loss 5.99274874 epoch total loss 5.65711927\n",
      "Trained batch 5327 batch loss 5.53382444 epoch total loss 5.65709591\n",
      "Trained batch 5328 batch loss 5.90289211 epoch total loss 5.65714169\n",
      "Trained batch 5329 batch loss 5.72863293 epoch total loss 5.65715551\n",
      "Trained batch 5330 batch loss 4.98131847 epoch total loss 5.6570282\n",
      "Trained batch 5331 batch loss 5.92246962 epoch total loss 5.65707779\n",
      "Trained batch 5332 batch loss 5.75832176 epoch total loss 5.65709686\n",
      "Trained batch 5333 batch loss 6.03392887 epoch total loss 5.65716743\n",
      "Trained batch 5334 batch loss 5.59414577 epoch total loss 5.65715551\n",
      "Trained batch 5335 batch loss 5.55633736 epoch total loss 5.65713692\n",
      "Trained batch 5336 batch loss 5.70900869 epoch total loss 5.65714645\n",
      "Trained batch 5337 batch loss 5.56299591 epoch total loss 5.65712881\n",
      "Trained batch 5338 batch loss 5.79573441 epoch total loss 5.65715456\n",
      "Trained batch 5339 batch loss 5.29655075 epoch total loss 5.65708685\n",
      "Trained batch 5340 batch loss 5.72111797 epoch total loss 5.65709877\n",
      "Trained batch 5341 batch loss 5.29025412 epoch total loss 5.65703058\n",
      "Trained batch 5342 batch loss 5.68028927 epoch total loss 5.6570344\n",
      "Trained batch 5343 batch loss 5.92475128 epoch total loss 5.65708447\n",
      "Trained batch 5344 batch loss 4.30515623 epoch total loss 5.65683126\n",
      "Trained batch 5345 batch loss 5.06346321 epoch total loss 5.65672\n",
      "Trained batch 5346 batch loss 5.40103292 epoch total loss 5.65667248\n",
      "Trained batch 5347 batch loss 5.105093 epoch total loss 5.65656948\n",
      "Trained batch 5348 batch loss 5.01353 epoch total loss 5.65644884\n",
      "Trained batch 5349 batch loss 5.19665527 epoch total loss 5.65636301\n",
      "Trained batch 5350 batch loss 4.44957256 epoch total loss 5.65613747\n",
      "Trained batch 5351 batch loss 5.45141363 epoch total loss 5.65609932\n",
      "Trained batch 5352 batch loss 6.14812851 epoch total loss 5.65619135\n",
      "Trained batch 5353 batch loss 5.58300114 epoch total loss 5.65617752\n",
      "Trained batch 5354 batch loss 5.43743324 epoch total loss 5.65613651\n",
      "Trained batch 5355 batch loss 5.70853424 epoch total loss 5.65614653\n",
      "Trained batch 5356 batch loss 5.54353905 epoch total loss 5.65612507\n",
      "Trained batch 5357 batch loss 5.13079405 epoch total loss 5.65602732\n",
      "Trained batch 5358 batch loss 5.67072868 epoch total loss 5.6560297\n",
      "Trained batch 5359 batch loss 6.691185 epoch total loss 5.65622282\n",
      "Trained batch 5360 batch loss 5.99288321 epoch total loss 5.65628576\n",
      "Trained batch 5361 batch loss 5.9306736 epoch total loss 5.65633726\n",
      "Trained batch 5362 batch loss 5.6033783 epoch total loss 5.65632725\n",
      "Trained batch 5363 batch loss 6.93094635 epoch total loss 5.65656519\n",
      "Trained batch 5364 batch loss 6.16706 epoch total loss 5.65666056\n",
      "Trained batch 5365 batch loss 5.67224741 epoch total loss 5.65666342\n",
      "Trained batch 5366 batch loss 4.53756332 epoch total loss 5.65645456\n",
      "Trained batch 5367 batch loss 4.88196802 epoch total loss 5.65631056\n",
      "Trained batch 5368 batch loss 4.43320131 epoch total loss 5.65608263\n",
      "Trained batch 5369 batch loss 5.06620502 epoch total loss 5.65597296\n",
      "Trained batch 5370 batch loss 5.14646053 epoch total loss 5.65587807\n",
      "Trained batch 5371 batch loss 6.20609903 epoch total loss 5.65598059\n",
      "Trained batch 5372 batch loss 5.28216 epoch total loss 5.65591097\n",
      "Trained batch 5373 batch loss 5.16416121 epoch total loss 5.65581942\n",
      "Trained batch 5374 batch loss 5.5404 epoch total loss 5.65579796\n",
      "Trained batch 5375 batch loss 5.48634052 epoch total loss 5.65576649\n",
      "Trained batch 5376 batch loss 6.16594458 epoch total loss 5.65586138\n",
      "Trained batch 5377 batch loss 4.55916 epoch total loss 5.65565729\n",
      "Trained batch 5378 batch loss 5.00516033 epoch total loss 5.65553617\n",
      "Trained batch 5379 batch loss 5.58416653 epoch total loss 5.6555233\n",
      "Trained batch 5380 batch loss 5.46115971 epoch total loss 5.65548706\n",
      "Trained batch 5381 batch loss 5.79732609 epoch total loss 5.65551329\n",
      "Trained batch 5382 batch loss 4.8082695 epoch total loss 5.65535593\n",
      "Trained batch 5383 batch loss 5.70625544 epoch total loss 5.65536547\n",
      "Trained batch 5384 batch loss 6.13318253 epoch total loss 5.65545416\n",
      "Trained batch 5385 batch loss 5.41745663 epoch total loss 5.65541\n",
      "Trained batch 5386 batch loss 5.6143074 epoch total loss 5.65540266\n",
      "Trained batch 5387 batch loss 5.44831038 epoch total loss 5.65536404\n",
      "Trained batch 5388 batch loss 6.28214312 epoch total loss 5.65548038\n",
      "Trained batch 5389 batch loss 5.98288059 epoch total loss 5.65554094\n",
      "Trained batch 5390 batch loss 6.36619043 epoch total loss 5.65567255\n",
      "Trained batch 5391 batch loss 6.20063877 epoch total loss 5.65577412\n",
      "Trained batch 5392 batch loss 4.97141647 epoch total loss 5.6556468\n",
      "Trained batch 5393 batch loss 6.4611845 epoch total loss 5.65579605\n",
      "Trained batch 5394 batch loss 5.68421173 epoch total loss 5.6558013\n",
      "Trained batch 5395 batch loss 5.92558336 epoch total loss 5.65585136\n",
      "Trained batch 5396 batch loss 6.41248226 epoch total loss 5.65599155\n",
      "Trained batch 5397 batch loss 5.9825325 epoch total loss 5.65605211\n",
      "Trained batch 5398 batch loss 5.91778278 epoch total loss 5.65610075\n",
      "Trained batch 5399 batch loss 5.6496048 epoch total loss 5.65609932\n",
      "Trained batch 5400 batch loss 5.93731976 epoch total loss 5.65615177\n",
      "Trained batch 5401 batch loss 5.89021349 epoch total loss 5.65619516\n",
      "Trained batch 5402 batch loss 6.39160347 epoch total loss 5.65633154\n",
      "Trained batch 5403 batch loss 5.80438232 epoch total loss 5.65635872\n",
      "Trained batch 5404 batch loss 5.94467354 epoch total loss 5.65641212\n",
      "Trained batch 5405 batch loss 4.99497366 epoch total loss 5.65628958\n",
      "Trained batch 5406 batch loss 5.41127491 epoch total loss 5.65624475\n",
      "Trained batch 5407 batch loss 5.90664196 epoch total loss 5.65629101\n",
      "Trained batch 5408 batch loss 5.94137764 epoch total loss 5.65634346\n",
      "Trained batch 5409 batch loss 5.73470974 epoch total loss 5.65635777\n",
      "Trained batch 5410 batch loss 5.50243664 epoch total loss 5.65632963\n",
      "Trained batch 5411 batch loss 5.61418724 epoch total loss 5.65632153\n",
      "Trained batch 5412 batch loss 5.59276104 epoch total loss 5.6563096\n",
      "Trained batch 5413 batch loss 5.87919664 epoch total loss 5.65635061\n",
      "Trained batch 5414 batch loss 6.73535442 epoch total loss 5.65655\n",
      "Trained batch 5415 batch loss 7.0244503 epoch total loss 5.65680313\n",
      "Trained batch 5416 batch loss 7.02097225 epoch total loss 5.6570549\n",
      "Trained batch 5417 batch loss 7.14156055 epoch total loss 5.65732861\n",
      "Trained batch 5418 batch loss 6.9538269 epoch total loss 5.65756798\n",
      "Trained batch 5419 batch loss 6.93271446 epoch total loss 5.65780354\n",
      "Trained batch 5420 batch loss 6.8703084 epoch total loss 5.65802717\n",
      "Trained batch 5421 batch loss 6.70308876 epoch total loss 5.65822029\n",
      "Trained batch 5422 batch loss 6.49035454 epoch total loss 5.65837336\n",
      "Trained batch 5423 batch loss 5.98089075 epoch total loss 5.65843296\n",
      "Trained batch 5424 batch loss 6.17592812 epoch total loss 5.65852833\n",
      "Trained batch 5425 batch loss 5.25302076 epoch total loss 5.65845346\n",
      "Trained batch 5426 batch loss 6.176054 epoch total loss 5.65854883\n",
      "Trained batch 5427 batch loss 6.37751532 epoch total loss 5.65868139\n",
      "Trained batch 5428 batch loss 6.65940762 epoch total loss 5.65886593\n",
      "Trained batch 5429 batch loss 6.34077358 epoch total loss 5.65899134\n",
      "Trained batch 5430 batch loss 5.70815945 epoch total loss 5.6590004\n",
      "Trained batch 5431 batch loss 6.25334454 epoch total loss 5.65911\n",
      "Trained batch 5432 batch loss 5.7911272 epoch total loss 5.65913439\n",
      "Trained batch 5433 batch loss 4.12235498 epoch total loss 5.65885162\n",
      "Trained batch 5434 batch loss 4.40955067 epoch total loss 5.65862179\n",
      "Trained batch 5435 batch loss 4.86004257 epoch total loss 5.65847492\n",
      "Trained batch 5436 batch loss 5.65002632 epoch total loss 5.65847349\n",
      "Trained batch 5437 batch loss 6.06927109 epoch total loss 5.65854883\n",
      "Trained batch 5438 batch loss 4.90842104 epoch total loss 5.65841055\n",
      "Trained batch 5439 batch loss 5.49388885 epoch total loss 5.65838051\n",
      "Trained batch 5440 batch loss 5.89950609 epoch total loss 5.65842485\n",
      "Trained batch 5441 batch loss 5.05260563 epoch total loss 5.65831375\n",
      "Trained batch 5442 batch loss 5.47856712 epoch total loss 5.65828085\n",
      "Trained batch 5443 batch loss 6.06339931 epoch total loss 5.65835476\n",
      "Trained batch 5444 batch loss 5.67191601 epoch total loss 5.65835762\n",
      "Trained batch 5445 batch loss 4.92160606 epoch total loss 5.6582222\n",
      "Trained batch 5446 batch loss 4.77257729 epoch total loss 5.6580596\n",
      "Trained batch 5447 batch loss 5.71974802 epoch total loss 5.65807104\n",
      "Trained batch 5448 batch loss 5.63507843 epoch total loss 5.65806675\n",
      "Trained batch 5449 batch loss 5.85899353 epoch total loss 5.65810394\n",
      "Trained batch 5450 batch loss 3.48460793 epoch total loss 5.65770483\n",
      "Trained batch 5451 batch loss 4.98741 epoch total loss 5.65758228\n",
      "Trained batch 5452 batch loss 4.00626278 epoch total loss 5.65727901\n",
      "Trained batch 5453 batch loss 4.18931 epoch total loss 5.65701\n",
      "Trained batch 5454 batch loss 4.28822947 epoch total loss 5.65675926\n",
      "Trained batch 5455 batch loss 4.25173759 epoch total loss 5.65650177\n",
      "Trained batch 5456 batch loss 4.5806551 epoch total loss 5.65630436\n",
      "Trained batch 5457 batch loss 4.13791 epoch total loss 5.65602636\n",
      "Trained batch 5458 batch loss 4.38312721 epoch total loss 5.65579319\n",
      "Trained batch 5459 batch loss 4.08517408 epoch total loss 5.65550566\n",
      "Trained batch 5460 batch loss 4.31919622 epoch total loss 5.65526056\n",
      "Trained batch 5461 batch loss 4.50761271 epoch total loss 5.65505028\n",
      "Trained batch 5462 batch loss 5.66500235 epoch total loss 5.65505219\n",
      "Trained batch 5463 batch loss 5.18011189 epoch total loss 5.65496492\n",
      "Trained batch 5464 batch loss 5.5353694 epoch total loss 5.65494299\n",
      "Trained batch 5465 batch loss 5.56070042 epoch total loss 5.65492582\n",
      "Trained batch 5466 batch loss 5.89733696 epoch total loss 5.65497\n",
      "Trained batch 5467 batch loss 5.58554602 epoch total loss 5.65495729\n",
      "Trained batch 5468 batch loss 5.00857544 epoch total loss 5.65483904\n",
      "Trained batch 5469 batch loss 5.45817518 epoch total loss 5.65480328\n",
      "Trained batch 5470 batch loss 5.76729727 epoch total loss 5.65482378\n",
      "Trained batch 5471 batch loss 6.03733921 epoch total loss 5.65489388\n",
      "Trained batch 5472 batch loss 5.95288372 epoch total loss 5.65494823\n",
      "Trained batch 5473 batch loss 5.62078333 epoch total loss 5.65494204\n",
      "Trained batch 5474 batch loss 5.9247 epoch total loss 5.65499115\n",
      "Trained batch 5475 batch loss 4.43929863 epoch total loss 5.65476942\n",
      "Trained batch 5476 batch loss 4.94621754 epoch total loss 5.65463972\n",
      "Trained batch 5477 batch loss 4.97248602 epoch total loss 5.65451527\n",
      "Trained batch 5478 batch loss 4.35839415 epoch total loss 5.65427828\n",
      "Trained batch 5479 batch loss 3.47594857 epoch total loss 5.65388107\n",
      "Trained batch 5480 batch loss 3.55839753 epoch total loss 5.65349865\n",
      "Trained batch 5481 batch loss 3.9746685 epoch total loss 5.65319204\n",
      "Trained batch 5482 batch loss 4.0570941 epoch total loss 5.65290117\n",
      "Trained batch 5483 batch loss 4.29674 epoch total loss 5.65265369\n",
      "Trained batch 5484 batch loss 4.28187 epoch total loss 5.65240335\n",
      "Trained batch 5485 batch loss 4.49493694 epoch total loss 5.65219259\n",
      "Trained batch 5486 batch loss 4.26934481 epoch total loss 5.65194035\n",
      "Trained batch 5487 batch loss 4.15474653 epoch total loss 5.65166759\n",
      "Trained batch 5488 batch loss 4.44792128 epoch total loss 5.65144777\n",
      "Trained batch 5489 batch loss 4.31849527 epoch total loss 5.65120506\n",
      "Trained batch 5490 batch loss 4.20764828 epoch total loss 5.65094185\n",
      "Trained batch 5491 batch loss 4.28122234 epoch total loss 5.65069246\n",
      "Trained batch 5492 batch loss 4.60699 epoch total loss 5.65050268\n",
      "Trained batch 5493 batch loss 4.35713959 epoch total loss 5.65026712\n",
      "Trained batch 5494 batch loss 4.25911522 epoch total loss 5.65001392\n",
      "Trained batch 5495 batch loss 4.23380852 epoch total loss 5.64975643\n",
      "Trained batch 5496 batch loss 4.30759048 epoch total loss 5.64951229\n",
      "Trained batch 5497 batch loss 4.31703186 epoch total loss 5.64926958\n",
      "Trained batch 5498 batch loss 4.65451145 epoch total loss 5.64908886\n",
      "Trained batch 5499 batch loss 4.59602833 epoch total loss 5.64889717\n",
      "Trained batch 5500 batch loss 4.76056051 epoch total loss 5.64873552\n",
      "Trained batch 5501 batch loss 4.14147854 epoch total loss 5.64846134\n",
      "Trained batch 5502 batch loss 5.19914103 epoch total loss 5.64838\n",
      "Trained batch 5503 batch loss 5.40886307 epoch total loss 5.64833593\n",
      "Trained batch 5504 batch loss 5.17070675 epoch total loss 5.64824915\n",
      "Trained batch 5505 batch loss 5.47385788 epoch total loss 5.64821768\n",
      "Trained batch 5506 batch loss 5.64575863 epoch total loss 5.6482172\n",
      "Trained batch 5507 batch loss 5.64607716 epoch total loss 5.64821672\n",
      "Trained batch 5508 batch loss 4.29992294 epoch total loss 5.64797211\n",
      "Trained batch 5509 batch loss 4.30045319 epoch total loss 5.64772749\n",
      "Trained batch 5510 batch loss 5.4551549 epoch total loss 5.64769268\n",
      "Trained batch 5511 batch loss 5.51435423 epoch total loss 5.64766836\n",
      "Trained batch 5512 batch loss 5.6995244 epoch total loss 5.6476779\n",
      "Trained batch 5513 batch loss 5.87981796 epoch total loss 5.64772\n",
      "Trained batch 5514 batch loss 5.86189747 epoch total loss 5.64775848\n",
      "Trained batch 5515 batch loss 6.3486352 epoch total loss 5.6478858\n",
      "Trained batch 5516 batch loss 5.33352947 epoch total loss 5.64782906\n",
      "Trained batch 5517 batch loss 5.32548237 epoch total loss 5.6477704\n",
      "Trained batch 5518 batch loss 5.44528675 epoch total loss 5.64773369\n",
      "Trained batch 5519 batch loss 5.44848633 epoch total loss 5.64769793\n",
      "Trained batch 5520 batch loss 5.45970964 epoch total loss 5.64766359\n",
      "Trained batch 5521 batch loss 4.92921352 epoch total loss 5.64753342\n",
      "Trained batch 5522 batch loss 4.99430275 epoch total loss 5.64741516\n",
      "Trained batch 5523 batch loss 6.44430542 epoch total loss 5.64755964\n",
      "Trained batch 5524 batch loss 6.31248951 epoch total loss 5.64768\n",
      "Trained batch 5525 batch loss 6.24536705 epoch total loss 5.64778805\n",
      "Trained batch 5526 batch loss 5.92379904 epoch total loss 5.64783812\n",
      "Trained batch 5527 batch loss 6.49295807 epoch total loss 5.6479907\n",
      "Trained batch 5528 batch loss 5.87625742 epoch total loss 5.64803219\n",
      "Trained batch 5529 batch loss 6.15702057 epoch total loss 5.64812422\n",
      "Trained batch 5530 batch loss 5.97489214 epoch total loss 5.64818335\n",
      "Trained batch 5531 batch loss 6.22538948 epoch total loss 5.6482873\n",
      "Trained batch 5532 batch loss 6.77540255 epoch total loss 5.64849138\n",
      "Trained batch 5533 batch loss 5.75614309 epoch total loss 5.64851046\n",
      "Trained batch 5534 batch loss 5.89455318 epoch total loss 5.6485548\n",
      "Trained batch 5535 batch loss 6.38318062 epoch total loss 5.64868784\n",
      "Trained batch 5536 batch loss 6.87312317 epoch total loss 5.64890862\n",
      "Trained batch 5537 batch loss 7.32184124 epoch total loss 5.64921093\n",
      "Trained batch 5538 batch loss 5.65583038 epoch total loss 5.64921236\n",
      "Trained batch 5539 batch loss 5.78328419 epoch total loss 5.64923668\n",
      "Trained batch 5540 batch loss 6.13488483 epoch total loss 5.64932394\n",
      "Trained batch 5541 batch loss 6.19832039 epoch total loss 5.6494236\n",
      "Trained batch 5542 batch loss 6.02592945 epoch total loss 5.64949131\n",
      "Trained batch 5543 batch loss 5.95876217 epoch total loss 5.6495471\n",
      "Trained batch 5544 batch loss 6.26700687 epoch total loss 5.64965868\n",
      "Trained batch 5545 batch loss 6.78468037 epoch total loss 5.64986324\n",
      "Trained batch 5546 batch loss 5.24458075 epoch total loss 5.64979029\n",
      "Trained batch 5547 batch loss 5.27982903 epoch total loss 5.64972353\n",
      "Trained batch 5548 batch loss 6.36313248 epoch total loss 5.6498518\n",
      "Trained batch 5549 batch loss 6.4555521 epoch total loss 5.64999723\n",
      "Trained batch 5550 batch loss 6.23700953 epoch total loss 5.65010262\n",
      "Trained batch 5551 batch loss 6.39462185 epoch total loss 5.65023708\n",
      "Trained batch 5552 batch loss 5.71061468 epoch total loss 5.65024757\n",
      "Trained batch 5553 batch loss 5.96178436 epoch total loss 5.65030384\n",
      "Trained batch 5554 batch loss 5.93651295 epoch total loss 5.65035534\n",
      "Trained batch 5555 batch loss 6.05398226 epoch total loss 5.65042782\n",
      "Trained batch 5556 batch loss 5.58356762 epoch total loss 5.6504159\n",
      "Trained batch 5557 batch loss 6.49084616 epoch total loss 5.65056705\n",
      "Trained batch 5558 batch loss 6.30070591 epoch total loss 5.65068388\n",
      "Trained batch 5559 batch loss 4.7514782 epoch total loss 5.65052223\n",
      "Trained batch 5560 batch loss 5.76305103 epoch total loss 5.65054274\n",
      "Trained batch 5561 batch loss 4.98042679 epoch total loss 5.6504221\n",
      "Trained batch 5562 batch loss 5.77769375 epoch total loss 5.65044498\n",
      "Trained batch 5563 batch loss 5.6542778 epoch total loss 5.65044594\n",
      "Trained batch 5564 batch loss 5.84067822 epoch total loss 5.65048\n",
      "Trained batch 5565 batch loss 5.85986185 epoch total loss 5.65051746\n",
      "Trained batch 5566 batch loss 5.81424809 epoch total loss 5.65054655\n",
      "Trained batch 5567 batch loss 6.11238194 epoch total loss 5.65063\n",
      "Trained batch 5568 batch loss 5.4958353 epoch total loss 5.65060234\n",
      "Trained batch 5569 batch loss 5.99676275 epoch total loss 5.65066433\n",
      "Trained batch 5570 batch loss 5.94483089 epoch total loss 5.65071726\n",
      "Trained batch 5571 batch loss 6.0205164 epoch total loss 5.65078354\n",
      "Trained batch 5572 batch loss 6.10895395 epoch total loss 5.65086603\n",
      "Trained batch 5573 batch loss 6.53532124 epoch total loss 5.65102482\n",
      "Trained batch 5574 batch loss 6.7027235 epoch total loss 5.65121317\n",
      "Trained batch 5575 batch loss 6.63230228 epoch total loss 5.6513896\n",
      "Trained batch 5576 batch loss 6.84163475 epoch total loss 5.65160275\n",
      "Trained batch 5577 batch loss 6.22504902 epoch total loss 5.65170574\n",
      "Trained batch 5578 batch loss 6.14332485 epoch total loss 5.65179348\n",
      "Trained batch 5579 batch loss 6.44138622 epoch total loss 5.6519351\n",
      "Trained batch 5580 batch loss 5.28684044 epoch total loss 5.65187\n",
      "Trained batch 5581 batch loss 4.56969738 epoch total loss 5.65167618\n",
      "Trained batch 5582 batch loss 5.97352028 epoch total loss 5.6517334\n",
      "Trained batch 5583 batch loss 5.86888313 epoch total loss 5.6517725\n",
      "Trained batch 5584 batch loss 5.64198828 epoch total loss 5.65177059\n",
      "Trained batch 5585 batch loss 6.05707932 epoch total loss 5.65184355\n",
      "Trained batch 5586 batch loss 5.57280254 epoch total loss 5.65182924\n",
      "Trained batch 5587 batch loss 5.86646032 epoch total loss 5.65186739\n",
      "Trained batch 5588 batch loss 6.03248835 epoch total loss 5.65193605\n",
      "Trained batch 5589 batch loss 5.88269901 epoch total loss 5.65197706\n",
      "Trained batch 5590 batch loss 6.14966297 epoch total loss 5.65206623\n",
      "Trained batch 5591 batch loss 5.56696033 epoch total loss 5.65205097\n",
      "Trained batch 5592 batch loss 5.41098785 epoch total loss 5.65200758\n",
      "Trained batch 5593 batch loss 6.26732922 epoch total loss 5.65211773\n",
      "Trained batch 5594 batch loss 6.52723694 epoch total loss 5.65227413\n",
      "Trained batch 5595 batch loss 6.27675533 epoch total loss 5.65238619\n",
      "Trained batch 5596 batch loss 6.04262304 epoch total loss 5.65245581\n",
      "Trained batch 5597 batch loss 6.73270035 epoch total loss 5.65264893\n",
      "Trained batch 5598 batch loss 6.05339813 epoch total loss 5.65272\n",
      "Trained batch 5599 batch loss 6.70181751 epoch total loss 5.65290737\n",
      "Trained batch 5600 batch loss 6.41849709 epoch total loss 5.65304422\n",
      "Trained batch 5601 batch loss 6.43090677 epoch total loss 5.65318298\n",
      "Trained batch 5602 batch loss 5.88625336 epoch total loss 5.65322495\n",
      "Trained batch 5603 batch loss 6.36381292 epoch total loss 5.65335131\n",
      "Trained batch 5604 batch loss 5.24315929 epoch total loss 5.65327835\n",
      "Trained batch 5605 batch loss 5.35933304 epoch total loss 5.6532259\n",
      "Trained batch 5606 batch loss 6.04726505 epoch total loss 5.65329599\n",
      "Trained batch 5607 batch loss 5.60635376 epoch total loss 5.65328741\n",
      "Trained batch 5608 batch loss 5.7106123 epoch total loss 5.6532979\n",
      "Trained batch 5609 batch loss 6.14655304 epoch total loss 5.65338564\n",
      "Trained batch 5610 batch loss 5.97194576 epoch total loss 5.65344238\n",
      "Trained batch 5611 batch loss 6.08685637 epoch total loss 5.65351963\n",
      "Trained batch 5612 batch loss 5.97615576 epoch total loss 5.65357733\n",
      "Trained batch 5613 batch loss 4.96013165 epoch total loss 5.65345383\n",
      "Trained batch 5614 batch loss 5.18998957 epoch total loss 5.65337133\n",
      "Trained batch 5615 batch loss 5.31724453 epoch total loss 5.65331125\n",
      "Trained batch 5616 batch loss 4.14373589 epoch total loss 5.65304232\n",
      "Trained batch 5617 batch loss 5.93044233 epoch total loss 5.65309191\n",
      "Trained batch 5618 batch loss 5.36167145 epoch total loss 5.65304\n",
      "Trained batch 5619 batch loss 6.16226959 epoch total loss 5.65313053\n",
      "Trained batch 5620 batch loss 5.78664207 epoch total loss 5.65315437\n",
      "Trained batch 5621 batch loss 6.1244545 epoch total loss 5.6532383\n",
      "Trained batch 5622 batch loss 5.71640253 epoch total loss 5.65324926\n",
      "Trained batch 5623 batch loss 6.3456769 epoch total loss 5.65337276\n",
      "Trained batch 5624 batch loss 6.19309902 epoch total loss 5.65346861\n",
      "Trained batch 5625 batch loss 6.16324234 epoch total loss 5.65355921\n",
      "Trained batch 5626 batch loss 5.46939802 epoch total loss 5.65352631\n",
      "Trained batch 5627 batch loss 5.90814495 epoch total loss 5.65357161\n",
      "Trained batch 5628 batch loss 6.18564463 epoch total loss 5.6536665\n",
      "Trained batch 5629 batch loss 6.14337063 epoch total loss 5.65375328\n",
      "Trained batch 5630 batch loss 6.16910315 epoch total loss 5.65384483\n",
      "Trained batch 5631 batch loss 6.32549286 epoch total loss 5.65396404\n",
      "Trained batch 5632 batch loss 6.20360279 epoch total loss 5.65406179\n",
      "Trained batch 5633 batch loss 6.70999765 epoch total loss 5.65424919\n",
      "Trained batch 5634 batch loss 6.20315838 epoch total loss 5.65434694\n",
      "Trained batch 5635 batch loss 6.58000898 epoch total loss 5.65451097\n",
      "Trained batch 5636 batch loss 6.50594807 epoch total loss 5.65466213\n",
      "Trained batch 5637 batch loss 5.81956673 epoch total loss 5.6546917\n",
      "Trained batch 5638 batch loss 6.43598413 epoch total loss 5.65483\n",
      "Trained batch 5639 batch loss 6.25103188 epoch total loss 5.65493584\n",
      "Trained batch 5640 batch loss 7.13409376 epoch total loss 5.6551981\n",
      "Trained batch 5641 batch loss 6.40516806 epoch total loss 5.65533113\n",
      "Trained batch 5642 batch loss 5.778162 epoch total loss 5.65535259\n",
      "Trained batch 5643 batch loss 5.80840206 epoch total loss 5.65538\n",
      "Trained batch 5644 batch loss 6.36939049 epoch total loss 5.65550613\n",
      "Trained batch 5645 batch loss 5.91338205 epoch total loss 5.65555191\n",
      "Trained batch 5646 batch loss 5.30527258 epoch total loss 5.65549\n",
      "Trained batch 5647 batch loss 5.62032509 epoch total loss 5.65548372\n",
      "Trained batch 5648 batch loss 5.54564381 epoch total loss 5.65546417\n",
      "Trained batch 5649 batch loss 6.32117 epoch total loss 5.65558195\n",
      "Trained batch 5650 batch loss 5.91221523 epoch total loss 5.65562725\n",
      "Trained batch 5651 batch loss 5.79175 epoch total loss 5.65565157\n",
      "Trained batch 5652 batch loss 4.64727211 epoch total loss 5.65547276\n",
      "Trained batch 5653 batch loss 5.81254 epoch total loss 5.65550041\n",
      "Trained batch 5654 batch loss 6.47124767 epoch total loss 5.65564489\n",
      "Trained batch 5655 batch loss 5.89892387 epoch total loss 5.65568781\n",
      "Trained batch 5656 batch loss 5.97535276 epoch total loss 5.65574408\n",
      "Trained batch 5657 batch loss 5.57274199 epoch total loss 5.65572929\n",
      "Trained batch 5658 batch loss 5.63088036 epoch total loss 5.655725\n",
      "Trained batch 5659 batch loss 5.68461466 epoch total loss 5.65573025\n",
      "Trained batch 5660 batch loss 5.21243 epoch total loss 5.65565205\n",
      "Trained batch 5661 batch loss 6.29605198 epoch total loss 5.65576506\n",
      "Trained batch 5662 batch loss 5.94260454 epoch total loss 5.65581608\n",
      "Trained batch 5663 batch loss 6.27671146 epoch total loss 5.65592575\n",
      "Trained batch 5664 batch loss 6.47628784 epoch total loss 5.65607071\n",
      "Trained batch 5665 batch loss 6.09005451 epoch total loss 5.65614748\n",
      "Trained batch 5666 batch loss 6.02276373 epoch total loss 5.65621185\n",
      "Trained batch 5667 batch loss 5.90802383 epoch total loss 5.65625668\n",
      "Trained batch 5668 batch loss 6.29301453 epoch total loss 5.65636873\n",
      "Trained batch 5669 batch loss 6.22150326 epoch total loss 5.65646839\n",
      "Trained batch 5670 batch loss 6.38224888 epoch total loss 5.65659666\n",
      "Trained batch 5671 batch loss 6.17359352 epoch total loss 5.65668774\n",
      "Trained batch 5672 batch loss 6.29596567 epoch total loss 5.65680075\n",
      "Trained batch 5673 batch loss 5.61177969 epoch total loss 5.65679264\n",
      "Trained batch 5674 batch loss 4.95004368 epoch total loss 5.65666771\n",
      "Trained batch 5675 batch loss 5.10816097 epoch total loss 5.65657091\n",
      "Trained batch 5676 batch loss 3.76943541 epoch total loss 5.65623856\n",
      "Trained batch 5677 batch loss 5.71806526 epoch total loss 5.65624952\n",
      "Trained batch 5678 batch loss 5.78251457 epoch total loss 5.65627193\n",
      "Trained batch 5679 batch loss 5.51916885 epoch total loss 5.65624809\n",
      "Trained batch 5680 batch loss 5.49723148 epoch total loss 5.65622\n",
      "Trained batch 5681 batch loss 6.28251076 epoch total loss 5.65633059\n",
      "Trained batch 5682 batch loss 5.46702051 epoch total loss 5.65629721\n",
      "Trained batch 5683 batch loss 6.34740734 epoch total loss 5.6564188\n",
      "Trained batch 5684 batch loss 6.60637665 epoch total loss 5.65658569\n",
      "Trained batch 5685 batch loss 6.26025677 epoch total loss 5.65669203\n",
      "Trained batch 5686 batch loss 5.762918 epoch total loss 5.65671062\n",
      "Trained batch 5687 batch loss 6.22313 epoch total loss 5.65681028\n",
      "Trained batch 5688 batch loss 6.2746048 epoch total loss 5.656919\n",
      "Trained batch 5689 batch loss 6.37605572 epoch total loss 5.65704536\n",
      "Trained batch 5690 batch loss 5.3614769 epoch total loss 5.65699339\n",
      "Trained batch 5691 batch loss 5.3857317 epoch total loss 5.65694571\n",
      "Trained batch 5692 batch loss 5.63623047 epoch total loss 5.65694189\n",
      "Trained batch 5693 batch loss 5.42461 epoch total loss 5.65690136\n",
      "Trained batch 5694 batch loss 6.30419207 epoch total loss 5.65701485\n",
      "Trained batch 5695 batch loss 4.89263344 epoch total loss 5.65688086\n",
      "Trained batch 5696 batch loss 6.29795504 epoch total loss 5.65699339\n",
      "Trained batch 5697 batch loss 6.36495113 epoch total loss 5.65711784\n",
      "Trained batch 5698 batch loss 6.9281621 epoch total loss 5.65734053\n",
      "Trained batch 5699 batch loss 6.79665279 epoch total loss 5.6575408\n",
      "Trained batch 5700 batch loss 5.82877398 epoch total loss 5.65757036\n",
      "Trained batch 5701 batch loss 5.64295673 epoch total loss 5.65756798\n",
      "Trained batch 5702 batch loss 5.60899544 epoch total loss 5.65755939\n",
      "Trained batch 5703 batch loss 5.56392431 epoch total loss 5.65754318\n",
      "Trained batch 5704 batch loss 5.4706893 epoch total loss 5.65751028\n",
      "Trained batch 5705 batch loss 6.58329344 epoch total loss 5.65767288\n",
      "Trained batch 5706 batch loss 6.62844324 epoch total loss 5.65784311\n",
      "Trained batch 5707 batch loss 6.14602757 epoch total loss 5.65792847\n",
      "Trained batch 5708 batch loss 5.53267288 epoch total loss 5.65790701\n",
      "Trained batch 5709 batch loss 6.21244955 epoch total loss 5.65800381\n",
      "Trained batch 5710 batch loss 4.18805647 epoch total loss 5.65774632\n",
      "Trained batch 5711 batch loss 4.04029083 epoch total loss 5.65746355\n",
      "Trained batch 5712 batch loss 4.57954073 epoch total loss 5.65727472\n",
      "Trained batch 5713 batch loss 5.30792713 epoch total loss 5.65721369\n",
      "Trained batch 5714 batch loss 4.62909317 epoch total loss 5.65703392\n",
      "Trained batch 5715 batch loss 4.6039052 epoch total loss 5.65684938\n",
      "Trained batch 5716 batch loss 4.83031082 epoch total loss 5.6567049\n",
      "Trained batch 5717 batch loss 4.59484243 epoch total loss 5.65651941\n",
      "Trained batch 5718 batch loss 4.42981768 epoch total loss 5.65630484\n",
      "Trained batch 5719 batch loss 4.55673599 epoch total loss 5.65611219\n",
      "Trained batch 5720 batch loss 5.19079161 epoch total loss 5.65603113\n",
      "Trained batch 5721 batch loss 5.67804766 epoch total loss 5.65603495\n",
      "Trained batch 5722 batch loss 4.71403074 epoch total loss 5.65587044\n",
      "Trained batch 5723 batch loss 4.38439846 epoch total loss 5.65564823\n",
      "Trained batch 5724 batch loss 4.3181448 epoch total loss 5.65541458\n",
      "Trained batch 5725 batch loss 4.39478493 epoch total loss 5.65519428\n",
      "Trained batch 5726 batch loss 5.98574448 epoch total loss 5.65525246\n",
      "Trained batch 5727 batch loss 6.35739422 epoch total loss 5.655375\n",
      "Trained batch 5728 batch loss 6.14277649 epoch total loss 5.65546\n",
      "Trained batch 5729 batch loss 6.98575211 epoch total loss 5.6556921\n",
      "Trained batch 5730 batch loss 6.52765703 epoch total loss 5.65584421\n",
      "Trained batch 5731 batch loss 6.36609936 epoch total loss 5.65596819\n",
      "Trained batch 5732 batch loss 5.92320251 epoch total loss 5.65601492\n",
      "Trained batch 5733 batch loss 6.22802162 epoch total loss 5.65611458\n",
      "Trained batch 5734 batch loss 6.06152153 epoch total loss 5.65618515\n",
      "Trained batch 5735 batch loss 6.30566216 epoch total loss 5.65629816\n",
      "Trained batch 5736 batch loss 6.33994293 epoch total loss 5.65641737\n",
      "Trained batch 5737 batch loss 5.99898815 epoch total loss 5.65647697\n",
      "Trained batch 5738 batch loss 5.69133806 epoch total loss 5.65648317\n",
      "Trained batch 5739 batch loss 6.15723276 epoch total loss 5.65657043\n",
      "Trained batch 5740 batch loss 5.9995203 epoch total loss 5.65663052\n",
      "Trained batch 5741 batch loss 5.24793339 epoch total loss 5.65655947\n",
      "Trained batch 5742 batch loss 4.53543282 epoch total loss 5.65636396\n",
      "Trained batch 5743 batch loss 5.06336355 epoch total loss 5.65626049\n",
      "Trained batch 5744 batch loss 5.53627634 epoch total loss 5.65624\n",
      "Trained batch 5745 batch loss 4.52819777 epoch total loss 5.65604353\n",
      "Trained batch 5746 batch loss 4.65958786 epoch total loss 5.65587\n",
      "Trained batch 5747 batch loss 4.31355047 epoch total loss 5.65563679\n",
      "Trained batch 5748 batch loss 5.12608337 epoch total loss 5.65554476\n",
      "Trained batch 5749 batch loss 4.57970858 epoch total loss 5.65535736\n",
      "Trained batch 5750 batch loss 5.16477585 epoch total loss 5.65527201\n",
      "Trained batch 5751 batch loss 4.80651522 epoch total loss 5.65512466\n",
      "Trained batch 5752 batch loss 5.24103069 epoch total loss 5.65505219\n",
      "Trained batch 5753 batch loss 4.07621527 epoch total loss 5.654778\n",
      "Trained batch 5754 batch loss 4.42156696 epoch total loss 5.6545639\n",
      "Trained batch 5755 batch loss 4.55829477 epoch total loss 5.65437317\n",
      "Trained batch 5756 batch loss 4.69499 epoch total loss 5.65420675\n",
      "Trained batch 5757 batch loss 3.79365492 epoch total loss 5.65388346\n",
      "Trained batch 5758 batch loss 4.59508801 epoch total loss 5.6536994\n",
      "Trained batch 5759 batch loss 4.36275959 epoch total loss 5.65347528\n",
      "Trained batch 5760 batch loss 5.63542414 epoch total loss 5.65347242\n",
      "Trained batch 5761 batch loss 5.61836624 epoch total loss 5.65346622\n",
      "Trained batch 5762 batch loss 5.98909378 epoch total loss 5.6535244\n",
      "Trained batch 5763 batch loss 6.03577805 epoch total loss 5.65359068\n",
      "Trained batch 5764 batch loss 5.70258236 epoch total loss 5.65359926\n",
      "Trained batch 5765 batch loss 6.40080833 epoch total loss 5.65372896\n",
      "Trained batch 5766 batch loss 6.66710091 epoch total loss 5.65390444\n",
      "Trained batch 5767 batch loss 5.2427969 epoch total loss 5.65383339\n",
      "Trained batch 5768 batch loss 5.48625374 epoch total loss 5.6538043\n",
      "Trained batch 5769 batch loss 6.22734737 epoch total loss 5.65390348\n",
      "Trained batch 5770 batch loss 6.29805326 epoch total loss 5.65401506\n",
      "Trained batch 5771 batch loss 5.86318874 epoch total loss 5.6540513\n",
      "Trained batch 5772 batch loss 6.222785 epoch total loss 5.65415\n",
      "Trained batch 5773 batch loss 5.94421816 epoch total loss 5.6542\n",
      "Trained batch 5774 batch loss 6.08611584 epoch total loss 5.65427494\n",
      "Trained batch 5775 batch loss 5.38381481 epoch total loss 5.65422821\n",
      "Trained batch 5776 batch loss 6.23564386 epoch total loss 5.65432882\n",
      "Trained batch 5777 batch loss 6.5958581 epoch total loss 5.6544919\n",
      "Trained batch 5778 batch loss 6.14545393 epoch total loss 5.65457678\n",
      "Trained batch 5779 batch loss 5.9340291 epoch total loss 5.65462494\n",
      "Trained batch 5780 batch loss 5.57575512 epoch total loss 5.65461159\n",
      "Trained batch 5781 batch loss 5.60080814 epoch total loss 5.65460253\n",
      "Trained batch 5782 batch loss 5.51296806 epoch total loss 5.65457773\n",
      "Trained batch 5783 batch loss 5.22962093 epoch total loss 5.65450478\n",
      "Trained batch 5784 batch loss 5.68771648 epoch total loss 5.6545105\n",
      "Trained batch 5785 batch loss 5.99474621 epoch total loss 5.65456915\n",
      "Trained batch 5786 batch loss 5.45152044 epoch total loss 5.65453386\n",
      "Trained batch 5787 batch loss 5.42374611 epoch total loss 5.65449381\n",
      "Trained batch 5788 batch loss 5.88765907 epoch total loss 5.65453386\n",
      "Trained batch 5789 batch loss 5.90363789 epoch total loss 5.65457726\n",
      "Trained batch 5790 batch loss 4.75231361 epoch total loss 5.65442133\n",
      "Trained batch 5791 batch loss 5.94183445 epoch total loss 5.65447092\n",
      "Trained batch 5792 batch loss 5.28975916 epoch total loss 5.65440798\n",
      "Trained batch 5793 batch loss 5.15307093 epoch total loss 5.65432119\n",
      "Trained batch 5794 batch loss 6.17321253 epoch total loss 5.65441084\n",
      "Trained batch 5795 batch loss 6.09661531 epoch total loss 5.65448713\n",
      "Trained batch 5796 batch loss 5.88131332 epoch total loss 5.65452623\n",
      "Trained batch 5797 batch loss 4.90402317 epoch total loss 5.65439606\n",
      "Trained batch 5798 batch loss 4.97339821 epoch total loss 5.65427876\n",
      "Trained batch 5799 batch loss 5.71541834 epoch total loss 5.65428925\n",
      "Trained batch 5800 batch loss 5.55370283 epoch total loss 5.65427208\n",
      "Trained batch 5801 batch loss 5.76954222 epoch total loss 5.65429163\n",
      "Trained batch 5802 batch loss 5.4666481 epoch total loss 5.6542592\n",
      "Trained batch 5803 batch loss 5.84362793 epoch total loss 5.65429163\n",
      "Trained batch 5804 batch loss 5.98694229 epoch total loss 5.65434933\n",
      "Trained batch 5805 batch loss 5.95021057 epoch total loss 5.65440035\n",
      "Trained batch 5806 batch loss 5.88492298 epoch total loss 5.6544404\n",
      "Trained batch 5807 batch loss 5.81203079 epoch total loss 5.65446758\n",
      "Trained batch 5808 batch loss 5.90995836 epoch total loss 5.65451145\n",
      "Trained batch 5809 batch loss 5.89015102 epoch total loss 5.65455198\n",
      "Trained batch 5810 batch loss 5.87336588 epoch total loss 5.65459\n",
      "Trained batch 5811 batch loss 5.62717867 epoch total loss 5.65458536\n",
      "Trained batch 5812 batch loss 5.41222858 epoch total loss 5.65454435\n",
      "Trained batch 5813 batch loss 5.37695265 epoch total loss 5.65449619\n",
      "Trained batch 5814 batch loss 5.93548 epoch total loss 5.65454435\n",
      "Trained batch 5815 batch loss 5.41307068 epoch total loss 5.65450287\n",
      "Trained batch 5816 batch loss 5.83204651 epoch total loss 5.65453339\n",
      "Trained batch 5817 batch loss 5.09594488 epoch total loss 5.65443754\n",
      "Trained batch 5818 batch loss 6.05612278 epoch total loss 5.65450621\n",
      "Trained batch 5819 batch loss 5.63263702 epoch total loss 5.65450239\n",
      "Trained batch 5820 batch loss 5.02862358 epoch total loss 5.6543951\n",
      "Trained batch 5821 batch loss 5.89475679 epoch total loss 5.65443611\n",
      "Trained batch 5822 batch loss 6.60164547 epoch total loss 5.65459871\n",
      "Trained batch 5823 batch loss 6.40589714 epoch total loss 5.65472794\n",
      "Trained batch 5824 batch loss 4.9832654 epoch total loss 5.65461302\n",
      "Trained batch 5825 batch loss 4.24851131 epoch total loss 5.65437174\n",
      "Trained batch 5826 batch loss 5.35341549 epoch total loss 5.65432\n",
      "Trained batch 5827 batch loss 5.80721092 epoch total loss 5.65434599\n",
      "Trained batch 5828 batch loss 6.22437143 epoch total loss 5.65444374\n",
      "Trained batch 5829 batch loss 5.92935371 epoch total loss 5.65449095\n",
      "Trained batch 5830 batch loss 4.34864473 epoch total loss 5.65426683\n",
      "Trained batch 5831 batch loss 4.57915401 epoch total loss 5.6540823\n",
      "Trained batch 5832 batch loss 4.77548504 epoch total loss 5.65393162\n",
      "Trained batch 5833 batch loss 5.13851833 epoch total loss 5.6538434\n",
      "Trained batch 5834 batch loss 6.02362728 epoch total loss 5.65390635\n",
      "Trained batch 5835 batch loss 5.80683088 epoch total loss 5.65393305\n",
      "Trained batch 5836 batch loss 5.58431 epoch total loss 5.6539216\n",
      "Trained batch 5837 batch loss 5.51273918 epoch total loss 5.65389681\n",
      "Trained batch 5838 batch loss 5.16650772 epoch total loss 5.65381384\n",
      "Trained batch 5839 batch loss 5.7600112 epoch total loss 5.65383244\n",
      "Trained batch 5840 batch loss 5.72945499 epoch total loss 5.65384531\n",
      "Trained batch 5841 batch loss 5.63351059 epoch total loss 5.65384197\n",
      "Trained batch 5842 batch loss 5.58030415 epoch total loss 5.65382957\n",
      "Trained batch 5843 batch loss 5.55907536 epoch total loss 5.65381336\n",
      "Trained batch 5844 batch loss 5.8407197 epoch total loss 5.65384483\n",
      "Trained batch 5845 batch loss 5.91232967 epoch total loss 5.65388966\n",
      "Trained batch 5846 batch loss 5.57893 epoch total loss 5.65387678\n",
      "Trained batch 5847 batch loss 5.43728256 epoch total loss 5.65383959\n",
      "Trained batch 5848 batch loss 5.75403309 epoch total loss 5.65385675\n",
      "Trained batch 5849 batch loss 5.49179554 epoch total loss 5.6538291\n",
      "Trained batch 5850 batch loss 5.60946369 epoch total loss 5.65382147\n",
      "Trained batch 5851 batch loss 5.1118 epoch total loss 5.65372896\n",
      "Trained batch 5852 batch loss 5.87374449 epoch total loss 5.65376711\n",
      "Trained batch 5853 batch loss 5.86639404 epoch total loss 5.65380335\n",
      "Trained batch 5854 batch loss 5.34060192 epoch total loss 5.65375\n",
      "Trained batch 5855 batch loss 5.43770599 epoch total loss 5.65371275\n",
      "Trained batch 5856 batch loss 6.2052 epoch total loss 5.65380716\n",
      "Trained batch 5857 batch loss 5.4930439 epoch total loss 5.65377951\n",
      "Trained batch 5858 batch loss 5.66258717 epoch total loss 5.65378141\n",
      "Trained batch 5859 batch loss 5.12010717 epoch total loss 5.65369034\n",
      "Trained batch 5860 batch loss 5.77662754 epoch total loss 5.6537118\n",
      "Trained batch 5861 batch loss 5.5463419 epoch total loss 5.6536932\n",
      "Trained batch 5862 batch loss 5.50724316 epoch total loss 5.6536684\n",
      "Trained batch 5863 batch loss 5.60173702 epoch total loss 5.65366\n",
      "Trained batch 5864 batch loss 5.66857147 epoch total loss 5.6536622\n",
      "Trained batch 5865 batch loss 5.41999531 epoch total loss 5.65362263\n",
      "Trained batch 5866 batch loss 6.11376095 epoch total loss 5.65370083\n",
      "Trained batch 5867 batch loss 5.12904644 epoch total loss 5.65361166\n",
      "Trained batch 5868 batch loss 5.38770151 epoch total loss 5.65356588\n",
      "Trained batch 5869 batch loss 6.00128078 epoch total loss 5.65362501\n",
      "Trained batch 5870 batch loss 5.8730154 epoch total loss 5.6536622\n",
      "Trained batch 5871 batch loss 6.17025661 epoch total loss 5.65375042\n",
      "Trained batch 5872 batch loss 5.73654 epoch total loss 5.65376472\n",
      "Trained batch 5873 batch loss 5.61226225 epoch total loss 5.65375757\n",
      "Trained batch 5874 batch loss 5.65159035 epoch total loss 5.65375757\n",
      "Trained batch 5875 batch loss 5.73602 epoch total loss 5.6537714\n",
      "Trained batch 5876 batch loss 5.23800373 epoch total loss 5.65370035\n",
      "Trained batch 5877 batch loss 4.91642666 epoch total loss 5.65357542\n",
      "Trained batch 5878 batch loss 5.27398682 epoch total loss 5.65351057\n",
      "Trained batch 5879 batch loss 5.87619209 epoch total loss 5.65354824\n",
      "Trained batch 5880 batch loss 5.91594028 epoch total loss 5.65359259\n",
      "Trained batch 5881 batch loss 5.63802433 epoch total loss 5.65358973\n",
      "Trained batch 5882 batch loss 5.83459044 epoch total loss 5.65362072\n",
      "Trained batch 5883 batch loss 5.83709 epoch total loss 5.65365171\n",
      "Trained batch 5884 batch loss 5.47987652 epoch total loss 5.65362215\n",
      "Trained batch 5885 batch loss 5.89687538 epoch total loss 5.65366411\n",
      "Trained batch 5886 batch loss 5.33713055 epoch total loss 5.65361\n",
      "Trained batch 5887 batch loss 5.92927 epoch total loss 5.65365696\n",
      "Trained batch 5888 batch loss 6.39028454 epoch total loss 5.65378189\n",
      "Trained batch 5889 batch loss 5.60116673 epoch total loss 5.65377331\n",
      "Trained batch 5890 batch loss 5.34185696 epoch total loss 5.65372038\n",
      "Trained batch 5891 batch loss 6.02780962 epoch total loss 5.6537838\n",
      "Trained batch 5892 batch loss 5.62623501 epoch total loss 5.65377903\n",
      "Trained batch 5893 batch loss 5.92661762 epoch total loss 5.65382528\n",
      "Trained batch 5894 batch loss 6.40760803 epoch total loss 5.65395308\n",
      "Trained batch 5895 batch loss 5.53202248 epoch total loss 5.65393209\n",
      "Trained batch 5896 batch loss 5.63096142 epoch total loss 5.65392828\n",
      "Trained batch 5897 batch loss 5.99499893 epoch total loss 5.65398645\n",
      "Trained batch 5898 batch loss 6.14693451 epoch total loss 5.65407038\n",
      "Trained batch 5899 batch loss 6.0228 epoch total loss 5.65413284\n",
      "Trained batch 5900 batch loss 6.38387585 epoch total loss 5.65425634\n",
      "Trained batch 5901 batch loss 6.55878305 epoch total loss 5.65441\n",
      "Trained batch 5902 batch loss 6.28674793 epoch total loss 5.6545167\n",
      "Trained batch 5903 batch loss 5.6335144 epoch total loss 5.65451288\n",
      "Trained batch 5904 batch loss 5.83376 epoch total loss 5.65454292\n",
      "Trained batch 5905 batch loss 6.12157202 epoch total loss 5.65462208\n",
      "Trained batch 5906 batch loss 5.81856585 epoch total loss 5.65465\n",
      "Trained batch 5907 batch loss 3.31321955 epoch total loss 5.65425348\n",
      "Trained batch 5908 batch loss 4.25829649 epoch total loss 5.65401697\n",
      "Trained batch 5909 batch loss 3.91704893 epoch total loss 5.65372324\n",
      "Trained batch 5910 batch loss 4.19233036 epoch total loss 5.65347576\n",
      "Trained batch 5911 batch loss 3.81644726 epoch total loss 5.65316534\n",
      "Trained batch 5912 batch loss 4.94455481 epoch total loss 5.65304565\n",
      "Trained batch 5913 batch loss 5.54551792 epoch total loss 5.65302753\n",
      "Trained batch 5914 batch loss 5.8333149 epoch total loss 5.65305758\n",
      "Trained batch 5915 batch loss 5.5969162 epoch total loss 5.65304852\n",
      "Trained batch 5916 batch loss 5.63236618 epoch total loss 5.65304518\n",
      "Trained batch 5917 batch loss 5.96775198 epoch total loss 5.65309811\n",
      "Trained batch 5918 batch loss 7.31359339 epoch total loss 5.65337896\n",
      "Trained batch 5919 batch loss 6.02350521 epoch total loss 5.65344143\n",
      "Trained batch 5920 batch loss 5.84600544 epoch total loss 5.65347385\n",
      "Trained batch 5921 batch loss 5.76876879 epoch total loss 5.65349388\n",
      "Trained batch 5922 batch loss 6.06735802 epoch total loss 5.6535635\n",
      "Trained batch 5923 batch loss 5.62999344 epoch total loss 5.65355921\n",
      "Trained batch 5924 batch loss 5.93175888 epoch total loss 5.65360641\n",
      "Trained batch 5925 batch loss 5.55380249 epoch total loss 5.65358973\n",
      "Trained batch 5926 batch loss 4.93899 epoch total loss 5.65346909\n",
      "Trained batch 5927 batch loss 4.78260231 epoch total loss 5.65332174\n",
      "Trained batch 5928 batch loss 5.2892952 epoch total loss 5.65326\n",
      "Trained batch 5929 batch loss 5.7097 epoch total loss 5.65327024\n",
      "Trained batch 5930 batch loss 5.97033 epoch total loss 5.65332317\n",
      "Trained batch 5931 batch loss 6.01107788 epoch total loss 5.65338373\n",
      "Trained batch 5932 batch loss 5.91522217 epoch total loss 5.6534276\n",
      "Trained batch 5933 batch loss 5.2860651 epoch total loss 5.65336561\n",
      "Trained batch 5934 batch loss 5.90906382 epoch total loss 5.653409\n",
      "Trained batch 5935 batch loss 5.56231594 epoch total loss 5.65339375\n",
      "Trained batch 5936 batch loss 6.79673672 epoch total loss 5.65358639\n",
      "Trained batch 5937 batch loss 6.9508543 epoch total loss 5.6538043\n",
      "Trained batch 5938 batch loss 6.22920799 epoch total loss 5.65390158\n",
      "Trained batch 5939 batch loss 6.20924282 epoch total loss 5.65399551\n",
      "Trained batch 5940 batch loss 6.22504425 epoch total loss 5.65409184\n",
      "Trained batch 5941 batch loss 6.26922846 epoch total loss 5.65419531\n",
      "Trained batch 5942 batch loss 5.76997375 epoch total loss 5.65421486\n",
      "Trained batch 5943 batch loss 6.1749773 epoch total loss 5.6543026\n",
      "Trained batch 5944 batch loss 6.56031895 epoch total loss 5.65445471\n",
      "Trained batch 5945 batch loss 5.5135994 epoch total loss 5.65443039\n",
      "Trained batch 5946 batch loss 6.44528866 epoch total loss 5.65456343\n",
      "Trained batch 5947 batch loss 6.00196314 epoch total loss 5.65462255\n",
      "Trained batch 5948 batch loss 6.22017193 epoch total loss 5.65471697\n",
      "Trained batch 5949 batch loss 6.2814436 epoch total loss 5.65482235\n",
      "Trained batch 5950 batch loss 6.1134119 epoch total loss 5.6548996\n",
      "Trained batch 5951 batch loss 7.20652533 epoch total loss 5.65516043\n",
      "Trained batch 5952 batch loss 5.40124559 epoch total loss 5.65511799\n",
      "Trained batch 5953 batch loss 6.6536088 epoch total loss 5.65528536\n",
      "Trained batch 5954 batch loss 5.03236389 epoch total loss 5.65518045\n",
      "Trained batch 5955 batch loss 5.04073381 epoch total loss 5.65507698\n",
      "Trained batch 5956 batch loss 5.66646814 epoch total loss 5.65507936\n",
      "Trained batch 5957 batch loss 4.87764168 epoch total loss 5.65494919\n",
      "Trained batch 5958 batch loss 5.60903215 epoch total loss 5.65494156\n",
      "Trained batch 5959 batch loss 5.20460224 epoch total loss 5.65486574\n",
      "Trained batch 5960 batch loss 4.34804296 epoch total loss 5.6546464\n",
      "Trained batch 5961 batch loss 5.15563345 epoch total loss 5.65456247\n",
      "Trained batch 5962 batch loss 5.46667194 epoch total loss 5.654531\n",
      "Trained batch 5963 batch loss 5.56558323 epoch total loss 5.65451622\n",
      "Trained batch 5964 batch loss 6.11100388 epoch total loss 5.65459204\n",
      "Trained batch 5965 batch loss 5.87746286 epoch total loss 5.65462971\n",
      "Trained batch 5966 batch loss 6.37947845 epoch total loss 5.6547513\n",
      "Trained batch 5967 batch loss 6.05493355 epoch total loss 5.65481853\n",
      "Trained batch 5968 batch loss 4.82217789 epoch total loss 5.65467834\n",
      "Trained batch 5969 batch loss 5.27405739 epoch total loss 5.65461445\n",
      "Trained batch 5970 batch loss 5.79292059 epoch total loss 5.65463781\n",
      "Trained batch 5971 batch loss 5.99295139 epoch total loss 5.65469408\n",
      "Trained batch 5972 batch loss 5.76768398 epoch total loss 5.65471363\n",
      "Trained batch 5973 batch loss 5.74165392 epoch total loss 5.65472841\n",
      "Trained batch 5974 batch loss 4.8325038 epoch total loss 5.65459061\n",
      "Trained batch 5975 batch loss 4.84802341 epoch total loss 5.65445518\n",
      "Trained batch 5976 batch loss 5.14353848 epoch total loss 5.65437031\n",
      "Trained batch 5977 batch loss 5.02404499 epoch total loss 5.65426445\n",
      "Trained batch 5978 batch loss 4.5470829 epoch total loss 5.65407944\n",
      "Trained batch 5979 batch loss 6.35906792 epoch total loss 5.65419722\n",
      "Trained batch 5980 batch loss 5.85586357 epoch total loss 5.65423107\n",
      "Trained batch 5981 batch loss 5.74083042 epoch total loss 5.65424538\n",
      "Trained batch 5982 batch loss 6.21678 epoch total loss 5.65433931\n",
      "Trained batch 5983 batch loss 6.53487873 epoch total loss 5.65448666\n",
      "Trained batch 5984 batch loss 5.59228182 epoch total loss 5.65447617\n",
      "Trained batch 5985 batch loss 5.36549902 epoch total loss 5.65442848\n",
      "Trained batch 5986 batch loss 5.68864202 epoch total loss 5.65443373\n",
      "Trained batch 5987 batch loss 5.76262093 epoch total loss 5.65445185\n",
      "Trained batch 5988 batch loss 5.81636906 epoch total loss 5.65447903\n",
      "Trained batch 5989 batch loss 5.82225895 epoch total loss 5.65450668\n",
      "Trained batch 5990 batch loss 5.4555068 epoch total loss 5.65447378\n",
      "Trained batch 5991 batch loss 5.4579668 epoch total loss 5.65444088\n",
      "Trained batch 5992 batch loss 5.6074338 epoch total loss 5.65443325\n",
      "Trained batch 5993 batch loss 5.4463954 epoch total loss 5.65439844\n",
      "Trained batch 5994 batch loss 5.85695314 epoch total loss 5.65443182\n",
      "Trained batch 5995 batch loss 6.17602491 epoch total loss 5.6545186\n",
      "Trained batch 5996 batch loss 6.11813164 epoch total loss 5.65459585\n",
      "Trained batch 5997 batch loss 5.72343731 epoch total loss 5.6546073\n",
      "Trained batch 5998 batch loss 6.15956783 epoch total loss 5.6546917\n",
      "Trained batch 5999 batch loss 6.13282537 epoch total loss 5.65477133\n",
      "Trained batch 6000 batch loss 5.79996681 epoch total loss 5.65479565\n",
      "Trained batch 6001 batch loss 4.71277094 epoch total loss 5.65463829\n",
      "Trained batch 6002 batch loss 6.55863953 epoch total loss 5.65478897\n",
      "Trained batch 6003 batch loss 6.69453144 epoch total loss 5.65496206\n",
      "Trained batch 6004 batch loss 5.63608027 epoch total loss 5.6549592\n",
      "Trained batch 6005 batch loss 6.09932518 epoch total loss 5.65503311\n",
      "Trained batch 6006 batch loss 5.93871307 epoch total loss 5.65508\n",
      "Trained batch 6007 batch loss 5.56732368 epoch total loss 5.65506506\n",
      "Trained batch 6008 batch loss 6.29135323 epoch total loss 5.65517139\n",
      "Trained batch 6009 batch loss 5.37832165 epoch total loss 5.65512562\n",
      "Trained batch 6010 batch loss 6.64315224 epoch total loss 5.65529\n",
      "Trained batch 6011 batch loss 6.2282238 epoch total loss 5.65538502\n",
      "Trained batch 6012 batch loss 6.33651924 epoch total loss 5.65549803\n",
      "Trained batch 6013 batch loss 6.14128304 epoch total loss 5.65557909\n",
      "Trained batch 6014 batch loss 6.40574455 epoch total loss 5.65570354\n",
      "Trained batch 6015 batch loss 6.4040513 epoch total loss 5.655828\n",
      "Trained batch 6016 batch loss 6.18578386 epoch total loss 5.65591621\n",
      "Trained batch 6017 batch loss 5.92646217 epoch total loss 5.65596104\n",
      "Trained batch 6018 batch loss 6.28553915 epoch total loss 5.65606546\n",
      "Trained batch 6019 batch loss 6.31339645 epoch total loss 5.65617466\n",
      "Trained batch 6020 batch loss 6.12021923 epoch total loss 5.65625191\n",
      "Trained batch 6021 batch loss 5.8352623 epoch total loss 5.65628195\n",
      "Trained batch 6022 batch loss 6.08918619 epoch total loss 5.65635395\n",
      "Trained batch 6023 batch loss 6.31633806 epoch total loss 5.65646315\n",
      "Trained batch 6024 batch loss 5.57936859 epoch total loss 5.65645027\n",
      "Trained batch 6025 batch loss 6.33024216 epoch total loss 5.65656233\n",
      "Trained batch 6026 batch loss 6.6008091 epoch total loss 5.65671921\n",
      "Trained batch 6027 batch loss 6.43297195 epoch total loss 5.65684843\n",
      "Trained batch 6028 batch loss 6.57247829 epoch total loss 5.65700054\n",
      "Trained batch 6029 batch loss 6.63920498 epoch total loss 5.65716362\n",
      "Trained batch 6030 batch loss 6.39998913 epoch total loss 5.65728664\n",
      "Trained batch 6031 batch loss 6.29344416 epoch total loss 5.65739202\n",
      "Trained batch 6032 batch loss 5.93668079 epoch total loss 5.65743828\n",
      "Trained batch 6033 batch loss 6.03695822 epoch total loss 5.65750074\n",
      "Trained batch 6034 batch loss 6.03333759 epoch total loss 5.65756369\n",
      "Trained batch 6035 batch loss 6.64731169 epoch total loss 5.65772772\n",
      "Trained batch 6036 batch loss 6.30152321 epoch total loss 5.65783405\n",
      "Trained batch 6037 batch loss 6.37947083 epoch total loss 5.65795374\n",
      "Trained batch 6038 batch loss 6.26740026 epoch total loss 5.65805435\n",
      "Trained batch 6039 batch loss 5.97331381 epoch total loss 5.65810633\n",
      "Trained batch 6040 batch loss 5.84933901 epoch total loss 5.6581378\n",
      "Trained batch 6041 batch loss 5.63237524 epoch total loss 5.65813351\n",
      "Trained batch 6042 batch loss 6.32889652 epoch total loss 5.65824461\n",
      "Trained batch 6043 batch loss 6.013134 epoch total loss 5.65830278\n",
      "Trained batch 6044 batch loss 6.24485874 epoch total loss 5.6584\n",
      "Trained batch 6045 batch loss 6.29653931 epoch total loss 5.65850592\n",
      "Trained batch 6046 batch loss 6.01762 epoch total loss 5.65856552\n",
      "Trained batch 6047 batch loss 5.57136536 epoch total loss 5.65855122\n",
      "Trained batch 6048 batch loss 5.65180492 epoch total loss 5.65855\n",
      "Trained batch 6049 batch loss 6.14340687 epoch total loss 5.65863037\n",
      "Trained batch 6050 batch loss 5.55525732 epoch total loss 5.6586132\n",
      "Trained batch 6051 batch loss 6.211339 epoch total loss 5.65870428\n",
      "Trained batch 6052 batch loss 6.17605829 epoch total loss 5.65878963\n",
      "Trained batch 6053 batch loss 6.06825733 epoch total loss 5.65885735\n",
      "Trained batch 6054 batch loss 5.5978632 epoch total loss 5.65884686\n",
      "Trained batch 6055 batch loss 5.79566526 epoch total loss 5.65886974\n",
      "Trained batch 6056 batch loss 5.0334816 epoch total loss 5.65876675\n",
      "Trained batch 6057 batch loss 5.26224327 epoch total loss 5.65870142\n",
      "Trained batch 6058 batch loss 6.04267788 epoch total loss 5.65876484\n",
      "Trained batch 6059 batch loss 6.24310732 epoch total loss 5.65886116\n",
      "Trained batch 6060 batch loss 5.72005 epoch total loss 5.6588707\n",
      "Trained batch 6061 batch loss 5.87335396 epoch total loss 5.65890646\n",
      "Trained batch 6062 batch loss 5.74537086 epoch total loss 5.65892076\n",
      "Trained batch 6063 batch loss 6.36233234 epoch total loss 5.65903711\n",
      "Trained batch 6064 batch loss 6.04865 epoch total loss 5.65910101\n",
      "Trained batch 6065 batch loss 5.66916275 epoch total loss 5.65910244\n",
      "Trained batch 6066 batch loss 4.92576885 epoch total loss 5.6589818\n",
      "Trained batch 6067 batch loss 5.3547411 epoch total loss 5.65893173\n",
      "Trained batch 6068 batch loss 5.89728212 epoch total loss 5.65897131\n",
      "Trained batch 6069 batch loss 5.59513187 epoch total loss 5.65896034\n",
      "Trained batch 6070 batch loss 6.32487059 epoch total loss 5.65907\n",
      "Trained batch 6071 batch loss 6.53851557 epoch total loss 5.65921497\n",
      "Trained batch 6072 batch loss 5.21888065 epoch total loss 5.65914249\n",
      "Trained batch 6073 batch loss 5.5540266 epoch total loss 5.65912533\n",
      "Trained batch 6074 batch loss 6.28348827 epoch total loss 5.65922832\n",
      "Trained batch 6075 batch loss 5.58694458 epoch total loss 5.6592164\n",
      "Trained batch 6076 batch loss 5.43345308 epoch total loss 5.65917921\n",
      "Trained batch 6077 batch loss 5.18724489 epoch total loss 5.65910149\n",
      "Trained batch 6078 batch loss 5.52262402 epoch total loss 5.65907907\n",
      "Trained batch 6079 batch loss 6.00376511 epoch total loss 5.65913582\n",
      "Trained batch 6080 batch loss 5.69449 epoch total loss 5.65914202\n",
      "Trained batch 6081 batch loss 5.3458662 epoch total loss 5.65909052\n",
      "Trained batch 6082 batch loss 5.69995451 epoch total loss 5.65909719\n",
      "Trained batch 6083 batch loss 5.52762318 epoch total loss 5.65907526\n",
      "Trained batch 6084 batch loss 5.5473609 epoch total loss 5.65905714\n",
      "Trained batch 6085 batch loss 5.81035042 epoch total loss 5.65908146\n",
      "Trained batch 6086 batch loss 5.35549831 epoch total loss 5.65903187\n",
      "Trained batch 6087 batch loss 5.78097105 epoch total loss 5.6590519\n",
      "Trained batch 6088 batch loss 5.28294039 epoch total loss 5.65899\n",
      "Trained batch 6089 batch loss 5.66545773 epoch total loss 5.65899038\n",
      "Trained batch 6090 batch loss 5.88708591 epoch total loss 5.65902805\n",
      "Trained batch 6091 batch loss 5.06447506 epoch total loss 5.65893078\n",
      "Trained batch 6092 batch loss 4.89764309 epoch total loss 5.65880585\n",
      "Trained batch 6093 batch loss 5.55406141 epoch total loss 5.65878868\n",
      "Trained batch 6094 batch loss 5.12037754 epoch total loss 5.65870047\n",
      "Trained batch 6095 batch loss 5.39206123 epoch total loss 5.6586566\n",
      "Trained batch 6096 batch loss 5.60715771 epoch total loss 5.65864801\n",
      "Trained batch 6097 batch loss 5.52711058 epoch total loss 5.65862608\n",
      "Trained batch 6098 batch loss 5.5734539 epoch total loss 5.65861225\n",
      "Trained batch 6099 batch loss 5.18345737 epoch total loss 5.65853453\n",
      "Trained batch 6100 batch loss 4.35358334 epoch total loss 5.6583209\n",
      "Trained batch 6101 batch loss 4.39620495 epoch total loss 5.65811396\n",
      "Trained batch 6102 batch loss 5.37053442 epoch total loss 5.65806675\n",
      "Trained batch 6103 batch loss 6.08966923 epoch total loss 5.65813732\n",
      "Trained batch 6104 batch loss 5.99672508 epoch total loss 5.65819311\n",
      "Trained batch 6105 batch loss 5.74300623 epoch total loss 5.65820646\n",
      "Trained batch 6106 batch loss 6.24366713 epoch total loss 5.65830231\n",
      "Trained batch 6107 batch loss 5.55874348 epoch total loss 5.65828609\n",
      "Trained batch 6108 batch loss 6.64099026 epoch total loss 5.65844679\n",
      "Trained batch 6109 batch loss 5.57433939 epoch total loss 5.65843296\n",
      "Trained batch 6110 batch loss 5.66724 epoch total loss 5.65843439\n",
      "Trained batch 6111 batch loss 5.97468376 epoch total loss 5.65848684\n",
      "Trained batch 6112 batch loss 5.65444613 epoch total loss 5.65848637\n",
      "Trained batch 6113 batch loss 5.96178532 epoch total loss 5.65853596\n",
      "Trained batch 6114 batch loss 4.3096323 epoch total loss 5.6583147\n",
      "Trained batch 6115 batch loss 6.0013628 epoch total loss 5.65837097\n",
      "Trained batch 6116 batch loss 6.56907415 epoch total loss 5.65851974\n",
      "Trained batch 6117 batch loss 6.50951719 epoch total loss 5.65865898\n",
      "Trained batch 6118 batch loss 5.50744152 epoch total loss 5.65863419\n",
      "Trained batch 6119 batch loss 5.38376856 epoch total loss 5.65858889\n",
      "Trained batch 6120 batch loss 6.08407831 epoch total loss 5.65865898\n",
      "Trained batch 6121 batch loss 5.79398108 epoch total loss 5.65868092\n",
      "Trained batch 6122 batch loss 6.84076 epoch total loss 5.65887356\n",
      "Trained batch 6123 batch loss 6.25699186 epoch total loss 5.65897179\n",
      "Trained batch 6124 batch loss 4.96982384 epoch total loss 5.65885878\n",
      "Trained batch 6125 batch loss 5.7927165 epoch total loss 5.65888071\n",
      "Trained batch 6126 batch loss 5.38892365 epoch total loss 5.65883684\n",
      "Trained batch 6127 batch loss 4.85776424 epoch total loss 5.65870667\n",
      "Trained batch 6128 batch loss 4.68024826 epoch total loss 5.65854692\n",
      "Trained batch 6129 batch loss 5.223001 epoch total loss 5.6584754\n",
      "Trained batch 6130 batch loss 5.52675438 epoch total loss 5.65845442\n",
      "Trained batch 6131 batch loss 6.23390675 epoch total loss 5.65854836\n",
      "Trained batch 6132 batch loss 5.56097221 epoch total loss 5.65853262\n",
      "Trained batch 6133 batch loss 5.7180562 epoch total loss 5.65854216\n",
      "Trained batch 6134 batch loss 6.07716179 epoch total loss 5.65861082\n",
      "Trained batch 6135 batch loss 5.1998806 epoch total loss 5.65853596\n",
      "Trained batch 6136 batch loss 5.79060173 epoch total loss 5.65855694\n",
      "Trained batch 6137 batch loss 5.12828779 epoch total loss 5.65847063\n",
      "Trained batch 6138 batch loss 5.01808167 epoch total loss 5.65836668\n",
      "Trained batch 6139 batch loss 4.97223663 epoch total loss 5.6582551\n",
      "Trained batch 6140 batch loss 4.02295399 epoch total loss 5.65798855\n",
      "Trained batch 6141 batch loss 4.1370821 epoch total loss 5.65774107\n",
      "Trained batch 6142 batch loss 3.77523327 epoch total loss 5.65743399\n",
      "Trained batch 6143 batch loss 3.43673921 epoch total loss 5.65707302\n",
      "Trained batch 6144 batch loss 4.56040096 epoch total loss 5.65689421\n",
      "Trained batch 6145 batch loss 5.58662415 epoch total loss 5.65688229\n",
      "Trained batch 6146 batch loss 5.8403511 epoch total loss 5.65691233\n",
      "Trained batch 6147 batch loss 6.17752552 epoch total loss 5.65699673\n",
      "Trained batch 6148 batch loss 5.42989349 epoch total loss 5.65695953\n",
      "Trained batch 6149 batch loss 5.7633009 epoch total loss 5.6569767\n",
      "Trained batch 6150 batch loss 5.15691233 epoch total loss 5.65689516\n",
      "Trained batch 6151 batch loss 4.23488235 epoch total loss 5.65666389\n",
      "Trained batch 6152 batch loss 3.71582437 epoch total loss 5.65634823\n",
      "Trained batch 6153 batch loss 5.56005192 epoch total loss 5.65633249\n",
      "Trained batch 6154 batch loss 6.17172146 epoch total loss 5.65641642\n",
      "Trained batch 6155 batch loss 5.83545113 epoch total loss 5.6564455\n",
      "Trained batch 6156 batch loss 5.96822357 epoch total loss 5.65649605\n",
      "Trained batch 6157 batch loss 5.90613747 epoch total loss 5.65653658\n",
      "Trained batch 6158 batch loss 6.07961798 epoch total loss 5.65660524\n",
      "Trained batch 6159 batch loss 5.38241291 epoch total loss 5.6565609\n",
      "Trained batch 6160 batch loss 4.83097744 epoch total loss 5.65642691\n",
      "Trained batch 6161 batch loss 5.53687859 epoch total loss 5.65640736\n",
      "Trained batch 6162 batch loss 5.67319679 epoch total loss 5.65640974\n",
      "Trained batch 6163 batch loss 5.30354548 epoch total loss 5.65635252\n",
      "Trained batch 6164 batch loss 5.70633698 epoch total loss 5.6563611\n",
      "Trained batch 6165 batch loss 5.58889198 epoch total loss 5.65635\n",
      "Trained batch 6166 batch loss 5.37141037 epoch total loss 5.65630388\n",
      "Trained batch 6167 batch loss 5.92732811 epoch total loss 5.65634775\n",
      "Trained batch 6168 batch loss 5.05146694 epoch total loss 5.65624952\n",
      "Trained batch 6169 batch loss 5.64304876 epoch total loss 5.65624762\n",
      "Trained batch 6170 batch loss 5.63569164 epoch total loss 5.65624428\n",
      "Trained batch 6171 batch loss 6.23222876 epoch total loss 5.65633726\n",
      "Trained batch 6172 batch loss 6.22317696 epoch total loss 5.65642929\n",
      "Trained batch 6173 batch loss 5.95284367 epoch total loss 5.65647697\n",
      "Trained batch 6174 batch loss 4.92810869 epoch total loss 5.65635967\n",
      "Trained batch 6175 batch loss 6.3637476 epoch total loss 5.65647411\n",
      "Trained batch 6176 batch loss 6.24367 epoch total loss 5.656569\n",
      "Trained batch 6177 batch loss 6.12743616 epoch total loss 5.6566453\n",
      "Trained batch 6178 batch loss 6.32386684 epoch total loss 5.65675306\n",
      "Trained batch 6179 batch loss 6.71899748 epoch total loss 5.6569252\n",
      "Trained batch 6180 batch loss 6.05144024 epoch total loss 5.6569891\n",
      "Trained batch 6181 batch loss 5.73587036 epoch total loss 5.6570015\n",
      "Trained batch 6182 batch loss 6.18425465 epoch total loss 5.65708637\n",
      "Trained batch 6183 batch loss 5.48475456 epoch total loss 5.65705872\n",
      "Trained batch 6184 batch loss 5.83611488 epoch total loss 5.6570878\n",
      "Trained batch 6185 batch loss 5.66635323 epoch total loss 5.65708923\n",
      "Trained batch 6186 batch loss 5.88267231 epoch total loss 5.65712595\n",
      "Trained batch 6187 batch loss 5.78751 epoch total loss 5.65714693\n",
      "Trained batch 6188 batch loss 5.08834171 epoch total loss 5.65705538\n",
      "Trained batch 6189 batch loss 6.28802586 epoch total loss 5.65715742\n",
      "Trained batch 6190 batch loss 6.17974806 epoch total loss 5.65724182\n",
      "Trained batch 6191 batch loss 6.10588074 epoch total loss 5.6573143\n",
      "Trained batch 6192 batch loss 5.26550293 epoch total loss 5.65725136\n",
      "Trained batch 6193 batch loss 5.32272148 epoch total loss 5.65719748\n",
      "Trained batch 6194 batch loss 5.94023037 epoch total loss 5.65724325\n",
      "Trained batch 6195 batch loss 5.86744404 epoch total loss 5.65727711\n",
      "Trained batch 6196 batch loss 6.07251883 epoch total loss 5.65734434\n",
      "Trained batch 6197 batch loss 5.69375706 epoch total loss 5.65735054\n",
      "Trained batch 6198 batch loss 5.60476 epoch total loss 5.65734243\n",
      "Trained batch 6199 batch loss 5.8262763 epoch total loss 5.65736961\n",
      "Trained batch 6200 batch loss 5.59978294 epoch total loss 5.65736055\n",
      "Trained batch 6201 batch loss 5.69612217 epoch total loss 5.65736675\n",
      "Trained batch 6202 batch loss 5.31433678 epoch total loss 5.65731144\n",
      "Trained batch 6203 batch loss 5.46637058 epoch total loss 5.65728045\n",
      "Trained batch 6204 batch loss 5.99348068 epoch total loss 5.65733433\n",
      "Trained batch 6205 batch loss 6.16495895 epoch total loss 5.65741587\n",
      "Trained batch 6206 batch loss 6.18750191 epoch total loss 5.65750122\n",
      "Trained batch 6207 batch loss 6.21571636 epoch total loss 5.65759087\n",
      "Trained batch 6208 batch loss 4.58762836 epoch total loss 5.65741825\n",
      "Trained batch 6209 batch loss 5.6727314 epoch total loss 5.65742064\n",
      "Trained batch 6210 batch loss 5.66248274 epoch total loss 5.65742207\n",
      "Trained batch 6211 batch loss 5.69983673 epoch total loss 5.65742874\n",
      "Trained batch 6212 batch loss 6.37433 epoch total loss 5.65754414\n",
      "Trained batch 6213 batch loss 5.86113262 epoch total loss 5.65757656\n",
      "Trained batch 6214 batch loss 5.90917683 epoch total loss 5.65761709\n",
      "Trained batch 6215 batch loss 4.91598272 epoch total loss 5.65749741\n",
      "Trained batch 6216 batch loss 5.79178286 epoch total loss 5.65751934\n",
      "Trained batch 6217 batch loss 5.21714687 epoch total loss 5.65744877\n",
      "Trained batch 6218 batch loss 4.98411179 epoch total loss 5.65734053\n",
      "Trained batch 6219 batch loss 5.95643187 epoch total loss 5.65738869\n",
      "Trained batch 6220 batch loss 6.0086503 epoch total loss 5.65744495\n",
      "Trained batch 6221 batch loss 4.89331388 epoch total loss 5.65732241\n",
      "Trained batch 6222 batch loss 6.0757947 epoch total loss 5.65738964\n",
      "Trained batch 6223 batch loss 5.57647324 epoch total loss 5.65737677\n",
      "Trained batch 6224 batch loss 5.88454723 epoch total loss 5.65741301\n",
      "Trained batch 6225 batch loss 5.13849592 epoch total loss 5.65732908\n",
      "Trained batch 6226 batch loss 5.16383696 epoch total loss 5.65725\n",
      "Trained batch 6227 batch loss 5.11528206 epoch total loss 5.65716314\n",
      "Trained batch 6228 batch loss 6.14453602 epoch total loss 5.65724182\n",
      "Trained batch 6229 batch loss 4.825284 epoch total loss 5.65710783\n",
      "Trained batch 6230 batch loss 4.69110489 epoch total loss 5.65695286\n",
      "Trained batch 6231 batch loss 7.10921478 epoch total loss 5.65718603\n",
      "Trained batch 6232 batch loss 5.08092785 epoch total loss 5.65709352\n",
      "Trained batch 6233 batch loss 5.58629608 epoch total loss 5.65708208\n",
      "Trained batch 6234 batch loss 5.64744902 epoch total loss 5.65708065\n",
      "Trained batch 6235 batch loss 6.18748093 epoch total loss 5.657166\n",
      "Trained batch 6236 batch loss 5.71204567 epoch total loss 5.65717459\n",
      "Trained batch 6237 batch loss 5.20394373 epoch total loss 5.65710163\n",
      "Trained batch 6238 batch loss 5.57655668 epoch total loss 5.65708923\n",
      "Trained batch 6239 batch loss 5.42156696 epoch total loss 5.65705156\n",
      "Trained batch 6240 batch loss 5.56168175 epoch total loss 5.6570363\n",
      "Trained batch 6241 batch loss 6.71602201 epoch total loss 5.65720558\n",
      "Trained batch 6242 batch loss 6.92328835 epoch total loss 5.65740824\n",
      "Trained batch 6243 batch loss 6.09409332 epoch total loss 5.65747833\n",
      "Trained batch 6244 batch loss 6.06790161 epoch total loss 5.65754366\n",
      "Trained batch 6245 batch loss 5.9716773 epoch total loss 5.6575942\n",
      "Trained batch 6246 batch loss 5.73207283 epoch total loss 5.65760565\n",
      "Trained batch 6247 batch loss 6.77334 epoch total loss 5.65778446\n",
      "Trained batch 6248 batch loss 6.23108435 epoch total loss 5.65787601\n",
      "Trained batch 6249 batch loss 6.81240177 epoch total loss 5.65806103\n",
      "Trained batch 6250 batch loss 6.54719639 epoch total loss 5.65820312\n",
      "Trained batch 6251 batch loss 6.3812027 epoch total loss 5.658319\n",
      "Trained batch 6252 batch loss 5.66403484 epoch total loss 5.65832\n",
      "Trained batch 6253 batch loss 5.74119 epoch total loss 5.6583333\n",
      "Trained batch 6254 batch loss 5.99043036 epoch total loss 5.65838671\n",
      "Trained batch 6255 batch loss 5.69711399 epoch total loss 5.65839243\n",
      "Trained batch 6256 batch loss 5.22079754 epoch total loss 5.65832281\n",
      "Trained batch 6257 batch loss 6.65102243 epoch total loss 5.65848207\n",
      "Trained batch 6258 batch loss 6.7305069 epoch total loss 5.65865326\n",
      "Trained batch 6259 batch loss 5.98635626 epoch total loss 5.65870571\n",
      "Trained batch 6260 batch loss 4.95981216 epoch total loss 5.65859461\n",
      "Trained batch 6261 batch loss 6.03921938 epoch total loss 5.65865517\n",
      "Trained batch 6262 batch loss 5.11683321 epoch total loss 5.65856886\n",
      "Trained batch 6263 batch loss 5.78705883 epoch total loss 5.65858889\n",
      "Trained batch 6264 batch loss 5.66581917 epoch total loss 5.65859\n",
      "Trained batch 6265 batch loss 5.23854446 epoch total loss 5.65852261\n",
      "Trained batch 6266 batch loss 5.92952919 epoch total loss 5.658566\n",
      "Trained batch 6267 batch loss 5.80425119 epoch total loss 5.65858936\n",
      "Trained batch 6268 batch loss 5.74126625 epoch total loss 5.65860271\n",
      "Trained batch 6269 batch loss 5.6968894 epoch total loss 5.65860844\n",
      "Trained batch 6270 batch loss 6.80988884 epoch total loss 5.65879202\n",
      "Trained batch 6271 batch loss 6.23305607 epoch total loss 5.65888357\n",
      "Trained batch 6272 batch loss 5.84279394 epoch total loss 5.65891314\n",
      "Trained batch 6273 batch loss 5.93723679 epoch total loss 5.65895748\n",
      "Trained batch 6274 batch loss 5.69652176 epoch total loss 5.6589632\n",
      "Trained batch 6275 batch loss 5.97626925 epoch total loss 5.65901375\n",
      "Trained batch 6276 batch loss 5.99613857 epoch total loss 5.65906763\n",
      "Trained batch 6277 batch loss 6.00299883 epoch total loss 5.65912247\n",
      "Trained batch 6278 batch loss 6.9234333 epoch total loss 5.65932369\n",
      "Trained batch 6279 batch loss 6.64732 epoch total loss 5.65948105\n",
      "Trained batch 6280 batch loss 6.36356878 epoch total loss 5.65959311\n",
      "Trained batch 6281 batch loss 6.51649094 epoch total loss 5.65972948\n",
      "Trained batch 6282 batch loss 5.66709709 epoch total loss 5.65973091\n",
      "Trained batch 6283 batch loss 6.37381411 epoch total loss 5.65984488\n",
      "Trained batch 6284 batch loss 5.06047249 epoch total loss 5.65974903\n",
      "Trained batch 6285 batch loss 5.76502514 epoch total loss 5.65976572\n",
      "Trained batch 6286 batch loss 5.76469231 epoch total loss 5.65978289\n",
      "Trained batch 6287 batch loss 5.76754045 epoch total loss 5.65979958\n",
      "Trained batch 6288 batch loss 5.67453337 epoch total loss 5.65980196\n",
      "Trained batch 6289 batch loss 5.27569485 epoch total loss 5.6597414\n",
      "Trained batch 6290 batch loss 4.99811888 epoch total loss 5.6596365\n",
      "Trained batch 6291 batch loss 5.09578753 epoch total loss 5.65954733\n",
      "Trained batch 6292 batch loss 5.62676334 epoch total loss 5.65954161\n",
      "Trained batch 6293 batch loss 6.40047359 epoch total loss 5.65966\n",
      "Trained batch 6294 batch loss 6.04944611 epoch total loss 5.65972185\n",
      "Trained batch 6295 batch loss 6.43314 epoch total loss 5.65984488\n",
      "Trained batch 6296 batch loss 5.6630621 epoch total loss 5.65984535\n",
      "Trained batch 6297 batch loss 6.99270868 epoch total loss 5.66005707\n",
      "Trained batch 6298 batch loss 6.19091034 epoch total loss 5.66014147\n",
      "Trained batch 6299 batch loss 5.36282778 epoch total loss 5.66009426\n",
      "Trained batch 6300 batch loss 6.32784891 epoch total loss 5.6602\n",
      "Trained batch 6301 batch loss 4.31845379 epoch total loss 5.65998745\n",
      "Trained batch 6302 batch loss 5.49852848 epoch total loss 5.65996218\n",
      "Trained batch 6303 batch loss 4.7004776 epoch total loss 5.65980959\n",
      "Trained batch 6304 batch loss 5.16576767 epoch total loss 5.65973139\n",
      "Trained batch 6305 batch loss 6.31283855 epoch total loss 5.65983486\n",
      "Trained batch 6306 batch loss 5.74829674 epoch total loss 5.65984917\n",
      "Trained batch 6307 batch loss 6.32412338 epoch total loss 5.65995455\n",
      "Trained batch 6308 batch loss 6.27325821 epoch total loss 5.66005182\n",
      "Trained batch 6309 batch loss 5.99793 epoch total loss 5.66010475\n",
      "Trained batch 6310 batch loss 6.75196218 epoch total loss 5.66027832\n",
      "Trained batch 6311 batch loss 5.88354588 epoch total loss 5.66031361\n",
      "Trained batch 6312 batch loss 5.56462717 epoch total loss 5.66029882\n",
      "Trained batch 6313 batch loss 5.82039309 epoch total loss 5.6603241\n",
      "Trained batch 6314 batch loss 6.40568256 epoch total loss 5.66044188\n",
      "Trained batch 6315 batch loss 6.31720543 epoch total loss 5.66054583\n",
      "Trained batch 6316 batch loss 5.75260258 epoch total loss 5.66056061\n",
      "Trained batch 6317 batch loss 5.39358425 epoch total loss 5.66051865\n",
      "Trained batch 6318 batch loss 5.67696762 epoch total loss 5.66052103\n",
      "Trained batch 6319 batch loss 5.07135582 epoch total loss 5.66042757\n",
      "Trained batch 6320 batch loss 4.78718662 epoch total loss 5.66029\n",
      "Trained batch 6321 batch loss 5.29513836 epoch total loss 5.66023207\n",
      "Trained batch 6322 batch loss 5.65711403 epoch total loss 5.66023159\n",
      "Trained batch 6323 batch loss 5.84670687 epoch total loss 5.66026115\n",
      "Trained batch 6324 batch loss 5.002635 epoch total loss 5.66015768\n",
      "Trained batch 6325 batch loss 4.26379681 epoch total loss 5.6599369\n",
      "Trained batch 6326 batch loss 5.15975094 epoch total loss 5.65985823\n",
      "Trained batch 6327 batch loss 5.91013718 epoch total loss 5.65989733\n",
      "Trained batch 6328 batch loss 5.89339161 epoch total loss 5.65993452\n",
      "Trained batch 6329 batch loss 6.32870674 epoch total loss 5.66004038\n",
      "Trained batch 6330 batch loss 5.28428 epoch total loss 5.65998077\n",
      "Trained batch 6331 batch loss 5.65735054 epoch total loss 5.6599803\n",
      "Trained batch 6332 batch loss 5.46079731 epoch total loss 5.65994883\n",
      "Trained batch 6333 batch loss 5.17132664 epoch total loss 5.65987206\n",
      "Trained batch 6334 batch loss 5.68096256 epoch total loss 5.65987492\n",
      "Trained batch 6335 batch loss 5.61440849 epoch total loss 5.65986776\n",
      "Trained batch 6336 batch loss 5.91896152 epoch total loss 5.65990829\n",
      "Trained batch 6337 batch loss 5.73420954 epoch total loss 5.65992\n",
      "Trained batch 6338 batch loss 6.36827517 epoch total loss 5.6600318\n",
      "Trained batch 6339 batch loss 4.74595 epoch total loss 5.65988779\n",
      "Trained batch 6340 batch loss 5.66741371 epoch total loss 5.65988874\n",
      "Trained batch 6341 batch loss 5.84511852 epoch total loss 5.65991783\n",
      "Trained batch 6342 batch loss 5.23687363 epoch total loss 5.65985155\n",
      "Trained batch 6343 batch loss 5.31787109 epoch total loss 5.65979719\n",
      "Trained batch 6344 batch loss 4.95885324 epoch total loss 5.65968657\n",
      "Trained batch 6345 batch loss 6.23726463 epoch total loss 5.65977764\n",
      "Trained batch 6346 batch loss 5.60911417 epoch total loss 5.65976954\n",
      "Trained batch 6347 batch loss 6.26379585 epoch total loss 5.65986538\n",
      "Trained batch 6348 batch loss 5.73223305 epoch total loss 5.65987635\n",
      "Trained batch 6349 batch loss 6.26476049 epoch total loss 5.65997171\n",
      "Trained batch 6350 batch loss 6.43889904 epoch total loss 5.66009426\n",
      "Trained batch 6351 batch loss 4.35465097 epoch total loss 5.65988874\n",
      "Trained batch 6352 batch loss 5.5027318 epoch total loss 5.65986395\n",
      "Trained batch 6353 batch loss 5.18862629 epoch total loss 5.65978956\n",
      "Trained batch 6354 batch loss 5.94024706 epoch total loss 5.65983391\n",
      "Trained batch 6355 batch loss 5.39984703 epoch total loss 5.6597929\n",
      "Trained batch 6356 batch loss 4.92985535 epoch total loss 5.65967798\n",
      "Trained batch 6357 batch loss 5.39856672 epoch total loss 5.65963697\n",
      "Trained batch 6358 batch loss 5.58800364 epoch total loss 5.65962601\n",
      "Trained batch 6359 batch loss 4.59085369 epoch total loss 5.65945768\n",
      "Trained batch 6360 batch loss 4.36409473 epoch total loss 5.65925407\n",
      "Trained batch 6361 batch loss 5.24905634 epoch total loss 5.6591897\n",
      "Trained batch 6362 batch loss 5.52593517 epoch total loss 5.65916872\n",
      "Trained batch 6363 batch loss 5.43518925 epoch total loss 5.65913343\n",
      "Trained batch 6364 batch loss 5.73768616 epoch total loss 5.65914583\n",
      "Trained batch 6365 batch loss 5.25201035 epoch total loss 5.65908241\n",
      "Trained batch 6366 batch loss 5.95192 epoch total loss 5.65912867\n",
      "Trained batch 6367 batch loss 5.26477432 epoch total loss 5.65906668\n",
      "Trained batch 6368 batch loss 5.6364727 epoch total loss 5.65906334\n",
      "Trained batch 6369 batch loss 5.46800041 epoch total loss 5.6590333\n",
      "Trained batch 6370 batch loss 5.77578735 epoch total loss 5.6590519\n",
      "Trained batch 6371 batch loss 5.6868 epoch total loss 5.65905619\n",
      "Trained batch 6372 batch loss 5.25757885 epoch total loss 5.65899324\n",
      "Trained batch 6373 batch loss 5.3573761 epoch total loss 5.65894556\n",
      "Trained batch 6374 batch loss 4.10964346 epoch total loss 5.65870237\n",
      "Trained batch 6375 batch loss 5.24563265 epoch total loss 5.658638\n",
      "Trained batch 6376 batch loss 5.4236393 epoch total loss 5.65860081\n",
      "Trained batch 6377 batch loss 5.27337933 epoch total loss 5.65854025\n",
      "Trained batch 6378 batch loss 5.06639767 epoch total loss 5.65844727\n",
      "Trained batch 6379 batch loss 5.56819534 epoch total loss 5.65843296\n",
      "Trained batch 6380 batch loss 5.18718147 epoch total loss 5.65835905\n",
      "Trained batch 6381 batch loss 5.04584789 epoch total loss 5.65826321\n",
      "Trained batch 6382 batch loss 5.19919395 epoch total loss 5.65819168\n",
      "Trained batch 6383 batch loss 4.76536322 epoch total loss 5.65805149\n",
      "Trained batch 6384 batch loss 5.51856041 epoch total loss 5.65803\n",
      "Trained batch 6385 batch loss 4.94771481 epoch total loss 5.65791893\n",
      "Trained batch 6386 batch loss 5.74599123 epoch total loss 5.65793276\n",
      "Trained batch 6387 batch loss 6.20308304 epoch total loss 5.65801811\n",
      "Trained batch 6388 batch loss 5.34481859 epoch total loss 5.657969\n",
      "Trained batch 6389 batch loss 5.31478786 epoch total loss 5.65791559\n",
      "Trained batch 6390 batch loss 6.05273151 epoch total loss 5.6579771\n",
      "Trained batch 6391 batch loss 5.13975573 epoch total loss 5.65789604\n",
      "Trained batch 6392 batch loss 5.97758341 epoch total loss 5.65794563\n",
      "Trained batch 6393 batch loss 4.48463345 epoch total loss 5.65776205\n",
      "Trained batch 6394 batch loss 4.6904068 epoch total loss 5.65761137\n",
      "Trained batch 6395 batch loss 5.11067724 epoch total loss 5.65752554\n",
      "Trained batch 6396 batch loss 5.9896307 epoch total loss 5.65757704\n",
      "Trained batch 6397 batch loss 6.06079388 epoch total loss 5.65764046\n",
      "Trained batch 6398 batch loss 6.00141144 epoch total loss 5.65769386\n",
      "Trained batch 6399 batch loss 5.85374498 epoch total loss 5.65772486\n",
      "Trained batch 6400 batch loss 5.36993694 epoch total loss 5.65768\n",
      "Trained batch 6401 batch loss 5.30342102 epoch total loss 5.65762472\n",
      "Trained batch 6402 batch loss 4.9321003 epoch total loss 5.65751171\n",
      "Trained batch 6403 batch loss 4.77201939 epoch total loss 5.65737391\n",
      "Trained batch 6404 batch loss 5.08291101 epoch total loss 5.65728378\n",
      "Trained batch 6405 batch loss 5.70164 epoch total loss 5.65729094\n",
      "Trained batch 6406 batch loss 4.19777107 epoch total loss 5.65706348\n",
      "Trained batch 6407 batch loss 4.74951172 epoch total loss 5.65692186\n",
      "Trained batch 6408 batch loss 4.97469521 epoch total loss 5.65681553\n",
      "Trained batch 6409 batch loss 4.22229195 epoch total loss 5.65659189\n",
      "Trained batch 6410 batch loss 5.24222088 epoch total loss 5.65652704\n",
      "Trained batch 6411 batch loss 4.6671381 epoch total loss 5.65637302\n",
      "Trained batch 6412 batch loss 4.76790619 epoch total loss 5.65623474\n",
      "Trained batch 6413 batch loss 5.42502737 epoch total loss 5.65619898\n",
      "Trained batch 6414 batch loss 5.18262768 epoch total loss 5.65612507\n",
      "Trained batch 6415 batch loss 5.49926758 epoch total loss 5.65610075\n",
      "Trained batch 6416 batch loss 5.35274696 epoch total loss 5.65605354\n",
      "Trained batch 6417 batch loss 5.32801437 epoch total loss 5.65600204\n",
      "Trained batch 6418 batch loss 5.80555725 epoch total loss 5.65602541\n",
      "Trained batch 6419 batch loss 5.09170914 epoch total loss 5.65593719\n",
      "Trained batch 6420 batch loss 5.17780828 epoch total loss 5.65586281\n",
      "Trained batch 6421 batch loss 5.58432484 epoch total loss 5.65585232\n",
      "Trained batch 6422 batch loss 5.97933 epoch total loss 5.65590286\n",
      "Trained batch 6423 batch loss 6.05351734 epoch total loss 5.65596485\n",
      "Trained batch 6424 batch loss 5.89247561 epoch total loss 5.65600109\n",
      "Trained batch 6425 batch loss 5.7435627 epoch total loss 5.65601492\n",
      "Trained batch 6426 batch loss 4.33051586 epoch total loss 5.65580845\n",
      "Trained batch 6427 batch loss 5.65553474 epoch total loss 5.65580893\n",
      "Trained batch 6428 batch loss 5.00258398 epoch total loss 5.65570736\n",
      "Trained batch 6429 batch loss 5.93988228 epoch total loss 5.65575171\n",
      "Trained batch 6430 batch loss 5.69743872 epoch total loss 5.65575838\n",
      "Trained batch 6431 batch loss 4.89117289 epoch total loss 5.65563965\n",
      "Trained batch 6432 batch loss 5.10295439 epoch total loss 5.65555334\n",
      "Trained batch 6433 batch loss 5.69879723 epoch total loss 5.65556\n",
      "Trained batch 6434 batch loss 5.84865379 epoch total loss 5.65559\n",
      "Trained batch 6435 batch loss 5.09672689 epoch total loss 5.65550327\n",
      "Trained batch 6436 batch loss 5.7481246 epoch total loss 5.65551805\n",
      "Trained batch 6437 batch loss 4.48523903 epoch total loss 5.6553359\n",
      "Trained batch 6438 batch loss 4.43875027 epoch total loss 5.65514708\n",
      "Trained batch 6439 batch loss 4.79595757 epoch total loss 5.65501356\n",
      "Trained batch 6440 batch loss 4.92953396 epoch total loss 5.65490103\n",
      "Trained batch 6441 batch loss 5.26798773 epoch total loss 5.65484095\n",
      "Trained batch 6442 batch loss 4.67029858 epoch total loss 5.65468836\n",
      "Trained batch 6443 batch loss 5.36470127 epoch total loss 5.65464354\n",
      "Trained batch 6444 batch loss 5.69523716 epoch total loss 5.65464973\n",
      "Trained batch 6445 batch loss 5.31767416 epoch total loss 5.65459728\n",
      "Trained batch 6446 batch loss 5.57801914 epoch total loss 5.65458536\n",
      "Trained batch 6447 batch loss 5.59778118 epoch total loss 5.6545763\n",
      "Trained batch 6448 batch loss 5.11960316 epoch total loss 5.65449381\n",
      "Trained batch 6449 batch loss 5.79262114 epoch total loss 5.65451527\n",
      "Trained batch 6450 batch loss 5.01620293 epoch total loss 5.65441608\n",
      "Trained batch 6451 batch loss 5.37897635 epoch total loss 5.65437365\n",
      "Trained batch 6452 batch loss 4.28513956 epoch total loss 5.65416145\n",
      "Trained batch 6453 batch loss 3.47678781 epoch total loss 5.65382385\n",
      "Trained batch 6454 batch loss 4.72647572 epoch total loss 5.65368032\n",
      "Trained batch 6455 batch loss 5.70958424 epoch total loss 5.65368891\n",
      "Trained batch 6456 batch loss 6.42263603 epoch total loss 5.65380812\n",
      "Trained batch 6457 batch loss 5.44160557 epoch total loss 5.65377522\n",
      "Trained batch 6458 batch loss 6.53137589 epoch total loss 5.65391111\n",
      "Trained batch 6459 batch loss 6.01153135 epoch total loss 5.65396643\n",
      "Trained batch 6460 batch loss 5.75595379 epoch total loss 5.65398264\n",
      "Trained batch 6461 batch loss 6.76614761 epoch total loss 5.6541543\n",
      "Trained batch 6462 batch loss 6.42733288 epoch total loss 5.65427399\n",
      "Trained batch 6463 batch loss 5.39571476 epoch total loss 5.65423393\n",
      "Trained batch 6464 batch loss 6.67429686 epoch total loss 5.65439177\n",
      "Trained batch 6465 batch loss 5.47047567 epoch total loss 5.65436316\n",
      "Trained batch 6466 batch loss 5.20276642 epoch total loss 5.65429306\n",
      "Trained batch 6467 batch loss 5.37405682 epoch total loss 5.65425\n",
      "Trained batch 6468 batch loss 5.14848185 epoch total loss 5.65417194\n",
      "Trained batch 6469 batch loss 6.99253416 epoch total loss 5.65437889\n",
      "Trained batch 6470 batch loss 7.57237196 epoch total loss 5.65467548\n",
      "Trained batch 6471 batch loss 6.3209734 epoch total loss 5.65477848\n",
      "Trained batch 6472 batch loss 7.56132126 epoch total loss 5.65507317\n",
      "Trained batch 6473 batch loss 6.56485748 epoch total loss 5.65521383\n",
      "Trained batch 6474 batch loss 6.82212162 epoch total loss 5.6553936\n",
      "Trained batch 6475 batch loss 7.51830244 epoch total loss 5.65568161\n",
      "Trained batch 6476 batch loss 6.24389172 epoch total loss 5.65577221\n",
      "Trained batch 6477 batch loss 6.18236876 epoch total loss 5.65585375\n",
      "Trained batch 6478 batch loss 6.01410389 epoch total loss 5.65590954\n",
      "Trained batch 6479 batch loss 6.49877787 epoch total loss 5.65603971\n",
      "Trained batch 6480 batch loss 4.72373962 epoch total loss 5.65589571\n",
      "Trained batch 6481 batch loss 7.42531 epoch total loss 5.65616846\n",
      "Trained batch 6482 batch loss 4.74010086 epoch total loss 5.65602684\n",
      "Trained batch 6483 batch loss 5.73956585 epoch total loss 5.65603971\n",
      "Trained batch 6484 batch loss 6.08947 epoch total loss 5.65610647\n",
      "Trained batch 6485 batch loss 6.59894848 epoch total loss 5.65625191\n",
      "Trained batch 6486 batch loss 6.3721509 epoch total loss 5.65636206\n",
      "Trained batch 6487 batch loss 5.96674156 epoch total loss 5.65640974\n",
      "Trained batch 6488 batch loss 6.31148052 epoch total loss 5.65651083\n",
      "Trained batch 6489 batch loss 6.14276886 epoch total loss 5.65658569\n",
      "Trained batch 6490 batch loss 5.92270517 epoch total loss 5.6566267\n",
      "Trained batch 6491 batch loss 6.50292873 epoch total loss 5.65675735\n",
      "Trained batch 6492 batch loss 6.4690361 epoch total loss 5.65688229\n",
      "Trained batch 6493 batch loss 6.22034931 epoch total loss 5.65696907\n",
      "Trained batch 6494 batch loss 6.56191921 epoch total loss 5.65710831\n",
      "Trained batch 6495 batch loss 5.09187794 epoch total loss 5.65702152\n",
      "Trained batch 6496 batch loss 5.73597956 epoch total loss 5.65703344\n",
      "Trained batch 6497 batch loss 5.95348167 epoch total loss 5.65707922\n",
      "Trained batch 6498 batch loss 5.55960751 epoch total loss 5.65706396\n",
      "Trained batch 6499 batch loss 5.73571682 epoch total loss 5.65707588\n",
      "Trained batch 6500 batch loss 5.70710373 epoch total loss 5.65708351\n",
      "Trained batch 6501 batch loss 5.11799335 epoch total loss 5.65700054\n",
      "Trained batch 6502 batch loss 5.30682087 epoch total loss 5.65694714\n",
      "Trained batch 6503 batch loss 5.96554375 epoch total loss 5.65699434\n",
      "Trained batch 6504 batch loss 6.64455032 epoch total loss 5.65714598\n",
      "Trained batch 6505 batch loss 6.15924597 epoch total loss 5.65722322\n",
      "Trained batch 6506 batch loss 6.0995388 epoch total loss 5.65729094\n",
      "Trained batch 6507 batch loss 6.25563383 epoch total loss 5.65738297\n",
      "Trained batch 6508 batch loss 6.69240379 epoch total loss 5.65754175\n",
      "Trained batch 6509 batch loss 6.11399746 epoch total loss 5.65761185\n",
      "Trained batch 6510 batch loss 5.73416805 epoch total loss 5.65762329\n",
      "Trained batch 6511 batch loss 5.42132521 epoch total loss 5.65758705\n",
      "Trained batch 6512 batch loss 4.62074184 epoch total loss 5.65742826\n",
      "Trained batch 6513 batch loss 5.50538445 epoch total loss 5.65740442\n",
      "Trained batch 6514 batch loss 5.68262386 epoch total loss 5.65740871\n",
      "Trained batch 6515 batch loss 6.53255844 epoch total loss 5.65754271\n",
      "Trained batch 6516 batch loss 6.42387295 epoch total loss 5.65766048\n",
      "Trained batch 6517 batch loss 6.3060112 epoch total loss 5.65775967\n",
      "Trained batch 6518 batch loss 5.08548212 epoch total loss 5.65767193\n",
      "Trained batch 6519 batch loss 5.40206766 epoch total loss 5.65763283\n",
      "Trained batch 6520 batch loss 6.18005562 epoch total loss 5.65771294\n",
      "Trained batch 6521 batch loss 5.65110397 epoch total loss 5.65771246\n",
      "Trained batch 6522 batch loss 5.73125076 epoch total loss 5.65772343\n",
      "Trained batch 6523 batch loss 5.42961454 epoch total loss 5.65768862\n",
      "Trained batch 6524 batch loss 5.03410912 epoch total loss 5.65759277\n",
      "Trained batch 6525 batch loss 5.54535151 epoch total loss 5.65757608\n",
      "Trained batch 6526 batch loss 6.01424599 epoch total loss 5.65763092\n",
      "Trained batch 6527 batch loss 5.69360065 epoch total loss 5.65763664\n",
      "Trained batch 6528 batch loss 5.56865883 epoch total loss 5.65762329\n",
      "Trained batch 6529 batch loss 6.20420456 epoch total loss 5.65770674\n",
      "Trained batch 6530 batch loss 5.83360672 epoch total loss 5.65773344\n",
      "Trained batch 6531 batch loss 6.26186848 epoch total loss 5.65782595\n",
      "Trained batch 6532 batch loss 5.56154108 epoch total loss 5.65781164\n",
      "Trained batch 6533 batch loss 5.72339058 epoch total loss 5.65782118\n",
      "Trained batch 6534 batch loss 5.69093466 epoch total loss 5.65782642\n",
      "Trained batch 6535 batch loss 5.27198219 epoch total loss 5.65776777\n",
      "Trained batch 6536 batch loss 5.1420536 epoch total loss 5.65768862\n",
      "Trained batch 6537 batch loss 5.51759148 epoch total loss 5.65766764\n",
      "Trained batch 6538 batch loss 5.98187923 epoch total loss 5.65771675\n",
      "Trained batch 6539 batch loss 5.65297747 epoch total loss 5.6577158\n",
      "Trained batch 6540 batch loss 5.24477 epoch total loss 5.65765285\n",
      "Trained batch 6541 batch loss 5.3621583 epoch total loss 5.65760803\n",
      "Trained batch 6542 batch loss 5.380898 epoch total loss 5.65756607\n",
      "Trained batch 6543 batch loss 5.91801453 epoch total loss 5.65760565\n",
      "Trained batch 6544 batch loss 5.42952251 epoch total loss 5.65757084\n",
      "Trained batch 6545 batch loss 5.77591753 epoch total loss 5.65758944\n",
      "Trained batch 6546 batch loss 5.5212779 epoch total loss 5.65756798\n",
      "Trained batch 6547 batch loss 5.7997 epoch total loss 5.65759\n",
      "Trained batch 6548 batch loss 5.94560814 epoch total loss 5.65763378\n",
      "Trained batch 6549 batch loss 5.95150614 epoch total loss 5.65767908\n",
      "Trained batch 6550 batch loss 5.03378773 epoch total loss 5.65758419\n",
      "Trained batch 6551 batch loss 6.46391106 epoch total loss 5.65770721\n",
      "Trained batch 6552 batch loss 6.51813602 epoch total loss 5.65783882\n",
      "Trained batch 6553 batch loss 6.77359581 epoch total loss 5.65800905\n",
      "Trained batch 6554 batch loss 6.84468365 epoch total loss 5.65819025\n",
      "Trained batch 6555 batch loss 6.1392417 epoch total loss 5.65826368\n",
      "Trained batch 6556 batch loss 5.56556177 epoch total loss 5.65824938\n",
      "Trained batch 6557 batch loss 6.27975702 epoch total loss 5.65834475\n",
      "Trained batch 6558 batch loss 6.05153561 epoch total loss 5.65840435\n",
      "Trained batch 6559 batch loss 5.88086271 epoch total loss 5.65843868\n",
      "Trained batch 6560 batch loss 6.07620764 epoch total loss 5.65850258\n",
      "Trained batch 6561 batch loss 5.41348171 epoch total loss 5.65846539\n",
      "Trained batch 6562 batch loss 5.51477671 epoch total loss 5.65844345\n",
      "Trained batch 6563 batch loss 5.71096039 epoch total loss 5.65845156\n",
      "Trained batch 6564 batch loss 5.43400478 epoch total loss 5.65841722\n",
      "Trained batch 6565 batch loss 5.7561512 epoch total loss 5.65843248\n",
      "Trained batch 6566 batch loss 5.51099 epoch total loss 5.65841\n",
      "Trained batch 6567 batch loss 5.29053974 epoch total loss 5.65835381\n",
      "Trained batch 6568 batch loss 5.53791332 epoch total loss 5.65833569\n",
      "Trained batch 6569 batch loss 5.74669266 epoch total loss 5.65834904\n",
      "Trained batch 6570 batch loss 5.00917149 epoch total loss 5.65825033\n",
      "Trained batch 6571 batch loss 5.58277035 epoch total loss 5.65823841\n",
      "Trained batch 6572 batch loss 5.01911354 epoch total loss 5.65814114\n",
      "Trained batch 6573 batch loss 4.66035175 epoch total loss 5.6579895\n",
      "Trained batch 6574 batch loss 5.90395498 epoch total loss 5.6580267\n",
      "Trained batch 6575 batch loss 5.6923275 epoch total loss 5.65803194\n",
      "Trained batch 6576 batch loss 6.10434437 epoch total loss 5.65809965\n",
      "Trained batch 6577 batch loss 4.98840332 epoch total loss 5.65799809\n",
      "Trained batch 6578 batch loss 5.08056259 epoch total loss 5.65791035\n",
      "Trained batch 6579 batch loss 4.77509689 epoch total loss 5.65777588\n",
      "Trained batch 6580 batch loss 4.74764729 epoch total loss 5.6576376\n",
      "Trained batch 6581 batch loss 5.67145729 epoch total loss 5.6576395\n",
      "Trained batch 6582 batch loss 5.96375656 epoch total loss 5.65768623\n",
      "Trained batch 6583 batch loss 6.19054222 epoch total loss 5.6577673\n",
      "Trained batch 6584 batch loss 5.75356913 epoch total loss 5.65778208\n",
      "Trained batch 6585 batch loss 5.95033264 epoch total loss 5.65782595\n",
      "Trained batch 6586 batch loss 6.50796318 epoch total loss 5.65795517\n",
      "Trained batch 6587 batch loss 5.47774124 epoch total loss 5.65792751\n",
      "Trained batch 6588 batch loss 5.73625183 epoch total loss 5.65793943\n",
      "Trained batch 6589 batch loss 5.22226429 epoch total loss 5.65787315\n",
      "Trained batch 6590 batch loss 4.93471575 epoch total loss 5.65776348\n",
      "Trained batch 6591 batch loss 4.89331818 epoch total loss 5.65764761\n",
      "Trained batch 6592 batch loss 4.57632923 epoch total loss 5.65748358\n",
      "Trained batch 6593 batch loss 5.21000576 epoch total loss 5.65741587\n",
      "Trained batch 6594 batch loss 6.71853542 epoch total loss 5.65757704\n",
      "Trained batch 6595 batch loss 6.80129671 epoch total loss 5.65775\n",
      "Trained batch 6596 batch loss 6.41647625 epoch total loss 5.65786552\n",
      "Trained batch 6597 batch loss 5.93312 epoch total loss 5.65790749\n",
      "Trained batch 6598 batch loss 5.9820118 epoch total loss 5.65795612\n",
      "Trained batch 6599 batch loss 6.03425598 epoch total loss 5.65801334\n",
      "Trained batch 6600 batch loss 5.83426857 epoch total loss 5.65804052\n",
      "Trained batch 6601 batch loss 5.49111795 epoch total loss 5.65801525\n",
      "Trained batch 6602 batch loss 6.13455105 epoch total loss 5.65808725\n",
      "Trained batch 6603 batch loss 6.75575352 epoch total loss 5.65825319\n",
      "Trained batch 6604 batch loss 6.31759453 epoch total loss 5.65835285\n",
      "Trained batch 6605 batch loss 6.54674816 epoch total loss 5.65848732\n",
      "Trained batch 6606 batch loss 6.53846836 epoch total loss 5.65862036\n",
      "Trained batch 6607 batch loss 6.19465065 epoch total loss 5.6587019\n",
      "Trained batch 6608 batch loss 6.45044 epoch total loss 5.65882158\n",
      "Trained batch 6609 batch loss 6.40973759 epoch total loss 5.65893507\n",
      "Trained batch 6610 batch loss 6.03072882 epoch total loss 5.65899134\n",
      "Trained batch 6611 batch loss 6.44710875 epoch total loss 5.65911055\n",
      "Trained batch 6612 batch loss 5.87419701 epoch total loss 5.65914297\n",
      "Trained batch 6613 batch loss 6.10246658 epoch total loss 5.65920973\n",
      "Trained batch 6614 batch loss 5.83781528 epoch total loss 5.65923691\n",
      "Trained batch 6615 batch loss 5.77006912 epoch total loss 5.65925312\n",
      "Trained batch 6616 batch loss 5.47342253 epoch total loss 5.65922499\n",
      "Trained batch 6617 batch loss 6.22016239 epoch total loss 5.65931\n",
      "Trained batch 6618 batch loss 6.01224804 epoch total loss 5.65936279\n",
      "Trained batch 6619 batch loss 5.93705511 epoch total loss 5.65940475\n",
      "Trained batch 6620 batch loss 6.23963547 epoch total loss 5.65949249\n",
      "Trained batch 6621 batch loss 5.85849905 epoch total loss 5.65952253\n",
      "Trained batch 6622 batch loss 6.19609356 epoch total loss 5.6596036\n",
      "Trained batch 6623 batch loss 5.73837709 epoch total loss 5.65961552\n",
      "Trained batch 6624 batch loss 4.79745054 epoch total loss 5.65948534\n",
      "Trained batch 6625 batch loss 5.05316114 epoch total loss 5.65939379\n",
      "Trained batch 6626 batch loss 4.67965889 epoch total loss 5.65924597\n",
      "Trained batch 6627 batch loss 4.47302818 epoch total loss 5.65906715\n",
      "Trained batch 6628 batch loss 4.58795786 epoch total loss 5.65890551\n",
      "Trained batch 6629 batch loss 4.21863 epoch total loss 5.65868855\n",
      "Trained batch 6630 batch loss 5.22831726 epoch total loss 5.65862322\n",
      "Trained batch 6631 batch loss 6.46706486 epoch total loss 5.65874529\n",
      "Trained batch 6632 batch loss 6.23542213 epoch total loss 5.65883207\n",
      "Trained batch 6633 batch loss 5.83825779 epoch total loss 5.65885925\n",
      "Trained batch 6634 batch loss 6.18204784 epoch total loss 5.65893841\n",
      "Trained batch 6635 batch loss 5.69192696 epoch total loss 5.65894365\n",
      "Trained batch 6636 batch loss 5.40383911 epoch total loss 5.65890455\n",
      "Trained batch 6637 batch loss 5.89812756 epoch total loss 5.65894079\n",
      "Trained batch 6638 batch loss 4.92515278 epoch total loss 5.65883064\n",
      "Trained batch 6639 batch loss 5.02251148 epoch total loss 5.6587348\n",
      "Trained batch 6640 batch loss 5.21837234 epoch total loss 5.65866852\n",
      "Trained batch 6641 batch loss 6.06315 epoch total loss 5.65872908\n",
      "Trained batch 6642 batch loss 5.86074734 epoch total loss 5.65875959\n",
      "Trained batch 6643 batch loss 5.53650093 epoch total loss 5.658741\n",
      "Trained batch 6644 batch loss 6.04507828 epoch total loss 5.65879917\n",
      "Trained batch 6645 batch loss 5.85867357 epoch total loss 5.65882969\n",
      "Trained batch 6646 batch loss 6.21933 epoch total loss 5.65891361\n",
      "Trained batch 6647 batch loss 5.76374912 epoch total loss 5.65893\n",
      "Trained batch 6648 batch loss 6.47326374 epoch total loss 5.65905237\n",
      "Trained batch 6649 batch loss 6.6986475 epoch total loss 5.65920877\n",
      "Trained batch 6650 batch loss 5.49587536 epoch total loss 5.65918398\n",
      "Trained batch 6651 batch loss 4.91304874 epoch total loss 5.65907192\n",
      "Trained batch 6652 batch loss 4.60949707 epoch total loss 5.65891409\n",
      "Trained batch 6653 batch loss 6.76450729 epoch total loss 5.65908051\n",
      "Trained batch 6654 batch loss 5.94911766 epoch total loss 5.65912437\n",
      "Trained batch 6655 batch loss 5.93138218 epoch total loss 5.65916491\n",
      "Trained batch 6656 batch loss 6.38461876 epoch total loss 5.65927362\n",
      "Trained batch 6657 batch loss 5.71032619 epoch total loss 5.65928125\n",
      "Trained batch 6658 batch loss 5.34152317 epoch total loss 5.65923357\n",
      "Trained batch 6659 batch loss 5.36458206 epoch total loss 5.65918875\n",
      "Trained batch 6660 batch loss 5.49260092 epoch total loss 5.65916395\n",
      "Trained batch 6661 batch loss 5.39164209 epoch total loss 5.65912342\n",
      "Trained batch 6662 batch loss 5.53933191 epoch total loss 5.6591053\n",
      "Trained batch 6663 batch loss 4.68621254 epoch total loss 5.65896\n",
      "Trained batch 6664 batch loss 4.8632679 epoch total loss 5.65884\n",
      "Trained batch 6665 batch loss 4.90195608 epoch total loss 5.65872669\n",
      "Trained batch 6666 batch loss 6.18273449 epoch total loss 5.65880537\n",
      "Trained batch 6667 batch loss 6.14877081 epoch total loss 5.6588788\n",
      "Trained batch 6668 batch loss 6.39832211 epoch total loss 5.65899\n",
      "Trained batch 6669 batch loss 6.21346331 epoch total loss 5.65907335\n",
      "Trained batch 6670 batch loss 6.0094986 epoch total loss 5.65912533\n",
      "Trained batch 6671 batch loss 6.65028429 epoch total loss 5.65927362\n",
      "Trained batch 6672 batch loss 5.91695595 epoch total loss 5.65931273\n",
      "Trained batch 6673 batch loss 6.31651497 epoch total loss 5.65941095\n",
      "Trained batch 6674 batch loss 6.87470531 epoch total loss 5.65959311\n",
      "Trained batch 6675 batch loss 6.1384182 epoch total loss 5.65966463\n",
      "Trained batch 6676 batch loss 5.38057232 epoch total loss 5.65962267\n",
      "Trained batch 6677 batch loss 5.32952642 epoch total loss 5.65957308\n",
      "Trained batch 6678 batch loss 4.25445557 epoch total loss 5.65936232\n",
      "Trained batch 6679 batch loss 5.16279459 epoch total loss 5.65928841\n",
      "Trained batch 6680 batch loss 4.86406422 epoch total loss 5.6591692\n",
      "Trained batch 6681 batch loss 5.95903397 epoch total loss 5.6592145\n",
      "Trained batch 6682 batch loss 4.65805817 epoch total loss 5.65906429\n",
      "Trained batch 6683 batch loss 4.76095 epoch total loss 5.65893\n",
      "Trained batch 6684 batch loss 6.28887367 epoch total loss 5.65902424\n",
      "Trained batch 6685 batch loss 5.42129707 epoch total loss 5.65898895\n",
      "Trained batch 6686 batch loss 4.81279 epoch total loss 5.65886211\n",
      "Trained batch 6687 batch loss 5.0905571 epoch total loss 5.65877724\n",
      "Trained batch 6688 batch loss 6.72587299 epoch total loss 5.6589365\n",
      "Trained batch 6689 batch loss 6.20383453 epoch total loss 5.65901804\n",
      "Trained batch 6690 batch loss 5.92019 epoch total loss 5.65905714\n",
      "Trained batch 6691 batch loss 6.41843843 epoch total loss 5.65917063\n",
      "Trained batch 6692 batch loss 5.75575781 epoch total loss 5.65918493\n",
      "Trained batch 6693 batch loss 5.25663757 epoch total loss 5.65912485\n",
      "Trained batch 6694 batch loss 6.5361948 epoch total loss 5.65925598\n",
      "Trained batch 6695 batch loss 6.10206795 epoch total loss 5.65932178\n",
      "Trained batch 6696 batch loss 5.76509094 epoch total loss 5.659338\n",
      "Trained batch 6697 batch loss 6.85856247 epoch total loss 5.65951681\n",
      "Trained batch 6698 batch loss 6.25813866 epoch total loss 5.65960646\n",
      "Trained batch 6699 batch loss 5.2410512 epoch total loss 5.65954399\n",
      "Trained batch 6700 batch loss 4.55030775 epoch total loss 5.65937853\n",
      "Trained batch 6701 batch loss 5.65763569 epoch total loss 5.65937805\n",
      "Trained batch 6702 batch loss 6.14987755 epoch total loss 5.65945101\n",
      "Trained batch 6703 batch loss 6.07039881 epoch total loss 5.65951252\n",
      "Trained batch 6704 batch loss 6.65565777 epoch total loss 5.65966082\n",
      "Trained batch 6705 batch loss 5.75039816 epoch total loss 5.65967464\n",
      "Trained batch 6706 batch loss 4.71039629 epoch total loss 5.65953302\n",
      "Trained batch 6707 batch loss 4.99657059 epoch total loss 5.65943384\n",
      "Trained batch 6708 batch loss 5.38568592 epoch total loss 5.65939331\n",
      "Trained batch 6709 batch loss 5.03456306 epoch total loss 5.65930033\n",
      "Trained batch 6710 batch loss 5.89972448 epoch total loss 5.65933609\n",
      "Trained batch 6711 batch loss 5.53756762 epoch total loss 5.65931797\n",
      "Trained batch 6712 batch loss 5.26594925 epoch total loss 5.65925932\n",
      "Trained batch 6713 batch loss 6.0812254 epoch total loss 5.65932226\n",
      "Trained batch 6714 batch loss 5.88317633 epoch total loss 5.65935564\n",
      "Trained batch 6715 batch loss 5.65977859 epoch total loss 5.65935564\n",
      "Trained batch 6716 batch loss 6.30858421 epoch total loss 5.65945244\n",
      "Trained batch 6717 batch loss 6.33197689 epoch total loss 5.65955257\n",
      "Trained batch 6718 batch loss 5.76396513 epoch total loss 5.65956831\n",
      "Trained batch 6719 batch loss 5.81882381 epoch total loss 5.65959215\n",
      "Trained batch 6720 batch loss 5.58571815 epoch total loss 5.65958118\n",
      "Trained batch 6721 batch loss 5.70359039 epoch total loss 5.65958786\n",
      "Trained batch 6722 batch loss 5.86876869 epoch total loss 5.65961885\n",
      "Trained batch 6723 batch loss 5.63479233 epoch total loss 5.65961552\n",
      "Trained batch 6724 batch loss 6.09060574 epoch total loss 5.65967941\n",
      "Trained batch 6725 batch loss 5.76826 epoch total loss 5.65969563\n",
      "Trained batch 6726 batch loss 5.82846451 epoch total loss 5.6597209\n",
      "Trained batch 6727 batch loss 4.60950422 epoch total loss 5.6595645\n",
      "Trained batch 6728 batch loss 5.41315222 epoch total loss 5.65952826\n",
      "Trained batch 6729 batch loss 5.46002579 epoch total loss 5.65949869\n",
      "Trained batch 6730 batch loss 5.79749966 epoch total loss 5.65951872\n",
      "Trained batch 6731 batch loss 5.56550598 epoch total loss 5.65950489\n",
      "Trained batch 6732 batch loss 5.45471621 epoch total loss 5.65947437\n",
      "Trained batch 6733 batch loss 5.51861763 epoch total loss 5.65945387\n",
      "Trained batch 6734 batch loss 5.01527309 epoch total loss 5.65935802\n",
      "Trained batch 6735 batch loss 5.13622141 epoch total loss 5.6592803\n",
      "Trained batch 6736 batch loss 5.1449542 epoch total loss 5.65920401\n",
      "Trained batch 6737 batch loss 5.06660366 epoch total loss 5.65911627\n",
      "Trained batch 6738 batch loss 4.43284893 epoch total loss 5.65893412\n",
      "Trained batch 6739 batch loss 5.56813478 epoch total loss 5.65892029\n",
      "Trained batch 6740 batch loss 4.93172455 epoch total loss 5.658813\n",
      "Trained batch 6741 batch loss 5.31773949 epoch total loss 5.65876198\n",
      "Trained batch 6742 batch loss 5.78070402 epoch total loss 5.65878\n",
      "Trained batch 6743 batch loss 5.9837203 epoch total loss 5.65882826\n",
      "Trained batch 6744 batch loss 5.45353651 epoch total loss 5.65879822\n",
      "Trained batch 6745 batch loss 5.40498543 epoch total loss 5.65876055\n",
      "Trained batch 6746 batch loss 5.62890291 epoch total loss 5.65875626\n",
      "Trained batch 6747 batch loss 5.7952013 epoch total loss 5.65877676\n",
      "Trained batch 6748 batch loss 5.50880241 epoch total loss 5.65875435\n",
      "Trained batch 6749 batch loss 4.72967243 epoch total loss 5.65861654\n",
      "Trained batch 6750 batch loss 5.77496338 epoch total loss 5.65863371\n",
      "Trained batch 6751 batch loss 5.43695164 epoch total loss 5.65860081\n",
      "Trained batch 6752 batch loss 5.53475809 epoch total loss 5.65858269\n",
      "Trained batch 6753 batch loss 5.05025291 epoch total loss 5.65849257\n",
      "Trained batch 6754 batch loss 5.63791752 epoch total loss 5.65848923\n",
      "Trained batch 6755 batch loss 5.75280333 epoch total loss 5.65850353\n",
      "Trained batch 6756 batch loss 5.19271374 epoch total loss 5.65843439\n",
      "Trained batch 6757 batch loss 5.07271528 epoch total loss 5.65834808\n",
      "Trained batch 6758 batch loss 5.58346128 epoch total loss 5.65833664\n",
      "Trained batch 6759 batch loss 6.2808466 epoch total loss 5.65842867\n",
      "Trained batch 6760 batch loss 6.0020237 epoch total loss 5.65847969\n",
      "Trained batch 6761 batch loss 5.89524794 epoch total loss 5.65851498\n",
      "Trained batch 6762 batch loss 5.53029251 epoch total loss 5.6584959\n",
      "Trained batch 6763 batch loss 5.81420326 epoch total loss 5.65851879\n",
      "Trained batch 6764 batch loss 5.99506855 epoch total loss 5.65856886\n",
      "Trained batch 6765 batch loss 6.16525936 epoch total loss 5.65864325\n",
      "Trained batch 6766 batch loss 5.80179787 epoch total loss 5.65866423\n",
      "Trained batch 6767 batch loss 6.76169586 epoch total loss 5.6588273\n",
      "Trained batch 6768 batch loss 4.90047836 epoch total loss 5.65871572\n",
      "Trained batch 6769 batch loss 4.79578 epoch total loss 5.65858841\n",
      "Trained batch 6770 batch loss 4.54191971 epoch total loss 5.65842342\n",
      "Trained batch 6771 batch loss 4.76376629 epoch total loss 5.65829182\n",
      "Trained batch 6772 batch loss 5.48313808 epoch total loss 5.65826607\n",
      "Trained batch 6773 batch loss 4.94539356 epoch total loss 5.65816069\n",
      "Trained batch 6774 batch loss 5.02776241 epoch total loss 5.6580677\n",
      "Trained batch 6775 batch loss 4.42714214 epoch total loss 5.65788555\n",
      "Trained batch 6776 batch loss 4.90200329 epoch total loss 5.65777445\n",
      "Trained batch 6777 batch loss 5.24874401 epoch total loss 5.65771389\n",
      "Trained batch 6778 batch loss 6.25859737 epoch total loss 5.65780258\n",
      "Trained batch 6779 batch loss 6.1686306 epoch total loss 5.65787792\n",
      "Trained batch 6780 batch loss 4.5563 epoch total loss 5.65771532\n",
      "Trained batch 6781 batch loss 5.62431765 epoch total loss 5.65771\n",
      "Trained batch 6782 batch loss 6.82092667 epoch total loss 5.65788174\n",
      "Trained batch 6783 batch loss 5.07198334 epoch total loss 5.65779495\n",
      "Trained batch 6784 batch loss 5.31577826 epoch total loss 5.65774488\n",
      "Trained batch 6785 batch loss 6.98039532 epoch total loss 5.65794\n",
      "Trained batch 6786 batch loss 6.51964712 epoch total loss 5.65806675\n",
      "Trained batch 6787 batch loss 6.98446369 epoch total loss 5.65826225\n",
      "Trained batch 6788 batch loss 5.94196224 epoch total loss 5.65830374\n",
      "Trained batch 6789 batch loss 6.53716 epoch total loss 5.65843344\n",
      "Trained batch 6790 batch loss 6.31124 epoch total loss 5.65852976\n",
      "Trained batch 6791 batch loss 6.51780462 epoch total loss 5.6586566\n",
      "Trained batch 6792 batch loss 7.20967388 epoch total loss 5.658885\n",
      "Trained batch 6793 batch loss 5.21599293 epoch total loss 5.65881968\n",
      "Trained batch 6794 batch loss 5.23352861 epoch total loss 5.65875721\n",
      "Trained batch 6795 batch loss 5.58756351 epoch total loss 5.65874672\n",
      "Trained batch 6796 batch loss 4.5542469 epoch total loss 5.65858412\n",
      "Trained batch 6797 batch loss 5.38843918 epoch total loss 5.65854406\n",
      "Trained batch 6798 batch loss 5.70048237 epoch total loss 5.65855026\n",
      "Trained batch 6799 batch loss 5.35233593 epoch total loss 5.65850496\n",
      "Trained batch 6800 batch loss 5.8940897 epoch total loss 5.65854\n",
      "Trained batch 6801 batch loss 5.78267574 epoch total loss 5.65855789\n",
      "Trained batch 6802 batch loss 5.95524454 epoch total loss 5.65860176\n",
      "Trained batch 6803 batch loss 5.47488308 epoch total loss 5.65857506\n",
      "Trained batch 6804 batch loss 5.23274088 epoch total loss 5.65851259\n",
      "Trained batch 6805 batch loss 4.08946419 epoch total loss 5.65828228\n",
      "Trained batch 6806 batch loss 5.75884199 epoch total loss 5.65829659\n",
      "Trained batch 6807 batch loss 5.39788151 epoch total loss 5.65825844\n",
      "Trained batch 6808 batch loss 5.12002373 epoch total loss 5.65817976\n",
      "Trained batch 6809 batch loss 4.86921501 epoch total loss 5.65806389\n",
      "Trained batch 6810 batch loss 4.7669673 epoch total loss 5.65793276\n",
      "Trained batch 6811 batch loss 4.33429432 epoch total loss 5.65773869\n",
      "Trained batch 6812 batch loss 4.71847439 epoch total loss 5.65760088\n",
      "Trained batch 6813 batch loss 5.06495476 epoch total loss 5.6575141\n",
      "Trained batch 6814 batch loss 5.78934813 epoch total loss 5.65753365\n",
      "Trained batch 6815 batch loss 5.18790627 epoch total loss 5.6574645\n",
      "Trained batch 6816 batch loss 5.44506311 epoch total loss 5.65743351\n",
      "Trained batch 6817 batch loss 5.98327112 epoch total loss 5.65748119\n",
      "Trained batch 6818 batch loss 5.95411873 epoch total loss 5.65752459\n",
      "Trained batch 6819 batch loss 5.16953945 epoch total loss 5.65745306\n",
      "Trained batch 6820 batch loss 5.49348211 epoch total loss 5.65742874\n",
      "Trained batch 6821 batch loss 5.34433317 epoch total loss 5.65738297\n",
      "Trained batch 6822 batch loss 5.89528084 epoch total loss 5.6574173\n",
      "Trained batch 6823 batch loss 6.03358 epoch total loss 5.65747309\n",
      "Trained batch 6824 batch loss 5.25367928 epoch total loss 5.65741396\n",
      "Trained batch 6825 batch loss 5.67858267 epoch total loss 5.65741682\n",
      "Trained batch 6826 batch loss 5.8329134 epoch total loss 5.65744257\n",
      "Trained batch 6827 batch loss 5.50925255 epoch total loss 5.65742064\n",
      "Trained batch 6828 batch loss 5.51349163 epoch total loss 5.65739918\n",
      "Trained batch 6829 batch loss 5.26453209 epoch total loss 5.65734196\n",
      "Trained batch 6830 batch loss 5.53000927 epoch total loss 5.65732336\n",
      "Trained batch 6831 batch loss 5.66846228 epoch total loss 5.65732527\n",
      "Trained batch 6832 batch loss 5.44386673 epoch total loss 5.6572938\n",
      "Trained batch 6833 batch loss 5.89426136 epoch total loss 5.65732861\n",
      "Trained batch 6834 batch loss 6.64066553 epoch total loss 5.65747261\n",
      "Trained batch 6835 batch loss 6.46137142 epoch total loss 5.65759039\n",
      "Trained batch 6836 batch loss 6.80199337 epoch total loss 5.65775728\n",
      "Trained batch 6837 batch loss 6.75183105 epoch total loss 5.65791702\n",
      "Trained batch 6838 batch loss 6.86024284 epoch total loss 5.65809298\n",
      "Trained batch 6839 batch loss 6.47772217 epoch total loss 5.65821266\n",
      "Trained batch 6840 batch loss 5.99707508 epoch total loss 5.65826178\n",
      "Trained batch 6841 batch loss 5.52940369 epoch total loss 5.65824318\n",
      "Trained batch 6842 batch loss 5.71610355 epoch total loss 5.65825176\n",
      "Trained batch 6843 batch loss 5.69035196 epoch total loss 5.65825653\n",
      "Trained batch 6844 batch loss 5.26836061 epoch total loss 5.6582\n",
      "Trained batch 6845 batch loss 6.03413868 epoch total loss 5.65825462\n",
      "Trained batch 6846 batch loss 7.03520489 epoch total loss 5.65845585\n",
      "Trained batch 6847 batch loss 5.77260733 epoch total loss 5.65847254\n",
      "Trained batch 6848 batch loss 5.96559143 epoch total loss 5.65851736\n",
      "Trained batch 6849 batch loss 6.05859041 epoch total loss 5.65857601\n",
      "Trained batch 6850 batch loss 5.66909409 epoch total loss 5.65857744\n",
      "Trained batch 6851 batch loss 5.51368237 epoch total loss 5.65855646\n",
      "Trained batch 6852 batch loss 4.71458817 epoch total loss 5.65841866\n",
      "Trained batch 6853 batch loss 5.65564823 epoch total loss 5.65841818\n",
      "Trained batch 6854 batch loss 5.56739807 epoch total loss 5.65840483\n",
      "Trained batch 6855 batch loss 6.0477581 epoch total loss 5.65846157\n",
      "Trained batch 6856 batch loss 5.81426907 epoch total loss 5.65848398\n",
      "Trained batch 6857 batch loss 5.59139156 epoch total loss 5.65847397\n",
      "Trained batch 6858 batch loss 5.74075317 epoch total loss 5.65848637\n",
      "Trained batch 6859 batch loss 6.29792118 epoch total loss 5.65857935\n",
      "Trained batch 6860 batch loss 5.57844734 epoch total loss 5.65856743\n",
      "Trained batch 6861 batch loss 5.7283268 epoch total loss 5.65857744\n",
      "Trained batch 6862 batch loss 4.96110535 epoch total loss 5.65847588\n",
      "Trained batch 6863 batch loss 5.47888231 epoch total loss 5.65844965\n",
      "Trained batch 6864 batch loss 5.90472555 epoch total loss 5.65848589\n",
      "Trained batch 6865 batch loss 5.47823811 epoch total loss 5.65845966\n",
      "Trained batch 6866 batch loss 5.21314764 epoch total loss 5.65839481\n",
      "Trained batch 6867 batch loss 6.26510906 epoch total loss 5.65848351\n",
      "Trained batch 6868 batch loss 5.85102272 epoch total loss 5.65851116\n",
      "Trained batch 6869 batch loss 6.09554911 epoch total loss 5.65857458\n",
      "Trained batch 6870 batch loss 5.77139473 epoch total loss 5.65859079\n",
      "Trained batch 6871 batch loss 5.28976202 epoch total loss 5.65853691\n",
      "Trained batch 6872 batch loss 5.68449402 epoch total loss 5.65854073\n",
      "Trained batch 6873 batch loss 5.90050888 epoch total loss 5.65857601\n",
      "Trained batch 6874 batch loss 4.890028 epoch total loss 5.65846443\n",
      "Trained batch 6875 batch loss 4.25574398 epoch total loss 5.65826035\n",
      "Trained batch 6876 batch loss 5.2916317 epoch total loss 5.65820694\n",
      "Trained batch 6877 batch loss 5.31654119 epoch total loss 5.65815735\n",
      "Trained batch 6878 batch loss 5.510499 epoch total loss 5.65813589\n",
      "Trained batch 6879 batch loss 4.76557541 epoch total loss 5.65800619\n",
      "Trained batch 6880 batch loss 6.57442331 epoch total loss 5.65813971\n",
      "Trained batch 6881 batch loss 6.36805248 epoch total loss 5.6582427\n",
      "Trained batch 6882 batch loss 6.09786844 epoch total loss 5.6583066\n",
      "Trained batch 6883 batch loss 5.3840847 epoch total loss 5.65826654\n",
      "Trained batch 6884 batch loss 5.87302065 epoch total loss 5.65829754\n",
      "Trained batch 6885 batch loss 5.60534143 epoch total loss 5.65828943\n",
      "Trained batch 6886 batch loss 6.37878561 epoch total loss 5.65839434\n",
      "Trained batch 6887 batch loss 4.92726517 epoch total loss 5.658288\n",
      "Trained batch 6888 batch loss 6.46767902 epoch total loss 5.65840578\n",
      "Trained batch 6889 batch loss 5.41018677 epoch total loss 5.65836954\n",
      "Trained batch 6890 batch loss 6.64456844 epoch total loss 5.65851259\n",
      "Trained batch 6891 batch loss 6.26538897 epoch total loss 5.65860081\n",
      "Trained batch 6892 batch loss 5.44506264 epoch total loss 5.65857\n",
      "Trained batch 6893 batch loss 6.58265781 epoch total loss 5.6587038\n",
      "Trained batch 6894 batch loss 5.47024298 epoch total loss 5.65867615\n",
      "Trained batch 6895 batch loss 6.70112181 epoch total loss 5.6588273\n",
      "Trained batch 6896 batch loss 6.91695213 epoch total loss 5.65901\n",
      "Trained batch 6897 batch loss 6.35612535 epoch total loss 5.65911055\n",
      "Trained batch 6898 batch loss 6.30492496 epoch total loss 5.65920448\n",
      "Trained batch 6899 batch loss 6.91502953 epoch total loss 5.65938616\n",
      "Trained batch 6900 batch loss 6.7382946 epoch total loss 5.65954256\n",
      "Trained batch 6901 batch loss 6.05728436 epoch total loss 5.65960026\n",
      "Trained batch 6902 batch loss 5.44373703 epoch total loss 5.65956926\n",
      "Trained batch 6903 batch loss 6.39571762 epoch total loss 5.6596756\n",
      "Trained batch 6904 batch loss 5.9298048 epoch total loss 5.6597147\n",
      "Trained batch 6905 batch loss 6.30286503 epoch total loss 5.65980816\n",
      "Trained batch 6906 batch loss 6.04074287 epoch total loss 5.65986347\n",
      "Trained batch 6907 batch loss 5.8382659 epoch total loss 5.65988922\n",
      "Trained batch 6908 batch loss 5.456604 epoch total loss 5.65986\n",
      "Trained batch 6909 batch loss 6.22627306 epoch total loss 5.65994215\n",
      "Trained batch 6910 batch loss 5.39528084 epoch total loss 5.65990353\n",
      "Trained batch 6911 batch loss 5.15564871 epoch total loss 5.65983057\n",
      "Trained batch 6912 batch loss 5.03280926 epoch total loss 5.65974\n",
      "Trained batch 6913 batch loss 4.47289658 epoch total loss 5.65956783\n",
      "Trained batch 6914 batch loss 4.12348604 epoch total loss 5.6593461\n",
      "Trained batch 6915 batch loss 3.80326104 epoch total loss 5.65907764\n",
      "Trained batch 6916 batch loss 5.54303074 epoch total loss 5.65906096\n",
      "Trained batch 6917 batch loss 5.08210754 epoch total loss 5.65897751\n",
      "Trained batch 6918 batch loss 5.83902264 epoch total loss 5.65900373\n",
      "Trained batch 6919 batch loss 6.08014107 epoch total loss 5.65906477\n",
      "Trained batch 6920 batch loss 4.96546364 epoch total loss 5.65896463\n",
      "Trained batch 6921 batch loss 5.81961155 epoch total loss 5.658988\n",
      "Trained batch 6922 batch loss 5.91818762 epoch total loss 5.65902519\n",
      "Trained batch 6923 batch loss 5.16035891 epoch total loss 5.65895319\n",
      "Trained batch 6924 batch loss 3.94952965 epoch total loss 5.65870619\n",
      "Trained batch 6925 batch loss 5.35761 epoch total loss 5.65866327\n",
      "Trained batch 6926 batch loss 5.20054531 epoch total loss 5.65859699\n",
      "Trained batch 6927 batch loss 5.46745205 epoch total loss 5.65856934\n",
      "Trained batch 6928 batch loss 5.42286587 epoch total loss 5.658535\n",
      "Trained batch 6929 batch loss 4.71274 epoch total loss 5.65839863\n",
      "Trained batch 6930 batch loss 4.82281256 epoch total loss 5.65827799\n",
      "Trained batch 6931 batch loss 4.70919037 epoch total loss 5.65814161\n",
      "Trained batch 6932 batch loss 5.35093784 epoch total loss 5.65809727\n",
      "Trained batch 6933 batch loss 5.17795801 epoch total loss 5.65802813\n",
      "Trained batch 6934 batch loss 5.15504169 epoch total loss 5.65795565\n",
      "Trained batch 6935 batch loss 5.53146029 epoch total loss 5.65793753\n",
      "Trained batch 6936 batch loss 5.9713192 epoch total loss 5.65798283\n",
      "Trained batch 6937 batch loss 6.09088659 epoch total loss 5.65804529\n",
      "Trained batch 6938 batch loss 7.05697203 epoch total loss 5.65824699\n",
      "Trained batch 6939 batch loss 7.15752602 epoch total loss 5.658463\n",
      "Trained batch 6940 batch loss 6.91554737 epoch total loss 5.65864372\n",
      "Trained batch 6941 batch loss 5.29937077 epoch total loss 5.65859222\n",
      "Trained batch 6942 batch loss 6.01486874 epoch total loss 5.65864372\n",
      "Trained batch 6943 batch loss 5.84037399 epoch total loss 5.65867\n",
      "Trained batch 6944 batch loss 4.34926558 epoch total loss 5.65848112\n",
      "Trained batch 6945 batch loss 5.66573334 epoch total loss 5.6584816\n",
      "Trained batch 6946 batch loss 5.67425537 epoch total loss 5.65848446\n",
      "Trained batch 6947 batch loss 5.79404068 epoch total loss 5.65850353\n",
      "Trained batch 6948 batch loss 5.09699965 epoch total loss 5.65842295\n",
      "Trained batch 6949 batch loss 5.86925697 epoch total loss 5.65845346\n",
      "Trained batch 6950 batch loss 5.45510292 epoch total loss 5.65842438\n",
      "Trained batch 6951 batch loss 5.44556379 epoch total loss 5.65839386\n",
      "Trained batch 6952 batch loss 5.21549797 epoch total loss 5.65833\n",
      "Trained batch 6953 batch loss 5.35139465 epoch total loss 5.65828609\n",
      "Trained batch 6954 batch loss 6.01189232 epoch total loss 5.65833664\n",
      "Trained batch 6955 batch loss 5.06698513 epoch total loss 5.65825176\n",
      "Trained batch 6956 batch loss 5.17173815 epoch total loss 5.65818167\n",
      "Trained batch 6957 batch loss 5.81309319 epoch total loss 5.65820408\n",
      "Trained batch 6958 batch loss 5.60735321 epoch total loss 5.65819645\n",
      "Trained batch 6959 batch loss 5.62273169 epoch total loss 5.6581912\n",
      "Trained batch 6960 batch loss 6.06609 epoch total loss 5.65825\n",
      "Trained batch 6961 batch loss 6.12581301 epoch total loss 5.65831661\n",
      "Trained batch 6962 batch loss 5.81853676 epoch total loss 5.65834\n",
      "Trained batch 6963 batch loss 5.98739433 epoch total loss 5.65838718\n",
      "Trained batch 6964 batch loss 6.29659605 epoch total loss 5.65847921\n",
      "Trained batch 6965 batch loss 6.29284954 epoch total loss 5.65857029\n",
      "Trained batch 6966 batch loss 6.18992138 epoch total loss 5.65864658\n",
      "Trained batch 6967 batch loss 5.75874805 epoch total loss 5.65866089\n",
      "Trained batch 6968 batch loss 5.26283884 epoch total loss 5.65860415\n",
      "Trained batch 6969 batch loss 6.03266525 epoch total loss 5.65865755\n",
      "Trained batch 6970 batch loss 6.03891706 epoch total loss 5.65871191\n",
      "Trained batch 6971 batch loss 6.34809589 epoch total loss 5.65881062\n",
      "Trained batch 6972 batch loss 5.68630123 epoch total loss 5.65881491\n",
      "Trained batch 6973 batch loss 5.67843151 epoch total loss 5.65881777\n",
      "Trained batch 6974 batch loss 6.2879591 epoch total loss 5.65890837\n",
      "Trained batch 6975 batch loss 4.51858139 epoch total loss 5.65874481\n",
      "Trained batch 6976 batch loss 5.74434185 epoch total loss 5.65875769\n",
      "Trained batch 6977 batch loss 5.47360516 epoch total loss 5.65873098\n",
      "Trained batch 6978 batch loss 6.11402416 epoch total loss 5.65879583\n",
      "Trained batch 6979 batch loss 5.69996309 epoch total loss 5.65880156\n",
      "Trained batch 6980 batch loss 6.63847446 epoch total loss 5.65894175\n",
      "Trained batch 6981 batch loss 5.42502403 epoch total loss 5.65890837\n",
      "Trained batch 6982 batch loss 5.03330326 epoch total loss 5.6588192\n",
      "Trained batch 6983 batch loss 5.16575527 epoch total loss 5.65874815\n",
      "Trained batch 6984 batch loss 5.32873058 epoch total loss 5.65870094\n",
      "Trained batch 6985 batch loss 5.20216751 epoch total loss 5.65863562\n",
      "Trained batch 6986 batch loss 6.11739683 epoch total loss 5.65870142\n",
      "Trained batch 6987 batch loss 5.79140425 epoch total loss 5.65872049\n",
      "Trained batch 6988 batch loss 5.52122 epoch total loss 5.65870047\n",
      "Trained batch 6989 batch loss 5.1958251 epoch total loss 5.65863419\n",
      "Trained batch 6990 batch loss 5.80620813 epoch total loss 5.65865517\n",
      "Trained batch 6991 batch loss 4.55208874 epoch total loss 5.65849686\n",
      "Trained batch 6992 batch loss 5.03543139 epoch total loss 5.65840769\n",
      "Trained batch 6993 batch loss 5.29770422 epoch total loss 5.65835571\n",
      "Trained batch 6994 batch loss 5.48215961 epoch total loss 5.65833044\n",
      "Trained batch 6995 batch loss 4.87200451 epoch total loss 5.65821791\n",
      "Trained batch 6996 batch loss 5.61326647 epoch total loss 5.65821171\n",
      "Trained batch 6997 batch loss 6.28311825 epoch total loss 5.6583004\n",
      "Trained batch 6998 batch loss 5.50697374 epoch total loss 5.65827894\n",
      "Trained batch 6999 batch loss 5.95157099 epoch total loss 5.65832138\n",
      "Trained batch 7000 batch loss 6.08171558 epoch total loss 5.65838146\n",
      "Trained batch 7001 batch loss 5.93259668 epoch total loss 5.65842104\n",
      "Trained batch 7002 batch loss 5.49384 epoch total loss 5.6583972\n",
      "Trained batch 7003 batch loss 7.02527809 epoch total loss 5.65859222\n",
      "Trained batch 7004 batch loss 5.4342227 epoch total loss 5.65856028\n",
      "Trained batch 7005 batch loss 5.80319 epoch total loss 5.65858078\n",
      "Trained batch 7006 batch loss 5.23068905 epoch total loss 5.65851974\n",
      "Trained batch 7007 batch loss 6.20573902 epoch total loss 5.65859795\n",
      "Trained batch 7008 batch loss 5.93231297 epoch total loss 5.65863752\n",
      "Trained batch 7009 batch loss 5.87591887 epoch total loss 5.65866804\n",
      "Trained batch 7010 batch loss 5.63227272 epoch total loss 5.6586647\n",
      "Trained batch 7011 batch loss 6.1897316 epoch total loss 5.65874052\n",
      "Trained batch 7012 batch loss 5.41233253 epoch total loss 5.65870571\n",
      "Trained batch 7013 batch loss 4.68743277 epoch total loss 5.65856695\n",
      "Trained batch 7014 batch loss 5.81973553 epoch total loss 5.65859032\n",
      "Trained batch 7015 batch loss 5.87547731 epoch total loss 5.65862083\n",
      "Trained batch 7016 batch loss 7.1316371 epoch total loss 5.65883112\n",
      "Trained batch 7017 batch loss 5.53582382 epoch total loss 5.65881348\n",
      "Trained batch 7018 batch loss 5.91769 epoch total loss 5.65885067\n",
      "Trained batch 7019 batch loss 5.72051 epoch total loss 5.65885878\n",
      "Trained batch 7020 batch loss 6.15551567 epoch total loss 5.65893\n",
      "Trained batch 7021 batch loss 6.21264505 epoch total loss 5.6590085\n",
      "Trained batch 7022 batch loss 5.47340202 epoch total loss 5.6589818\n",
      "Trained batch 7023 batch loss 7.24869442 epoch total loss 5.6592083\n",
      "Trained batch 7024 batch loss 6.35336256 epoch total loss 5.659307\n",
      "Trained batch 7025 batch loss 5.62393188 epoch total loss 5.65930223\n",
      "Trained batch 7026 batch loss 5.83746624 epoch total loss 5.65932751\n",
      "Trained batch 7027 batch loss 5.40978146 epoch total loss 5.65929174\n",
      "Trained batch 7028 batch loss 4.6470089 epoch total loss 5.65914822\n",
      "Trained batch 7029 batch loss 5.55046177 epoch total loss 5.65913248\n",
      "Trained batch 7030 batch loss 6.4469471 epoch total loss 5.65924454\n",
      "Trained batch 7031 batch loss 5.25189972 epoch total loss 5.65918636\n",
      "Trained batch 7032 batch loss 3.74687958 epoch total loss 5.65891409\n",
      "Trained batch 7033 batch loss 5.40245 epoch total loss 5.65887785\n",
      "Trained batch 7034 batch loss 5.76190853 epoch total loss 5.65889215\n",
      "Trained batch 7035 batch loss 5.42282724 epoch total loss 5.65885878\n",
      "Trained batch 7036 batch loss 4.69689894 epoch total loss 5.65872145\n",
      "Trained batch 7037 batch loss 4.89162779 epoch total loss 5.65861273\n",
      "Trained batch 7038 batch loss 4.82107162 epoch total loss 5.65849352\n",
      "Trained batch 7039 batch loss 5.54140854 epoch total loss 5.65847683\n",
      "Trained batch 7040 batch loss 5.57480955 epoch total loss 5.65846491\n",
      "Trained batch 7041 batch loss 5.50336218 epoch total loss 5.65844297\n",
      "Trained batch 7042 batch loss 4.19876862 epoch total loss 5.65823603\n",
      "Trained batch 7043 batch loss 6.17552185 epoch total loss 5.65830946\n",
      "Trained batch 7044 batch loss 5.74993706 epoch total loss 5.65832233\n",
      "Trained batch 7045 batch loss 4.66857052 epoch total loss 5.65818167\n",
      "Trained batch 7046 batch loss 5.06948042 epoch total loss 5.65809822\n",
      "Trained batch 7047 batch loss 6.00009155 epoch total loss 5.65814686\n",
      "Trained batch 7048 batch loss 5.41622353 epoch total loss 5.658113\n",
      "Trained batch 7049 batch loss 5.71206379 epoch total loss 5.65812\n",
      "Trained batch 7050 batch loss 6.24448061 epoch total loss 5.6582036\n",
      "Trained batch 7051 batch loss 6.2300024 epoch total loss 5.65828466\n",
      "Trained batch 7052 batch loss 6.21208334 epoch total loss 5.65836334\n",
      "Trained batch 7053 batch loss 5.95326614 epoch total loss 5.65840483\n",
      "Trained batch 7054 batch loss 6.54589558 epoch total loss 5.65853119\n",
      "Trained batch 7055 batch loss 6.09134912 epoch total loss 5.65859222\n",
      "Trained batch 7056 batch loss 5.93046188 epoch total loss 5.65863037\n",
      "Trained batch 7057 batch loss 5.90294933 epoch total loss 5.65866518\n",
      "Trained batch 7058 batch loss 6.32324076 epoch total loss 5.65875912\n",
      "Trained batch 7059 batch loss 5.37379026 epoch total loss 5.65871906\n",
      "Trained batch 7060 batch loss 5.60583782 epoch total loss 5.65871143\n",
      "Trained batch 7061 batch loss 5.98520231 epoch total loss 5.65875769\n",
      "Trained batch 7062 batch loss 5.76517487 epoch total loss 5.65877295\n",
      "Trained batch 7063 batch loss 5.90390682 epoch total loss 5.65880728\n",
      "Trained batch 7064 batch loss 6.65304565 epoch total loss 5.65894794\n",
      "Trained batch 7065 batch loss 5.74887609 epoch total loss 5.65896082\n",
      "Trained batch 7066 batch loss 4.95762253 epoch total loss 5.65886164\n",
      "Trained batch 7067 batch loss 5.55694342 epoch total loss 5.65884733\n",
      "Trained batch 7068 batch loss 5.59312439 epoch total loss 5.65883827\n",
      "Trained batch 7069 batch loss 5.79931259 epoch total loss 5.6588583\n",
      "Trained batch 7070 batch loss 5.76655 epoch total loss 5.65887356\n",
      "Trained batch 7071 batch loss 5.52055359 epoch total loss 5.65885353\n",
      "Trained batch 7072 batch loss 4.01143169 epoch total loss 5.65862083\n",
      "Trained batch 7073 batch loss 5.94937325 epoch total loss 5.65866184\n",
      "Trained batch 7074 batch loss 6.27428055 epoch total loss 5.65874863\n",
      "Trained batch 7075 batch loss 6.38839722 epoch total loss 5.65885162\n",
      "Trained batch 7076 batch loss 6.11014843 epoch total loss 5.65891504\n",
      "Trained batch 7077 batch loss 6.27665424 epoch total loss 5.65900278\n",
      "Trained batch 7078 batch loss 5.94381523 epoch total loss 5.65904331\n",
      "Trained batch 7079 batch loss 5.72527122 epoch total loss 5.65905285\n",
      "Trained batch 7080 batch loss 6.41065598 epoch total loss 5.65915871\n",
      "Trained batch 7081 batch loss 6.08897066 epoch total loss 5.65921974\n",
      "Trained batch 7082 batch loss 5.99789524 epoch total loss 5.65926695\n",
      "Trained batch 7083 batch loss 6.077775 epoch total loss 5.65932608\n",
      "Trained batch 7084 batch loss 6.08643103 epoch total loss 5.65938663\n",
      "Trained batch 7085 batch loss 6.33853245 epoch total loss 5.65948248\n",
      "Trained batch 7086 batch loss 6.05947304 epoch total loss 5.65953875\n",
      "Trained batch 7087 batch loss 6.15520573 epoch total loss 5.65960884\n",
      "Trained batch 7088 batch loss 6.34413099 epoch total loss 5.65970564\n",
      "Trained batch 7089 batch loss 5.88469696 epoch total loss 5.65973711\n",
      "Trained batch 7090 batch loss 6.21834517 epoch total loss 5.65981579\n",
      "Trained batch 7091 batch loss 5.73031807 epoch total loss 5.6598258\n",
      "Trained batch 7092 batch loss 5.93684292 epoch total loss 5.6598649\n",
      "Trained batch 7093 batch loss 6.25561333 epoch total loss 5.65994883\n",
      "Trained batch 7094 batch loss 4.79853249 epoch total loss 5.65982676\n",
      "Trained batch 7095 batch loss 6.20304 epoch total loss 5.65990353\n",
      "Trained batch 7096 batch loss 6.07536411 epoch total loss 5.6599617\n",
      "Trained batch 7097 batch loss 5.71097946 epoch total loss 5.65996933\n",
      "Trained batch 7098 batch loss 5.66411877 epoch total loss 5.65997\n",
      "Trained batch 7099 batch loss 5.77273226 epoch total loss 5.65998554\n",
      "Trained batch 7100 batch loss 6.37065744 epoch total loss 5.66008568\n",
      "Trained batch 7101 batch loss 6.62778759 epoch total loss 5.66022205\n",
      "Trained batch 7102 batch loss 5.27165318 epoch total loss 5.66016769\n",
      "Trained batch 7103 batch loss 5.40595341 epoch total loss 5.66013193\n",
      "Trained batch 7104 batch loss 6.10859299 epoch total loss 5.66019535\n",
      "Trained batch 7105 batch loss 5.29054594 epoch total loss 5.6601429\n",
      "Trained batch 7106 batch loss 5.36715603 epoch total loss 5.66010189\n",
      "Trained batch 7107 batch loss 5.75672531 epoch total loss 5.66011572\n",
      "Trained batch 7108 batch loss 5.23718643 epoch total loss 5.66005611\n",
      "Trained batch 7109 batch loss 5.5532608 epoch total loss 5.66004133\n",
      "Trained batch 7110 batch loss 4.69677162 epoch total loss 5.65990591\n",
      "Trained batch 7111 batch loss 5.24505138 epoch total loss 5.65984774\n",
      "Trained batch 7112 batch loss 5.99913454 epoch total loss 5.65989542\n",
      "Trained batch 7113 batch loss 5.18087244 epoch total loss 5.65982771\n",
      "Trained batch 7114 batch loss 5.67595863 epoch total loss 5.65983\n",
      "Trained batch 7115 batch loss 5.5857296 epoch total loss 5.6598196\n",
      "Trained batch 7116 batch loss 5.48217916 epoch total loss 5.65979433\n",
      "Trained batch 7117 batch loss 5.606112 epoch total loss 5.6597867\n",
      "Trained batch 7118 batch loss 5.17937469 epoch total loss 5.65971947\n",
      "Trained batch 7119 batch loss 5.23119259 epoch total loss 5.65965891\n",
      "Trained batch 7120 batch loss 4.5403676 epoch total loss 5.65950155\n",
      "Trained batch 7121 batch loss 5.59446955 epoch total loss 5.65949249\n",
      "Trained batch 7122 batch loss 6.0437088 epoch total loss 5.65954638\n",
      "Trained batch 7123 batch loss 5.3466711 epoch total loss 5.65950251\n",
      "Trained batch 7124 batch loss 5.79432964 epoch total loss 5.6595211\n",
      "Trained batch 7125 batch loss 6.07405 epoch total loss 5.65957928\n",
      "Trained batch 7126 batch loss 5.27282333 epoch total loss 5.65952539\n",
      "Trained batch 7127 batch loss 6.01802588 epoch total loss 5.65957594\n",
      "Trained batch 7128 batch loss 5.7673769 epoch total loss 5.65959072\n",
      "Trained batch 7129 batch loss 5.65727234 epoch total loss 5.65959024\n",
      "Trained batch 7130 batch loss 5.1722703 epoch total loss 5.65952206\n",
      "Trained batch 7131 batch loss 5.37015533 epoch total loss 5.65948153\n",
      "Trained batch 7132 batch loss 5.34042645 epoch total loss 5.6594367\n",
      "Trained batch 7133 batch loss 5.44236946 epoch total loss 5.65940619\n",
      "Trained batch 7134 batch loss 6.16648483 epoch total loss 5.65947723\n",
      "Trained batch 7135 batch loss 5.94592762 epoch total loss 5.65951729\n",
      "Trained batch 7136 batch loss 6.25415802 epoch total loss 5.65960073\n",
      "Trained batch 7137 batch loss 4.91269207 epoch total loss 5.65949631\n",
      "Trained batch 7138 batch loss 4.72755146 epoch total loss 5.65936565\n",
      "Trained batch 7139 batch loss 5.07687616 epoch total loss 5.65928411\n",
      "Trained batch 7140 batch loss 4.95824051 epoch total loss 5.65918589\n",
      "Trained batch 7141 batch loss 5.35619068 epoch total loss 5.65914297\n",
      "Trained batch 7142 batch loss 6.05481148 epoch total loss 5.65919876\n",
      "Trained batch 7143 batch loss 5.74529839 epoch total loss 5.65921068\n",
      "Trained batch 7144 batch loss 5.98605728 epoch total loss 5.65925646\n",
      "Trained batch 7145 batch loss 6.09919834 epoch total loss 5.65931749\n",
      "Trained batch 7146 batch loss 5.51761913 epoch total loss 5.65929794\n",
      "Trained batch 7147 batch loss 5.37970734 epoch total loss 5.65925884\n",
      "Trained batch 7148 batch loss 5.30378628 epoch total loss 5.65920925\n",
      "Trained batch 7149 batch loss 6.07226372 epoch total loss 5.65926647\n",
      "Trained batch 7150 batch loss 5.10218716 epoch total loss 5.65918875\n",
      "Trained batch 7151 batch loss 5.62069845 epoch total loss 5.6591835\n",
      "Trained batch 7152 batch loss 4.92980099 epoch total loss 5.65908146\n",
      "Trained batch 7153 batch loss 5.30389166 epoch total loss 5.65903187\n",
      "Trained batch 7154 batch loss 5.3461504 epoch total loss 5.65898848\n",
      "Trained batch 7155 batch loss 5.36485 epoch total loss 5.65894699\n",
      "Trained batch 7156 batch loss 5.5801487 epoch total loss 5.65893602\n",
      "Trained batch 7157 batch loss 5.35743093 epoch total loss 5.65889454\n",
      "Trained batch 7158 batch loss 5.53068 epoch total loss 5.65887642\n",
      "Trained batch 7159 batch loss 5.83635807 epoch total loss 5.65890121\n",
      "Trained batch 7160 batch loss 5.54309082 epoch total loss 5.658885\n",
      "Trained batch 7161 batch loss 4.7421 epoch total loss 5.65875721\n",
      "Trained batch 7162 batch loss 5.86426 epoch total loss 5.65878582\n",
      "Trained batch 7163 batch loss 5.78557968 epoch total loss 5.65880346\n",
      "Trained batch 7164 batch loss 5.95569086 epoch total loss 5.65884495\n",
      "Trained batch 7165 batch loss 6.40498209 epoch total loss 5.65894938\n",
      "Trained batch 7166 batch loss 5.58285141 epoch total loss 5.65893841\n",
      "Trained batch 7167 batch loss 6.14444542 epoch total loss 5.65900612\n",
      "Trained batch 7168 batch loss 6.11947393 epoch total loss 5.65907049\n",
      "Trained batch 7169 batch loss 5.35521173 epoch total loss 5.65902853\n",
      "Trained batch 7170 batch loss 5.50473213 epoch total loss 5.6590066\n",
      "Trained batch 7171 batch loss 5.48493481 epoch total loss 5.65898228\n",
      "Trained batch 7172 batch loss 5.72611094 epoch total loss 5.65899181\n",
      "Trained batch 7173 batch loss 5.08611679 epoch total loss 5.65891171\n",
      "Trained batch 7174 batch loss 4.45053434 epoch total loss 5.65874338\n",
      "Trained batch 7175 batch loss 4.68104124 epoch total loss 5.65860701\n",
      "Trained batch 7176 batch loss 4.72519398 epoch total loss 5.65847683\n",
      "Trained batch 7177 batch loss 4.43181372 epoch total loss 5.65830612\n",
      "Trained batch 7178 batch loss 4.29550314 epoch total loss 5.65811682\n",
      "Trained batch 7179 batch loss 5.15990925 epoch total loss 5.6580472\n",
      "Trained batch 7180 batch loss 5.49509716 epoch total loss 5.65802479\n",
      "Trained batch 7181 batch loss 4.022542 epoch total loss 5.65779686\n",
      "Trained batch 7182 batch loss 5.56620407 epoch total loss 5.65778446\n",
      "Trained batch 7183 batch loss 5.53009224 epoch total loss 5.65776682\n",
      "Trained batch 7184 batch loss 4.90679169 epoch total loss 5.65766191\n",
      "Trained batch 7185 batch loss 4.63671589 epoch total loss 5.65752\n",
      "Trained batch 7186 batch loss 4.63550282 epoch total loss 5.65737772\n",
      "Trained batch 7187 batch loss 3.93047714 epoch total loss 5.65713739\n",
      "Trained batch 7188 batch loss 5.48603725 epoch total loss 5.65711355\n",
      "Trained batch 7189 batch loss 4.13410378 epoch total loss 5.65690136\n",
      "Trained batch 7190 batch loss 3.73887801 epoch total loss 5.65663481\n",
      "Trained batch 7191 batch loss 3.35125732 epoch total loss 5.6563139\n",
      "Trained batch 7192 batch loss 3.68749285 epoch total loss 5.65604\n",
      "Trained batch 7193 batch loss 3.84033823 epoch total loss 5.65578794\n",
      "Trained batch 7194 batch loss 3.95212674 epoch total loss 5.65555096\n",
      "Trained batch 7195 batch loss 4.02083111 epoch total loss 5.65532398\n",
      "Trained batch 7196 batch loss 4.75230694 epoch total loss 5.65519857\n",
      "Trained batch 7197 batch loss 4.00875568 epoch total loss 5.65496969\n",
      "Trained batch 7198 batch loss 4.50258064 epoch total loss 5.65480947\n",
      "Trained batch 7199 batch loss 4.00092077 epoch total loss 5.65457964\n",
      "Trained batch 7200 batch loss 5.72791433 epoch total loss 5.65458965\n",
      "Trained batch 7201 batch loss 6.30945873 epoch total loss 5.65468073\n",
      "Trained batch 7202 batch loss 5.74199343 epoch total loss 5.65469265\n",
      "Trained batch 7203 batch loss 5.7339282 epoch total loss 5.65470409\n",
      "Trained batch 7204 batch loss 6.00365973 epoch total loss 5.65475225\n",
      "Trained batch 7205 batch loss 6.27001286 epoch total loss 5.65483761\n",
      "Trained batch 7206 batch loss 5.56614542 epoch total loss 5.65482521\n",
      "Trained batch 7207 batch loss 5.90370178 epoch total loss 5.65485954\n",
      "Trained batch 7208 batch loss 6.36815119 epoch total loss 5.65495872\n",
      "Trained batch 7209 batch loss 6.01471 epoch total loss 5.65500879\n",
      "Trained batch 7210 batch loss 7.4966259 epoch total loss 5.6552639\n",
      "Trained batch 7211 batch loss 6.84255219 epoch total loss 5.65542889\n",
      "Trained batch 7212 batch loss 6.41715813 epoch total loss 5.65553427\n",
      "Trained batch 7213 batch loss 5.80347 epoch total loss 5.65555525\n",
      "Trained batch 7214 batch loss 5.69743681 epoch total loss 5.65556145\n",
      "Trained batch 7215 batch loss 5.31094885 epoch total loss 5.65551376\n",
      "Trained batch 7216 batch loss 5.59768867 epoch total loss 5.65550566\n",
      "Trained batch 7217 batch loss 5.45788431 epoch total loss 5.655478\n",
      "Trained batch 7218 batch loss 5.22608 epoch total loss 5.65541887\n",
      "Trained batch 7219 batch loss 5.92401457 epoch total loss 5.65545607\n",
      "Trained batch 7220 batch loss 5.4449873 epoch total loss 5.65542698\n",
      "Trained batch 7221 batch loss 5.77555561 epoch total loss 5.65544415\n",
      "Trained batch 7222 batch loss 6.15692139 epoch total loss 5.65551329\n",
      "Trained batch 7223 batch loss 4.49097824 epoch total loss 5.65535212\n",
      "Trained batch 7224 batch loss 5.7060008 epoch total loss 5.65535927\n",
      "Trained batch 7225 batch loss 4.69118118 epoch total loss 5.65522623\n",
      "Trained batch 7226 batch loss 4.87296534 epoch total loss 5.65511751\n",
      "Trained batch 7227 batch loss 6.20800209 epoch total loss 5.65519381\n",
      "Trained batch 7228 batch loss 4.7295351 epoch total loss 5.65506601\n",
      "Trained batch 7229 batch loss 6.24020576 epoch total loss 5.6551466\n",
      "Trained batch 7230 batch loss 5.69856739 epoch total loss 5.6551528\n",
      "Trained batch 7231 batch loss 5.5208745 epoch total loss 5.65513372\n",
      "Trained batch 7232 batch loss 5.74535 epoch total loss 5.6551466\n",
      "Trained batch 7233 batch loss 4.61514282 epoch total loss 5.65500259\n",
      "Trained batch 7234 batch loss 5.23948097 epoch total loss 5.6549449\n",
      "Trained batch 7235 batch loss 5.66246414 epoch total loss 5.65494633\n",
      "Trained batch 7236 batch loss 5.07643414 epoch total loss 5.65486622\n",
      "Trained batch 7237 batch loss 5.68765593 epoch total loss 5.65487099\n",
      "Trained batch 7238 batch loss 6.45928764 epoch total loss 5.65498209\n",
      "Trained batch 7239 batch loss 5.2263279 epoch total loss 5.65492296\n",
      "Trained batch 7240 batch loss 5.27867603 epoch total loss 5.65487099\n",
      "Trained batch 7241 batch loss 5.617033 epoch total loss 5.65486574\n",
      "Trained batch 7242 batch loss 5.04519415 epoch total loss 5.65478182\n",
      "Trained batch 7243 batch loss 6.25950289 epoch total loss 5.65486526\n",
      "Trained batch 7244 batch loss 5.68085194 epoch total loss 5.6548686\n",
      "Trained batch 7245 batch loss 5.59202099 epoch total loss 5.65486\n",
      "Trained batch 7246 batch loss 4.26647186 epoch total loss 5.65466833\n",
      "Trained batch 7247 batch loss 5.49333286 epoch total loss 5.65464592\n",
      "Trained batch 7248 batch loss 4.6096282 epoch total loss 5.65450144\n",
      "Trained batch 7249 batch loss 5.06540203 epoch total loss 5.65442038\n",
      "Trained batch 7250 batch loss 4.77027369 epoch total loss 5.65429831\n",
      "Trained batch 7251 batch loss 4.20871449 epoch total loss 5.65409899\n",
      "Trained batch 7252 batch loss 5.06025791 epoch total loss 5.65401697\n",
      "Trained batch 7253 batch loss 4.96300697 epoch total loss 5.6539216\n",
      "Trained batch 7254 batch loss 5.28125811 epoch total loss 5.65387058\n",
      "Trained batch 7255 batch loss 5.03822231 epoch total loss 5.65378571\n",
      "Trained batch 7256 batch loss 4.49725246 epoch total loss 5.65362597\n",
      "Trained batch 7257 batch loss 5.35764027 epoch total loss 5.65358543\n",
      "Trained batch 7258 batch loss 5.55503416 epoch total loss 5.65357208\n",
      "Trained batch 7259 batch loss 5.01020765 epoch total loss 5.65348339\n",
      "Trained batch 7260 batch loss 5.96974 epoch total loss 5.65352678\n",
      "Trained batch 7261 batch loss 5.21026707 epoch total loss 5.65346575\n",
      "Trained batch 7262 batch loss 4.87584686 epoch total loss 5.65335894\n",
      "Trained batch 7263 batch loss 5.14660835 epoch total loss 5.65328932\n",
      "Trained batch 7264 batch loss 4.8975997 epoch total loss 5.65318537\n",
      "Trained batch 7265 batch loss 5.77343798 epoch total loss 5.65320206\n",
      "Trained batch 7266 batch loss 5.3727417 epoch total loss 5.65316296\n",
      "Trained batch 7267 batch loss 5.95286894 epoch total loss 5.65320444\n",
      "Trained batch 7268 batch loss 5.16376495 epoch total loss 5.65313721\n",
      "Trained batch 7269 batch loss 6.26131535 epoch total loss 5.65322065\n",
      "Trained batch 7270 batch loss 4.95268631 epoch total loss 5.65312433\n",
      "Trained batch 7271 batch loss 4.65274763 epoch total loss 5.652987\n",
      "Trained batch 7272 batch loss 5.61903477 epoch total loss 5.65298176\n",
      "Trained batch 7273 batch loss 5.62804317 epoch total loss 5.65297842\n",
      "Trained batch 7274 batch loss 6.90761566 epoch total loss 5.65315104\n",
      "Trained batch 7275 batch loss 5.21087074 epoch total loss 5.65309\n",
      "Trained batch 7276 batch loss 5.87525129 epoch total loss 5.65312052\n",
      "Trained batch 7277 batch loss 5.86404228 epoch total loss 5.6531496\n",
      "Trained batch 7278 batch loss 6.18580723 epoch total loss 5.65322304\n",
      "Trained batch 7279 batch loss 5.26067591 epoch total loss 5.65316916\n",
      "Trained batch 7280 batch loss 3.84110427 epoch total loss 5.65292025\n",
      "Trained batch 7281 batch loss 5.36659622 epoch total loss 5.65288067\n",
      "Trained batch 7282 batch loss 5.66247129 epoch total loss 5.6528821\n",
      "Trained batch 7283 batch loss 4.3674736 epoch total loss 5.65270567\n",
      "Trained batch 7284 batch loss 6.02837753 epoch total loss 5.65275717\n",
      "Trained batch 7285 batch loss 5.70004511 epoch total loss 5.65276337\n",
      "Trained batch 7286 batch loss 5.89816666 epoch total loss 5.65279722\n",
      "Trained batch 7287 batch loss 6.0189414 epoch total loss 5.65284777\n",
      "Trained batch 7288 batch loss 5.64503765 epoch total loss 5.65284634\n",
      "Trained batch 7289 batch loss 5.31510258 epoch total loss 5.65280056\n",
      "Trained batch 7290 batch loss 6.13660669 epoch total loss 5.65286684\n",
      "Trained batch 7291 batch loss 6.25547314 epoch total loss 5.65294933\n",
      "Trained batch 7292 batch loss 6.91835403 epoch total loss 5.65312243\n",
      "Trained batch 7293 batch loss 6.10046959 epoch total loss 5.65318394\n",
      "Trained batch 7294 batch loss 6.03167534 epoch total loss 5.65323591\n",
      "Trained batch 7295 batch loss 6.4747653 epoch total loss 5.65334892\n",
      "Trained batch 7296 batch loss 5.46442318 epoch total loss 5.65332317\n",
      "Trained batch 7297 batch loss 5.91576576 epoch total loss 5.65335894\n",
      "Trained batch 7298 batch loss 5.45249081 epoch total loss 5.65333128\n",
      "Trained batch 7299 batch loss 5.56117439 epoch total loss 5.65331888\n",
      "Trained batch 7300 batch loss 5.26327515 epoch total loss 5.653265\n",
      "Trained batch 7301 batch loss 4.89720726 epoch total loss 5.653162\n",
      "Trained batch 7302 batch loss 5.60862637 epoch total loss 5.6531558\n",
      "Trained batch 7303 batch loss 4.7375 epoch total loss 5.6530304\n",
      "Trained batch 7304 batch loss 5.46113157 epoch total loss 5.65300417\n",
      "Trained batch 7305 batch loss 4.26521444 epoch total loss 5.65281439\n",
      "Trained batch 7306 batch loss 4.77876759 epoch total loss 5.6526947\n",
      "Trained batch 7307 batch loss 5.10162067 epoch total loss 5.65261889\n",
      "Trained batch 7308 batch loss 5.46393442 epoch total loss 5.65259314\n",
      "Trained batch 7309 batch loss 6.03268242 epoch total loss 5.65264511\n",
      "Trained batch 7310 batch loss 6.472826 epoch total loss 5.65275717\n",
      "Trained batch 7311 batch loss 6.61572409 epoch total loss 5.65288925\n",
      "Trained batch 7312 batch loss 4.75530434 epoch total loss 5.65276623\n",
      "Trained batch 7313 batch loss 4.14176321 epoch total loss 5.65255976\n",
      "Trained batch 7314 batch loss 5.70771122 epoch total loss 5.65256691\n",
      "Trained batch 7315 batch loss 5.62908554 epoch total loss 5.65256357\n",
      "Trained batch 7316 batch loss 5.66361046 epoch total loss 5.65256548\n",
      "Trained batch 7317 batch loss 5.78178692 epoch total loss 5.65258312\n",
      "Trained batch 7318 batch loss 4.87124825 epoch total loss 5.65247631\n",
      "Trained batch 7319 batch loss 6.41628408 epoch total loss 5.65258074\n",
      "Trained batch 7320 batch loss 5.50142813 epoch total loss 5.65255976\n",
      "Trained batch 7321 batch loss 6.15750647 epoch total loss 5.6526289\n",
      "Trained batch 7322 batch loss 5.54393482 epoch total loss 5.65261364\n",
      "Trained batch 7323 batch loss 6.22114849 epoch total loss 5.65269136\n",
      "Trained batch 7324 batch loss 6.2074337 epoch total loss 5.65276718\n",
      "Trained batch 7325 batch loss 5.72009325 epoch total loss 5.65277624\n",
      "Trained batch 7326 batch loss 4.88622284 epoch total loss 5.65267181\n",
      "Trained batch 7327 batch loss 5.88928127 epoch total loss 5.65270424\n",
      "Trained batch 7328 batch loss 5.79043055 epoch total loss 5.65272284\n",
      "Trained batch 7329 batch loss 5.68749189 epoch total loss 5.6527276\n",
      "Trained batch 7330 batch loss 4.50516796 epoch total loss 5.65257072\n",
      "Trained batch 7331 batch loss 5.62454128 epoch total loss 5.65256691\n",
      "Trained batch 7332 batch loss 5.93436956 epoch total loss 5.65260553\n",
      "Trained batch 7333 batch loss 4.51302814 epoch total loss 5.65244961\n",
      "Trained batch 7334 batch loss 5.60373402 epoch total loss 5.65244341\n",
      "Trained batch 7335 batch loss 6.34677362 epoch total loss 5.6525383\n",
      "Trained batch 7336 batch loss 5.52757692 epoch total loss 5.65252113\n",
      "Trained batch 7337 batch loss 5.61117744 epoch total loss 5.65251541\n",
      "Trained batch 7338 batch loss 6.14150047 epoch total loss 5.65258169\n",
      "Trained batch 7339 batch loss 6.0262146 epoch total loss 5.65263271\n",
      "Trained batch 7340 batch loss 5.86490059 epoch total loss 5.65266132\n",
      "Trained batch 7341 batch loss 5.1685791 epoch total loss 5.65259552\n",
      "Trained batch 7342 batch loss 4.49897099 epoch total loss 5.65243864\n",
      "Trained batch 7343 batch loss 5.63853168 epoch total loss 5.65243626\n",
      "Trained batch 7344 batch loss 5.52568579 epoch total loss 5.65241909\n",
      "Trained batch 7345 batch loss 6.13594055 epoch total loss 5.65248537\n",
      "Trained batch 7346 batch loss 5.35265779 epoch total loss 5.65244436\n",
      "Trained batch 7347 batch loss 6.21395779 epoch total loss 5.65252066\n",
      "Trained batch 7348 batch loss 5.0158596 epoch total loss 5.65243435\n",
      "Trained batch 7349 batch loss 4.94413185 epoch total loss 5.65233803\n",
      "Trained batch 7350 batch loss 6.0920887 epoch total loss 5.65239811\n",
      "Trained batch 7351 batch loss 5.62406349 epoch total loss 5.65239429\n",
      "Trained batch 7352 batch loss 6.59625912 epoch total loss 5.65252304\n",
      "Trained batch 7353 batch loss 5.82587337 epoch total loss 5.65254593\n",
      "Trained batch 7354 batch loss 6.21042299 epoch total loss 5.65262222\n",
      "Trained batch 7355 batch loss 6.38127232 epoch total loss 5.65272141\n",
      "Trained batch 7356 batch loss 6.84766579 epoch total loss 5.65288401\n",
      "Trained batch 7357 batch loss 6.06807709 epoch total loss 5.65294\n",
      "Trained batch 7358 batch loss 6.41582 epoch total loss 5.65304327\n",
      "Trained batch 7359 batch loss 5.67165232 epoch total loss 5.65304613\n",
      "Trained batch 7360 batch loss 6.29296684 epoch total loss 5.65313292\n",
      "Trained batch 7361 batch loss 6.59826088 epoch total loss 5.65326118\n",
      "Trained batch 7362 batch loss 6.57766104 epoch total loss 5.65338707\n",
      "Trained batch 7363 batch loss 5.68773127 epoch total loss 5.65339136\n",
      "Trained batch 7364 batch loss 6.44404221 epoch total loss 5.65349913\n",
      "Trained batch 7365 batch loss 5.9044652 epoch total loss 5.65353346\n",
      "Trained batch 7366 batch loss 5.46646738 epoch total loss 5.65350771\n",
      "Trained batch 7367 batch loss 5.67446184 epoch total loss 5.65351057\n",
      "Trained batch 7368 batch loss 6.72223377 epoch total loss 5.65365601\n",
      "Trained batch 7369 batch loss 6.49099779 epoch total loss 5.65376949\n",
      "Trained batch 7370 batch loss 6.56949711 epoch total loss 5.65389395\n",
      "Trained batch 7371 batch loss 6.39760399 epoch total loss 5.65399504\n",
      "Trained batch 7372 batch loss 6.28096 epoch total loss 5.65408039\n",
      "Trained batch 7373 batch loss 6.2322917 epoch total loss 5.65415812\n",
      "Trained batch 7374 batch loss 5.72571087 epoch total loss 5.65416813\n",
      "Trained batch 7375 batch loss 5.83692074 epoch total loss 5.65419292\n",
      "Trained batch 7376 batch loss 6.16119862 epoch total loss 5.65426159\n",
      "Trained batch 7377 batch loss 5.85348368 epoch total loss 5.65428829\n",
      "Trained batch 7378 batch loss 5.358531 epoch total loss 5.65424824\n",
      "Trained batch 7379 batch loss 5.88449717 epoch total loss 5.65427923\n",
      "Trained batch 7380 batch loss 5.26713037 epoch total loss 5.6542263\n",
      "Trained batch 7381 batch loss 5.39413929 epoch total loss 5.65419149\n",
      "Trained batch 7382 batch loss 5.69081593 epoch total loss 5.65419626\n",
      "Trained batch 7383 batch loss 6.5327034 epoch total loss 5.65431499\n",
      "Trained batch 7384 batch loss 4.59964371 epoch total loss 5.65417242\n",
      "Trained batch 7385 batch loss 5.66702366 epoch total loss 5.65417433\n",
      "Trained batch 7386 batch loss 5.07550383 epoch total loss 5.65409613\n",
      "Trained batch 7387 batch loss 6.59824848 epoch total loss 5.65422344\n",
      "Trained batch 7388 batch loss 6.43909645 epoch total loss 5.65433\n",
      "Trained batch 7389 batch loss 5.90260029 epoch total loss 5.65436316\n",
      "Trained batch 7390 batch loss 5.00921631 epoch total loss 5.65427589\n",
      "Trained batch 7391 batch loss 4.98658562 epoch total loss 5.65418577\n",
      "Trained batch 7392 batch loss 6.11433744 epoch total loss 5.65424776\n",
      "Trained batch 7393 batch loss 4.73373365 epoch total loss 5.65412331\n",
      "Trained batch 7394 batch loss 4.41195917 epoch total loss 5.65395498\n",
      "Trained batch 7395 batch loss 5.5718503 epoch total loss 5.65394354\n",
      "Trained batch 7396 batch loss 6.04408836 epoch total loss 5.65399647\n",
      "Trained batch 7397 batch loss 5.68996525 epoch total loss 5.65400124\n",
      "Trained batch 7398 batch loss 4.23618698 epoch total loss 5.65380955\n",
      "Trained batch 7399 batch loss 5.23417759 epoch total loss 5.6537528\n",
      "Trained batch 7400 batch loss 5.01413536 epoch total loss 5.6536665\n",
      "Trained batch 7401 batch loss 4.79658508 epoch total loss 5.65355062\n",
      "Trained batch 7402 batch loss 5.2486639 epoch total loss 5.65349627\n",
      "Trained batch 7403 batch loss 5.75439358 epoch total loss 5.65351\n",
      "Trained batch 7404 batch loss 6.01969242 epoch total loss 5.65355921\n",
      "Trained batch 7405 batch loss 5.29255486 epoch total loss 5.65351057\n",
      "Trained batch 7406 batch loss 6.39814377 epoch total loss 5.65361118\n",
      "Trained batch 7407 batch loss 5.99184227 epoch total loss 5.65365696\n",
      "Trained batch 7408 batch loss 5.51304626 epoch total loss 5.65363789\n",
      "Trained batch 7409 batch loss 4.76611614 epoch total loss 5.65351772\n",
      "Trained batch 7410 batch loss 6.39782715 epoch total loss 5.65361834\n",
      "Trained batch 7411 batch loss 5.36224842 epoch total loss 5.65357924\n",
      "Trained batch 7412 batch loss 4.38770056 epoch total loss 5.65340853\n",
      "Trained batch 7413 batch loss 5.04441881 epoch total loss 5.65332603\n",
      "Trained batch 7414 batch loss 5.93518877 epoch total loss 5.6533637\n",
      "Trained batch 7415 batch loss 5.4529438 epoch total loss 5.653337\n",
      "Trained batch 7416 batch loss 5.12103033 epoch total loss 5.653265\n",
      "Trained batch 7417 batch loss 5.54826164 epoch total loss 5.65325069\n",
      "Trained batch 7418 batch loss 5.63323689 epoch total loss 5.65324783\n",
      "Trained batch 7419 batch loss 5.45561743 epoch total loss 5.65322161\n",
      "Trained batch 7420 batch loss 6.34258461 epoch total loss 5.65331459\n",
      "Trained batch 7421 batch loss 5.5312767 epoch total loss 5.6532979\n",
      "Trained batch 7422 batch loss 5.22442341 epoch total loss 5.65324\n",
      "Trained batch 7423 batch loss 5.33136845 epoch total loss 5.65319681\n",
      "Trained batch 7424 batch loss 5.64741611 epoch total loss 5.65319633\n",
      "Trained batch 7425 batch loss 4.66993523 epoch total loss 5.65306377\n",
      "Trained batch 7426 batch loss 4.20432568 epoch total loss 5.65286875\n",
      "Trained batch 7427 batch loss 6.00348759 epoch total loss 5.65291595\n",
      "Trained batch 7428 batch loss 4.51405907 epoch total loss 5.65276289\n",
      "Trained batch 7429 batch loss 5.78582668 epoch total loss 5.65278053\n",
      "Trained batch 7430 batch loss 6.22978878 epoch total loss 5.65285826\n",
      "Trained batch 7431 batch loss 5.76876879 epoch total loss 5.65287399\n",
      "Trained batch 7432 batch loss 5.14677382 epoch total loss 5.65280628\n",
      "Trained batch 7433 batch loss 5.64311886 epoch total loss 5.65280533\n",
      "Trained batch 7434 batch loss 5.60196972 epoch total loss 5.65279818\n",
      "Trained batch 7435 batch loss 5.2891078 epoch total loss 5.65274954\n",
      "Trained batch 7436 batch loss 5.73196697 epoch total loss 5.65276\n",
      "Trained batch 7437 batch loss 5.83434439 epoch total loss 5.65278435\n",
      "Trained batch 7438 batch loss 6.35631 epoch total loss 5.65287876\n",
      "Trained batch 7439 batch loss 6.29834652 epoch total loss 5.65296555\n",
      "Trained batch 7440 batch loss 5.67189598 epoch total loss 5.65296793\n",
      "Trained batch 7441 batch loss 5.50839615 epoch total loss 5.65294838\n",
      "Trained batch 7442 batch loss 5.73844 epoch total loss 5.65296\n",
      "Trained batch 7443 batch loss 4.28557777 epoch total loss 5.65277624\n",
      "Trained batch 7444 batch loss 5.10754 epoch total loss 5.65270329\n",
      "Trained batch 7445 batch loss 5.40494537 epoch total loss 5.65267\n",
      "Trained batch 7446 batch loss 5.87366438 epoch total loss 5.6527\n",
      "Trained batch 7447 batch loss 6.11809 epoch total loss 5.65276241\n",
      "Trained batch 7448 batch loss 5.62056589 epoch total loss 5.65275812\n",
      "Trained batch 7449 batch loss 5.56522465 epoch total loss 5.65274668\n",
      "Trained batch 7450 batch loss 5.52644444 epoch total loss 5.65272951\n",
      "Trained batch 7451 batch loss 5.60909748 epoch total loss 5.65272379\n",
      "Trained batch 7452 batch loss 5.12019062 epoch total loss 5.65265226\n",
      "Trained batch 7453 batch loss 5.52351713 epoch total loss 5.6526351\n",
      "Trained batch 7454 batch loss 5.6770649 epoch total loss 5.65263844\n",
      "Trained batch 7455 batch loss 5.80714703 epoch total loss 5.65265942\n",
      "Trained batch 7456 batch loss 4.82135868 epoch total loss 5.65254736\n",
      "Trained batch 7457 batch loss 4.68615246 epoch total loss 5.65241814\n",
      "Trained batch 7458 batch loss 5.67098427 epoch total loss 5.65242052\n",
      "Trained batch 7459 batch loss 5.95459175 epoch total loss 5.65246105\n",
      "Trained batch 7460 batch loss 6.19368839 epoch total loss 5.65253401\n",
      "Trained batch 7461 batch loss 5.4911375 epoch total loss 5.65251255\n",
      "Trained batch 7462 batch loss 5.75381947 epoch total loss 5.6525259\n",
      "Trained batch 7463 batch loss 6.09810066 epoch total loss 5.65258551\n",
      "Trained batch 7464 batch loss 5.01618385 epoch total loss 5.6525\n",
      "Trained batch 7465 batch loss 5.30967236 epoch total loss 5.65245438\n",
      "Trained batch 7466 batch loss 5.01596546 epoch total loss 5.65236902\n",
      "Trained batch 7467 batch loss 5.24376202 epoch total loss 5.65231371\n",
      "Trained batch 7468 batch loss 5.64299059 epoch total loss 5.65231276\n",
      "Trained batch 7469 batch loss 6.23106194 epoch total loss 5.65239048\n",
      "Trained batch 7470 batch loss 5.61709452 epoch total loss 5.65238571\n",
      "Trained batch 7471 batch loss 5.71718884 epoch total loss 5.65239429\n",
      "Trained batch 7472 batch loss 5.82769871 epoch total loss 5.65241814\n",
      "Trained batch 7473 batch loss 5.78420162 epoch total loss 5.65243578\n",
      "Trained batch 7474 batch loss 6.44189548 epoch total loss 5.65254116\n",
      "Trained batch 7475 batch loss 6.08026361 epoch total loss 5.65259886\n",
      "Trained batch 7476 batch loss 6.39275217 epoch total loss 5.65269804\n",
      "Trained batch 7477 batch loss 5.76011086 epoch total loss 5.65271282\n",
      "Trained batch 7478 batch loss 6.77751732 epoch total loss 5.65286303\n",
      "Trained batch 7479 batch loss 4.51480246 epoch total loss 5.65271091\n",
      "Trained batch 7480 batch loss 6.55628872 epoch total loss 5.65283155\n",
      "Trained batch 7481 batch loss 6.34435272 epoch total loss 5.65292406\n",
      "Trained batch 7482 batch loss 6.07427597 epoch total loss 5.65298033\n",
      "Trained batch 7483 batch loss 5.86026669 epoch total loss 5.65300798\n",
      "Trained batch 7484 batch loss 6.72926426 epoch total loss 5.65315151\n",
      "Trained batch 7485 batch loss 5.80763912 epoch total loss 5.65317249\n",
      "Trained batch 7486 batch loss 5.59219837 epoch total loss 5.65316439\n",
      "Trained batch 7487 batch loss 6.65021181 epoch total loss 5.65329742\n",
      "Trained batch 7488 batch loss 6.10448837 epoch total loss 5.65335798\n",
      "Trained batch 7489 batch loss 6.27381897 epoch total loss 5.65344048\n",
      "Trained batch 7490 batch loss 6.57841921 epoch total loss 5.65356398\n",
      "Trained batch 7491 batch loss 6.75437355 epoch total loss 5.65371084\n",
      "Trained batch 7492 batch loss 6.63866234 epoch total loss 5.65384245\n",
      "Trained batch 7493 batch loss 6.77848339 epoch total loss 5.65399218\n",
      "Trained batch 7494 batch loss 6.58554077 epoch total loss 5.65411663\n",
      "Trained batch 7495 batch loss 6.39536095 epoch total loss 5.65421534\n",
      "Trained batch 7496 batch loss 5.94141483 epoch total loss 5.65425348\n",
      "Trained batch 7497 batch loss 6.32481337 epoch total loss 5.65434313\n",
      "Trained batch 7498 batch loss 6.21244526 epoch total loss 5.65441704\n",
      "Trained batch 7499 batch loss 6.44724464 epoch total loss 5.6545229\n",
      "Trained batch 7500 batch loss 6.06291294 epoch total loss 5.65457726\n",
      "Trained batch 7501 batch loss 6.39620924 epoch total loss 5.65467596\n",
      "Trained batch 7502 batch loss 5.90577888 epoch total loss 5.65470934\n",
      "Trained batch 7503 batch loss 6.83506775 epoch total loss 5.6548667\n",
      "Trained batch 7504 batch loss 5.98650837 epoch total loss 5.65491104\n",
      "Trained batch 7505 batch loss 6.10235214 epoch total loss 5.65497065\n",
      "Trained batch 7506 batch loss 5.45638084 epoch total loss 5.65494442\n",
      "Trained batch 7507 batch loss 5.61342621 epoch total loss 5.6549387\n",
      "Trained batch 7508 batch loss 5.61957216 epoch total loss 5.65493441\n",
      "Trained batch 7509 batch loss 6.44164944 epoch total loss 5.65503883\n",
      "Trained batch 7510 batch loss 6.71053696 epoch total loss 5.6551795\n",
      "Trained batch 7511 batch loss 6.76093435 epoch total loss 5.65532684\n",
      "Trained batch 7512 batch loss 6.71973324 epoch total loss 5.65546846\n",
      "Trained batch 7513 batch loss 6.29694 epoch total loss 5.65555382\n",
      "Trained batch 7514 batch loss 4.62863111 epoch total loss 5.65541697\n",
      "Trained batch 7515 batch loss 6.71767426 epoch total loss 5.65555859\n",
      "Trained batch 7516 batch loss 6.22811842 epoch total loss 5.65563488\n",
      "Trained batch 7517 batch loss 5.59026575 epoch total loss 5.65562582\n",
      "Trained batch 7518 batch loss 6.15105438 epoch total loss 5.6556921\n",
      "Trained batch 7519 batch loss 5.08301353 epoch total loss 5.65561581\n",
      "Trained batch 7520 batch loss 6.36987209 epoch total loss 5.6557107\n",
      "Trained batch 7521 batch loss 6.59253216 epoch total loss 5.65583563\n",
      "Trained batch 7522 batch loss 5.52603483 epoch total loss 5.65581846\n",
      "Trained batch 7523 batch loss 5.97063112 epoch total loss 5.65586\n",
      "Trained batch 7524 batch loss 6.06582117 epoch total loss 5.65591478\n",
      "Trained batch 7525 batch loss 6.3678112 epoch total loss 5.6560092\n",
      "Trained batch 7526 batch loss 5.80980587 epoch total loss 5.65602922\n",
      "Trained batch 7527 batch loss 6.4417119 epoch total loss 5.65613365\n",
      "Trained batch 7528 batch loss 6.05903625 epoch total loss 5.65618706\n",
      "Trained batch 7529 batch loss 5.31977 epoch total loss 5.65614271\n",
      "Trained batch 7530 batch loss 6.10019875 epoch total loss 5.65620184\n",
      "Trained batch 7531 batch loss 5.41140175 epoch total loss 5.65616894\n",
      "Trained batch 7532 batch loss 5.71542 epoch total loss 5.65617704\n",
      "Trained batch 7533 batch loss 4.22108841 epoch total loss 5.65598679\n",
      "Trained batch 7534 batch loss 4.94301319 epoch total loss 5.6558919\n",
      "Trained batch 7535 batch loss 4.81024551 epoch total loss 5.65577936\n",
      "Trained batch 7536 batch loss 4.18590736 epoch total loss 5.65558434\n",
      "Trained batch 7537 batch loss 4.27520084 epoch total loss 5.65540123\n",
      "Trained batch 7538 batch loss 4.47644138 epoch total loss 5.65524483\n",
      "Trained batch 7539 batch loss 4.51029587 epoch total loss 5.65509319\n",
      "Trained batch 7540 batch loss 5.33209658 epoch total loss 5.65505028\n",
      "Trained batch 7541 batch loss 4.34473515 epoch total loss 5.65487623\n",
      "Trained batch 7542 batch loss 6.22546053 epoch total loss 5.65495205\n",
      "Trained batch 7543 batch loss 5.90522385 epoch total loss 5.65498543\n",
      "Trained batch 7544 batch loss 6.06014204 epoch total loss 5.65503883\n",
      "Trained batch 7545 batch loss 5.78883505 epoch total loss 5.65505648\n",
      "Trained batch 7546 batch loss 5.68884039 epoch total loss 5.65506077\n",
      "Trained batch 7547 batch loss 5.82319832 epoch total loss 5.65508318\n",
      "Trained batch 7548 batch loss 5.68715048 epoch total loss 5.65508747\n",
      "Trained batch 7549 batch loss 5.6547513 epoch total loss 5.65508795\n",
      "Trained batch 7550 batch loss 5.96805859 epoch total loss 5.65512943\n",
      "Trained batch 7551 batch loss 5.9732275 epoch total loss 5.65517139\n",
      "Trained batch 7552 batch loss 5.68234444 epoch total loss 5.65517521\n",
      "Trained batch 7553 batch loss 5.39418221 epoch total loss 5.65514088\n",
      "Trained batch 7554 batch loss 5.88413858 epoch total loss 5.65517092\n",
      "Trained batch 7555 batch loss 6.19475079 epoch total loss 5.65524244\n",
      "Trained batch 7556 batch loss 6.91204119 epoch total loss 5.65540838\n",
      "Trained batch 7557 batch loss 6.46564865 epoch total loss 5.65551567\n",
      "Trained batch 7558 batch loss 6.30226326 epoch total loss 5.65560102\n",
      "Trained batch 7559 batch loss 6.33459 epoch total loss 5.65569067\n",
      "Trained batch 7560 batch loss 6.34815836 epoch total loss 5.65578222\n",
      "Trained batch 7561 batch loss 6.16985703 epoch total loss 5.65585\n",
      "Trained batch 7562 batch loss 6.71884251 epoch total loss 5.6559906\n",
      "Trained batch 7563 batch loss 6.35194969 epoch total loss 5.65608263\n",
      "Trained batch 7564 batch loss 5.12689209 epoch total loss 5.65601254\n",
      "Trained batch 7565 batch loss 4.73367929 epoch total loss 5.65589046\n",
      "Trained batch 7566 batch loss 5.17799091 epoch total loss 5.65582752\n",
      "Trained batch 7567 batch loss 5.46942902 epoch total loss 5.65580273\n",
      "Trained batch 7568 batch loss 5.63186502 epoch total loss 5.6558\n",
      "Trained batch 7569 batch loss 5.13715363 epoch total loss 5.6557312\n",
      "Trained batch 7570 batch loss 5.4218154 epoch total loss 5.6557\n",
      "Trained batch 7571 batch loss 6.01453495 epoch total loss 5.65574789\n",
      "Trained batch 7572 batch loss 5.83541727 epoch total loss 5.65577173\n",
      "Trained batch 7573 batch loss 5.36857605 epoch total loss 5.65573359\n",
      "Trained batch 7574 batch loss 5.41205883 epoch total loss 5.65570116\n",
      "Trained batch 7575 batch loss 6.17343426 epoch total loss 5.65576935\n",
      "Trained batch 7576 batch loss 5.1338582 epoch total loss 5.6557\n",
      "Trained batch 7577 batch loss 6.18136644 epoch total loss 5.65576935\n",
      "Trained batch 7578 batch loss 4.18924665 epoch total loss 5.65557575\n",
      "Trained batch 7579 batch loss 3.93625903 epoch total loss 5.65534925\n",
      "Trained batch 7580 batch loss 5.51865 epoch total loss 5.65533113\n",
      "Trained batch 7581 batch loss 5.88005 epoch total loss 5.6553607\n",
      "Trained batch 7582 batch loss 5.14704323 epoch total loss 5.65529394\n",
      "Trained batch 7583 batch loss 4.88943 epoch total loss 5.65519285\n",
      "Trained batch 7584 batch loss 6.16596889 epoch total loss 5.65526\n",
      "Trained batch 7585 batch loss 5.74871874 epoch total loss 5.65527248\n",
      "Trained batch 7586 batch loss 5.15748596 epoch total loss 5.65520668\n",
      "Trained batch 7587 batch loss 5.36599779 epoch total loss 5.65516901\n",
      "Trained batch 7588 batch loss 5.62778521 epoch total loss 5.6551652\n",
      "Trained batch 7589 batch loss 5.56749296 epoch total loss 5.65515375\n",
      "Trained batch 7590 batch loss 5.10524797 epoch total loss 5.65508127\n",
      "Trained batch 7591 batch loss 5.15875816 epoch total loss 5.65501595\n",
      "Trained batch 7592 batch loss 5.50730801 epoch total loss 5.6549964\n",
      "Trained batch 7593 batch loss 5.57570362 epoch total loss 5.6549859\n",
      "Trained batch 7594 batch loss 5.39980412 epoch total loss 5.65495205\n",
      "Trained batch 7595 batch loss 5.28285503 epoch total loss 5.65490294\n",
      "Trained batch 7596 batch loss 4.94951534 epoch total loss 5.65481\n",
      "Trained batch 7597 batch loss 4.05076361 epoch total loss 5.65459919\n",
      "Trained batch 7598 batch loss 6.03045464 epoch total loss 5.6546483\n",
      "Trained batch 7599 batch loss 5.34782219 epoch total loss 5.65460825\n",
      "Trained batch 7600 batch loss 5.8396039 epoch total loss 5.65463257\n",
      "Trained batch 7601 batch loss 4.60105324 epoch total loss 5.65449381\n",
      "Trained batch 7602 batch loss 5.26145601 epoch total loss 5.65444231\n",
      "Trained batch 7603 batch loss 5.31591225 epoch total loss 5.65439796\n",
      "Trained batch 7604 batch loss 4.88952255 epoch total loss 5.65429735\n",
      "Trained batch 7605 batch loss 4.80394 epoch total loss 5.65418577\n",
      "Trained batch 7606 batch loss 5.57051373 epoch total loss 5.6541748\n",
      "Trained batch 7607 batch loss 5.07918262 epoch total loss 5.65409899\n",
      "Trained batch 7608 batch loss 5.71506882 epoch total loss 5.65410709\n",
      "Trained batch 7609 batch loss 5.53808784 epoch total loss 5.65409184\n",
      "Trained batch 7610 batch loss 5.57245 epoch total loss 5.65408134\n",
      "Trained batch 7611 batch loss 5.81411743 epoch total loss 5.65410233\n",
      "Trained batch 7612 batch loss 6.01486206 epoch total loss 5.65414953\n",
      "Trained batch 7613 batch loss 5.78852701 epoch total loss 5.65416718\n",
      "Trained batch 7614 batch loss 5.22718906 epoch total loss 5.65411139\n",
      "Trained batch 7615 batch loss 5.21502399 epoch total loss 5.65405369\n",
      "Trained batch 7616 batch loss 5.73959303 epoch total loss 5.65406466\n",
      "Trained batch 7617 batch loss 5.09753227 epoch total loss 5.6539917\n",
      "Trained batch 7618 batch loss 5.56885433 epoch total loss 5.65398073\n",
      "Trained batch 7619 batch loss 4.62199831 epoch total loss 5.65384483\n",
      "Trained batch 7620 batch loss 5.44035339 epoch total loss 5.65381718\n",
      "Trained batch 7621 batch loss 4.80886173 epoch total loss 5.65370607\n",
      "Trained batch 7622 batch loss 5.19901466 epoch total loss 5.65364647\n",
      "Trained batch 7623 batch loss 5.53651142 epoch total loss 5.65363121\n",
      "Trained batch 7624 batch loss 5.05614853 epoch total loss 5.65355253\n",
      "Trained batch 7625 batch loss 5.73731136 epoch total loss 5.6535635\n",
      "Trained batch 7626 batch loss 6.00880909 epoch total loss 5.65360975\n",
      "Trained batch 7627 batch loss 5.74729204 epoch total loss 5.65362215\n",
      "Trained batch 7628 batch loss 5.37645912 epoch total loss 5.65358543\n",
      "Trained batch 7629 batch loss 5.31370783 epoch total loss 5.65354109\n",
      "Trained batch 7630 batch loss 5.7600646 epoch total loss 5.65355492\n",
      "Trained batch 7631 batch loss 5.56929493 epoch total loss 5.65354395\n",
      "Trained batch 7632 batch loss 5.73831367 epoch total loss 5.65355539\n",
      "Trained batch 7633 batch loss 5.8009181 epoch total loss 5.65357447\n",
      "Trained batch 7634 batch loss 5.6369257 epoch total loss 5.65357208\n",
      "Trained batch 7635 batch loss 6.36685753 epoch total loss 5.65366602\n",
      "Trained batch 7636 batch loss 5.49937201 epoch total loss 5.65364552\n",
      "Trained batch 7637 batch loss 6.53987741 epoch total loss 5.65376139\n",
      "Trained batch 7638 batch loss 6.08505344 epoch total loss 5.65381813\n",
      "Trained batch 7639 batch loss 5.87635803 epoch total loss 5.65384722\n",
      "Trained batch 7640 batch loss 5.8087945 epoch total loss 5.65386724\n",
      "Trained batch 7641 batch loss 5.42023754 epoch total loss 5.6538372\n",
      "Trained batch 7642 batch loss 6.15475702 epoch total loss 5.65390301\n",
      "Trained batch 7643 batch loss 5.46547318 epoch total loss 5.65387821\n",
      "Trained batch 7644 batch loss 4.95894718 epoch total loss 5.65378666\n",
      "Trained batch 7645 batch loss 6.63591576 epoch total loss 5.65391541\n",
      "Trained batch 7646 batch loss 5.96539879 epoch total loss 5.65395594\n",
      "Trained batch 7647 batch loss 5.73683 epoch total loss 5.6539669\n",
      "Trained batch 7648 batch loss 5.76959324 epoch total loss 5.65398216\n",
      "Trained batch 7649 batch loss 6.40806293 epoch total loss 5.65408039\n",
      "Trained batch 7650 batch loss 5.33697319 epoch total loss 5.65403891\n",
      "Trained batch 7651 batch loss 5.48925972 epoch total loss 5.65401745\n",
      "Trained batch 7652 batch loss 5.67877293 epoch total loss 5.65402079\n",
      "Trained batch 7653 batch loss 6.09278965 epoch total loss 5.65407801\n",
      "Trained batch 7654 batch loss 6.11063766 epoch total loss 5.65413761\n",
      "Trained batch 7655 batch loss 5.59787512 epoch total loss 5.65413046\n",
      "Trained batch 7656 batch loss 4.49329424 epoch total loss 5.65397835\n",
      "Trained batch 7657 batch loss 4.92587185 epoch total loss 5.65388346\n",
      "Trained batch 7658 batch loss 4.86395454 epoch total loss 5.65378\n",
      "Trained batch 7659 batch loss 6.12894058 epoch total loss 5.65384197\n",
      "Trained batch 7660 batch loss 5.2382555 epoch total loss 5.65378809\n",
      "Trained batch 7661 batch loss 5.57059717 epoch total loss 5.65377712\n",
      "Trained batch 7662 batch loss 4.9066534 epoch total loss 5.65367937\n",
      "Trained batch 7663 batch loss 6.07275057 epoch total loss 5.65373421\n",
      "Trained batch 7664 batch loss 6.12998343 epoch total loss 5.6537962\n",
      "Trained batch 7665 batch loss 4.84733534 epoch total loss 5.65369129\n",
      "Trained batch 7666 batch loss 6.18349648 epoch total loss 5.65376043\n",
      "Trained batch 7667 batch loss 5.03777409 epoch total loss 5.65368032\n",
      "Trained batch 7668 batch loss 5.75853348 epoch total loss 5.65369368\n",
      "Trained batch 7669 batch loss 5.8188014 epoch total loss 5.65371561\n",
      "Trained batch 7670 batch loss 5.9455986 epoch total loss 5.65375328\n",
      "Trained batch 7671 batch loss 5.78542614 epoch total loss 5.65377045\n",
      "Trained batch 7672 batch loss 5.70724106 epoch total loss 5.6537776\n",
      "Trained batch 7673 batch loss 5.18377113 epoch total loss 5.65371609\n",
      "Trained batch 7674 batch loss 5.1698494 epoch total loss 5.65365314\n",
      "Trained batch 7675 batch loss 5.2000494 epoch total loss 5.65359354\n",
      "Trained batch 7676 batch loss 4.99814224 epoch total loss 5.65350866\n",
      "Trained batch 7677 batch loss 4.76731586 epoch total loss 5.65339279\n",
      "Trained batch 7678 batch loss 4.85820675 epoch total loss 5.65328932\n",
      "Trained batch 7679 batch loss 4.72514725 epoch total loss 5.65316868\n",
      "Trained batch 7680 batch loss 4.56802368 epoch total loss 5.65302753\n",
      "Trained batch 7681 batch loss 4.7818675 epoch total loss 5.65291405\n",
      "Trained batch 7682 batch loss 4.51020718 epoch total loss 5.65276527\n",
      "Trained batch 7683 batch loss 5.45083332 epoch total loss 5.65273857\n",
      "Trained batch 7684 batch loss 4.97361755 epoch total loss 5.65265036\n",
      "Trained batch 7685 batch loss 4.54200602 epoch total loss 5.65250587\n",
      "Trained batch 7686 batch loss 4.91363621 epoch total loss 5.65241\n",
      "Trained batch 7687 batch loss 5.54541588 epoch total loss 5.6523962\n",
      "Trained batch 7688 batch loss 5.4091115 epoch total loss 5.65236473\n",
      "Trained batch 7689 batch loss 5.48008919 epoch total loss 5.65234232\n",
      "Trained batch 7690 batch loss 5.80024958 epoch total loss 5.65236139\n",
      "Trained batch 7691 batch loss 5.49045277 epoch total loss 5.65234089\n",
      "Trained batch 7692 batch loss 5.97278738 epoch total loss 5.65238237\n",
      "Trained batch 7693 batch loss 5.51730537 epoch total loss 5.65236473\n",
      "Trained batch 7694 batch loss 4.8891058 epoch total loss 5.65226555\n",
      "Trained batch 7695 batch loss 5.32830906 epoch total loss 5.65222359\n",
      "Trained batch 7696 batch loss 4.51977587 epoch total loss 5.65207624\n",
      "Trained batch 7697 batch loss 5.13123608 epoch total loss 5.65200901\n",
      "Trained batch 7698 batch loss 5.05762959 epoch total loss 5.65193176\n",
      "Trained batch 7699 batch loss 5.57973289 epoch total loss 5.65192223\n",
      "Trained batch 7700 batch loss 5.51776886 epoch total loss 5.65190506\n",
      "Trained batch 7701 batch loss 6.06660366 epoch total loss 5.65195894\n",
      "Trained batch 7702 batch loss 5.08270502 epoch total loss 5.65188456\n",
      "Trained batch 7703 batch loss 3.9846828 epoch total loss 5.65166807\n",
      "Trained batch 7704 batch loss 5.01018715 epoch total loss 5.6515851\n",
      "Trained batch 7705 batch loss 5.23084545 epoch total loss 5.65153074\n",
      "Trained batch 7706 batch loss 6.10840273 epoch total loss 5.65159\n",
      "Trained batch 7707 batch loss 5.12457848 epoch total loss 5.65152168\n",
      "Trained batch 7708 batch loss 5.9203825 epoch total loss 5.65155649\n",
      "Trained batch 7709 batch loss 6.33842421 epoch total loss 5.65164614\n",
      "Trained batch 7710 batch loss 5.96845675 epoch total loss 5.65168715\n",
      "Trained batch 7711 batch loss 5.38351107 epoch total loss 5.65165234\n",
      "Trained batch 7712 batch loss 5.293993 epoch total loss 5.65160561\n",
      "Trained batch 7713 batch loss 4.81772518 epoch total loss 5.65149736\n",
      "Trained batch 7714 batch loss 5.34657574 epoch total loss 5.65145826\n",
      "Trained batch 7715 batch loss 5.15737677 epoch total loss 5.65139389\n",
      "Trained batch 7716 batch loss 5.68672943 epoch total loss 5.65139866\n",
      "Trained batch 7717 batch loss 5.33692551 epoch total loss 5.65135765\n",
      "Trained batch 7718 batch loss 4.42056322 epoch total loss 5.65119839\n",
      "Trained batch 7719 batch loss 4.93391609 epoch total loss 5.6511054\n",
      "Trained batch 7720 batch loss 5.55748129 epoch total loss 5.65109348\n",
      "Trained batch 7721 batch loss 4.98045635 epoch total loss 5.6510067\n",
      "Trained batch 7722 batch loss 6.18890476 epoch total loss 5.65107584\n",
      "Trained batch 7723 batch loss 6.07904768 epoch total loss 5.65113115\n",
      "Trained batch 7724 batch loss 6.09120464 epoch total loss 5.65118837\n",
      "Trained batch 7725 batch loss 6.67576218 epoch total loss 5.65132093\n",
      "Trained batch 7726 batch loss 6.68950844 epoch total loss 5.6514554\n",
      "Trained batch 7727 batch loss 5.62544203 epoch total loss 5.65145206\n",
      "Trained batch 7728 batch loss 4.51239109 epoch total loss 5.65130472\n",
      "Trained batch 7729 batch loss 4.94802237 epoch total loss 5.65121365\n",
      "Trained batch 7730 batch loss 5.01400757 epoch total loss 5.65113163\n",
      "Trained batch 7731 batch loss 4.47419071 epoch total loss 5.65097904\n",
      "Trained batch 7732 batch loss 4.52335119 epoch total loss 5.65083313\n",
      "Trained batch 7733 batch loss 5.85147762 epoch total loss 5.65085936\n",
      "Trained batch 7734 batch loss 5.82530689 epoch total loss 5.65088177\n",
      "Trained batch 7735 batch loss 5.2044363 epoch total loss 5.65082359\n",
      "Trained batch 7736 batch loss 5.87615156 epoch total loss 5.65085268\n",
      "Trained batch 7737 batch loss 6.0729084 epoch total loss 5.65090752\n",
      "Trained batch 7738 batch loss 4.52208614 epoch total loss 5.6507616\n",
      "Trained batch 7739 batch loss 6.00317144 epoch total loss 5.65080738\n",
      "Trained batch 7740 batch loss 6.55284309 epoch total loss 5.65092421\n",
      "Trained batch 7741 batch loss 7.61177921 epoch total loss 5.65117741\n",
      "Trained batch 7742 batch loss 5.254632 epoch total loss 5.65112638\n",
      "Trained batch 7743 batch loss 5.58495331 epoch total loss 5.6511178\n",
      "Trained batch 7744 batch loss 5.23257446 epoch total loss 5.65106392\n",
      "Trained batch 7745 batch loss 5.54897261 epoch total loss 5.65105104\n",
      "Trained batch 7746 batch loss 6.63073254 epoch total loss 5.65117741\n",
      "Trained batch 7747 batch loss 7.07515621 epoch total loss 5.65136099\n",
      "Trained batch 7748 batch loss 5.6617403 epoch total loss 5.65136194\n",
      "Trained batch 7749 batch loss 6.82166052 epoch total loss 5.6515131\n",
      "Trained batch 7750 batch loss 5.13369083 epoch total loss 5.65144587\n",
      "Trained batch 7751 batch loss 5.44468594 epoch total loss 5.65141964\n",
      "Trained batch 7752 batch loss 6.51315784 epoch total loss 5.65153027\n",
      "Trained batch 7753 batch loss 6.77227783 epoch total loss 5.65167522\n",
      "Trained batch 7754 batch loss 4.80418444 epoch total loss 5.65156603\n",
      "Trained batch 7755 batch loss 5.06639767 epoch total loss 5.65149069\n",
      "Trained batch 7756 batch loss 5.72844124 epoch total loss 5.6515\n",
      "Trained batch 7757 batch loss 4.7102108 epoch total loss 5.65137911\n",
      "Trained batch 7758 batch loss 4.5726881 epoch total loss 5.65124\n",
      "Trained batch 7759 batch loss 4.98699236 epoch total loss 5.65115452\n",
      "Trained batch 7760 batch loss 5.49417353 epoch total loss 5.65113449\n",
      "Trained batch 7761 batch loss 5.62911844 epoch total loss 5.65113163\n",
      "Trained batch 7762 batch loss 6.0790844 epoch total loss 5.65118694\n",
      "Trained batch 7763 batch loss 5.06615257 epoch total loss 5.6511116\n",
      "Trained batch 7764 batch loss 4.01187229 epoch total loss 5.65090036\n",
      "Trained batch 7765 batch loss 5.48626375 epoch total loss 5.65087891\n",
      "Trained batch 7766 batch loss 6.56197834 epoch total loss 5.65099621\n",
      "Trained batch 7767 batch loss 5.95974445 epoch total loss 5.65103626\n",
      "Trained batch 7768 batch loss 6.08475447 epoch total loss 5.65109205\n",
      "Trained batch 7769 batch loss 6.15259886 epoch total loss 5.65115643\n",
      "Trained batch 7770 batch loss 5.69781876 epoch total loss 5.65116262\n",
      "Trained batch 7771 batch loss 5.86370659 epoch total loss 5.65119028\n",
      "Trained batch 7772 batch loss 5.234231 epoch total loss 5.6511364\n",
      "Trained batch 7773 batch loss 5.44672298 epoch total loss 5.65111\n",
      "Trained batch 7774 batch loss 5.75566626 epoch total loss 5.65112305\n",
      "Trained batch 7775 batch loss 5.5619669 epoch total loss 5.6511116\n",
      "Trained batch 7776 batch loss 6.24445 epoch total loss 5.65118837\n",
      "Trained batch 7777 batch loss 6.39951 epoch total loss 5.65128422\n",
      "Trained batch 7778 batch loss 5.21615171 epoch total loss 5.65122843\n",
      "Trained batch 7779 batch loss 6.3140583 epoch total loss 5.6513133\n",
      "Trained batch 7780 batch loss 4.79033947 epoch total loss 5.65120268\n",
      "Trained batch 7781 batch loss 5.89782619 epoch total loss 5.65123415\n",
      "Trained batch 7782 batch loss 5.57458687 epoch total loss 5.65122461\n",
      "Trained batch 7783 batch loss 6.12209225 epoch total loss 5.65128469\n",
      "Trained batch 7784 batch loss 4.59022903 epoch total loss 5.65114832\n",
      "Trained batch 7785 batch loss 5.54895401 epoch total loss 5.65113544\n",
      "Trained batch 7786 batch loss 5.96051216 epoch total loss 5.6511755\n",
      "Trained batch 7787 batch loss 5.07721567 epoch total loss 5.65110159\n",
      "Trained batch 7788 batch loss 5.497437 epoch total loss 5.65108156\n",
      "Trained batch 7789 batch loss 5.6966753 epoch total loss 5.65108728\n",
      "Trained batch 7790 batch loss 5.47715712 epoch total loss 5.65106487\n",
      "Trained batch 7791 batch loss 5.44705677 epoch total loss 5.65103865\n",
      "Trained batch 7792 batch loss 5.42128801 epoch total loss 5.65100908\n",
      "Trained batch 7793 batch loss 6.15504646 epoch total loss 5.65107393\n",
      "Trained batch 7794 batch loss 5.61467743 epoch total loss 5.65106916\n",
      "Trained batch 7795 batch loss 5.62066507 epoch total loss 5.65106535\n",
      "Trained batch 7796 batch loss 6.44224739 epoch total loss 5.65116692\n",
      "Trained batch 7797 batch loss 6.78831673 epoch total loss 5.65131283\n",
      "Trained batch 7798 batch loss 6.05959845 epoch total loss 5.6513648\n",
      "Trained batch 7799 batch loss 6.99273682 epoch total loss 5.65153694\n",
      "Trained batch 7800 batch loss 5.99400711 epoch total loss 5.65158033\n",
      "Trained batch 7801 batch loss 6.25582314 epoch total loss 5.65165758\n",
      "Trained batch 7802 batch loss 6.09521 epoch total loss 5.65171432\n",
      "Trained batch 7803 batch loss 5.84360123 epoch total loss 5.65173912\n",
      "Trained batch 7804 batch loss 6.2772131 epoch total loss 5.65181923\n",
      "Trained batch 7805 batch loss 5.47999859 epoch total loss 5.65179729\n",
      "Trained batch 7806 batch loss 5.00668955 epoch total loss 5.6517148\n",
      "Trained batch 7807 batch loss 6.80999947 epoch total loss 5.6518631\n",
      "Trained batch 7808 batch loss 4.34710407 epoch total loss 5.65169573\n",
      "Trained batch 7809 batch loss 4.61306858 epoch total loss 5.65156269\n",
      "Trained batch 7810 batch loss 4.31158781 epoch total loss 5.65139151\n",
      "Trained batch 7811 batch loss 6.15323353 epoch total loss 5.6514554\n",
      "Trained batch 7812 batch loss 4.37058926 epoch total loss 5.65129185\n",
      "Trained batch 7813 batch loss 5.15179825 epoch total loss 5.65122795\n",
      "Trained batch 7814 batch loss 6.15327406 epoch total loss 5.65129185\n",
      "Trained batch 7815 batch loss 5.22568512 epoch total loss 5.65123749\n",
      "Trained batch 7816 batch loss 6.0259223 epoch total loss 5.65128565\n",
      "Trained batch 7817 batch loss 6.03162575 epoch total loss 5.65133429\n",
      "Trained batch 7818 batch loss 6.38900232 epoch total loss 5.6514287\n",
      "Trained batch 7819 batch loss 5.05388165 epoch total loss 5.65135241\n",
      "Trained batch 7820 batch loss 5.76003551 epoch total loss 5.65136671\n",
      "Trained batch 7821 batch loss 5.57731152 epoch total loss 5.65135717\n",
      "Trained batch 7822 batch loss 5.73328161 epoch total loss 5.65136814\n",
      "Trained batch 7823 batch loss 6.02980804 epoch total loss 5.6514163\n",
      "Trained batch 7824 batch loss 5.80471659 epoch total loss 5.65143585\n",
      "Trained batch 7825 batch loss 6.5640831 epoch total loss 5.65155268\n",
      "Trained batch 7826 batch loss 6.2268734 epoch total loss 5.65162611\n",
      "Trained batch 7827 batch loss 7.1279645 epoch total loss 5.65181494\n",
      "Trained batch 7828 batch loss 5.22107792 epoch total loss 5.65176\n",
      "Trained batch 7829 batch loss 6.32598877 epoch total loss 5.65184593\n",
      "Trained batch 7830 batch loss 6.82606697 epoch total loss 5.65199566\n",
      "Trained batch 7831 batch loss 6.35340786 epoch total loss 5.65208483\n",
      "Trained batch 7832 batch loss 5.63724613 epoch total loss 5.65208292\n",
      "Trained batch 7833 batch loss 5.60159063 epoch total loss 5.65207624\n",
      "Trained batch 7834 batch loss 5.78461885 epoch total loss 5.65209341\n",
      "Trained batch 7835 batch loss 5.71477318 epoch total loss 5.65210152\n",
      "Trained batch 7836 batch loss 4.64358234 epoch total loss 5.65197277\n",
      "Trained batch 7837 batch loss 5.0318718 epoch total loss 5.65189362\n",
      "Trained batch 7838 batch loss 5.7484169 epoch total loss 5.65190601\n",
      "Trained batch 7839 batch loss 4.96363163 epoch total loss 5.65181875\n",
      "Trained batch 7840 batch loss 4.64472914 epoch total loss 5.65169\n",
      "Trained batch 7841 batch loss 5.10201454 epoch total loss 5.65162\n",
      "Trained batch 7842 batch loss 5.46981621 epoch total loss 5.65159655\n",
      "Trained batch 7843 batch loss 6.15118313 epoch total loss 5.65166044\n",
      "Trained batch 7844 batch loss 5.83847046 epoch total loss 5.65168428\n",
      "Trained batch 7845 batch loss 5.9983325 epoch total loss 5.65172863\n",
      "Trained batch 7846 batch loss 5.81462955 epoch total loss 5.65174961\n",
      "Trained batch 7847 batch loss 6.39161587 epoch total loss 5.65184402\n",
      "Trained batch 7848 batch loss 5.13627958 epoch total loss 5.65177822\n",
      "Trained batch 7849 batch loss 6.04553699 epoch total loss 5.65182877\n",
      "Trained batch 7850 batch loss 6.39198446 epoch total loss 5.6519227\n",
      "Trained batch 7851 batch loss 5.5663209 epoch total loss 5.65191174\n",
      "Trained batch 7852 batch loss 5.84877682 epoch total loss 5.65193701\n",
      "Trained batch 7853 batch loss 5.75651026 epoch total loss 5.65195036\n",
      "Trained batch 7854 batch loss 5.80028439 epoch total loss 5.65196943\n",
      "Trained batch 7855 batch loss 5.75923729 epoch total loss 5.65198278\n",
      "Trained batch 7856 batch loss 4.84862185 epoch total loss 5.65188026\n",
      "Trained batch 7857 batch loss 4.8733325 epoch total loss 5.65178156\n",
      "Trained batch 7858 batch loss 5.93897676 epoch total loss 5.6518178\n",
      "Trained batch 7859 batch loss 6.19320202 epoch total loss 5.65188646\n",
      "Trained batch 7860 batch loss 6.53626204 epoch total loss 5.651999\n",
      "Trained batch 7861 batch loss 6.31092596 epoch total loss 5.65208292\n",
      "Trained batch 7862 batch loss 5.95632 epoch total loss 5.65212154\n",
      "Trained batch 7863 batch loss 5.75541115 epoch total loss 5.65213442\n",
      "Trained batch 7864 batch loss 5.77513123 epoch total loss 5.65215\n",
      "Trained batch 7865 batch loss 5.11095142 epoch total loss 5.65208101\n",
      "Trained batch 7866 batch loss 4.97486591 epoch total loss 5.65199518\n",
      "Trained batch 7867 batch loss 6.18313456 epoch total loss 5.65206289\n",
      "Trained batch 7868 batch loss 4.93896723 epoch total loss 5.65197182\n",
      "Trained batch 7869 batch loss 6.90204287 epoch total loss 5.6521306\n",
      "Trained batch 7870 batch loss 6.03215837 epoch total loss 5.65217876\n",
      "Trained batch 7871 batch loss 5.23233891 epoch total loss 5.65212536\n",
      "Trained batch 7872 batch loss 5.48442268 epoch total loss 5.6521039\n",
      "Trained batch 7873 batch loss 5.84544659 epoch total loss 5.65212822\n",
      "Trained batch 7874 batch loss 5.80997658 epoch total loss 5.65214825\n",
      "Trained batch 7875 batch loss 5.66400051 epoch total loss 5.65214968\n",
      "Trained batch 7876 batch loss 6.07441235 epoch total loss 5.65220356\n",
      "Trained batch 7877 batch loss 5.45397854 epoch total loss 5.65217829\n",
      "Trained batch 7878 batch loss 5.62055492 epoch total loss 5.652174\n",
      "Trained batch 7879 batch loss 5.73682737 epoch total loss 5.65218496\n",
      "Trained batch 7880 batch loss 6.05529165 epoch total loss 5.65223598\n",
      "Trained batch 7881 batch loss 6.26355743 epoch total loss 5.65231371\n",
      "Trained batch 7882 batch loss 5.86125565 epoch total loss 5.65234\n",
      "Trained batch 7883 batch loss 5.99852467 epoch total loss 5.6523838\n",
      "Trained batch 7884 batch loss 6.3339324 epoch total loss 5.65247\n",
      "Trained batch 7885 batch loss 5.62653542 epoch total loss 5.65246677\n",
      "Trained batch 7886 batch loss 6.12585497 epoch total loss 5.65252638\n",
      "Trained batch 7887 batch loss 5.8138485 epoch total loss 5.65254688\n",
      "Trained batch 7888 batch loss 5.47143078 epoch total loss 5.65252399\n",
      "Trained batch 7889 batch loss 5.71499395 epoch total loss 5.6525321\n",
      "Trained batch 7890 batch loss 5.84620571 epoch total loss 5.65255642\n",
      "Trained batch 7891 batch loss 5.22470284 epoch total loss 5.65250254\n",
      "Trained batch 7892 batch loss 5.03604603 epoch total loss 5.65242434\n",
      "Trained batch 7893 batch loss 4.70705843 epoch total loss 5.65230465\n",
      "Trained batch 7894 batch loss 5.10894299 epoch total loss 5.65223598\n",
      "Trained batch 7895 batch loss 4.58517933 epoch total loss 5.65210104\n",
      "Trained batch 7896 batch loss 4.98354149 epoch total loss 5.65201616\n",
      "Trained batch 7897 batch loss 3.55482316 epoch total loss 5.65175056\n",
      "Trained batch 7898 batch loss 5.18211555 epoch total loss 5.65169144\n",
      "Trained batch 7899 batch loss 4.86619616 epoch total loss 5.65159225\n",
      "Trained batch 7900 batch loss 5.86723709 epoch total loss 5.65161943\n",
      "Trained batch 7901 batch loss 5.67585611 epoch total loss 5.6516223\n",
      "Trained batch 7902 batch loss 5.50509691 epoch total loss 5.6516037\n",
      "Trained batch 7903 batch loss 6.65890694 epoch total loss 5.65173149\n",
      "Trained batch 7904 batch loss 6.68030119 epoch total loss 5.65186119\n",
      "Trained batch 7905 batch loss 5.69622231 epoch total loss 5.65186691\n",
      "Trained batch 7906 batch loss 6.29573584 epoch total loss 5.65194845\n",
      "Trained batch 7907 batch loss 6.00694847 epoch total loss 5.65199327\n",
      "Trained batch 7908 batch loss 6.3139 epoch total loss 5.6520772\n",
      "Trained batch 7909 batch loss 6.16123486 epoch total loss 5.65214109\n",
      "Trained batch 7910 batch loss 5.73145771 epoch total loss 5.65215111\n",
      "Trained batch 7911 batch loss 6.02113676 epoch total loss 5.65219736\n",
      "Trained batch 7912 batch loss 5.84902859 epoch total loss 5.65222216\n",
      "Trained batch 7913 batch loss 6.15524578 epoch total loss 5.65228605\n",
      "Trained batch 7914 batch loss 6.1083622 epoch total loss 5.65234375\n",
      "Trained batch 7915 batch loss 5.0429306 epoch total loss 5.65226698\n",
      "Trained batch 7916 batch loss 5.06982136 epoch total loss 5.65219307\n",
      "Trained batch 7917 batch loss 4.66848421 epoch total loss 5.65206909\n",
      "Trained batch 7918 batch loss 5.8745923 epoch total loss 5.65209723\n",
      "Trained batch 7919 batch loss 6.16984749 epoch total loss 5.65216208\n",
      "Trained batch 7920 batch loss 5.16446686 epoch total loss 5.65210056\n",
      "Trained batch 7921 batch loss 6.01698303 epoch total loss 5.65214634\n",
      "Trained batch 7922 batch loss 5.75183773 epoch total loss 5.65215874\n",
      "Trained batch 7923 batch loss 7.17598724 epoch total loss 5.65235138\n",
      "Trained batch 7924 batch loss 5.80733871 epoch total loss 5.65237093\n",
      "Trained batch 7925 batch loss 5.96777248 epoch total loss 5.65241098\n",
      "Trained batch 7926 batch loss 6.80456924 epoch total loss 5.65255594\n",
      "Trained batch 7927 batch loss 5.03040409 epoch total loss 5.65247774\n",
      "Trained batch 7928 batch loss 5.71592331 epoch total loss 5.65248585\n",
      "Trained batch 7929 batch loss 5.56004286 epoch total loss 5.65247393\n",
      "Trained batch 7930 batch loss 5.5521369 epoch total loss 5.65246105\n",
      "Trained batch 7931 batch loss 5.4337883 epoch total loss 5.6524334\n",
      "Trained batch 7932 batch loss 5.88602734 epoch total loss 5.65246296\n",
      "Trained batch 7933 batch loss 5.31039429 epoch total loss 5.65241957\n",
      "Trained batch 7934 batch loss 6.08389091 epoch total loss 5.65247393\n",
      "Trained batch 7935 batch loss 5.56853867 epoch total loss 5.65246344\n",
      "Trained batch 7936 batch loss 5.86016846 epoch total loss 5.65248966\n",
      "Trained batch 7937 batch loss 5.3165741 epoch total loss 5.65244722\n",
      "Trained batch 7938 batch loss 5.96659756 epoch total loss 5.65248632\n",
      "Trained batch 7939 batch loss 5.68066311 epoch total loss 5.65248966\n",
      "Trained batch 7940 batch loss 5.56027699 epoch total loss 5.65247822\n",
      "Trained batch 7941 batch loss 5.15097237 epoch total loss 5.65241528\n",
      "Trained batch 7942 batch loss 5.45194578 epoch total loss 5.65239\n",
      "Trained batch 7943 batch loss 5.67048216 epoch total loss 5.65239239\n",
      "Trained batch 7944 batch loss 4.89565849 epoch total loss 5.65229702\n",
      "Trained batch 7945 batch loss 5.57579613 epoch total loss 5.65228701\n",
      "Trained batch 7946 batch loss 4.70393467 epoch total loss 5.6521678\n",
      "Trained batch 7947 batch loss 5.60188103 epoch total loss 5.6521616\n",
      "Trained batch 7948 batch loss 6.13100863 epoch total loss 5.65222168\n",
      "Trained batch 7949 batch loss 6.75604057 epoch total loss 5.65236092\n",
      "Trained batch 7950 batch loss 6.32589102 epoch total loss 5.65244532\n",
      "Trained batch 7951 batch loss 5.90232182 epoch total loss 5.65247679\n",
      "Trained batch 7952 batch loss 5.59665918 epoch total loss 5.65247\n",
      "Trained batch 7953 batch loss 6.14405966 epoch total loss 5.6525321\n",
      "Trained batch 7954 batch loss 5.25465631 epoch total loss 5.65248156\n",
      "Trained batch 7955 batch loss 4.37809944 epoch total loss 5.65232182\n",
      "Trained batch 7956 batch loss 3.92490721 epoch total loss 5.65210485\n",
      "Trained batch 7957 batch loss 5.58373499 epoch total loss 5.65209579\n",
      "Trained batch 7958 batch loss 5.43041134 epoch total loss 5.65206766\n",
      "Trained batch 7959 batch loss 5.62172508 epoch total loss 5.65206385\n",
      "Trained batch 7960 batch loss 6.0195055 epoch total loss 5.65211\n",
      "Trained batch 7961 batch loss 6.67481232 epoch total loss 5.65223885\n",
      "Trained batch 7962 batch loss 6.59757376 epoch total loss 5.65235758\n",
      "Trained batch 7963 batch loss 6.11986923 epoch total loss 5.65241623\n",
      "Trained batch 7964 batch loss 6.23045158 epoch total loss 5.65248871\n",
      "Trained batch 7965 batch loss 6.13597965 epoch total loss 5.65254974\n",
      "Trained batch 7966 batch loss 6.07180309 epoch total loss 5.6526022\n",
      "Trained batch 7967 batch loss 6.21134949 epoch total loss 5.65267229\n",
      "Trained batch 7968 batch loss 5.88933563 epoch total loss 5.65270233\n",
      "Trained batch 7969 batch loss 5.13422537 epoch total loss 5.652637\n",
      "Trained batch 7970 batch loss 6.13444614 epoch total loss 5.65269709\n",
      "Trained batch 7971 batch loss 4.87333727 epoch total loss 5.65259933\n",
      "Trained batch 7972 batch loss 5.06566238 epoch total loss 5.6525259\n",
      "Trained batch 7973 batch loss 6.12345934 epoch total loss 5.65258551\n",
      "Trained batch 7974 batch loss 6.46701336 epoch total loss 5.65268755\n",
      "Trained batch 7975 batch loss 5.44773388 epoch total loss 5.65266228\n",
      "Trained batch 7976 batch loss 5.69803905 epoch total loss 5.652668\n",
      "Trained batch 7977 batch loss 6.5880785 epoch total loss 5.6527853\n",
      "Trained batch 7978 batch loss 5.57741642 epoch total loss 5.65277624\n",
      "Trained batch 7979 batch loss 4.8597846 epoch total loss 5.65267658\n",
      "Trained batch 7980 batch loss 4.14363098 epoch total loss 5.65248775\n",
      "Trained batch 7981 batch loss 3.96004725 epoch total loss 5.65227556\n",
      "Trained batch 7982 batch loss 4.21731472 epoch total loss 5.65209627\n",
      "Trained batch 7983 batch loss 3.96912909 epoch total loss 5.65188503\n",
      "Trained batch 7984 batch loss 4.25788 epoch total loss 5.65171051\n",
      "Trained batch 7985 batch loss 3.95306206 epoch total loss 5.65149784\n",
      "Trained batch 7986 batch loss 3.20687842 epoch total loss 5.65119171\n",
      "Trained batch 7987 batch loss 3.27225113 epoch total loss 5.65089417\n",
      "Trained batch 7988 batch loss 4.11884546 epoch total loss 5.650702\n",
      "Trained batch 7989 batch loss 4.36126137 epoch total loss 5.65054035\n",
      "Trained batch 7990 batch loss 5.18697 epoch total loss 5.65048265\n",
      "Trained batch 7991 batch loss 4.65770769 epoch total loss 5.6503582\n",
      "Trained batch 7992 batch loss 4.72259521 epoch total loss 5.65024185\n",
      "Trained batch 7993 batch loss 4.63200045 epoch total loss 5.65011454\n",
      "Trained batch 7994 batch loss 4.45693064 epoch total loss 5.64996529\n",
      "Trained batch 7995 batch loss 4.21612263 epoch total loss 5.649786\n",
      "Trained batch 7996 batch loss 3.98051572 epoch total loss 5.64957714\n",
      "Trained batch 7997 batch loss 3.58036947 epoch total loss 5.6493187\n",
      "Trained batch 7998 batch loss 5.20878124 epoch total loss 5.64926338\n",
      "Trained batch 7999 batch loss 5.77677155 epoch total loss 5.64927959\n",
      "Trained batch 8000 batch loss 6.10859203 epoch total loss 5.64933681\n",
      "Trained batch 8001 batch loss 5.0101757 epoch total loss 5.64925718\n",
      "Trained batch 8002 batch loss 5.93791866 epoch total loss 5.64929342\n",
      "Trained batch 8003 batch loss 4.9190321 epoch total loss 5.64920187\n",
      "Trained batch 8004 batch loss 5.75167561 epoch total loss 5.64921427\n",
      "Trained batch 8005 batch loss 5.85235071 epoch total loss 5.64923954\n",
      "Trained batch 8006 batch loss 6.22052383 epoch total loss 5.64931107\n",
      "Trained batch 8007 batch loss 5.7404871 epoch total loss 5.64932251\n",
      "Trained batch 8008 batch loss 5.91617 epoch total loss 5.64935589\n",
      "Trained batch 8009 batch loss 4.66498041 epoch total loss 5.64923286\n",
      "Trained batch 8010 batch loss 5.69621706 epoch total loss 5.64923859\n",
      "Trained batch 8011 batch loss 5.01821709 epoch total loss 5.64916\n",
      "Trained batch 8012 batch loss 3.91364 epoch total loss 5.64894342\n",
      "Trained batch 8013 batch loss 4.38696098 epoch total loss 5.64878607\n",
      "Trained batch 8014 batch loss 5.56173611 epoch total loss 5.6487751\n",
      "Trained batch 8015 batch loss 5.85766172 epoch total loss 5.64880133\n",
      "Trained batch 8016 batch loss 5.41758633 epoch total loss 5.64877272\n",
      "Trained batch 8017 batch loss 5.41816521 epoch total loss 5.64874411\n",
      "Trained batch 8018 batch loss 5.46720028 epoch total loss 5.64872169\n",
      "Trained batch 8019 batch loss 5.44141197 epoch total loss 5.64869547\n",
      "Trained batch 8020 batch loss 5.78606701 epoch total loss 5.64871264\n",
      "Trained batch 8021 batch loss 5.72772169 epoch total loss 5.64872217\n",
      "Trained batch 8022 batch loss 5.81615639 epoch total loss 5.64874315\n",
      "Trained batch 8023 batch loss 5.85283184 epoch total loss 5.64876842\n",
      "Trained batch 8024 batch loss 5.77570391 epoch total loss 5.64878464\n",
      "Trained batch 8025 batch loss 5.73906946 epoch total loss 5.6487956\n",
      "Trained batch 8026 batch loss 4.9880743 epoch total loss 5.64871359\n",
      "Trained batch 8027 batch loss 5.72294712 epoch total loss 5.64872265\n",
      "Trained batch 8028 batch loss 5.04151535 epoch total loss 5.64864731\n",
      "Trained batch 8029 batch loss 5.43218279 epoch total loss 5.64862061\n",
      "Trained batch 8030 batch loss 5.83740711 epoch total loss 5.64864397\n",
      "Trained batch 8031 batch loss 4.10045481 epoch total loss 5.64845133\n",
      "Trained batch 8032 batch loss 5.40320396 epoch total loss 5.64842033\n",
      "Trained batch 8033 batch loss 5.62358809 epoch total loss 5.64841747\n",
      "Trained batch 8034 batch loss 5.06822586 epoch total loss 5.64834499\n",
      "Trained batch 8035 batch loss 4.92395687 epoch total loss 5.64825535\n",
      "Trained batch 8036 batch loss 5.37592793 epoch total loss 5.64822102\n",
      "Trained batch 8037 batch loss 5.69085407 epoch total loss 5.64822674\n",
      "Trained batch 8038 batch loss 5.62533426 epoch total loss 5.64822388\n",
      "Trained batch 8039 batch loss 5.47400045 epoch total loss 5.64820194\n",
      "Trained batch 8040 batch loss 6.80158949 epoch total loss 5.64834499\n",
      "Trained batch 8041 batch loss 6.4797554 epoch total loss 5.64844847\n",
      "Trained batch 8042 batch loss 4.4001255 epoch total loss 5.64829302\n",
      "Trained batch 8043 batch loss 4.56666136 epoch total loss 5.64815855\n",
      "Trained batch 8044 batch loss 5.56996536 epoch total loss 5.64814901\n",
      "Trained batch 8045 batch loss 5.98233604 epoch total loss 5.6481905\n",
      "Trained batch 8046 batch loss 5.81608438 epoch total loss 5.64821148\n",
      "Trained batch 8047 batch loss 5.48505783 epoch total loss 5.64819098\n",
      "Trained batch 8048 batch loss 6.32303381 epoch total loss 5.6482749\n",
      "Trained batch 8049 batch loss 5.74918175 epoch total loss 5.64828777\n",
      "Trained batch 8050 batch loss 5.83607292 epoch total loss 5.64831066\n",
      "Trained batch 8051 batch loss 5.82058764 epoch total loss 5.64833212\n",
      "Trained batch 8052 batch loss 5.76667261 epoch total loss 5.6483469\n",
      "Trained batch 8053 batch loss 4.94743299 epoch total loss 5.64826\n",
      "Trained batch 8054 batch loss 6.17678118 epoch total loss 5.64832544\n",
      "Trained batch 8055 batch loss 4.78052807 epoch total loss 5.64821768\n",
      "Trained batch 8056 batch loss 5.04626083 epoch total loss 5.64814329\n",
      "Trained batch 8057 batch loss 5.77114391 epoch total loss 5.64815807\n",
      "Trained batch 8058 batch loss 5.46414042 epoch total loss 5.64813566\n",
      "Trained batch 8059 batch loss 5.80828714 epoch total loss 5.64815521\n",
      "Trained batch 8060 batch loss 5.548244 epoch total loss 5.64814281\n",
      "Trained batch 8061 batch loss 5.2226553 epoch total loss 5.64809\n",
      "Trained batch 8062 batch loss 5.86466217 epoch total loss 5.64811659\n",
      "Trained batch 8063 batch loss 6.54090881 epoch total loss 5.64822721\n",
      "Trained batch 8064 batch loss 5.66524506 epoch total loss 5.64822912\n",
      "Trained batch 8065 batch loss 6.15428305 epoch total loss 5.64829159\n",
      "Trained batch 8066 batch loss 6.31598282 epoch total loss 5.64837456\n",
      "Trained batch 8067 batch loss 6.08804607 epoch total loss 5.64842939\n",
      "Trained batch 8068 batch loss 5.21281433 epoch total loss 5.64837503\n",
      "Trained batch 8069 batch loss 5.06824923 epoch total loss 5.64830303\n",
      "Trained batch 8070 batch loss 5.72234344 epoch total loss 5.64831209\n",
      "Trained batch 8071 batch loss 4.42569637 epoch total loss 5.64816046\n",
      "Trained batch 8072 batch loss 5.4037962 epoch total loss 5.64813042\n",
      "Trained batch 8073 batch loss 4.15123367 epoch total loss 5.64794493\n",
      "Trained batch 8074 batch loss 4.63887119 epoch total loss 5.64782\n",
      "Trained batch 8075 batch loss 5.72533655 epoch total loss 5.64783\n",
      "Trained batch 8076 batch loss 5.98268652 epoch total loss 5.64787149\n",
      "Trained batch 8077 batch loss 5.03634787 epoch total loss 5.64779568\n",
      "Trained batch 8078 batch loss 4.00478935 epoch total loss 5.64759207\n",
      "Trained batch 8079 batch loss 4.29619884 epoch total loss 5.64742517\n",
      "Trained batch 8080 batch loss 4.8269887 epoch total loss 5.64732361\n",
      "Trained batch 8081 batch loss 6.09824467 epoch total loss 5.6473794\n",
      "Trained batch 8082 batch loss 5.2433691 epoch total loss 5.64732933\n",
      "Trained batch 8083 batch loss 5.28712034 epoch total loss 5.64728498\n",
      "Trained batch 8084 batch loss 5.2602663 epoch total loss 5.6472373\n",
      "Trained batch 8085 batch loss 5.21443653 epoch total loss 5.6471839\n",
      "Trained batch 8086 batch loss 5.52583122 epoch total loss 5.64716911\n",
      "Trained batch 8087 batch loss 5.66829586 epoch total loss 5.6471715\n",
      "Trained batch 8088 batch loss 5.1822834 epoch total loss 5.64711428\n",
      "Trained batch 8089 batch loss 4.84137 epoch total loss 5.64701414\n",
      "Trained batch 8090 batch loss 5.82693768 epoch total loss 5.64703655\n",
      "Trained batch 8091 batch loss 5.52475548 epoch total loss 5.64702129\n",
      "Trained batch 8092 batch loss 5.87872219 epoch total loss 5.64705\n",
      "Trained batch 8093 batch loss 5.95077133 epoch total loss 5.64708757\n",
      "Trained batch 8094 batch loss 5.09629107 epoch total loss 5.64701939\n",
      "Trained batch 8095 batch loss 5.14958525 epoch total loss 5.64695787\n",
      "Trained batch 8096 batch loss 5.67084 epoch total loss 5.64696121\n",
      "Trained batch 8097 batch loss 4.83687401 epoch total loss 5.64686108\n",
      "Trained batch 8098 batch loss 5.99930191 epoch total loss 5.64690447\n",
      "Trained batch 8099 batch loss 5.03763866 epoch total loss 5.64682961\n",
      "Trained batch 8100 batch loss 5.78820801 epoch total loss 5.64684725\n",
      "Trained batch 8101 batch loss 6.16460514 epoch total loss 5.64691067\n",
      "Trained batch 8102 batch loss 6.53303766 epoch total loss 5.64702\n",
      "Trained batch 8103 batch loss 5.9132905 epoch total loss 5.64705276\n",
      "Trained batch 8104 batch loss 5.40374184 epoch total loss 5.64702272\n",
      "Trained batch 8105 batch loss 6.40196228 epoch total loss 5.64711618\n",
      "Trained batch 8106 batch loss 5.94539452 epoch total loss 5.6471529\n",
      "Trained batch 8107 batch loss 4.31602764 epoch total loss 5.64698839\n",
      "Trained batch 8108 batch loss 6.88350582 epoch total loss 5.64714098\n",
      "Trained batch 8109 batch loss 5.9537878 epoch total loss 5.64717865\n",
      "Trained batch 8110 batch loss 5.45773888 epoch total loss 5.64715528\n",
      "Trained batch 8111 batch loss 5.91391087 epoch total loss 5.64718819\n",
      "Trained batch 8112 batch loss 6.29133177 epoch total loss 5.64726782\n",
      "Trained batch 8113 batch loss 6.71159506 epoch total loss 5.64739895\n",
      "Trained batch 8114 batch loss 6.28977871 epoch total loss 5.6474781\n",
      "Trained batch 8115 batch loss 6.42946148 epoch total loss 5.64757442\n",
      "Trained batch 8116 batch loss 6.655653 epoch total loss 5.64769888\n",
      "Trained batch 8117 batch loss 6.87405 epoch total loss 5.64785\n",
      "Trained batch 8118 batch loss 6.30751133 epoch total loss 5.6479311\n",
      "Trained batch 8119 batch loss 6.22911549 epoch total loss 5.6480031\n",
      "Trained batch 8120 batch loss 5.63164473 epoch total loss 5.64800119\n",
      "Trained batch 8121 batch loss 6.18255043 epoch total loss 5.648067\n",
      "Trained batch 8122 batch loss 6.47002506 epoch total loss 5.64816809\n",
      "Trained batch 8123 batch loss 6.70539045 epoch total loss 5.64829874\n",
      "Trained batch 8124 batch loss 6.72160339 epoch total loss 5.64843082\n",
      "Trained batch 8125 batch loss 5.68483734 epoch total loss 5.64843512\n",
      "Trained batch 8126 batch loss 6.57021618 epoch total loss 5.6485486\n",
      "Trained batch 8127 batch loss 5.54055882 epoch total loss 5.64853525\n",
      "Trained batch 8128 batch loss 5.88666439 epoch total loss 5.64856434\n",
      "Trained batch 8129 batch loss 4.90678358 epoch total loss 5.64847326\n",
      "Trained batch 8130 batch loss 6.25449562 epoch total loss 5.64854765\n",
      "Trained batch 8131 batch loss 6.08412075 epoch total loss 5.64860153\n",
      "Trained batch 8132 batch loss 4.97876787 epoch total loss 5.64851904\n",
      "Trained batch 8133 batch loss 5.55724096 epoch total loss 5.64850807\n",
      "Trained batch 8134 batch loss 5.51543522 epoch total loss 5.64849186\n",
      "Trained batch 8135 batch loss 4.97171307 epoch total loss 5.64840889\n",
      "Trained batch 8136 batch loss 4.59983826 epoch total loss 5.64828\n",
      "Trained batch 8137 batch loss 4.03186226 epoch total loss 5.6480813\n",
      "Trained batch 8138 batch loss 5.11510849 epoch total loss 5.6480155\n",
      "Trained batch 8139 batch loss 5.08256626 epoch total loss 5.64794588\n",
      "Trained batch 8140 batch loss 5.28197289 epoch total loss 5.64790106\n",
      "Trained batch 8141 batch loss 5.26425 epoch total loss 5.64785385\n",
      "Trained batch 8142 batch loss 5.12707806 epoch total loss 5.64779043\n",
      "Trained batch 8143 batch loss 4.90239906 epoch total loss 5.64769888\n",
      "Trained batch 8144 batch loss 6.60567665 epoch total loss 5.64781618\n",
      "Trained batch 8145 batch loss 6.30104208 epoch total loss 5.64789629\n",
      "Trained batch 8146 batch loss 4.90632582 epoch total loss 5.64780569\n",
      "Trained batch 8147 batch loss 4.68114948 epoch total loss 5.64768648\n",
      "Trained batch 8148 batch loss 3.62170219 epoch total loss 5.64743805\n",
      "Trained batch 8149 batch loss 5.29785585 epoch total loss 5.64739513\n",
      "Trained batch 8150 batch loss 4.38500309 epoch total loss 5.64724\n",
      "Trained batch 8151 batch loss 6.19168425 epoch total loss 5.64730692\n",
      "Trained batch 8152 batch loss 5.2402916 epoch total loss 5.64725733\n",
      "Trained batch 8153 batch loss 5.33 epoch total loss 5.64721823\n",
      "Trained batch 8154 batch loss 5.02212143 epoch total loss 5.64714146\n",
      "Trained batch 8155 batch loss 5.27156782 epoch total loss 5.64709568\n",
      "Trained batch 8156 batch loss 6.17275667 epoch total loss 5.64716\n",
      "Trained batch 8157 batch loss 5.57871151 epoch total loss 5.64715147\n",
      "Trained batch 8158 batch loss 5.78757858 epoch total loss 5.64716911\n",
      "Trained batch 8159 batch loss 6.06249332 epoch total loss 5.64722\n",
      "Trained batch 8160 batch loss 6.43817472 epoch total loss 5.64731693\n",
      "Trained batch 8161 batch loss 6.28460598 epoch total loss 5.64739513\n",
      "Trained batch 8162 batch loss 6.14935637 epoch total loss 5.64745617\n",
      "Trained batch 8163 batch loss 5.23890781 epoch total loss 5.6474061\n",
      "Trained batch 8164 batch loss 4.89968967 epoch total loss 5.64731455\n",
      "Trained batch 8165 batch loss 4.96655655 epoch total loss 5.6472311\n",
      "Trained batch 8166 batch loss 5.7722559 epoch total loss 5.64724636\n",
      "Trained batch 8167 batch loss 5.3921 epoch total loss 5.64721489\n",
      "Trained batch 8168 batch loss 5.62588549 epoch total loss 5.64721203\n",
      "Trained batch 8169 batch loss 5.32712078 epoch total loss 5.6471734\n",
      "Trained batch 8170 batch loss 5.6278019 epoch total loss 5.64717102\n",
      "Trained batch 8171 batch loss 5.84230614 epoch total loss 5.64719486\n",
      "Trained batch 8172 batch loss 6.03192234 epoch total loss 5.64724207\n",
      "Trained batch 8173 batch loss 5.05345821 epoch total loss 5.64716959\n",
      "Trained batch 8174 batch loss 4.91533756 epoch total loss 5.64708\n",
      "Trained batch 8175 batch loss 5.78634596 epoch total loss 5.64709663\n",
      "Trained batch 8176 batch loss 6.15127945 epoch total loss 5.64715862\n",
      "Trained batch 8177 batch loss 5.57532549 epoch total loss 5.64714956\n",
      "Trained batch 8178 batch loss 6.15101 epoch total loss 5.64721155\n",
      "Trained batch 8179 batch loss 5.72519159 epoch total loss 5.64722109\n",
      "Trained batch 8180 batch loss 5.20602608 epoch total loss 5.64716721\n",
      "Trained batch 8181 batch loss 5.68185616 epoch total loss 5.6471715\n",
      "Trained batch 8182 batch loss 6.70019817 epoch total loss 5.64730024\n",
      "Trained batch 8183 batch loss 5.84440136 epoch total loss 5.64732409\n",
      "Trained batch 8184 batch loss 6.06644106 epoch total loss 5.64737558\n",
      "Trained batch 8185 batch loss 6.32597065 epoch total loss 5.64745808\n",
      "Trained batch 8186 batch loss 6.17572689 epoch total loss 5.64752293\n",
      "Trained batch 8187 batch loss 5.41877031 epoch total loss 5.64749479\n",
      "Trained batch 8188 batch loss 5.56634283 epoch total loss 5.64748478\n",
      "Trained batch 8189 batch loss 5.36131573 epoch total loss 5.64744949\n",
      "Trained batch 8190 batch loss 5.3357029 epoch total loss 5.64741135\n",
      "Trained batch 8191 batch loss 3.67972541 epoch total loss 5.6471715\n",
      "Trained batch 8192 batch loss 3.8888793 epoch total loss 5.64695692\n",
      "Trained batch 8193 batch loss 4.72593212 epoch total loss 5.64684439\n",
      "Trained batch 8194 batch loss 4.31947947 epoch total loss 5.64668274\n",
      "Trained batch 8195 batch loss 4.66792202 epoch total loss 5.64656305\n",
      "Trained batch 8196 batch loss 5.68274164 epoch total loss 5.64656782\n",
      "Trained batch 8197 batch loss 5.57128191 epoch total loss 5.64655828\n",
      "Trained batch 8198 batch loss 5.97186 epoch total loss 5.64659834\n",
      "Trained batch 8199 batch loss 5.80904436 epoch total loss 5.64661789\n",
      "Trained batch 8200 batch loss 5.3006711 epoch total loss 5.64657593\n",
      "Trained batch 8201 batch loss 5.68483257 epoch total loss 5.64658\n",
      "Trained batch 8202 batch loss 5.15938473 epoch total loss 5.64652109\n",
      "Trained batch 8203 batch loss 5.91395283 epoch total loss 5.64655352\n",
      "Trained batch 8204 batch loss 5.75632572 epoch total loss 5.64656734\n",
      "Trained batch 8205 batch loss 6.10393286 epoch total loss 5.64662313\n",
      "Trained batch 8206 batch loss 5.28142166 epoch total loss 5.64657879\n",
      "Trained batch 8207 batch loss 5.53607893 epoch total loss 5.64656496\n",
      "Trained batch 8208 batch loss 5.45007515 epoch total loss 5.64654112\n",
      "Trained batch 8209 batch loss 5.33846617 epoch total loss 5.64650345\n",
      "Trained batch 8210 batch loss 5.95356464 epoch total loss 5.64654112\n",
      "Trained batch 8211 batch loss 5.91077137 epoch total loss 5.64657307\n",
      "Trained batch 8212 batch loss 6.78542614 epoch total loss 5.64671183\n",
      "Trained batch 8213 batch loss 4.90095568 epoch total loss 5.64662123\n",
      "Trained batch 8214 batch loss 5.09159 epoch total loss 5.64655352\n",
      "Trained batch 8215 batch loss 5.36213732 epoch total loss 5.64651871\n",
      "Trained batch 8216 batch loss 5.89516544 epoch total loss 5.64654922\n",
      "Trained batch 8217 batch loss 5.86440468 epoch total loss 5.64657545\n",
      "Trained batch 8218 batch loss 5.65764046 epoch total loss 5.6465764\n",
      "Trained batch 8219 batch loss 5.66573143 epoch total loss 5.64657879\n",
      "Trained batch 8220 batch loss 5.71945906 epoch total loss 5.64658737\n",
      "Trained batch 8221 batch loss 5.91819572 epoch total loss 5.64662027\n",
      "Trained batch 8222 batch loss 5.99117374 epoch total loss 5.64666271\n",
      "Trained batch 8223 batch loss 5.80024 epoch total loss 5.64668131\n",
      "Trained batch 8224 batch loss 5.39133406 epoch total loss 5.64665031\n",
      "Trained batch 8225 batch loss 4.86062527 epoch total loss 5.64655447\n",
      "Trained batch 8226 batch loss 5.03059578 epoch total loss 5.64647961\n",
      "Trained batch 8227 batch loss 5.44110107 epoch total loss 5.64645481\n",
      "Trained batch 8228 batch loss 3.9747982 epoch total loss 5.64625168\n",
      "Trained batch 8229 batch loss 4.95956421 epoch total loss 5.64616823\n",
      "Trained batch 8230 batch loss 5.14938307 epoch total loss 5.64610815\n",
      "Trained batch 8231 batch loss 5.20857096 epoch total loss 5.64605474\n",
      "Trained batch 8232 batch loss 5.3270421 epoch total loss 5.64601612\n",
      "Trained batch 8233 batch loss 5.16506672 epoch total loss 5.64595747\n",
      "Trained batch 8234 batch loss 4.98110294 epoch total loss 5.64587688\n",
      "Trained batch 8235 batch loss 5.19202805 epoch total loss 5.64582157\n",
      "Trained batch 8236 batch loss 5.00051737 epoch total loss 5.64574289\n",
      "Trained batch 8237 batch loss 4.95169926 epoch total loss 5.64565897\n",
      "Trained batch 8238 batch loss 5.44772577 epoch total loss 5.64563513\n",
      "Trained batch 8239 batch loss 5.05064583 epoch total loss 5.64556313\n",
      "Trained batch 8240 batch loss 5.52292633 epoch total loss 5.64554834\n",
      "Trained batch 8241 batch loss 4.90764141 epoch total loss 5.64545822\n",
      "Trained batch 8242 batch loss 5.22128344 epoch total loss 5.6454072\n",
      "Trained batch 8243 batch loss 6.26267052 epoch total loss 5.64548206\n",
      "Trained batch 8244 batch loss 6.26670074 epoch total loss 5.64555693\n",
      "Trained batch 8245 batch loss 4.39030552 epoch total loss 5.64540482\n",
      "Trained batch 8246 batch loss 5.40396118 epoch total loss 5.64537525\n",
      "Trained batch 8247 batch loss 5.08365 epoch total loss 5.64530706\n",
      "Trained batch 8248 batch loss 5.31697 epoch total loss 5.64526701\n",
      "Trained batch 8249 batch loss 5.55221748 epoch total loss 5.64525557\n",
      "Trained batch 8250 batch loss 5.86275673 epoch total loss 5.64528227\n",
      "Trained batch 8251 batch loss 5.7291441 epoch total loss 5.64529276\n",
      "Trained batch 8252 batch loss 5.77648306 epoch total loss 5.64530849\n",
      "Trained batch 8253 batch loss 6.37853336 epoch total loss 5.64539719\n",
      "Trained batch 8254 batch loss 5.5039959 epoch total loss 5.6453805\n",
      "Trained batch 8255 batch loss 6.00026035 epoch total loss 5.64542341\n",
      "Trained batch 8256 batch loss 5.55388641 epoch total loss 5.64541245\n",
      "Trained batch 8257 batch loss 5.52743626 epoch total loss 5.64539814\n",
      "Trained batch 8258 batch loss 4.3885088 epoch total loss 5.64524555\n",
      "Trained batch 8259 batch loss 4.90070248 epoch total loss 5.64515543\n",
      "Trained batch 8260 batch loss 4.81334734 epoch total loss 5.64505482\n",
      "Trained batch 8261 batch loss 5.01027107 epoch total loss 5.64497805\n",
      "Trained batch 8262 batch loss 4.70178556 epoch total loss 5.64486408\n",
      "Trained batch 8263 batch loss 5.41469574 epoch total loss 5.64483595\n",
      "Trained batch 8264 batch loss 5.68873692 epoch total loss 5.64484119\n",
      "Trained batch 8265 batch loss 4.43692303 epoch total loss 5.64469528\n",
      "Trained batch 8266 batch loss 5.05859756 epoch total loss 5.64462423\n",
      "Trained batch 8267 batch loss 4.47223377 epoch total loss 5.64448261\n",
      "Trained batch 8268 batch loss 6.05296135 epoch total loss 5.6445322\n",
      "Trained batch 8269 batch loss 5.73975754 epoch total loss 5.64454365\n",
      "Trained batch 8270 batch loss 5.97743368 epoch total loss 5.6445837\n",
      "Trained batch 8271 batch loss 5.25684881 epoch total loss 5.64453697\n",
      "Trained batch 8272 batch loss 5.57341814 epoch total loss 5.64452839\n",
      "Trained batch 8273 batch loss 5.23237 epoch total loss 5.64447832\n",
      "Trained batch 8274 batch loss 4.3617816 epoch total loss 5.64432335\n",
      "Trained batch 8275 batch loss 5.21034861 epoch total loss 5.64427137\n",
      "Trained batch 8276 batch loss 5.89738035 epoch total loss 5.64430189\n",
      "Trained batch 8277 batch loss 5.72608328 epoch total loss 5.6443119\n",
      "Trained batch 8278 batch loss 6.00967693 epoch total loss 5.64435577\n",
      "Trained batch 8279 batch loss 5.80265665 epoch total loss 5.64437437\n",
      "Trained batch 8280 batch loss 6.12565899 epoch total loss 5.64443254\n",
      "Trained batch 8281 batch loss 5.28207874 epoch total loss 5.64438868\n",
      "Trained batch 8282 batch loss 5.70639896 epoch total loss 5.64439631\n",
      "Trained batch 8283 batch loss 5.44276428 epoch total loss 5.64437199\n",
      "Trained batch 8284 batch loss 5.40086699 epoch total loss 5.64434242\n",
      "Trained batch 8285 batch loss 4.78190041 epoch total loss 5.64423847\n",
      "Trained batch 8286 batch loss 6.21704149 epoch total loss 5.64430761\n",
      "Trained batch 8287 batch loss 4.75584698 epoch total loss 5.64420033\n",
      "Trained batch 8288 batch loss 5.13034344 epoch total loss 5.64413834\n",
      "Trained batch 8289 batch loss 5.74829 epoch total loss 5.64415073\n",
      "Trained batch 8290 batch loss 5.39097834 epoch total loss 5.64412\n",
      "Trained batch 8291 batch loss 7.06377649 epoch total loss 5.6442914\n",
      "Trained batch 8292 batch loss 6.71849823 epoch total loss 5.6444211\n",
      "Trained batch 8293 batch loss 6.22427797 epoch total loss 5.64449072\n",
      "Trained batch 8294 batch loss 6.89859581 epoch total loss 5.64464188\n",
      "Trained batch 8295 batch loss 6.88217545 epoch total loss 5.64479113\n",
      "Trained batch 8296 batch loss 6.55172253 epoch total loss 5.64490032\n",
      "Trained batch 8297 batch loss 7.37497425 epoch total loss 5.6451087\n",
      "Trained batch 8298 batch loss 6.25050735 epoch total loss 5.64518166\n",
      "Trained batch 8299 batch loss 5.67154455 epoch total loss 5.64518499\n",
      "Trained batch 8300 batch loss 5.6883812 epoch total loss 5.64519024\n",
      "Trained batch 8301 batch loss 6.4550457 epoch total loss 5.64528751\n",
      "Trained batch 8302 batch loss 5.78210211 epoch total loss 5.64530373\n",
      "Trained batch 8303 batch loss 5.38997412 epoch total loss 5.64527321\n",
      "Trained batch 8304 batch loss 5.95417118 epoch total loss 5.6453104\n",
      "Trained batch 8305 batch loss 6.69018459 epoch total loss 5.64543629\n",
      "Trained batch 8306 batch loss 6.79190731 epoch total loss 5.64557457\n",
      "Trained batch 8307 batch loss 5.56855965 epoch total loss 5.64556551\n",
      "Trained batch 8308 batch loss 6.75858498 epoch total loss 5.64569902\n",
      "Trained batch 8309 batch loss 6.48773861 epoch total loss 5.64580059\n",
      "Trained batch 8310 batch loss 6.66704559 epoch total loss 5.64592361\n",
      "Trained batch 8311 batch loss 6.01474476 epoch total loss 5.64596796\n",
      "Trained batch 8312 batch loss 5.94175339 epoch total loss 5.64600372\n",
      "Trained batch 8313 batch loss 6.10749197 epoch total loss 5.64605951\n",
      "Trained batch 8314 batch loss 5.57351494 epoch total loss 5.64605093\n",
      "Trained batch 8315 batch loss 5.25235939 epoch total loss 5.64600372\n",
      "Trained batch 8316 batch loss 4.77627 epoch total loss 5.6458993\n",
      "Trained batch 8317 batch loss 6.1342144 epoch total loss 5.64595747\n",
      "Trained batch 8318 batch loss 6.0524025 epoch total loss 5.64600611\n",
      "Trained batch 8319 batch loss 5.80945349 epoch total loss 5.64602566\n",
      "Trained batch 8320 batch loss 6.13370132 epoch total loss 5.64608431\n",
      "Trained batch 8321 batch loss 6.48550415 epoch total loss 5.64618492\n",
      "Trained batch 8322 batch loss 5.93830872 epoch total loss 5.64622\n",
      "Trained batch 8323 batch loss 6.53392839 epoch total loss 5.64632702\n",
      "Trained batch 8324 batch loss 5.92725515 epoch total loss 5.6463604\n",
      "Trained batch 8325 batch loss 5.6393671 epoch total loss 5.64636\n",
      "Trained batch 8326 batch loss 3.83952808 epoch total loss 5.64614296\n",
      "Trained batch 8327 batch loss 5.59675026 epoch total loss 5.64613724\n",
      "Trained batch 8328 batch loss 4.43018818 epoch total loss 5.64599085\n",
      "Trained batch 8329 batch loss 4.85980225 epoch total loss 5.64589643\n",
      "Trained batch 8330 batch loss 4.16934347 epoch total loss 5.64571905\n",
      "Trained batch 8331 batch loss 5.26743174 epoch total loss 5.64567327\n",
      "Trained batch 8332 batch loss 4.88861752 epoch total loss 5.6455822\n",
      "Trained batch 8333 batch loss 5.29135132 epoch total loss 5.64554\n",
      "Trained batch 8334 batch loss 4.8594408 epoch total loss 5.64544582\n",
      "Trained batch 8335 batch loss 4.84636736 epoch total loss 5.64535\n",
      "Trained batch 8336 batch loss 5.56111526 epoch total loss 5.64534\n",
      "Trained batch 8337 batch loss 5.07907677 epoch total loss 5.64527225\n",
      "Trained batch 8338 batch loss 5.08767033 epoch total loss 5.64520502\n",
      "Trained batch 8339 batch loss 4.58676434 epoch total loss 5.64507771\n",
      "Trained batch 8340 batch loss 5.19099808 epoch total loss 5.64502335\n",
      "Trained batch 8341 batch loss 4.49302244 epoch total loss 5.64488506\n",
      "Trained batch 8342 batch loss 4.55988598 epoch total loss 5.64475489\n",
      "Trained batch 8343 batch loss 4.33900833 epoch total loss 5.64459848\n",
      "Trained batch 8344 batch loss 5.51563358 epoch total loss 5.64458323\n",
      "Trained batch 8345 batch loss 5.76391315 epoch total loss 5.64459753\n",
      "Trained batch 8346 batch loss 5.0081706 epoch total loss 5.64452124\n",
      "Trained batch 8347 batch loss 5.29092026 epoch total loss 5.6444788\n",
      "Trained batch 8348 batch loss 5.65699291 epoch total loss 5.64448\n",
      "Trained batch 8349 batch loss 6.80211067 epoch total loss 5.64461851\n",
      "Trained batch 8350 batch loss 5.79974794 epoch total loss 5.64463758\n",
      "Trained batch 8351 batch loss 5.51795769 epoch total loss 5.64462233\n",
      "Trained batch 8352 batch loss 5.28356886 epoch total loss 5.64457941\n",
      "Trained batch 8353 batch loss 6.21119 epoch total loss 5.64464712\n",
      "Trained batch 8354 batch loss 6.10557842 epoch total loss 5.64470243\n",
      "Trained batch 8355 batch loss 5.4091258 epoch total loss 5.6446743\n",
      "Trained batch 8356 batch loss 6.51237392 epoch total loss 5.64477825\n",
      "Trained batch 8357 batch loss 6.33317852 epoch total loss 5.64486027\n",
      "Trained batch 8358 batch loss 6.18140125 epoch total loss 5.64492416\n",
      "Trained batch 8359 batch loss 5.68311262 epoch total loss 5.64492893\n",
      "Trained batch 8360 batch loss 4.75468922 epoch total loss 5.64482212\n",
      "Trained batch 8361 batch loss 5.35587168 epoch total loss 5.64478779\n",
      "Trained batch 8362 batch loss 5.4479146 epoch total loss 5.64476442\n",
      "Trained batch 8363 batch loss 5.72398424 epoch total loss 5.64477348\n",
      "Trained batch 8364 batch loss 5.9343605 epoch total loss 5.64480829\n",
      "Trained batch 8365 batch loss 5.77976513 epoch total loss 5.6448245\n",
      "Trained batch 8366 batch loss 5.64010382 epoch total loss 5.64482403\n",
      "Trained batch 8367 batch loss 6.27537584 epoch total loss 5.64489937\n",
      "Trained batch 8368 batch loss 5.26728 epoch total loss 5.64485359\n",
      "Trained batch 8369 batch loss 4.4537015 epoch total loss 5.64471149\n",
      "Trained batch 8370 batch loss 5.62190533 epoch total loss 5.64470863\n",
      "Trained batch 8371 batch loss 5.55441952 epoch total loss 5.64469767\n",
      "Trained batch 8372 batch loss 6.81438065 epoch total loss 5.64483738\n",
      "Trained batch 8373 batch loss 6.09701157 epoch total loss 5.64489126\n",
      "Trained batch 8374 batch loss 6.24498177 epoch total loss 5.64496326\n",
      "Trained batch 8375 batch loss 5.3436389 epoch total loss 5.64492702\n",
      "Trained batch 8376 batch loss 5.47322464 epoch total loss 5.64490652\n",
      "Trained batch 8377 batch loss 7.12207699 epoch total loss 5.64508295\n",
      "Trained batch 8378 batch loss 5.30944681 epoch total loss 5.6450429\n",
      "Trained batch 8379 batch loss 4.25476646 epoch total loss 5.64487648\n",
      "Trained batch 8380 batch loss 5.45351934 epoch total loss 5.64485359\n",
      "Trained batch 8381 batch loss 5.87229919 epoch total loss 5.64488077\n",
      "Trained batch 8382 batch loss 4.79777193 epoch total loss 5.64477968\n",
      "Trained batch 8383 batch loss 5.06993437 epoch total loss 5.64471102\n",
      "Trained batch 8384 batch loss 5.49757481 epoch total loss 5.64469337\n",
      "Trained batch 8385 batch loss 5.77459335 epoch total loss 5.64470863\n",
      "Trained batch 8386 batch loss 4.12537766 epoch total loss 5.64452744\n",
      "Trained batch 8387 batch loss 4.81236649 epoch total loss 5.64442825\n",
      "Trained batch 8388 batch loss 4.22225 epoch total loss 5.64425898\n",
      "Trained batch 8389 batch loss 5.27805519 epoch total loss 5.64421511\n",
      "Trained batch 8390 batch loss 4.67890739 epoch total loss 5.6441\n",
      "Trained batch 8391 batch loss 4.8697505 epoch total loss 5.64400816\n",
      "Trained batch 8392 batch loss 5.22175121 epoch total loss 5.64395761\n",
      "Trained batch 8393 batch loss 5.74898243 epoch total loss 5.64397049\n",
      "Trained batch 8394 batch loss 5.4064703 epoch total loss 5.64394188\n",
      "Trained batch 8395 batch loss 5.62172413 epoch total loss 5.6439395\n",
      "Trained batch 8396 batch loss 5.7564888 epoch total loss 5.64395285\n",
      "Trained batch 8397 batch loss 5.67441177 epoch total loss 5.64395666\n",
      "Trained batch 8398 batch loss 6.10481167 epoch total loss 5.6440115\n",
      "Trained batch 8399 batch loss 5.8202858 epoch total loss 5.64403248\n",
      "Trained batch 8400 batch loss 5.47638512 epoch total loss 5.64401293\n",
      "Trained batch 8401 batch loss 5.3669672 epoch total loss 5.64398\n",
      "Trained batch 8402 batch loss 6.40911388 epoch total loss 5.6440711\n",
      "Trained batch 8403 batch loss 5.50564575 epoch total loss 5.64405441\n",
      "Trained batch 8404 batch loss 5.7175293 epoch total loss 5.644063\n",
      "Trained batch 8405 batch loss 5.89192581 epoch total loss 5.64409256\n",
      "Trained batch 8406 batch loss 6.18055725 epoch total loss 5.64415646\n",
      "Trained batch 8407 batch loss 5.80283594 epoch total loss 5.64417553\n",
      "Trained batch 8408 batch loss 6.80580807 epoch total loss 5.64431334\n",
      "Trained batch 8409 batch loss 5.9372592 epoch total loss 5.64434814\n",
      "Trained batch 8410 batch loss 5.45068359 epoch total loss 5.64432526\n",
      "Trained batch 8411 batch loss 5.18195438 epoch total loss 5.64427042\n",
      "Trained batch 8412 batch loss 5.83741665 epoch total loss 5.64429283\n",
      "Trained batch 8413 batch loss 5.73405361 epoch total loss 5.6443038\n",
      "Trained batch 8414 batch loss 5.97783 epoch total loss 5.64434338\n",
      "Trained batch 8415 batch loss 5.14378548 epoch total loss 5.64428377\n",
      "Trained batch 8416 batch loss 5.74421692 epoch total loss 5.64429569\n",
      "Trained batch 8417 batch loss 5.42781258 epoch total loss 5.64427042\n",
      "Trained batch 8418 batch loss 5.43147182 epoch total loss 5.64424515\n",
      "Trained batch 8419 batch loss 4.62236691 epoch total loss 5.64412355\n",
      "Trained batch 8420 batch loss 5.46297169 epoch total loss 5.6441021\n",
      "Trained batch 8421 batch loss 5.23612 epoch total loss 5.64405346\n",
      "Trained batch 8422 batch loss 5.7358489 epoch total loss 5.64406443\n",
      "Trained batch 8423 batch loss 5.44735765 epoch total loss 5.64404106\n",
      "Trained batch 8424 batch loss 5.68293715 epoch total loss 5.64404583\n",
      "Trained batch 8425 batch loss 4.66263103 epoch total loss 5.64392948\n",
      "Trained batch 8426 batch loss 5.79163 epoch total loss 5.64394712\n",
      "Trained batch 8427 batch loss 5.61856413 epoch total loss 5.64394379\n",
      "Trained batch 8428 batch loss 4.12223673 epoch total loss 5.64376307\n",
      "Trained batch 8429 batch loss 5.42711353 epoch total loss 5.64373732\n",
      "Trained batch 8430 batch loss 4.11767769 epoch total loss 5.64355612\n",
      "Trained batch 8431 batch loss 5.18621445 epoch total loss 5.64350224\n",
      "Trained batch 8432 batch loss 5.51441288 epoch total loss 5.64348698\n",
      "Trained batch 8433 batch loss 6.15562582 epoch total loss 5.64354801\n",
      "Trained batch 8434 batch loss 5.96692514 epoch total loss 5.64358664\n",
      "Trained batch 8435 batch loss 6.30177402 epoch total loss 5.64366436\n",
      "Trained batch 8436 batch loss 5.84336 epoch total loss 5.6436882\n",
      "Trained batch 8437 batch loss 6.08577728 epoch total loss 5.64374065\n",
      "Trained batch 8438 batch loss 4.26383781 epoch total loss 5.6435771\n",
      "Trained batch 8439 batch loss 6.20288086 epoch total loss 5.64364338\n",
      "Trained batch 8440 batch loss 5.15457439 epoch total loss 5.64358568\n",
      "Trained batch 8441 batch loss 4.68828249 epoch total loss 5.64347219\n",
      "Trained batch 8442 batch loss 6.60900879 epoch total loss 5.64358664\n",
      "Trained batch 8443 batch loss 6.42182732 epoch total loss 5.64367914\n",
      "Trained batch 8444 batch loss 4.90337563 epoch total loss 5.6435914\n",
      "Trained batch 8445 batch loss 5.62600136 epoch total loss 5.64358902\n",
      "Trained batch 8446 batch loss 4.70375347 epoch total loss 5.64347792\n",
      "Trained batch 8447 batch loss 6.3765707 epoch total loss 5.64356422\n",
      "Trained batch 8448 batch loss 6.30996799 epoch total loss 5.6436429\n",
      "Trained batch 8449 batch loss 5.98464203 epoch total loss 5.64368343\n",
      "Trained batch 8450 batch loss 6.30827904 epoch total loss 5.64376211\n",
      "Trained batch 8451 batch loss 6.36469507 epoch total loss 5.64384699\n",
      "Trained batch 8452 batch loss 5.89013958 epoch total loss 5.64387655\n",
      "Trained batch 8453 batch loss 5.88035202 epoch total loss 5.64390421\n",
      "Trained batch 8454 batch loss 6.16686583 epoch total loss 5.6439662\n",
      "Trained batch 8455 batch loss 5.94127369 epoch total loss 5.64400148\n",
      "Trained batch 8456 batch loss 5.46183109 epoch total loss 5.64397955\n",
      "Trained batch 8457 batch loss 6.01143265 epoch total loss 5.64402294\n",
      "Trained batch 8458 batch loss 5.09702301 epoch total loss 5.64395857\n",
      "Trained batch 8459 batch loss 6.01451969 epoch total loss 5.64400244\n",
      "Trained batch 8460 batch loss 5.12014675 epoch total loss 5.64394093\n",
      "Trained batch 8461 batch loss 5.68921709 epoch total loss 5.64394569\n",
      "Trained batch 8462 batch loss 5.7966466 epoch total loss 5.64396381\n",
      "Trained batch 8463 batch loss 5.4917531 epoch total loss 5.64394617\n",
      "Trained batch 8464 batch loss 5.77803564 epoch total loss 5.64396191\n",
      "Trained batch 8465 batch loss 5.54851341 epoch total loss 5.64395046\n",
      "Trained batch 8466 batch loss 5.85460854 epoch total loss 5.64397526\n",
      "Trained batch 8467 batch loss 5.4881053 epoch total loss 5.64395666\n",
      "Trained batch 8468 batch loss 4.13976097 epoch total loss 5.64377928\n",
      "Trained batch 8469 batch loss 4.94988966 epoch total loss 5.64369726\n",
      "Trained batch 8470 batch loss 5.21399975 epoch total loss 5.64364672\n",
      "Trained batch 8471 batch loss 5.32109261 epoch total loss 5.64360857\n",
      "Trained batch 8472 batch loss 4.91924858 epoch total loss 5.64352274\n",
      "Trained batch 8473 batch loss 5.56052399 epoch total loss 5.64351273\n",
      "Trained batch 8474 batch loss 5.19008636 epoch total loss 5.64345932\n",
      "Trained batch 8475 batch loss 5.94338608 epoch total loss 5.64349508\n",
      "Trained batch 8476 batch loss 4.69708824 epoch total loss 5.64338303\n",
      "Trained batch 8477 batch loss 4.51489735 epoch total loss 5.64325\n",
      "Trained batch 8478 batch loss 5.33842421 epoch total loss 5.64321423\n",
      "Trained batch 8479 batch loss 4.84759331 epoch total loss 5.64312077\n",
      "Trained batch 8480 batch loss 5.04522324 epoch total loss 5.64305\n",
      "Trained batch 8481 batch loss 5.2363677 epoch total loss 5.64300251\n",
      "Trained batch 8482 batch loss 5.7587533 epoch total loss 5.64301586\n",
      "Trained batch 8483 batch loss 5.15555811 epoch total loss 5.64295864\n",
      "Trained batch 8484 batch loss 4.53866863 epoch total loss 5.64282846\n",
      "Trained batch 8485 batch loss 5.12886524 epoch total loss 5.64276791\n",
      "Trained batch 8486 batch loss 5.01374149 epoch total loss 5.642694\n",
      "Trained batch 8487 batch loss 4.85594368 epoch total loss 5.64260149\n",
      "Trained batch 8488 batch loss 5.55258179 epoch total loss 5.64259052\n",
      "Trained batch 8489 batch loss 5.12952709 epoch total loss 5.64253\n",
      "Trained batch 8490 batch loss 5.29024553 epoch total loss 5.64248848\n",
      "Trained batch 8491 batch loss 4.78201294 epoch total loss 5.64238691\n",
      "Trained batch 8492 batch loss 4.9558239 epoch total loss 5.64230633\n",
      "Trained batch 8493 batch loss 4.77691174 epoch total loss 5.64220428\n",
      "Trained batch 8494 batch loss 5.60484362 epoch total loss 5.6422\n",
      "Trained batch 8495 batch loss 4.78480101 epoch total loss 5.64209938\n",
      "Trained batch 8496 batch loss 5.99051285 epoch total loss 5.64214039\n",
      "Trained batch 8497 batch loss 6.15541935 epoch total loss 5.64220095\n",
      "Trained batch 8498 batch loss 5.71888781 epoch total loss 5.64221\n",
      "Trained batch 8499 batch loss 5.06292343 epoch total loss 5.64214182\n",
      "Trained batch 8500 batch loss 5.26421309 epoch total loss 5.64209747\n",
      "Trained batch 8501 batch loss 5.45283794 epoch total loss 5.64207506\n",
      "Trained batch 8502 batch loss 5.3869462 epoch total loss 5.64204502\n",
      "Trained batch 8503 batch loss 5.17920113 epoch total loss 5.64199066\n",
      "Trained batch 8504 batch loss 5.49458265 epoch total loss 5.6419735\n",
      "Trained batch 8505 batch loss 5.61614466 epoch total loss 5.64197063\n",
      "Trained batch 8506 batch loss 5.95701838 epoch total loss 5.64200783\n",
      "Trained batch 8507 batch loss 5.71432781 epoch total loss 5.64201641\n",
      "Trained batch 8508 batch loss 5.07760811 epoch total loss 5.64195\n",
      "Trained batch 8509 batch loss 5.52616596 epoch total loss 5.64193678\n",
      "Trained batch 8510 batch loss 5.4698596 epoch total loss 5.64191628\n",
      "Trained batch 8511 batch loss 5.78438568 epoch total loss 5.64193296\n",
      "Trained batch 8512 batch loss 5.77411461 epoch total loss 5.6419487\n",
      "Trained batch 8513 batch loss 5.34084177 epoch total loss 5.64191294\n",
      "Trained batch 8514 batch loss 4.95387554 epoch total loss 5.64183187\n",
      "Trained batch 8515 batch loss 6.07730579 epoch total loss 5.64188337\n",
      "Trained batch 8516 batch loss 6.19000673 epoch total loss 5.64194775\n",
      "Trained batch 8517 batch loss 5.48706865 epoch total loss 5.64192963\n",
      "Trained batch 8518 batch loss 5.68010902 epoch total loss 5.64193439\n",
      "Trained batch 8519 batch loss 5.31267929 epoch total loss 5.64189577\n",
      "Trained batch 8520 batch loss 5.02080774 epoch total loss 5.64182234\n",
      "Trained batch 8521 batch loss 5.9181776 epoch total loss 5.64185476\n",
      "Trained batch 8522 batch loss 5.90610838 epoch total loss 5.64188576\n",
      "Trained batch 8523 batch loss 4.89965439 epoch total loss 5.64179897\n",
      "Trained batch 8524 batch loss 5.59749603 epoch total loss 5.64179373\n",
      "Trained batch 8525 batch loss 5.95800161 epoch total loss 5.64183044\n",
      "Trained batch 8526 batch loss 5.74922371 epoch total loss 5.64184332\n",
      "Trained batch 8527 batch loss 5.50677538 epoch total loss 5.64182758\n",
      "Trained batch 8528 batch loss 4.94896889 epoch total loss 5.64174652\n",
      "Trained batch 8529 batch loss 6.08531475 epoch total loss 5.6417985\n",
      "Trained batch 8530 batch loss 5.18710709 epoch total loss 5.64174509\n",
      "Trained batch 8531 batch loss 5.50294447 epoch total loss 5.64172888\n",
      "Trained batch 8532 batch loss 4.93055582 epoch total loss 5.64164543\n",
      "Trained batch 8533 batch loss 5.3826189 epoch total loss 5.64161539\n",
      "Trained batch 8534 batch loss 5.39633942 epoch total loss 5.6415863\n",
      "Trained batch 8535 batch loss 5.06916 epoch total loss 5.64151907\n",
      "Trained batch 8536 batch loss 5.61588764 epoch total loss 5.64151621\n",
      "Trained batch 8537 batch loss 5.10630178 epoch total loss 5.64145374\n",
      "Trained batch 8538 batch loss 5.32387733 epoch total loss 5.64141655\n",
      "Trained batch 8539 batch loss 4.21404266 epoch total loss 5.64124966\n",
      "Trained batch 8540 batch loss 4.817132 epoch total loss 5.64115286\n",
      "Trained batch 8541 batch loss 5.36260319 epoch total loss 5.64112043\n",
      "Trained batch 8542 batch loss 5.78717327 epoch total loss 5.6411376\n",
      "Trained batch 8543 batch loss 5.82066154 epoch total loss 5.64115858\n",
      "Trained batch 8544 batch loss 5.79190063 epoch total loss 5.64117622\n",
      "Trained batch 8545 batch loss 5.96191502 epoch total loss 5.64121389\n",
      "Trained batch 8546 batch loss 6.00801659 epoch total loss 5.64125681\n",
      "Trained batch 8547 batch loss 6.20750713 epoch total loss 5.64132309\n",
      "Trained batch 8548 batch loss 5.43731308 epoch total loss 5.64129925\n",
      "Trained batch 8549 batch loss 4.67044878 epoch total loss 5.64118576\n",
      "Trained batch 8550 batch loss 5.87822676 epoch total loss 5.64121342\n",
      "Trained batch 8551 batch loss 5.85982513 epoch total loss 5.64123917\n",
      "Trained batch 8552 batch loss 6.00084591 epoch total loss 5.64128113\n",
      "Trained batch 8553 batch loss 5.88489676 epoch total loss 5.64130974\n",
      "Trained batch 8554 batch loss 6.15085888 epoch total loss 5.64136934\n",
      "Trained batch 8555 batch loss 6.24162817 epoch total loss 5.64143944\n",
      "Trained batch 8556 batch loss 6.12358093 epoch total loss 5.64149618\n",
      "Trained batch 8557 batch loss 5.73262644 epoch total loss 5.64150715\n",
      "Trained batch 8558 batch loss 5.87431717 epoch total loss 5.64153433\n",
      "Trained batch 8559 batch loss 5.29696083 epoch total loss 5.6414938\n",
      "Trained batch 8560 batch loss 6.04429817 epoch total loss 5.641541\n",
      "Trained batch 8561 batch loss 6.31539392 epoch total loss 5.64161968\n",
      "Trained batch 8562 batch loss 6.24625492 epoch total loss 5.64169025\n",
      "Trained batch 8563 batch loss 6.41795874 epoch total loss 5.64178085\n",
      "Trained batch 8564 batch loss 5.89051819 epoch total loss 5.64181\n",
      "Trained batch 8565 batch loss 5.8351512 epoch total loss 5.64183283\n",
      "Trained batch 8566 batch loss 5.51221371 epoch total loss 5.64181757\n",
      "Trained batch 8567 batch loss 5.06647301 epoch total loss 5.64175034\n",
      "Trained batch 8568 batch loss 4.8750248 epoch total loss 5.64166069\n",
      "Trained batch 8569 batch loss 4.99892616 epoch total loss 5.64158583\n",
      "Trained batch 8570 batch loss 5.69012594 epoch total loss 5.64159155\n",
      "Trained batch 8571 batch loss 5.80455 epoch total loss 5.64161062\n",
      "Trained batch 8572 batch loss 5.22301388 epoch total loss 5.64156199\n",
      "Trained batch 8573 batch loss 5.8344 epoch total loss 5.6415844\n",
      "Trained batch 8574 batch loss 5.5242815 epoch total loss 5.64157104\n",
      "Trained batch 8575 batch loss 6.50662756 epoch total loss 5.64167166\n",
      "Trained batch 8576 batch loss 5.53206 epoch total loss 5.64165878\n",
      "Trained batch 8577 batch loss 5.48362446 epoch total loss 5.64164066\n",
      "Trained batch 8578 batch loss 5.47436905 epoch total loss 5.64162111\n",
      "Trained batch 8579 batch loss 5.66024446 epoch total loss 5.64162302\n",
      "Trained batch 8580 batch loss 6.129179 epoch total loss 5.64168\n",
      "Trained batch 8581 batch loss 6.33308601 epoch total loss 5.64176035\n",
      "Trained batch 8582 batch loss 6.21843481 epoch total loss 5.64182758\n",
      "Trained batch 8583 batch loss 6.15918779 epoch total loss 5.64188814\n",
      "Trained batch 8584 batch loss 5.86391401 epoch total loss 5.64191389\n",
      "Trained batch 8585 batch loss 5.84710121 epoch total loss 5.64193773\n",
      "Trained batch 8586 batch loss 6.0662241 epoch total loss 5.64198732\n",
      "Trained batch 8587 batch loss 5.39547968 epoch total loss 5.64195824\n",
      "Trained batch 8588 batch loss 5.44532442 epoch total loss 5.64193535\n",
      "Trained batch 8589 batch loss 5.77208328 epoch total loss 5.64195061\n",
      "Trained batch 8590 batch loss 6.9274 epoch total loss 5.64210033\n",
      "Trained batch 8591 batch loss 7.01770353 epoch total loss 5.64226055\n",
      "Trained batch 8592 batch loss 5.54021931 epoch total loss 5.64224863\n",
      "Trained batch 8593 batch loss 5.89383268 epoch total loss 5.64227772\n",
      "Trained batch 8594 batch loss 5.3007741 epoch total loss 5.64223814\n",
      "Trained batch 8595 batch loss 5.17652035 epoch total loss 5.64218378\n",
      "Trained batch 8596 batch loss 5.72434616 epoch total loss 5.64219332\n",
      "Trained batch 8597 batch loss 4.74707794 epoch total loss 5.64208889\n",
      "Trained batch 8598 batch loss 4.40129185 epoch total loss 5.64194489\n",
      "Trained batch 8599 batch loss 4.53828859 epoch total loss 5.64181662\n",
      "Trained batch 8600 batch loss 5.14204311 epoch total loss 5.64175844\n",
      "Trained batch 8601 batch loss 5.76740646 epoch total loss 5.64177275\n",
      "Trained batch 8602 batch loss 6.08964252 epoch total loss 5.64182472\n",
      "Trained batch 8603 batch loss 6.42889118 epoch total loss 5.64191628\n",
      "Trained batch 8604 batch loss 6.55648804 epoch total loss 5.64202261\n",
      "Trained batch 8605 batch loss 6.33097 epoch total loss 5.64210272\n",
      "Trained batch 8606 batch loss 6.72906113 epoch total loss 5.64222908\n",
      "Trained batch 8607 batch loss 5.78824 epoch total loss 5.64224625\n",
      "Trained batch 8608 batch loss 5.44036388 epoch total loss 5.64222288\n",
      "Trained batch 8609 batch loss 5.56895351 epoch total loss 5.6422143\n",
      "Trained batch 8610 batch loss 5.68898 epoch total loss 5.64221954\n",
      "Trained batch 8611 batch loss 5.97648573 epoch total loss 5.64225864\n",
      "Trained batch 8612 batch loss 5.90323496 epoch total loss 5.64228868\n",
      "Trained batch 8613 batch loss 5.60149622 epoch total loss 5.64228392\n",
      "Trained batch 8614 batch loss 5.70097685 epoch total loss 5.64229059\n",
      "Trained batch 8615 batch loss 5.37567186 epoch total loss 5.6422596\n",
      "Trained batch 8616 batch loss 5.39496374 epoch total loss 5.64223099\n",
      "Trained batch 8617 batch loss 5.37660885 epoch total loss 5.6422\n",
      "Trained batch 8618 batch loss 4.42301846 epoch total loss 5.64205837\n",
      "Trained batch 8619 batch loss 5.25355244 epoch total loss 5.64201307\n",
      "Trained batch 8620 batch loss 6.29427719 epoch total loss 5.64208889\n",
      "Trained batch 8621 batch loss 5.43647623 epoch total loss 5.64206505\n",
      "Trained batch 8622 batch loss 6.10639572 epoch total loss 5.64211893\n",
      "Trained batch 8623 batch loss 6.33690405 epoch total loss 5.64219904\n",
      "Trained batch 8624 batch loss 6.11030149 epoch total loss 5.6422534\n",
      "Trained batch 8625 batch loss 4.69286966 epoch total loss 5.64214325\n",
      "Trained batch 8626 batch loss 5.10586452 epoch total loss 5.64208078\n",
      "Trained batch 8627 batch loss 5.46071482 epoch total loss 5.64206\n",
      "Trained batch 8628 batch loss 5.49753666 epoch total loss 5.64204311\n",
      "Trained batch 8629 batch loss 6.0562 epoch total loss 5.6420908\n",
      "Trained batch 8630 batch loss 5.74342871 epoch total loss 5.64210224\n",
      "Trained batch 8631 batch loss 6.53655 epoch total loss 5.64220572\n",
      "Trained batch 8632 batch loss 6.99772644 epoch total loss 5.64236259\n",
      "Trained batch 8633 batch loss 6.41165972 epoch total loss 5.64245176\n",
      "Trained batch 8634 batch loss 6.18797112 epoch total loss 5.64251471\n",
      "Trained batch 8635 batch loss 6.75703049 epoch total loss 5.64264393\n",
      "Trained batch 8636 batch loss 5.60529423 epoch total loss 5.64263964\n",
      "Trained batch 8637 batch loss 6.36325502 epoch total loss 5.64272308\n",
      "Trained batch 8638 batch loss 6.09286 epoch total loss 5.64277506\n",
      "Trained batch 8639 batch loss 6.29155922 epoch total loss 5.6428504\n",
      "Trained batch 8640 batch loss 5.30631256 epoch total loss 5.6428113\n",
      "Trained batch 8641 batch loss 5.30397367 epoch total loss 5.6427722\n",
      "Trained batch 8642 batch loss 4.13766193 epoch total loss 5.64259815\n",
      "Trained batch 8643 batch loss 4.14653301 epoch total loss 5.64242506\n",
      "Trained batch 8644 batch loss 5.88966465 epoch total loss 5.64245367\n",
      "Trained batch 8645 batch loss 5.98261738 epoch total loss 5.64249325\n",
      "Trained batch 8646 batch loss 4.65072632 epoch total loss 5.64237881\n",
      "Trained batch 8647 batch loss 5.85825 epoch total loss 5.64240408\n",
      "Trained batch 8648 batch loss 4.54871416 epoch total loss 5.64227724\n",
      "Trained batch 8649 batch loss 6.28805351 epoch total loss 5.6423521\n",
      "Trained batch 8650 batch loss 6.86011791 epoch total loss 5.64249277\n",
      "Trained batch 8651 batch loss 6.63928 epoch total loss 5.64260817\n",
      "Trained batch 8652 batch loss 4.36805534 epoch total loss 5.64246082\n",
      "Trained batch 8653 batch loss 6.57187939 epoch total loss 5.64256811\n",
      "Trained batch 8654 batch loss 6.2441597 epoch total loss 5.64263773\n",
      "Trained batch 8655 batch loss 4.932446 epoch total loss 5.64255571\n",
      "Trained batch 8656 batch loss 5.68334484 epoch total loss 5.64256048\n",
      "Trained batch 8657 batch loss 5.77271843 epoch total loss 5.64257574\n",
      "Trained batch 8658 batch loss 4.56399632 epoch total loss 5.64245081\n",
      "Trained batch 8659 batch loss 5.68948078 epoch total loss 5.64245653\n",
      "Trained batch 8660 batch loss 5.34816551 epoch total loss 5.64242268\n",
      "Trained batch 8661 batch loss 6.32692432 epoch total loss 5.64250183\n",
      "Trained batch 8662 batch loss 5.63657475 epoch total loss 5.64250088\n",
      "Trained batch 8663 batch loss 5.15581894 epoch total loss 5.64244509\n",
      "Trained batch 8664 batch loss 5.47530651 epoch total loss 5.64242554\n",
      "Trained batch 8665 batch loss 5.09753799 epoch total loss 5.64236307\n",
      "Trained batch 8666 batch loss 6.01329517 epoch total loss 5.64240551\n",
      "Trained batch 8667 batch loss 6.11503124 epoch total loss 5.64246\n",
      "Trained batch 8668 batch loss 6.1128273 epoch total loss 5.64251423\n",
      "Trained batch 8669 batch loss 4.28824234 epoch total loss 5.64235783\n",
      "Trained batch 8670 batch loss 6.68259192 epoch total loss 5.64247799\n",
      "Trained batch 8671 batch loss 6.65218687 epoch total loss 5.64259434\n",
      "Trained batch 8672 batch loss 6.75197649 epoch total loss 5.64272261\n",
      "Trained batch 8673 batch loss 6.26956463 epoch total loss 5.64279509\n",
      "Trained batch 8674 batch loss 6.19726 epoch total loss 5.64285851\n",
      "Trained batch 8675 batch loss 5.39476299 epoch total loss 5.64283\n",
      "Trained batch 8676 batch loss 4.52022886 epoch total loss 5.64270067\n",
      "Trained batch 8677 batch loss 5.93943119 epoch total loss 5.64273453\n",
      "Trained batch 8678 batch loss 5.57259226 epoch total loss 5.6427269\n",
      "Trained batch 8679 batch loss 6.31001 epoch total loss 5.64280319\n",
      "Trained batch 8680 batch loss 6.07014 epoch total loss 5.64285278\n",
      "Trained batch 8681 batch loss 6.11483145 epoch total loss 5.64290667\n",
      "Trained batch 8682 batch loss 6.19934225 epoch total loss 5.64297104\n",
      "Trained batch 8683 batch loss 6.11186743 epoch total loss 5.64302492\n",
      "Trained batch 8684 batch loss 4.19045925 epoch total loss 5.64285803\n",
      "Trained batch 8685 batch loss 4.79821777 epoch total loss 5.64276028\n",
      "Trained batch 8686 batch loss 5.06119251 epoch total loss 5.64269352\n",
      "Trained batch 8687 batch loss 5.06298 epoch total loss 5.64262676\n",
      "Trained batch 8688 batch loss 5.32407045 epoch total loss 5.64259\n",
      "Trained batch 8689 batch loss 4.52795 epoch total loss 5.64246178\n",
      "Trained batch 8690 batch loss 4.88429928 epoch total loss 5.64237452\n",
      "Trained batch 8691 batch loss 6.03046513 epoch total loss 5.64241934\n",
      "Trained batch 8692 batch loss 6.42965651 epoch total loss 5.64251\n",
      "Trained batch 8693 batch loss 4.70014668 epoch total loss 5.64240122\n",
      "Trained batch 8694 batch loss 5.30753517 epoch total loss 5.64236307\n",
      "Trained batch 8695 batch loss 5.25480461 epoch total loss 5.64231825\n",
      "Trained batch 8696 batch loss 5.28821039 epoch total loss 5.64227772\n",
      "Trained batch 8697 batch loss 5.11919594 epoch total loss 5.64221764\n",
      "Trained batch 8698 batch loss 7.50452232 epoch total loss 5.64243174\n",
      "Trained batch 8699 batch loss 6.4145484 epoch total loss 5.64252043\n",
      "Trained batch 8700 batch loss 5.79786682 epoch total loss 5.64253807\n",
      "Trained batch 8701 batch loss 5.75913239 epoch total loss 5.64255142\n",
      "Trained batch 8702 batch loss 6.21515369 epoch total loss 5.64261723\n",
      "Trained batch 8703 batch loss 5.85683632 epoch total loss 5.64264154\n",
      "Trained batch 8704 batch loss 6.04110146 epoch total loss 5.6426878\n",
      "Trained batch 8705 batch loss 6.04738 epoch total loss 5.64273405\n",
      "Trained batch 8706 batch loss 5.23194695 epoch total loss 5.64268684\n",
      "Trained batch 8707 batch loss 4.84209919 epoch total loss 5.64259481\n",
      "Trained batch 8708 batch loss 4.7561264 epoch total loss 5.64249325\n",
      "Trained batch 8709 batch loss 5.19530964 epoch total loss 5.64244223\n",
      "Trained batch 8710 batch loss 3.70628071 epoch total loss 5.64222\n",
      "Trained batch 8711 batch loss 5.73261261 epoch total loss 5.64223051\n",
      "Trained batch 8712 batch loss 5.66119099 epoch total loss 5.64223242\n",
      "Trained batch 8713 batch loss 6.07746935 epoch total loss 5.64228249\n",
      "Trained batch 8714 batch loss 5.7859726 epoch total loss 5.6422987\n",
      "Trained batch 8715 batch loss 4.9746 epoch total loss 5.64222193\n",
      "Trained batch 8716 batch loss 5.19431353 epoch total loss 5.64217091\n",
      "Trained batch 8717 batch loss 5.358325 epoch total loss 5.64213848\n",
      "Trained batch 8718 batch loss 5.9052968 epoch total loss 5.64216852\n",
      "Trained batch 8719 batch loss 5.66891432 epoch total loss 5.64217138\n",
      "Trained batch 8720 batch loss 5.46312523 epoch total loss 5.64215136\n",
      "Trained batch 8721 batch loss 4.58932447 epoch total loss 5.64203072\n",
      "Trained batch 8722 batch loss 5.81840038 epoch total loss 5.64205122\n",
      "Trained batch 8723 batch loss 5.22199059 epoch total loss 5.64200306\n",
      "Trained batch 8724 batch loss 5.63573265 epoch total loss 5.64200211\n",
      "Trained batch 8725 batch loss 5.78839493 epoch total loss 5.64201927\n",
      "Trained batch 8726 batch loss 6.28903723 epoch total loss 5.64209318\n",
      "Trained batch 8727 batch loss 5.53834105 epoch total loss 5.64208174\n",
      "Trained batch 8728 batch loss 5.34937525 epoch total loss 5.64204788\n",
      "Trained batch 8729 batch loss 5.61508226 epoch total loss 5.64204454\n",
      "Trained batch 8730 batch loss 6.48820686 epoch total loss 5.64214134\n",
      "Trained batch 8731 batch loss 5.78010464 epoch total loss 5.64215755\n",
      "Trained batch 8732 batch loss 4.78307152 epoch total loss 5.64205885\n",
      "Trained batch 8733 batch loss 5.24533606 epoch total loss 5.64201355\n",
      "Trained batch 8734 batch loss 4.80347252 epoch total loss 5.64191771\n",
      "Trained batch 8735 batch loss 6.44011116 epoch total loss 5.64200926\n",
      "Trained batch 8736 batch loss 6.29318047 epoch total loss 5.64208364\n",
      "Trained batch 8737 batch loss 6.1125474 epoch total loss 5.64213753\n",
      "Trained batch 8738 batch loss 6.19175529 epoch total loss 5.64220047\n",
      "Trained batch 8739 batch loss 5.24428749 epoch total loss 5.64215517\n",
      "Trained batch 8740 batch loss 5.91339397 epoch total loss 5.64218616\n",
      "Trained batch 8741 batch loss 5.92271709 epoch total loss 5.64221811\n",
      "Trained batch 8742 batch loss 6.05176353 epoch total loss 5.64226484\n",
      "Trained batch 8743 batch loss 6.26912212 epoch total loss 5.64233685\n",
      "Trained batch 8744 batch loss 6.40144157 epoch total loss 5.64242363\n",
      "Trained batch 8745 batch loss 6.63247538 epoch total loss 5.64253664\n",
      "Trained batch 8746 batch loss 6.41668224 epoch total loss 5.64262533\n",
      "Trained batch 8747 batch loss 5.14132357 epoch total loss 5.64256811\n",
      "Trained batch 8748 batch loss 3.67359447 epoch total loss 5.64234257\n",
      "Trained batch 8749 batch loss 5.29460049 epoch total loss 5.64230299\n",
      "Trained batch 8750 batch loss 4.05148602 epoch total loss 5.64212084\n",
      "Trained batch 8751 batch loss 4.52003384 epoch total loss 5.64199257\n",
      "Trained batch 8752 batch loss 4.79069138 epoch total loss 5.64189529\n",
      "Trained batch 8753 batch loss 4.33809805 epoch total loss 5.64174652\n",
      "Trained batch 8754 batch loss 4.13976669 epoch total loss 5.64157486\n",
      "Trained batch 8755 batch loss 4.04606485 epoch total loss 5.64139271\n",
      "Trained batch 8756 batch loss 4.38729906 epoch total loss 5.64124966\n",
      "Trained batch 8757 batch loss 4.76157665 epoch total loss 5.64114904\n",
      "Trained batch 8758 batch loss 4.45930099 epoch total loss 5.64101458\n",
      "Trained batch 8759 batch loss 4.93544626 epoch total loss 5.64093351\n",
      "Trained batch 8760 batch loss 4.19834948 epoch total loss 5.640769\n",
      "Trained batch 8761 batch loss 3.36317253 epoch total loss 5.64050913\n",
      "Trained batch 8762 batch loss 4.34191895 epoch total loss 5.64036131\n",
      "Trained batch 8763 batch loss 4.2675972 epoch total loss 5.64020443\n",
      "Trained batch 8764 batch loss 4.04786348 epoch total loss 5.64002275\n",
      "Trained batch 8765 batch loss 4.22139263 epoch total loss 5.63986111\n",
      "Trained batch 8766 batch loss 5.31114197 epoch total loss 5.63982391\n",
      "Trained batch 8767 batch loss 5.70745373 epoch total loss 5.63983154\n",
      "Trained batch 8768 batch loss 6.5932579 epoch total loss 5.63994026\n",
      "Trained batch 8769 batch loss 5.77822781 epoch total loss 5.639956\n",
      "Trained batch 8770 batch loss 5.46243238 epoch total loss 5.63993549\n",
      "Trained batch 8771 batch loss 4.28493881 epoch total loss 5.639781\n",
      "Trained batch 8772 batch loss 4.22258282 epoch total loss 5.63961935\n",
      "Trained batch 8773 batch loss 5.40409279 epoch total loss 5.63959265\n",
      "Trained batch 8774 batch loss 6.11953735 epoch total loss 5.63964748\n",
      "Trained batch 8775 batch loss 5.14987087 epoch total loss 5.63959122\n",
      "Trained batch 8776 batch loss 4.94198132 epoch total loss 5.63951159\n",
      "Trained batch 8777 batch loss 6.06225681 epoch total loss 5.63955975\n",
      "Trained batch 8778 batch loss 6.01614141 epoch total loss 5.63960266\n",
      "Trained batch 8779 batch loss 4.62934685 epoch total loss 5.63948774\n",
      "Trained batch 8780 batch loss 5.55357933 epoch total loss 5.63947821\n",
      "Trained batch 8781 batch loss 5.59495926 epoch total loss 5.63947296\n",
      "Trained batch 8782 batch loss 5.56027603 epoch total loss 5.63946342\n",
      "Trained batch 8783 batch loss 5.33776617 epoch total loss 5.63942909\n",
      "Trained batch 8784 batch loss 4.83705473 epoch total loss 5.63933754\n",
      "Trained batch 8785 batch loss 5.20999336 epoch total loss 5.6392889\n",
      "Trained batch 8786 batch loss 4.9189868 epoch total loss 5.63920689\n",
      "Trained batch 8787 batch loss 5.88033247 epoch total loss 5.63923407\n",
      "Trained batch 8788 batch loss 5.14309692 epoch total loss 5.6391778\n",
      "Trained batch 8789 batch loss 4.10888243 epoch total loss 5.63900375\n",
      "Trained batch 8790 batch loss 4.83726454 epoch total loss 5.6389122\n",
      "Trained batch 8791 batch loss 5.62807226 epoch total loss 5.63891125\n",
      "Trained batch 8792 batch loss 5.4438076 epoch total loss 5.63888931\n",
      "Trained batch 8793 batch loss 5.67282867 epoch total loss 5.63889265\n",
      "Trained batch 8794 batch loss 5.68942308 epoch total loss 5.63889837\n",
      "Trained batch 8795 batch loss 4.90748692 epoch total loss 5.63881493\n",
      "Trained batch 8796 batch loss 5.36179733 epoch total loss 5.63878393\n",
      "Trained batch 8797 batch loss 5.73535252 epoch total loss 5.63879442\n",
      "Trained batch 8798 batch loss 5.71530819 epoch total loss 5.63880348\n",
      "Trained batch 8799 batch loss 6.85609531 epoch total loss 5.63894176\n",
      "Trained batch 8800 batch loss 5.17318058 epoch total loss 5.63888836\n",
      "Trained batch 8801 batch loss 5.44043827 epoch total loss 5.63886595\n",
      "Trained batch 8802 batch loss 5.93260193 epoch total loss 5.63889933\n",
      "Trained batch 8803 batch loss 5.56385851 epoch total loss 5.63889074\n",
      "Trained batch 8804 batch loss 5.45480442 epoch total loss 5.63887\n",
      "Trained batch 8805 batch loss 5.20936441 epoch total loss 5.63882113\n",
      "Trained batch 8806 batch loss 5.86659336 epoch total loss 5.63884687\n",
      "Trained batch 8807 batch loss 5.56539536 epoch total loss 5.63883877\n",
      "Trained batch 8808 batch loss 6.00701284 epoch total loss 5.63888073\n",
      "Trained batch 8809 batch loss 5.68451214 epoch total loss 5.63888597\n",
      "Trained batch 8810 batch loss 4.91698313 epoch total loss 5.63880396\n",
      "Trained batch 8811 batch loss 5.28244638 epoch total loss 5.63876343\n",
      "Trained batch 8812 batch loss 5.16926098 epoch total loss 5.63871\n",
      "Trained batch 8813 batch loss 6.36628151 epoch total loss 5.63879251\n",
      "Trained batch 8814 batch loss 6.337502 epoch total loss 5.63887167\n",
      "Trained batch 8815 batch loss 5.99050903 epoch total loss 5.63891172\n",
      "Trained batch 8816 batch loss 5.72477102 epoch total loss 5.63892174\n",
      "Trained batch 8817 batch loss 6.27677917 epoch total loss 5.63899422\n",
      "Trained batch 8818 batch loss 5.79578 epoch total loss 5.63901186\n",
      "Trained batch 8819 batch loss 5.76509285 epoch total loss 5.63902664\n",
      "Trained batch 8820 batch loss 5.83550167 epoch total loss 5.63904858\n",
      "Trained batch 8821 batch loss 4.83795214 epoch total loss 5.63895798\n",
      "Trained batch 8822 batch loss 5.00393391 epoch total loss 5.63888597\n",
      "Trained batch 8823 batch loss 6.10074139 epoch total loss 5.63893843\n",
      "Trained batch 8824 batch loss 6.53518295 epoch total loss 5.63904\n",
      "Trained batch 8825 batch loss 5.4879055 epoch total loss 5.6390233\n",
      "Trained batch 8826 batch loss 5.89794683 epoch total loss 5.63905239\n",
      "Trained batch 8827 batch loss 5.44030952 epoch total loss 5.63903\n",
      "Trained batch 8828 batch loss 4.94210386 epoch total loss 5.6389513\n",
      "Trained batch 8829 batch loss 5.01255941 epoch total loss 5.63888025\n",
      "Trained batch 8830 batch loss 4.79838848 epoch total loss 5.63878489\n",
      "Trained batch 8831 batch loss 6.08002758 epoch total loss 5.63883448\n",
      "Trained batch 8832 batch loss 6.91845798 epoch total loss 5.63897943\n",
      "Trained batch 8833 batch loss 7.07015753 epoch total loss 5.63914108\n",
      "Trained batch 8834 batch loss 7.04274607 epoch total loss 5.63930035\n",
      "Trained batch 8835 batch loss 6.86184788 epoch total loss 5.63943863\n",
      "Trained batch 8836 batch loss 6.13955164 epoch total loss 5.63949537\n",
      "Trained batch 8837 batch loss 6.68166208 epoch total loss 5.63961363\n",
      "Trained batch 8838 batch loss 6.23515272 epoch total loss 5.63968086\n",
      "Trained batch 8839 batch loss 6.47311926 epoch total loss 5.63977528\n",
      "Trained batch 8840 batch loss 5.94027567 epoch total loss 5.63980913\n",
      "Trained batch 8841 batch loss 7.01054049 epoch total loss 5.63996458\n",
      "Trained batch 8842 batch loss 6.07038879 epoch total loss 5.64001322\n",
      "Trained batch 8843 batch loss 5.90371227 epoch total loss 5.64004278\n",
      "Trained batch 8844 batch loss 6.3039422 epoch total loss 5.64011812\n",
      "Trained batch 8845 batch loss 6.81343508 epoch total loss 5.64025068\n",
      "Trained batch 8846 batch loss 6.35220051 epoch total loss 5.64033079\n",
      "Trained batch 8847 batch loss 6.68771076 epoch total loss 5.64044905\n",
      "Trained batch 8848 batch loss 6.79488707 epoch total loss 5.6405797\n",
      "Trained batch 8849 batch loss 5.71189308 epoch total loss 5.64058733\n",
      "Trained batch 8850 batch loss 5.74203777 epoch total loss 5.64059877\n",
      "Trained batch 8851 batch loss 6.31728745 epoch total loss 5.64067554\n",
      "Trained batch 8852 batch loss 4.77330637 epoch total loss 5.64057732\n",
      "Trained batch 8853 batch loss 5.17988253 epoch total loss 5.64052534\n",
      "Trained batch 8854 batch loss 7.04952431 epoch total loss 5.6406846\n",
      "Trained batch 8855 batch loss 7.02147 epoch total loss 5.64084\n",
      "Trained batch 8856 batch loss 5.01684 epoch total loss 5.64076948\n",
      "Trained batch 8857 batch loss 6.0100193 epoch total loss 5.64081144\n",
      "Trained batch 8858 batch loss 5.52006149 epoch total loss 5.64079809\n",
      "Trained batch 8859 batch loss 5.6800375 epoch total loss 5.64080238\n",
      "Trained batch 8860 batch loss 6.09889603 epoch total loss 5.64085388\n",
      "Trained batch 8861 batch loss 5.53773642 epoch total loss 5.64084244\n",
      "Trained batch 8862 batch loss 5.15053368 epoch total loss 5.64078712\n",
      "Trained batch 8863 batch loss 6.30548477 epoch total loss 5.64086199\n",
      "Trained batch 8864 batch loss 5.01714468 epoch total loss 5.64079142\n",
      "Trained batch 8865 batch loss 5.64189386 epoch total loss 5.64079142\n",
      "Trained batch 8866 batch loss 6.37719965 epoch total loss 5.64087486\n",
      "Trained batch 8867 batch loss 5.20797443 epoch total loss 5.64082575\n",
      "Trained batch 8868 batch loss 5.45119047 epoch total loss 5.64080477\n",
      "Trained batch 8869 batch loss 4.98133469 epoch total loss 5.64073038\n",
      "Trained batch 8870 batch loss 4.98781729 epoch total loss 5.64065647\n",
      "Trained batch 8871 batch loss 5.22369 epoch total loss 5.64060974\n",
      "Trained batch 8872 batch loss 5.24697495 epoch total loss 5.64056492\n",
      "Trained batch 8873 batch loss 5.53604412 epoch total loss 5.64055347\n",
      "Trained batch 8874 batch loss 5.39232206 epoch total loss 5.64052486\n",
      "Trained batch 8875 batch loss 4.94072247 epoch total loss 5.64044619\n",
      "Trained batch 8876 batch loss 4.77968 epoch total loss 5.64034939\n",
      "Trained batch 8877 batch loss 4.92107201 epoch total loss 5.6402688\n",
      "Trained batch 8878 batch loss 4.5033226 epoch total loss 5.64014053\n",
      "Trained batch 8879 batch loss 4.84377813 epoch total loss 5.64005089\n",
      "Trained batch 8880 batch loss 4.78018475 epoch total loss 5.63995409\n",
      "Trained batch 8881 batch loss 5.1364212 epoch total loss 5.63989735\n",
      "Trained batch 8882 batch loss 5.88544559 epoch total loss 5.63992548\n",
      "Trained batch 8883 batch loss 5.36826849 epoch total loss 5.63989449\n",
      "Trained batch 8884 batch loss 5.62302113 epoch total loss 5.63989258\n",
      "Trained batch 8885 batch loss 5.67446661 epoch total loss 5.63989639\n",
      "Trained batch 8886 batch loss 5.07463551 epoch total loss 5.63983297\n",
      "Trained batch 8887 batch loss 5.12850094 epoch total loss 5.63977528\n",
      "Trained batch 8888 batch loss 5.22225666 epoch total loss 5.63972855\n",
      "Trained batch 8889 batch loss 5.75892544 epoch total loss 5.6397419\n",
      "Trained batch 8890 batch loss 5.94650602 epoch total loss 5.63977623\n",
      "Trained batch 8891 batch loss 5.67891312 epoch total loss 5.63978052\n",
      "Trained batch 8892 batch loss 4.95813274 epoch total loss 5.63970375\n",
      "Trained batch 8893 batch loss 5.72654104 epoch total loss 5.63971376\n",
      "Trained batch 8894 batch loss 5.22821617 epoch total loss 5.63966703\n",
      "Trained batch 8895 batch loss 5.61551952 epoch total loss 5.63966465\n",
      "Trained batch 8896 batch loss 5.90761471 epoch total loss 5.63969469\n",
      "Trained batch 8897 batch loss 5.55180502 epoch total loss 5.63968468\n",
      "Trained batch 8898 batch loss 5.42714214 epoch total loss 5.63966036\n",
      "Trained batch 8899 batch loss 5.39425659 epoch total loss 5.63963318\n",
      "Trained batch 8900 batch loss 5.36756611 epoch total loss 5.63960218\n",
      "Trained batch 8901 batch loss 5.89256668 epoch total loss 5.63963032\n",
      "Trained batch 8902 batch loss 6.47936773 epoch total loss 5.63972521\n",
      "Trained batch 8903 batch loss 4.96184635 epoch total loss 5.63964891\n",
      "Trained batch 8904 batch loss 5.31733513 epoch total loss 5.63961267\n",
      "Trained batch 8905 batch loss 5.58912802 epoch total loss 5.63960695\n",
      "Trained batch 8906 batch loss 5.50103569 epoch total loss 5.63959122\n",
      "Trained batch 8907 batch loss 5.33456802 epoch total loss 5.63955688\n",
      "Trained batch 8908 batch loss 5.51721907 epoch total loss 5.63954306\n",
      "Trained batch 8909 batch loss 5.4314909 epoch total loss 5.63951969\n",
      "Trained batch 8910 batch loss 5.47636175 epoch total loss 5.63950157\n",
      "Trained batch 8911 batch loss 5.47908211 epoch total loss 5.63948345\n",
      "Trained batch 8912 batch loss 5.10270405 epoch total loss 5.63942337\n",
      "Trained batch 8913 batch loss 6.95217276 epoch total loss 5.63957071\n",
      "Trained batch 8914 batch loss 4.83876705 epoch total loss 5.63948059\n",
      "Trained batch 8915 batch loss 4.40297222 epoch total loss 5.63934183\n",
      "Trained batch 8916 batch loss 5.17885351 epoch total loss 5.63929033\n",
      "Trained batch 8917 batch loss 5.37783623 epoch total loss 5.63926125\n",
      "Trained batch 8918 batch loss 5.33847618 epoch total loss 5.63922787\n",
      "Trained batch 8919 batch loss 3.75287437 epoch total loss 5.63901615\n",
      "Trained batch 8920 batch loss 4.19010878 epoch total loss 5.63885403\n",
      "Trained batch 8921 batch loss 5.26497936 epoch total loss 5.63881207\n",
      "Trained batch 8922 batch loss 5.19663048 epoch total loss 5.63876247\n",
      "Trained batch 8923 batch loss 5.63896275 epoch total loss 5.63876295\n",
      "Trained batch 8924 batch loss 5.65155268 epoch total loss 5.63876438\n",
      "Trained batch 8925 batch loss 5.2380228 epoch total loss 5.63871956\n",
      "Trained batch 8926 batch loss 5.86974335 epoch total loss 5.63874531\n",
      "Trained batch 8927 batch loss 5.36981 epoch total loss 5.63871527\n",
      "Trained batch 8928 batch loss 5.87643385 epoch total loss 5.63874197\n",
      "Trained batch 8929 batch loss 5.62090683 epoch total loss 5.63874\n",
      "Trained batch 8930 batch loss 5.68744421 epoch total loss 5.63874531\n",
      "Trained batch 8931 batch loss 5.65814686 epoch total loss 5.63874722\n",
      "Trained batch 8932 batch loss 6.33749723 epoch total loss 5.63882542\n",
      "Trained batch 8933 batch loss 6.06674862 epoch total loss 5.6388731\n",
      "Trained batch 8934 batch loss 6.28463554 epoch total loss 5.63894558\n",
      "Trained batch 8935 batch loss 6.36543369 epoch total loss 5.63902712\n",
      "Trained batch 8936 batch loss 5.21863842 epoch total loss 5.63898\n",
      "Trained batch 8937 batch loss 5.89189816 epoch total loss 5.63900805\n",
      "Trained batch 8938 batch loss 4.85002518 epoch total loss 5.63892031\n",
      "Trained batch 8939 batch loss 5.76438856 epoch total loss 5.63893414\n",
      "Trained batch 8940 batch loss 6.12451363 epoch total loss 5.63898849\n",
      "Trained batch 8941 batch loss 4.91789961 epoch total loss 5.63890791\n",
      "Trained batch 8942 batch loss 5.35182 epoch total loss 5.63887596\n",
      "Trained batch 8943 batch loss 4.25329256 epoch total loss 5.63872099\n",
      "Trained batch 8944 batch loss 6.47885275 epoch total loss 5.63881493\n",
      "Trained batch 8945 batch loss 6.2740078 epoch total loss 5.63888597\n",
      "Trained batch 8946 batch loss 6.2702837 epoch total loss 5.63895655\n",
      "Trained batch 8947 batch loss 5.40578175 epoch total loss 5.6389308\n",
      "Trained batch 8948 batch loss 4.86916161 epoch total loss 5.63884497\n",
      "Trained batch 8949 batch loss 6.58640623 epoch total loss 5.63895035\n",
      "Trained batch 8950 batch loss 5.55878592 epoch total loss 5.63894176\n",
      "Trained batch 8951 batch loss 6.5421 epoch total loss 5.63904238\n",
      "Trained batch 8952 batch loss 5.9101162 epoch total loss 5.6390729\n",
      "Trained batch 8953 batch loss 6.04600716 epoch total loss 5.63911867\n",
      "Trained batch 8954 batch loss 6.5077858 epoch total loss 5.63921547\n",
      "Trained batch 8955 batch loss 5.13442802 epoch total loss 5.63915873\n",
      "Trained batch 8956 batch loss 3.65416527 epoch total loss 5.638937\n",
      "Trained batch 8957 batch loss 3.95726681 epoch total loss 5.63874912\n",
      "Trained batch 8958 batch loss 3.57398701 epoch total loss 5.63851881\n",
      "Trained batch 8959 batch loss 5.11361456 epoch total loss 5.63846\n",
      "Trained batch 8960 batch loss 4.71009541 epoch total loss 5.63835669\n",
      "Trained batch 8961 batch loss 4.7702961 epoch total loss 5.63826\n",
      "Trained batch 8962 batch loss 4.63484669 epoch total loss 5.63814783\n",
      "Trained batch 8963 batch loss 5.40315437 epoch total loss 5.6381216\n",
      "Trained batch 8964 batch loss 5.52348852 epoch total loss 5.63810873\n",
      "Trained batch 8965 batch loss 5.58730316 epoch total loss 5.63810301\n",
      "Trained batch 8966 batch loss 5.61388206 epoch total loss 5.6381\n",
      "Trained batch 8967 batch loss 5.58158445 epoch total loss 5.63809395\n",
      "Trained batch 8968 batch loss 5.4595027 epoch total loss 5.6380744\n",
      "Trained batch 8969 batch loss 5.82263 epoch total loss 5.6380949\n",
      "Trained batch 8970 batch loss 5.24989796 epoch total loss 5.63805199\n",
      "Trained batch 8971 batch loss 5.69783497 epoch total loss 5.63805866\n",
      "Trained batch 8972 batch loss 5.38664341 epoch total loss 5.63803053\n",
      "Trained batch 8973 batch loss 5.62189865 epoch total loss 5.63802862\n",
      "Trained batch 8974 batch loss 5.51458359 epoch total loss 5.63801479\n",
      "Trained batch 8975 batch loss 5.23032475 epoch total loss 5.63796949\n",
      "Trained batch 8976 batch loss 5.68041706 epoch total loss 5.63797426\n",
      "Trained batch 8977 batch loss 5.66172314 epoch total loss 5.63797665\n",
      "Trained batch 8978 batch loss 5.33943272 epoch total loss 5.63794374\n",
      "Trained batch 8979 batch loss 5.74113 epoch total loss 5.63795519\n",
      "Trained batch 8980 batch loss 5.59955072 epoch total loss 5.63795042\n",
      "Trained batch 8981 batch loss 6.09303379 epoch total loss 5.63800144\n",
      "Trained batch 8982 batch loss 6.42394686 epoch total loss 5.63808918\n",
      "Trained batch 8983 batch loss 6.56209564 epoch total loss 5.63819218\n",
      "Trained batch 8984 batch loss 5.66357422 epoch total loss 5.63819504\n",
      "Trained batch 8985 batch loss 5.68901491 epoch total loss 5.63820028\n",
      "Trained batch 8986 batch loss 5.59631109 epoch total loss 5.63819599\n",
      "Trained batch 8987 batch loss 6.24611855 epoch total loss 5.6382637\n",
      "Trained batch 8988 batch loss 6.10380316 epoch total loss 5.63831568\n",
      "Trained batch 8989 batch loss 6.60417271 epoch total loss 5.63842297\n",
      "Trained batch 8990 batch loss 5.35076904 epoch total loss 5.63839102\n",
      "Trained batch 8991 batch loss 6.04060268 epoch total loss 5.63843584\n",
      "Trained batch 8992 batch loss 6.34957314 epoch total loss 5.63851452\n",
      "Trained batch 8993 batch loss 6.11369658 epoch total loss 5.63856745\n",
      "Trained batch 8994 batch loss 6.38279963 epoch total loss 5.63865\n",
      "Trained batch 8995 batch loss 6.62987137 epoch total loss 5.63876\n",
      "Trained batch 8996 batch loss 6.37645721 epoch total loss 5.63884211\n",
      "Trained batch 8997 batch loss 5.61382341 epoch total loss 5.63883924\n",
      "Trained batch 8998 batch loss 5.36884594 epoch total loss 5.6388092\n",
      "Trained batch 8999 batch loss 5.41642284 epoch total loss 5.63878441\n",
      "Trained batch 9000 batch loss 5.75783634 epoch total loss 5.63879776\n",
      "Trained batch 9001 batch loss 5.29641819 epoch total loss 5.63875961\n",
      "Trained batch 9002 batch loss 5.86652756 epoch total loss 5.63878536\n",
      "Trained batch 9003 batch loss 5.84161854 epoch total loss 5.6388073\n",
      "Trained batch 9004 batch loss 5.62162304 epoch total loss 5.63880539\n",
      "Trained batch 9005 batch loss 5.72816944 epoch total loss 5.6388154\n",
      "Trained batch 9006 batch loss 6.17257166 epoch total loss 5.63887453\n",
      "Trained batch 9007 batch loss 6.0850029 epoch total loss 5.63892412\n",
      "Trained batch 9008 batch loss 6.14815092 epoch total loss 5.63898087\n",
      "Trained batch 9009 batch loss 5.58897734 epoch total loss 5.63897514\n",
      "Trained batch 9010 batch loss 5.14660168 epoch total loss 5.63892078\n",
      "Trained batch 9011 batch loss 5.33396912 epoch total loss 5.63888645\n",
      "Trained batch 9012 batch loss 5.20878315 epoch total loss 5.63883877\n",
      "Trained batch 9013 batch loss 4.63548946 epoch total loss 5.63872766\n",
      "Trained batch 9014 batch loss 5.2882843 epoch total loss 5.63868856\n",
      "Trained batch 9015 batch loss 4.8645525 epoch total loss 5.63860273\n",
      "Trained batch 9016 batch loss 5.18474483 epoch total loss 5.63855219\n",
      "Trained batch 9017 batch loss 5.39035511 epoch total loss 5.63852501\n",
      "Trained batch 9018 batch loss 5.7324 epoch total loss 5.63853502\n",
      "Trained batch 9019 batch loss 4.0942831 epoch total loss 5.63836384\n",
      "Trained batch 9020 batch loss 4.7037611 epoch total loss 5.63826\n",
      "Trained batch 9021 batch loss 5.03427219 epoch total loss 5.63819313\n",
      "Trained batch 9022 batch loss 4.87992811 epoch total loss 5.63810921\n",
      "Trained batch 9023 batch loss 5.42379 epoch total loss 5.63808489\n",
      "Trained batch 9024 batch loss 5.19501 epoch total loss 5.63803577\n",
      "Trained batch 9025 batch loss 5.3970685 epoch total loss 5.63800955\n",
      "Trained batch 9026 batch loss 5.97059631 epoch total loss 5.63804626\n",
      "Trained batch 9027 batch loss 5.94267654 epoch total loss 5.63807964\n",
      "Trained batch 9028 batch loss 5.60034466 epoch total loss 5.63807583\n",
      "Trained batch 9029 batch loss 6.21385574 epoch total loss 5.63813972\n",
      "Trained batch 9030 batch loss 5.77252722 epoch total loss 5.63815451\n",
      "Trained batch 9031 batch loss 4.68982792 epoch total loss 5.6380496\n",
      "Trained batch 9032 batch loss 5.19575596 epoch total loss 5.63800049\n",
      "Trained batch 9033 batch loss 5.85423803 epoch total loss 5.63802481\n",
      "Trained batch 9034 batch loss 5.76700258 epoch total loss 5.63803864\n",
      "Trained batch 9035 batch loss 5.40299511 epoch total loss 5.63801289\n",
      "Trained batch 9036 batch loss 4.46839428 epoch total loss 5.63788319\n",
      "Trained batch 9037 batch loss 5.76474476 epoch total loss 5.63789749\n",
      "Trained batch 9038 batch loss 5.7773962 epoch total loss 5.63791275\n",
      "Trained batch 9039 batch loss 5.49527073 epoch total loss 5.63789701\n",
      "Trained batch 9040 batch loss 4.93008232 epoch total loss 5.63781881\n",
      "Trained batch 9041 batch loss 5.08142185 epoch total loss 5.6377573\n",
      "Trained batch 9042 batch loss 5.70087862 epoch total loss 5.63776445\n",
      "Trained batch 9043 batch loss 5.40127 epoch total loss 5.63773823\n",
      "Trained batch 9044 batch loss 5.41972208 epoch total loss 5.63771391\n",
      "Trained batch 9045 batch loss 5.64214134 epoch total loss 5.63771439\n",
      "Trained batch 9046 batch loss 5.35095692 epoch total loss 5.63768244\n",
      "Trained batch 9047 batch loss 6.01711941 epoch total loss 5.6377244\n",
      "Trained batch 9048 batch loss 4.51954937 epoch total loss 5.6376009\n",
      "Trained batch 9049 batch loss 5.62982178 epoch total loss 5.6376\n",
      "Trained batch 9050 batch loss 5.89696503 epoch total loss 5.63762856\n",
      "Trained batch 9051 batch loss 4.77330971 epoch total loss 5.63753319\n",
      "Trained batch 9052 batch loss 5.86301851 epoch total loss 5.63755798\n",
      "Trained batch 9053 batch loss 5.04443455 epoch total loss 5.63749218\n",
      "Trained batch 9054 batch loss 5.77426529 epoch total loss 5.63750744\n",
      "Trained batch 9055 batch loss 5.42466879 epoch total loss 5.63748407\n",
      "Trained batch 9056 batch loss 5.54546547 epoch total loss 5.63747406\n",
      "Trained batch 9057 batch loss 5.2854929 epoch total loss 5.63743496\n",
      "Trained batch 9058 batch loss 5.13419151 epoch total loss 5.63737965\n",
      "Trained batch 9059 batch loss 5.61513329 epoch total loss 5.63737679\n",
      "Trained batch 9060 batch loss 5.93914223 epoch total loss 5.63740969\n",
      "Trained batch 9061 batch loss 4.97135401 epoch total loss 5.63733673\n",
      "Trained batch 9062 batch loss 5.89914465 epoch total loss 5.63736534\n",
      "Trained batch 9063 batch loss 5.78145552 epoch total loss 5.63738108\n",
      "Trained batch 9064 batch loss 6.41536093 epoch total loss 5.63746691\n",
      "Trained batch 9065 batch loss 6.19812489 epoch total loss 5.6375289\n",
      "Trained batch 9066 batch loss 5.45698643 epoch total loss 5.63750887\n",
      "Trained batch 9067 batch loss 4.83001041 epoch total loss 5.6374197\n",
      "Trained batch 9068 batch loss 4.82786083 epoch total loss 5.63733053\n",
      "Trained batch 9069 batch loss 5.27023411 epoch total loss 5.63729\n",
      "Trained batch 9070 batch loss 5.0293293 epoch total loss 5.63722324\n",
      "Trained batch 9071 batch loss 4.45148373 epoch total loss 5.63709259\n",
      "Trained batch 9072 batch loss 5.3749752 epoch total loss 5.6370635\n",
      "Trained batch 9073 batch loss 5.72293472 epoch total loss 5.63707304\n",
      "Trained batch 9074 batch loss 5.36300373 epoch total loss 5.637043\n",
      "Trained batch 9075 batch loss 5.09047127 epoch total loss 5.63698244\n",
      "Trained batch 9076 batch loss 5.70707703 epoch total loss 5.63699055\n",
      "Trained batch 9077 batch loss 5.34257317 epoch total loss 5.63695812\n",
      "Trained batch 9078 batch loss 5.65167665 epoch total loss 5.63695955\n",
      "Trained batch 9079 batch loss 5.64596558 epoch total loss 5.63696051\n",
      "Trained batch 9080 batch loss 5.62852 epoch total loss 5.63695955\n",
      "Trained batch 9081 batch loss 5.78428 epoch total loss 5.63697577\n",
      "Trained batch 9082 batch loss 5.32549286 epoch total loss 5.63694143\n",
      "Trained batch 9083 batch loss 5.30604744 epoch total loss 5.63690519\n",
      "Trained batch 9084 batch loss 5.07719088 epoch total loss 5.63684368\n",
      "Trained batch 9085 batch loss 5.37011337 epoch total loss 5.63681412\n",
      "Trained batch 9086 batch loss 5.27115917 epoch total loss 5.63677359\n",
      "Trained batch 9087 batch loss 6.06296682 epoch total loss 5.63682079\n",
      "Trained batch 9088 batch loss 5.4316 epoch total loss 5.6367979\n",
      "Trained batch 9089 batch loss 5.85770226 epoch total loss 5.63682222\n",
      "Trained batch 9090 batch loss 5.87324476 epoch total loss 5.63684845\n",
      "Trained batch 9091 batch loss 5.66774654 epoch total loss 5.63685179\n",
      "Trained batch 9092 batch loss 6.02035952 epoch total loss 5.63689423\n",
      "Trained batch 9093 batch loss 5.09419727 epoch total loss 5.63683414\n",
      "Trained batch 9094 batch loss 5.18265152 epoch total loss 5.63678455\n",
      "Trained batch 9095 batch loss 5.88259315 epoch total loss 5.63681173\n",
      "Trained batch 9096 batch loss 6.49537277 epoch total loss 5.63690615\n",
      "Trained batch 9097 batch loss 4.73404694 epoch total loss 5.63680696\n",
      "Trained batch 9098 batch loss 4.98343 epoch total loss 5.63673496\n",
      "Trained batch 9099 batch loss 4.97383213 epoch total loss 5.63666201\n",
      "Trained batch 9100 batch loss 5.48491 epoch total loss 5.63664532\n",
      "Trained batch 9101 batch loss 6.15143967 epoch total loss 5.63670206\n",
      "Trained batch 9102 batch loss 5.34175301 epoch total loss 5.63666916\n",
      "Trained batch 9103 batch loss 6.0100975 epoch total loss 5.63671064\n",
      "Trained batch 9104 batch loss 5.33170795 epoch total loss 5.63667727\n",
      "Trained batch 9105 batch loss 5.33014297 epoch total loss 5.63664389\n",
      "Trained batch 9106 batch loss 5.64387 epoch total loss 5.63664436\n",
      "Trained batch 9107 batch loss 5.53745556 epoch total loss 5.63663387\n",
      "Trained batch 9108 batch loss 5.07838058 epoch total loss 5.63657236\n",
      "Trained batch 9109 batch loss 5.29827595 epoch total loss 5.63653517\n",
      "Trained batch 9110 batch loss 5.30909538 epoch total loss 5.6364994\n",
      "Trained batch 9111 batch loss 5.24332142 epoch total loss 5.63645601\n",
      "Trained batch 9112 batch loss 5.95486403 epoch total loss 5.63649082\n",
      "Trained batch 9113 batch loss 5.41555071 epoch total loss 5.6364665\n",
      "Trained batch 9114 batch loss 5.9615159 epoch total loss 5.63650179\n",
      "Trained batch 9115 batch loss 5.72921467 epoch total loss 5.63651228\n",
      "Trained batch 9116 batch loss 5.94116879 epoch total loss 5.63654566\n",
      "Trained batch 9117 batch loss 5.20778131 epoch total loss 5.63649845\n",
      "Trained batch 9118 batch loss 6.01535511 epoch total loss 5.63654\n",
      "Trained batch 9119 batch loss 5.199687 epoch total loss 5.63649225\n",
      "Trained batch 9120 batch loss 5.36106205 epoch total loss 5.63646173\n",
      "Trained batch 9121 batch loss 5.45518589 epoch total loss 5.63644218\n",
      "Trained batch 9122 batch loss 5.31124 epoch total loss 5.63640642\n",
      "Trained batch 9123 batch loss 4.40473795 epoch total loss 5.63627195\n",
      "Trained batch 9124 batch loss 4.44459248 epoch total loss 5.6361413\n",
      "Trained batch 9125 batch loss 3.79681444 epoch total loss 5.6359396\n",
      "Trained batch 9126 batch loss 6.22062683 epoch total loss 5.63600349\n",
      "Trained batch 9127 batch loss 5.87082863 epoch total loss 5.63602924\n",
      "Trained batch 9128 batch loss 6.19528389 epoch total loss 5.63609076\n",
      "Trained batch 9129 batch loss 5.89087582 epoch total loss 5.63611841\n",
      "Trained batch 9130 batch loss 6.31238127 epoch total loss 5.63619232\n",
      "Trained batch 9131 batch loss 6.3148756 epoch total loss 5.63626719\n",
      "Trained batch 9132 batch loss 6.46544266 epoch total loss 5.63635778\n",
      "Trained batch 9133 batch loss 6.86449909 epoch total loss 5.63649225\n",
      "Trained batch 9134 batch loss 6.25668669 epoch total loss 5.63656\n",
      "Trained batch 9135 batch loss 6.85893202 epoch total loss 5.63669395\n",
      "Trained batch 9136 batch loss 6.23819351 epoch total loss 5.63675976\n",
      "Trained batch 9137 batch loss 6.63821793 epoch total loss 5.63686943\n",
      "Trained batch 9138 batch loss 6.61636829 epoch total loss 5.63697672\n",
      "Trained batch 9139 batch loss 6.28057337 epoch total loss 5.63704681\n",
      "Trained batch 9140 batch loss 6.36013699 epoch total loss 5.63712597\n",
      "Trained batch 9141 batch loss 6.12285757 epoch total loss 5.6371789\n",
      "Trained batch 9142 batch loss 6.04178572 epoch total loss 5.63722324\n",
      "Trained batch 9143 batch loss 5.87014818 epoch total loss 5.63724899\n",
      "Trained batch 9144 batch loss 6.22189951 epoch total loss 5.63731289\n",
      "Trained batch 9145 batch loss 5.94033527 epoch total loss 5.63734627\n",
      "Trained batch 9146 batch loss 5.37758732 epoch total loss 5.63731813\n",
      "Trained batch 9147 batch loss 5.16328955 epoch total loss 5.63726616\n",
      "Trained batch 9148 batch loss 4.684268 epoch total loss 5.63716221\n",
      "Trained batch 9149 batch loss 6.36624336 epoch total loss 5.63724184\n",
      "Trained batch 9150 batch loss 4.85277367 epoch total loss 5.63715601\n",
      "Trained batch 9151 batch loss 6.28192329 epoch total loss 5.6372261\n",
      "Trained batch 9152 batch loss 5.82413578 epoch total loss 5.63724661\n",
      "Trained batch 9153 batch loss 6.14088631 epoch total loss 5.63730192\n",
      "Trained batch 9154 batch loss 6.34835052 epoch total loss 5.63737917\n",
      "Trained batch 9155 batch loss 5.82096338 epoch total loss 5.6373992\n",
      "Trained batch 9156 batch loss 5.91915369 epoch total loss 5.63742971\n",
      "Trained batch 9157 batch loss 6.30489492 epoch total loss 5.63750267\n",
      "Trained batch 9158 batch loss 6.14268589 epoch total loss 5.63755798\n",
      "Trained batch 9159 batch loss 6.95005941 epoch total loss 5.63770151\n",
      "Trained batch 9160 batch loss 6.85142517 epoch total loss 5.63783407\n",
      "Trained batch 9161 batch loss 7.15757 epoch total loss 5.63799953\n",
      "Trained batch 9162 batch loss 6.12533379 epoch total loss 5.63805294\n",
      "Trained batch 9163 batch loss 6.33572388 epoch total loss 5.63812876\n",
      "Trained batch 9164 batch loss 6.23754597 epoch total loss 5.63819456\n",
      "Trained batch 9165 batch loss 6.52701807 epoch total loss 5.63829136\n",
      "Trained batch 9166 batch loss 6.55962467 epoch total loss 5.63839197\n",
      "Trained batch 9167 batch loss 5.72332287 epoch total loss 5.63840103\n",
      "Trained batch 9168 batch loss 6.48741722 epoch total loss 5.63849354\n",
      "Trained batch 9169 batch loss 6.75168 epoch total loss 5.63861513\n",
      "Trained batch 9170 batch loss 6.27906275 epoch total loss 5.63868475\n",
      "Trained batch 9171 batch loss 5.45366764 epoch total loss 5.63866425\n",
      "Trained batch 9172 batch loss 5.93882084 epoch total loss 5.63869715\n",
      "Trained batch 9173 batch loss 6.08852291 epoch total loss 5.63874626\n",
      "Trained batch 9174 batch loss 6.09106445 epoch total loss 5.63879538\n",
      "Trained batch 9175 batch loss 6.46937 epoch total loss 5.63888597\n",
      "Trained batch 9176 batch loss 5.65813732 epoch total loss 5.63888788\n",
      "Trained batch 9177 batch loss 6.27415419 epoch total loss 5.63895702\n",
      "Trained batch 9178 batch loss 5.87995625 epoch total loss 5.63898277\n",
      "Trained batch 9179 batch loss 5.79066038 epoch total loss 5.63899946\n",
      "Trained batch 9180 batch loss 6.21768665 epoch total loss 5.6390624\n",
      "Trained batch 9181 batch loss 5.80054522 epoch total loss 5.63908\n",
      "Trained batch 9182 batch loss 6.74471951 epoch total loss 5.63920069\n",
      "Trained batch 9183 batch loss 5.47312737 epoch total loss 5.63918257\n",
      "Trained batch 9184 batch loss 6.36627102 epoch total loss 5.63926172\n",
      "Trained batch 9185 batch loss 6.77947378 epoch total loss 5.63938618\n",
      "Trained batch 9186 batch loss 6.21833944 epoch total loss 5.63944912\n",
      "Trained batch 9187 batch loss 6.07066059 epoch total loss 5.63949633\n",
      "Trained batch 9188 batch loss 4.70116138 epoch total loss 5.63939381\n",
      "Trained batch 9189 batch loss 4.15060282 epoch total loss 5.63923216\n",
      "Trained batch 9190 batch loss 5.60733891 epoch total loss 5.63922834\n",
      "Trained batch 9191 batch loss 5.21346188 epoch total loss 5.63918209\n",
      "Trained batch 9192 batch loss 4.94101524 epoch total loss 5.63910627\n",
      "Trained batch 9193 batch loss 4.67304325 epoch total loss 5.63900089\n",
      "Trained batch 9194 batch loss 4.77573967 epoch total loss 5.63890743\n",
      "Trained batch 9195 batch loss 5.38547659 epoch total loss 5.63888\n",
      "Trained batch 9196 batch loss 5.70294189 epoch total loss 5.63888693\n",
      "Trained batch 9197 batch loss 6.01202774 epoch total loss 5.63892746\n",
      "Trained batch 9198 batch loss 5.98348093 epoch total loss 5.63896513\n",
      "Trained batch 9199 batch loss 5.38509464 epoch total loss 5.63893747\n",
      "Trained batch 9200 batch loss 5.63173199 epoch total loss 5.638937\n",
      "Trained batch 9201 batch loss 5.31571579 epoch total loss 5.63890171\n",
      "Trained batch 9202 batch loss 5.94008541 epoch total loss 5.63893461\n",
      "Trained batch 9203 batch loss 5.90472269 epoch total loss 5.6389637\n",
      "Trained batch 9204 batch loss 6.03985691 epoch total loss 5.63900709\n",
      "Trained batch 9205 batch loss 5.44464874 epoch total loss 5.63898611\n",
      "Trained batch 9206 batch loss 5.84993172 epoch total loss 5.639009\n",
      "Trained batch 9207 batch loss 5.72222519 epoch total loss 5.63901806\n",
      "Trained batch 9208 batch loss 5.91211319 epoch total loss 5.6390481\n",
      "Trained batch 9209 batch loss 6.28073645 epoch total loss 5.63911772\n",
      "Trained batch 9210 batch loss 6.03621483 epoch total loss 5.63916111\n",
      "Trained batch 9211 batch loss 5.87142467 epoch total loss 5.63918591\n",
      "Trained batch 9212 batch loss 6.01319075 epoch total loss 5.63922644\n",
      "Trained batch 9213 batch loss 5.6696372 epoch total loss 5.63923\n",
      "Trained batch 9214 batch loss 5.93135548 epoch total loss 5.63926125\n",
      "Trained batch 9215 batch loss 4.82853842 epoch total loss 5.63917303\n",
      "Trained batch 9216 batch loss 4.46702909 epoch total loss 5.63904619\n",
      "Trained batch 9217 batch loss 4.95012665 epoch total loss 5.63897133\n",
      "Trained batch 9218 batch loss 5.29005384 epoch total loss 5.63893318\n",
      "Trained batch 9219 batch loss 5.82652 epoch total loss 5.63895369\n",
      "Trained batch 9220 batch loss 5.44040775 epoch total loss 5.63893223\n",
      "Trained batch 9221 batch loss 5.57903385 epoch total loss 5.63892603\n",
      "Trained batch 9222 batch loss 5.75513363 epoch total loss 5.63893843\n",
      "Trained batch 9223 batch loss 5.05344915 epoch total loss 5.63887501\n",
      "Trained batch 9224 batch loss 5.39283 epoch total loss 5.6388483\n",
      "Trained batch 9225 batch loss 5.13976574 epoch total loss 5.63879442\n",
      "Trained batch 9226 batch loss 6.08818 epoch total loss 5.63884354\n",
      "Trained batch 9227 batch loss 5.85203838 epoch total loss 5.63886642\n",
      "Trained batch 9228 batch loss 6.6404562 epoch total loss 5.63897514\n",
      "Trained batch 9229 batch loss 5.88911057 epoch total loss 5.63900232\n",
      "Trained batch 9230 batch loss 6.39227486 epoch total loss 5.63908386\n",
      "Trained batch 9231 batch loss 5.49725866 epoch total loss 5.63906813\n",
      "Trained batch 9232 batch loss 5.88744497 epoch total loss 5.63909483\n",
      "Trained batch 9233 batch loss 6.63388348 epoch total loss 5.63920259\n",
      "Trained batch 9234 batch loss 6.29636097 epoch total loss 5.63927364\n",
      "Trained batch 9235 batch loss 6.31122255 epoch total loss 5.6393466\n",
      "Trained batch 9236 batch loss 6.66761 epoch total loss 5.63945818\n",
      "Trained batch 9237 batch loss 6.30170155 epoch total loss 5.63952971\n",
      "Trained batch 9238 batch loss 5.79707146 epoch total loss 5.63954687\n",
      "Trained batch 9239 batch loss 5.43300247 epoch total loss 5.63952446\n",
      "Trained batch 9240 batch loss 5.05042171 epoch total loss 5.63946056\n",
      "Trained batch 9241 batch loss 5.94167376 epoch total loss 5.63949347\n",
      "Trained batch 9242 batch loss 5.34570122 epoch total loss 5.63946152\n",
      "Trained batch 9243 batch loss 5.29403 epoch total loss 5.63942385\n",
      "Trained batch 9244 batch loss 5.02900314 epoch total loss 5.63935757\n",
      "Trained batch 9245 batch loss 6.31243324 epoch total loss 5.63943052\n",
      "Trained batch 9246 batch loss 4.86584091 epoch total loss 5.63934708\n",
      "Trained batch 9247 batch loss 4.70396233 epoch total loss 5.63924599\n",
      "Trained batch 9248 batch loss 5.9128027 epoch total loss 5.63927555\n",
      "Trained batch 9249 batch loss 5.94237041 epoch total loss 5.63930798\n",
      "Trained batch 9250 batch loss 6.26433611 epoch total loss 5.63937569\n",
      "Trained batch 9251 batch loss 5.68971348 epoch total loss 5.63938141\n",
      "Trained batch 9252 batch loss 4.67032623 epoch total loss 5.63927698\n",
      "Trained batch 9253 batch loss 6.26379967 epoch total loss 5.63934469\n",
      "Trained batch 9254 batch loss 5.37050486 epoch total loss 5.63931561\n",
      "Trained batch 9255 batch loss 5.77194166 epoch total loss 5.63933\n",
      "Trained batch 9256 batch loss 5.60667706 epoch total loss 5.63932657\n",
      "Trained batch 9257 batch loss 5.77957249 epoch total loss 5.63934183\n",
      "Trained batch 9258 batch loss 4.73704052 epoch total loss 5.63924456\n",
      "Trained batch 9259 batch loss 6.09990072 epoch total loss 5.63929415\n",
      "Trained batch 9260 batch loss 5.05769777 epoch total loss 5.63923168\n",
      "Trained batch 9261 batch loss 5.85249186 epoch total loss 5.63925457\n",
      "Trained batch 9262 batch loss 5.22280884 epoch total loss 5.63920975\n",
      "Trained batch 9263 batch loss 4.60426521 epoch total loss 5.63909817\n",
      "Trained batch 9264 batch loss 5.87591219 epoch total loss 5.63912344\n",
      "Trained batch 9265 batch loss 4.40283585 epoch total loss 5.63899\n",
      "Trained batch 9266 batch loss 5.06723499 epoch total loss 5.63892841\n",
      "Trained batch 9267 batch loss 5.39203119 epoch total loss 5.63890123\n",
      "Trained batch 9268 batch loss 5.64872456 epoch total loss 5.63890219\n",
      "Trained batch 9269 batch loss 5.28574753 epoch total loss 5.63886404\n",
      "Trained batch 9270 batch loss 5.38300896 epoch total loss 5.63883686\n",
      "Trained batch 9271 batch loss 5.68779 epoch total loss 5.63884211\n",
      "Trained batch 9272 batch loss 6.30326319 epoch total loss 5.63891363\n",
      "Trained batch 9273 batch loss 5.38305807 epoch total loss 5.63888597\n",
      "Trained batch 9274 batch loss 4.25123501 epoch total loss 5.63873625\n",
      "Trained batch 9275 batch loss 4.37070084 epoch total loss 5.6386\n",
      "Trained batch 9276 batch loss 5.29999065 epoch total loss 5.63856316\n",
      "Trained batch 9277 batch loss 5.20292854 epoch total loss 5.63851643\n",
      "Trained batch 9278 batch loss 5.44641733 epoch total loss 5.63849545\n",
      "Trained batch 9279 batch loss 5.77271748 epoch total loss 5.63851\n",
      "Trained batch 9280 batch loss 5.1801033 epoch total loss 5.63846064\n",
      "Trained batch 9281 batch loss 5.56244946 epoch total loss 5.63845253\n",
      "Trained batch 9282 batch loss 5.81699181 epoch total loss 5.6384716\n",
      "Trained batch 9283 batch loss 5.60240889 epoch total loss 5.63846779\n",
      "Trained batch 9284 batch loss 5.56350613 epoch total loss 5.63845921\n",
      "Trained batch 9285 batch loss 5.43886614 epoch total loss 5.63843775\n",
      "Trained batch 9286 batch loss 5.52602196 epoch total loss 5.63842583\n",
      "Trained batch 9287 batch loss 5.68766546 epoch total loss 5.63843107\n",
      "Trained batch 9288 batch loss 5.92107964 epoch total loss 5.63846159\n",
      "Trained batch 9289 batch loss 5.93807316 epoch total loss 5.63849401\n",
      "Trained batch 9290 batch loss 5.08221054 epoch total loss 5.63843393\n",
      "Trained batch 9291 batch loss 5.46367931 epoch total loss 5.63841534\n",
      "Trained batch 9292 batch loss 5.28495026 epoch total loss 5.63837719\n",
      "Trained batch 9293 batch loss 6.04075336 epoch total loss 5.63842\n",
      "Trained batch 9294 batch loss 5.20600128 epoch total loss 5.63837385\n",
      "Trained batch 9295 batch loss 5.66392231 epoch total loss 5.63837671\n",
      "Trained batch 9296 batch loss 5.87304878 epoch total loss 5.63840199\n",
      "Trained batch 9297 batch loss 6.45192289 epoch total loss 5.63848972\n",
      "Trained batch 9298 batch loss 6.04333925 epoch total loss 5.63853312\n",
      "Trained batch 9299 batch loss 6.2229104 epoch total loss 5.63859606\n",
      "Trained batch 9300 batch loss 6.20172358 epoch total loss 5.63865662\n",
      "Trained batch 9301 batch loss 6.17211533 epoch total loss 5.63871431\n",
      "Trained batch 9302 batch loss 6.68309593 epoch total loss 5.63882637\n",
      "Trained batch 9303 batch loss 5.58897972 epoch total loss 5.63882113\n",
      "Trained batch 9304 batch loss 6.27532482 epoch total loss 5.63888931\n",
      "Trained batch 9305 batch loss 6.37887764 epoch total loss 5.63896894\n",
      "Trained batch 9306 batch loss 6.27346516 epoch total loss 5.63903713\n",
      "Trained batch 9307 batch loss 6.44525433 epoch total loss 5.63912392\n",
      "Trained batch 9308 batch loss 7.88514423 epoch total loss 5.6393652\n",
      "Trained batch 9309 batch loss 6.34667683 epoch total loss 5.63944101\n",
      "Trained batch 9310 batch loss 5.86835146 epoch total loss 5.63946581\n",
      "Trained batch 9311 batch loss 6.41083908 epoch total loss 5.6395483\n",
      "Trained batch 9312 batch loss 6.93062687 epoch total loss 5.63968706\n",
      "Trained batch 9313 batch loss 6.03236771 epoch total loss 5.63972902\n",
      "Trained batch 9314 batch loss 6.55388784 epoch total loss 5.63982725\n",
      "Trained batch 9315 batch loss 6.20263863 epoch total loss 5.63988781\n",
      "Trained batch 9316 batch loss 5.82986641 epoch total loss 5.63990784\n",
      "Trained batch 9317 batch loss 6.74849749 epoch total loss 5.64002705\n",
      "Trained batch 9318 batch loss 6.93893385 epoch total loss 5.64016628\n",
      "Trained batch 9319 batch loss 6.48772669 epoch total loss 5.64025736\n",
      "Trained batch 9320 batch loss 6.16647243 epoch total loss 5.6403141\n",
      "Trained batch 9321 batch loss 6.03152561 epoch total loss 5.64035606\n",
      "Trained batch 9322 batch loss 4.50134754 epoch total loss 5.64023352\n",
      "Trained batch 9323 batch loss 6.72056913 epoch total loss 5.64034939\n",
      "Trained batch 9324 batch loss 6.11567 epoch total loss 5.64040041\n",
      "Trained batch 9325 batch loss 6.24141788 epoch total loss 5.64046478\n",
      "Trained batch 9326 batch loss 6.08780193 epoch total loss 5.64051294\n",
      "Trained batch 9327 batch loss 5.68833923 epoch total loss 5.64051771\n",
      "Trained batch 9328 batch loss 6.14681482 epoch total loss 5.64057207\n",
      "Trained batch 9329 batch loss 6.41664696 epoch total loss 5.64065552\n",
      "Trained batch 9330 batch loss 7.34091282 epoch total loss 5.64083767\n",
      "Trained batch 9331 batch loss 6.66793776 epoch total loss 5.64094782\n",
      "Trained batch 9332 batch loss 5.1886096 epoch total loss 5.64089918\n",
      "Trained batch 9333 batch loss 5.14123058 epoch total loss 5.64084578\n",
      "Trained batch 9334 batch loss 4.37683868 epoch total loss 5.64071\n",
      "Trained batch 9335 batch loss 6.08245134 epoch total loss 5.64075708\n",
      "Trained batch 9336 batch loss 4.62171316 epoch total loss 5.64064789\n",
      "Trained batch 9337 batch loss 5.3145504 epoch total loss 5.64061308\n",
      "Trained batch 9338 batch loss 4.69281769 epoch total loss 5.64051151\n",
      "Trained batch 9339 batch loss 5.02749109 epoch total loss 5.64044619\n",
      "Trained batch 9340 batch loss 5.39213228 epoch total loss 5.64041901\n",
      "Trained batch 9341 batch loss 4.76463079 epoch total loss 5.64032555\n",
      "Trained batch 9342 batch loss 4.84396267 epoch total loss 5.64024\n",
      "Trained batch 9343 batch loss 3.96516228 epoch total loss 5.6400609\n",
      "Trained batch 9344 batch loss 4.43577957 epoch total loss 5.63993216\n",
      "Trained batch 9345 batch loss 4.66640472 epoch total loss 5.63982821\n",
      "Trained batch 9346 batch loss 4.24161053 epoch total loss 5.63967896\n",
      "Trained batch 9347 batch loss 5.85464573 epoch total loss 5.63970184\n",
      "Trained batch 9348 batch loss 4.97889614 epoch total loss 5.63963127\n",
      "Trained batch 9349 batch loss 4.28222179 epoch total loss 5.63948584\n",
      "Trained batch 9350 batch loss 4.48544502 epoch total loss 5.63936234\n",
      "Trained batch 9351 batch loss 4.40497923 epoch total loss 5.63923073\n",
      "Trained batch 9352 batch loss 4.33829117 epoch total loss 5.63909149\n",
      "Trained batch 9353 batch loss 4.12523651 epoch total loss 5.63893\n",
      "Trained batch 9354 batch loss 4.49326611 epoch total loss 5.6388073\n",
      "Trained batch 9355 batch loss 4.6443944 epoch total loss 5.63870096\n",
      "Trained batch 9356 batch loss 4.14084291 epoch total loss 5.63854074\n",
      "Trained batch 9357 batch loss 4.1656208 epoch total loss 5.63838339\n",
      "Trained batch 9358 batch loss 3.96484399 epoch total loss 5.63820457\n",
      "Trained batch 9359 batch loss 4.46014261 epoch total loss 5.63807869\n",
      "Trained batch 9360 batch loss 4.39808559 epoch total loss 5.63794613\n",
      "Trained batch 9361 batch loss 4.68027353 epoch total loss 5.63784361\n",
      "Trained batch 9362 batch loss 4.58280945 epoch total loss 5.63773108\n",
      "Trained batch 9363 batch loss 4.34449863 epoch total loss 5.63759279\n",
      "Trained batch 9364 batch loss 4.94544029 epoch total loss 5.63751888\n",
      "Trained batch 9365 batch loss 4.03854 epoch total loss 5.63734818\n",
      "Trained batch 9366 batch loss 4.34691191 epoch total loss 5.63721037\n",
      "Trained batch 9367 batch loss 4.86102867 epoch total loss 5.6371274\n",
      "Trained batch 9368 batch loss 4.87855625 epoch total loss 5.63704634\n",
      "Trained batch 9369 batch loss 5.88745785 epoch total loss 5.63707304\n",
      "Trained batch 9370 batch loss 5.2979188 epoch total loss 5.6370368\n",
      "Trained batch 9371 batch loss 5.4775 epoch total loss 5.63701963\n",
      "Trained batch 9372 batch loss 6.03393936 epoch total loss 5.63706207\n",
      "Trained batch 9373 batch loss 6.90528393 epoch total loss 5.63719749\n",
      "Trained batch 9374 batch loss 6.19774437 epoch total loss 5.63725758\n",
      "Trained batch 9375 batch loss 6.13037634 epoch total loss 5.63731\n",
      "Trained batch 9376 batch loss 6.10299 epoch total loss 5.63735962\n",
      "Trained batch 9377 batch loss 6.22834635 epoch total loss 5.63742256\n",
      "Trained batch 9378 batch loss 6.65575123 epoch total loss 5.6375308\n",
      "Trained batch 9379 batch loss 5.46385288 epoch total loss 5.63751268\n",
      "Trained batch 9380 batch loss 5.76606846 epoch total loss 5.63752604\n",
      "Trained batch 9381 batch loss 5.26278973 epoch total loss 5.63748598\n",
      "Trained batch 9382 batch loss 5.9600997 epoch total loss 5.63752079\n",
      "Trained batch 9383 batch loss 6.21102524 epoch total loss 5.63758183\n",
      "Trained batch 9384 batch loss 4.91139698 epoch total loss 5.6375041\n",
      "Trained batch 9385 batch loss 5.6998229 epoch total loss 5.63751078\n",
      "Trained batch 9386 batch loss 6.72773075 epoch total loss 5.63762665\n",
      "Trained batch 9387 batch loss 6.3059082 epoch total loss 5.6376977\n",
      "Trained batch 9388 batch loss 5.38518667 epoch total loss 5.63767099\n",
      "Trained batch 9389 batch loss 5.46179628 epoch total loss 5.6376524\n",
      "Trained batch 9390 batch loss 4.99262238 epoch total loss 5.63758373\n",
      "Trained batch 9391 batch loss 5.74451637 epoch total loss 5.63759518\n",
      "Trained batch 9392 batch loss 5.84266043 epoch total loss 5.63761711\n",
      "Trained batch 9393 batch loss 5.02184582 epoch total loss 5.63755178\n",
      "Trained batch 9394 batch loss 4.72222519 epoch total loss 5.63745451\n",
      "Trained batch 9395 batch loss 3.91745329 epoch total loss 5.6372714\n",
      "Trained batch 9396 batch loss 5.15581179 epoch total loss 5.63722\n",
      "Trained batch 9397 batch loss 5.96241856 epoch total loss 5.63725471\n",
      "Trained batch 9398 batch loss 4.76850605 epoch total loss 5.63716221\n",
      "Trained batch 9399 batch loss 4.5767107 epoch total loss 5.63704967\n",
      "Trained batch 9400 batch loss 4.68093872 epoch total loss 5.63694763\n",
      "Trained batch 9401 batch loss 4.97913074 epoch total loss 5.63687801\n",
      "Trained batch 9402 batch loss 4.98273706 epoch total loss 5.6368084\n",
      "Trained batch 9403 batch loss 5.33701324 epoch total loss 5.63677645\n",
      "Trained batch 9404 batch loss 5.47379208 epoch total loss 5.6367588\n",
      "Trained batch 9405 batch loss 5.46182919 epoch total loss 5.63674\n",
      "Trained batch 9406 batch loss 6.08332253 epoch total loss 5.63678789\n",
      "Trained batch 9407 batch loss 5.65395927 epoch total loss 5.63678932\n",
      "Trained batch 9408 batch loss 5.77199745 epoch total loss 5.6368041\n",
      "Trained batch 9409 batch loss 5.12556648 epoch total loss 5.63674927\n",
      "Trained batch 9410 batch loss 5.79461479 epoch total loss 5.63676596\n",
      "Trained batch 9411 batch loss 5.29106045 epoch total loss 5.63672972\n",
      "Trained batch 9412 batch loss 5.12526417 epoch total loss 5.63667536\n",
      "Trained batch 9413 batch loss 5.15414715 epoch total loss 5.63662386\n",
      "Trained batch 9414 batch loss 5.31560898 epoch total loss 5.63658953\n",
      "Trained batch 9415 batch loss 4.78094292 epoch total loss 5.63649893\n",
      "Trained batch 9416 batch loss 6.05411911 epoch total loss 5.63654327\n",
      "Trained batch 9417 batch loss 5.33079433 epoch total loss 5.63651085\n",
      "Trained batch 9418 batch loss 5.21883249 epoch total loss 5.6364665\n",
      "Trained batch 9419 batch loss 5.84597492 epoch total loss 5.63648891\n",
      "Trained batch 9420 batch loss 5.30596256 epoch total loss 5.63645363\n",
      "Trained batch 9421 batch loss 5.46685028 epoch total loss 5.63643599\n",
      "Trained batch 9422 batch loss 5.72072124 epoch total loss 5.63644505\n",
      "Trained batch 9423 batch loss 6.00987244 epoch total loss 5.6364851\n",
      "Trained batch 9424 batch loss 5.82389259 epoch total loss 5.63650465\n",
      "Trained batch 9425 batch loss 5.82182026 epoch total loss 5.6365242\n",
      "Trained batch 9426 batch loss 4.26708317 epoch total loss 5.63637877\n",
      "Trained batch 9427 batch loss 5.87911367 epoch total loss 5.63640451\n",
      "Trained batch 9428 batch loss 6.14980698 epoch total loss 5.63645887\n",
      "Trained batch 9429 batch loss 4.95872593 epoch total loss 5.63638687\n",
      "Trained batch 9430 batch loss 5.37576342 epoch total loss 5.63635921\n",
      "Trained batch 9431 batch loss 5.16469765 epoch total loss 5.63630915\n",
      "Trained batch 9432 batch loss 4.96970272 epoch total loss 5.63623857\n",
      "Trained batch 9433 batch loss 4.90846157 epoch total loss 5.63616133\n",
      "Trained batch 9434 batch loss 6.05676 epoch total loss 5.63620615\n",
      "Trained batch 9435 batch loss 5.67841339 epoch total loss 5.63621092\n",
      "Trained batch 9436 batch loss 5.89148903 epoch total loss 5.63623762\n",
      "Trained batch 9437 batch loss 5.99654865 epoch total loss 5.63627577\n",
      "Trained batch 9438 batch loss 5.94230747 epoch total loss 5.63630819\n",
      "Trained batch 9439 batch loss 6.2982769 epoch total loss 5.63637829\n",
      "Trained batch 9440 batch loss 6.87449408 epoch total loss 5.63650942\n",
      "Trained batch 9441 batch loss 6.01133299 epoch total loss 5.636549\n",
      "Trained batch 9442 batch loss 6.18825722 epoch total loss 5.63660765\n",
      "Trained batch 9443 batch loss 5.72374773 epoch total loss 5.63661671\n",
      "Trained batch 9444 batch loss 5.70426178 epoch total loss 5.63662338\n",
      "Trained batch 9445 batch loss 6.13418961 epoch total loss 5.63667631\n",
      "Trained batch 9446 batch loss 5.55280876 epoch total loss 5.63666725\n",
      "Trained batch 9447 batch loss 2.698493 epoch total loss 5.63635635\n",
      "Trained batch 9448 batch loss 4.83683205 epoch total loss 5.63627195\n",
      "Trained batch 9449 batch loss 5.67524815 epoch total loss 5.63627577\n",
      "Trained batch 9450 batch loss 4.7880311 epoch total loss 5.63618612\n",
      "Trained batch 9451 batch loss 5.98105431 epoch total loss 5.63622284\n",
      "Trained batch 9452 batch loss 5.38222313 epoch total loss 5.63619614\n",
      "Trained batch 9453 batch loss 5.43585253 epoch total loss 5.63617516\n",
      "Trained batch 9454 batch loss 5.18483591 epoch total loss 5.636127\n",
      "Trained batch 9455 batch loss 6.43547249 epoch total loss 5.6362114\n",
      "Trained batch 9456 batch loss 5.76777 epoch total loss 5.6362257\n",
      "Trained batch 9457 batch loss 5.94114304 epoch total loss 5.63625765\n",
      "Trained batch 9458 batch loss 5.80244541 epoch total loss 5.63627529\n",
      "Trained batch 9459 batch loss 5.66611481 epoch total loss 5.63627863\n",
      "Trained batch 9460 batch loss 6.04256058 epoch total loss 5.63632154\n",
      "Trained batch 9461 batch loss 6.02955198 epoch total loss 5.63636303\n",
      "Trained batch 9462 batch loss 6.35362339 epoch total loss 5.63643932\n",
      "Trained batch 9463 batch loss 4.94140863 epoch total loss 5.63636589\n",
      "Trained batch 9464 batch loss 6.18559361 epoch total loss 5.63642406\n",
      "Trained batch 9465 batch loss 6.18589735 epoch total loss 5.63648224\n",
      "Trained batch 9466 batch loss 5.97031403 epoch total loss 5.63651752\n",
      "Trained batch 9467 batch loss 5.36338615 epoch total loss 5.63648844\n",
      "Trained batch 9468 batch loss 5.52450609 epoch total loss 5.63647652\n",
      "Trained batch 9469 batch loss 5.5869751 epoch total loss 5.63647127\n",
      "Trained batch 9470 batch loss 5.37639046 epoch total loss 5.63644361\n",
      "Trained batch 9471 batch loss 5.56001806 epoch total loss 5.63643551\n",
      "Trained batch 9472 batch loss 6.31451035 epoch total loss 5.63650703\n",
      "Trained batch 9473 batch loss 5.95847559 epoch total loss 5.63654089\n",
      "Trained batch 9474 batch loss 5.88766098 epoch total loss 5.63656759\n",
      "Trained batch 9475 batch loss 5.93769 epoch total loss 5.63659906\n",
      "Trained batch 9476 batch loss 6.24067307 epoch total loss 5.63666296\n",
      "Trained batch 9477 batch loss 5.89528179 epoch total loss 5.63669\n",
      "Trained batch 9478 batch loss 6.86601353 epoch total loss 5.63682032\n",
      "Trained batch 9479 batch loss 6.3667984 epoch total loss 5.63689709\n",
      "Trained batch 9480 batch loss 6.18377399 epoch total loss 5.63695478\n",
      "Trained batch 9481 batch loss 6.17278385 epoch total loss 5.63701105\n",
      "Trained batch 9482 batch loss 6.76264906 epoch total loss 5.63713\n",
      "Trained batch 9483 batch loss 7.07616901 epoch total loss 5.63728142\n",
      "Trained batch 9484 batch loss 6.88145351 epoch total loss 5.63741255\n",
      "Trained batch 9485 batch loss 6.59699 epoch total loss 5.63751411\n",
      "Trained batch 9486 batch loss 5.77500248 epoch total loss 5.63752842\n",
      "Trained batch 9487 batch loss 6.61269236 epoch total loss 5.63763142\n",
      "Trained batch 9488 batch loss 4.45536232 epoch total loss 5.63750696\n",
      "Trained batch 9489 batch loss 6.04973698 epoch total loss 5.63755035\n",
      "Trained batch 9490 batch loss 6.52468681 epoch total loss 5.63764381\n",
      "Trained batch 9491 batch loss 6.59894753 epoch total loss 5.6377449\n",
      "Trained batch 9492 batch loss 6.44338942 epoch total loss 5.63783\n",
      "Trained batch 9493 batch loss 6.14486885 epoch total loss 5.63788319\n",
      "Trained batch 9494 batch loss 6.09152508 epoch total loss 5.63793087\n",
      "Trained batch 9495 batch loss 6.72697 epoch total loss 5.63804531\n",
      "Trained batch 9496 batch loss 5.48432684 epoch total loss 5.6380291\n",
      "Trained batch 9497 batch loss 5.59305668 epoch total loss 5.63802481\n",
      "Trained batch 9498 batch loss 5.80153465 epoch total loss 5.63804197\n",
      "Trained batch 9499 batch loss 6.31598854 epoch total loss 5.63811302\n",
      "Trained batch 9500 batch loss 4.55725527 epoch total loss 5.63799953\n",
      "Trained batch 9501 batch loss 5.00338888 epoch total loss 5.63793278\n",
      "Trained batch 9502 batch loss 4.61214542 epoch total loss 5.63782501\n",
      "Trained batch 9503 batch loss 4.33146191 epoch total loss 5.63768768\n",
      "Trained batch 9504 batch loss 5.52278042 epoch total loss 5.63767576\n",
      "Trained batch 9505 batch loss 4.54240131 epoch total loss 5.63756037\n",
      "Trained batch 9506 batch loss 4.17960835 epoch total loss 5.63740683\n",
      "Trained batch 9507 batch loss 5.57688904 epoch total loss 5.63740063\n",
      "Trained batch 9508 batch loss 4.68037462 epoch total loss 5.6373\n",
      "Trained batch 9509 batch loss 4.77296543 epoch total loss 5.63720942\n",
      "Trained batch 9510 batch loss 4.2952323 epoch total loss 5.63706827\n",
      "Trained batch 9511 batch loss 4.44102287 epoch total loss 5.63694239\n",
      "Trained batch 9512 batch loss 3.94272137 epoch total loss 5.63676453\n",
      "Trained batch 9513 batch loss 4.13975763 epoch total loss 5.63660717\n",
      "Trained batch 9514 batch loss 4.46289062 epoch total loss 5.63648367\n",
      "Trained batch 9515 batch loss 5.26303053 epoch total loss 5.63644457\n",
      "Trained batch 9516 batch loss 6.84595776 epoch total loss 5.63657188\n",
      "Trained batch 9517 batch loss 6.66546 epoch total loss 5.63667965\n",
      "Trained batch 9518 batch loss 5.6260004 epoch total loss 5.6366787\n",
      "Trained batch 9519 batch loss 6.35827827 epoch total loss 5.63675451\n",
      "Trained batch 9520 batch loss 6.61302185 epoch total loss 5.63685703\n",
      "Trained batch 9521 batch loss 5.78009224 epoch total loss 5.63687229\n",
      "Trained batch 9522 batch loss 6.00678444 epoch total loss 5.63691092\n",
      "Trained batch 9523 batch loss 5.91159725 epoch total loss 5.63694\n",
      "Trained batch 9524 batch loss 6.11516476 epoch total loss 5.63699\n",
      "Trained batch 9525 batch loss 6.1987052 epoch total loss 5.63704872\n",
      "Trained batch 9526 batch loss 6.17148685 epoch total loss 5.63710499\n",
      "Trained batch 9527 batch loss 6.3223877 epoch total loss 5.63717699\n",
      "Trained batch 9528 batch loss 6.58621359 epoch total loss 5.63727665\n",
      "Trained batch 9529 batch loss 6.74155092 epoch total loss 5.63739252\n",
      "Trained batch 9530 batch loss 6.38772297 epoch total loss 5.6374712\n",
      "Trained batch 9531 batch loss 5.87319851 epoch total loss 5.63749599\n",
      "Trained batch 9532 batch loss 5.52863312 epoch total loss 5.63748455\n",
      "Trained batch 9533 batch loss 5.8157 epoch total loss 5.63750362\n",
      "Trained batch 9534 batch loss 6.55630398 epoch total loss 5.63759947\n",
      "Trained batch 9535 batch loss 5.27602482 epoch total loss 5.6375618\n",
      "Trained batch 9536 batch loss 6.81083775 epoch total loss 5.6376853\n",
      "Trained batch 9537 batch loss 5.87746668 epoch total loss 5.63771057\n",
      "Trained batch 9538 batch loss 5.80234289 epoch total loss 5.63772726\n",
      "Trained batch 9539 batch loss 5.14449549 epoch total loss 5.63767576\n",
      "Trained batch 9540 batch loss 5.60345459 epoch total loss 5.63767195\n",
      "Trained batch 9541 batch loss 5.79772663 epoch total loss 5.63768864\n",
      "Trained batch 9542 batch loss 5.96490812 epoch total loss 5.63772297\n",
      "Trained batch 9543 batch loss 5.7689867 epoch total loss 5.6377368\n",
      "Trained batch 9544 batch loss 5.68778133 epoch total loss 5.63774204\n",
      "Trained batch 9545 batch loss 5.91619396 epoch total loss 5.63777113\n",
      "Trained batch 9546 batch loss 5.76163483 epoch total loss 5.63778448\n",
      "Trained batch 9547 batch loss 5.66221952 epoch total loss 5.63778687\n",
      "Trained batch 9548 batch loss 5.57584572 epoch total loss 5.63778\n",
      "Trained batch 9549 batch loss 5.31241 epoch total loss 5.63774633\n",
      "Trained batch 9550 batch loss 5.51110506 epoch total loss 5.63773298\n",
      "Trained batch 9551 batch loss 5.68400431 epoch total loss 5.63773775\n",
      "Trained batch 9552 batch loss 5.27382755 epoch total loss 5.6376996\n",
      "Trained batch 9553 batch loss 5.60594463 epoch total loss 5.63769627\n",
      "Trained batch 9554 batch loss 5.55503654 epoch total loss 5.63768768\n",
      "Trained batch 9555 batch loss 5.33584 epoch total loss 5.63765621\n",
      "Trained batch 9556 batch loss 5.62171745 epoch total loss 5.6376543\n",
      "Trained batch 9557 batch loss 6.21589184 epoch total loss 5.63771486\n",
      "Trained batch 9558 batch loss 5.63432455 epoch total loss 5.63771439\n",
      "Trained batch 9559 batch loss 5.25046444 epoch total loss 5.63767385\n",
      "Trained batch 9560 batch loss 5.65428972 epoch total loss 5.63767529\n",
      "Trained batch 9561 batch loss 5.95851803 epoch total loss 5.63770866\n",
      "Trained batch 9562 batch loss 7.54651165 epoch total loss 5.63790846\n",
      "Trained batch 9563 batch loss 6.03289795 epoch total loss 5.63794947\n",
      "Trained batch 9564 batch loss 6.71227646 epoch total loss 5.63806152\n",
      "Trained batch 9565 batch loss 5.00788403 epoch total loss 5.63799572\n",
      "Trained batch 9566 batch loss 6.13779163 epoch total loss 5.6380477\n",
      "Trained batch 9567 batch loss 6.2837038 epoch total loss 5.63811541\n",
      "Trained batch 9568 batch loss 6.33141232 epoch total loss 5.63818789\n",
      "Trained batch 9569 batch loss 6.18058 epoch total loss 5.63824463\n",
      "Trained batch 9570 batch loss 5.3789134 epoch total loss 5.63821745\n",
      "Trained batch 9571 batch loss 6.26025438 epoch total loss 5.63828278\n",
      "Trained batch 9572 batch loss 5.44971561 epoch total loss 5.63826275\n",
      "Trained batch 9573 batch loss 5.30837393 epoch total loss 5.63822842\n",
      "Trained batch 9574 batch loss 5.67775917 epoch total loss 5.63823271\n",
      "Trained batch 9575 batch loss 5.63426733 epoch total loss 5.63823223\n",
      "Trained batch 9576 batch loss 6.61603832 epoch total loss 5.63833475\n",
      "Trained batch 9577 batch loss 5.85878229 epoch total loss 5.63835764\n",
      "Trained batch 9578 batch loss 5.61694336 epoch total loss 5.63835526\n",
      "Trained batch 9579 batch loss 6.76334381 epoch total loss 5.63847256\n",
      "Trained batch 9580 batch loss 5.48127747 epoch total loss 5.63845634\n",
      "Trained batch 9581 batch loss 5.6658926 epoch total loss 5.63845873\n",
      "Trained batch 9582 batch loss 5.01178694 epoch total loss 5.6383934\n",
      "Trained batch 9583 batch loss 5.39579773 epoch total loss 5.63836813\n",
      "Trained batch 9584 batch loss 6.45165062 epoch total loss 5.63845301\n",
      "Trained batch 9585 batch loss 6.43525648 epoch total loss 5.63853598\n",
      "Trained batch 9586 batch loss 5.95065 epoch total loss 5.6385684\n",
      "Trained batch 9587 batch loss 6.28069496 epoch total loss 5.63863564\n",
      "Trained batch 9588 batch loss 6.12781048 epoch total loss 5.63868666\n",
      "Trained batch 9589 batch loss 6.34894466 epoch total loss 5.63876057\n",
      "Trained batch 9590 batch loss 6.50432205 epoch total loss 5.63885069\n",
      "Trained batch 9591 batch loss 5.363307 epoch total loss 5.63882208\n",
      "Trained batch 9592 batch loss 6.39186239 epoch total loss 5.63890028\n",
      "Trained batch 9593 batch loss 5.47432613 epoch total loss 5.63888311\n",
      "Trained batch 9594 batch loss 5.83412647 epoch total loss 5.63890362\n",
      "Trained batch 9595 batch loss 5.55219078 epoch total loss 5.63889456\n",
      "Trained batch 9596 batch loss 5.35749531 epoch total loss 5.63886547\n",
      "Trained batch 9597 batch loss 5.24969053 epoch total loss 5.63882494\n",
      "Trained batch 9598 batch loss 5.22240353 epoch total loss 5.63878155\n",
      "Trained batch 9599 batch loss 5.87692738 epoch total loss 5.63880587\n",
      "Trained batch 9600 batch loss 6.24089289 epoch total loss 5.63886881\n",
      "Trained batch 9601 batch loss 6.42077923 epoch total loss 5.63895035\n",
      "Trained batch 9602 batch loss 6.56229401 epoch total loss 5.63904667\n",
      "Trained batch 9603 batch loss 5.53296661 epoch total loss 5.63903522\n",
      "Trained batch 9604 batch loss 4.78490591 epoch total loss 5.63894653\n",
      "Trained batch 9605 batch loss 4.98766375 epoch total loss 5.63887882\n",
      "Trained batch 9606 batch loss 5.81041241 epoch total loss 5.63889647\n",
      "Trained batch 9607 batch loss 5.51933336 epoch total loss 5.63888407\n",
      "Trained batch 9608 batch loss 4.87714958 epoch total loss 5.63880491\n",
      "Trained batch 9609 batch loss 5.59960365 epoch total loss 5.63880062\n",
      "Trained batch 9610 batch loss 5.86491919 epoch total loss 5.63882399\n",
      "Trained batch 9611 batch loss 5.99531507 epoch total loss 5.63886118\n",
      "Trained batch 9612 batch loss 6.40720415 epoch total loss 5.63894081\n",
      "Trained batch 9613 batch loss 5.11707783 epoch total loss 5.63888645\n",
      "Trained batch 9614 batch loss 4.94643879 epoch total loss 5.63881445\n",
      "Trained batch 9615 batch loss 6.58416653 epoch total loss 5.63891315\n",
      "Trained batch 9616 batch loss 5.42491198 epoch total loss 5.63889074\n",
      "Trained batch 9617 batch loss 5.87553644 epoch total loss 5.63891554\n",
      "Trained batch 9618 batch loss 5.98086548 epoch total loss 5.63895082\n",
      "Trained batch 9619 batch loss 6.24763203 epoch total loss 5.63901424\n",
      "Trained batch 9620 batch loss 5.52400541 epoch total loss 5.63900185\n",
      "Trained batch 9621 batch loss 6.1276226 epoch total loss 5.63905287\n",
      "Trained batch 9622 batch loss 6.11589909 epoch total loss 5.63910246\n",
      "Trained batch 9623 batch loss 5.92684555 epoch total loss 5.6391325\n",
      "Trained batch 9624 batch loss 5.78013325 epoch total loss 5.63914728\n",
      "Trained batch 9625 batch loss 6.22589779 epoch total loss 5.63920832\n",
      "Trained batch 9626 batch loss 6.13975 epoch total loss 5.63926029\n",
      "Trained batch 9627 batch loss 6.24885273 epoch total loss 5.63932371\n",
      "Trained batch 9628 batch loss 5.5271368 epoch total loss 5.63931227\n",
      "Trained batch 9629 batch loss 5.79592323 epoch total loss 5.63932848\n",
      "Trained batch 9630 batch loss 5.88781071 epoch total loss 5.63935423\n",
      "Trained batch 9631 batch loss 5.50967789 epoch total loss 5.6393404\n",
      "Trained batch 9632 batch loss 6.13301182 epoch total loss 5.6393919\n",
      "Trained batch 9633 batch loss 5.45700788 epoch total loss 5.63937283\n",
      "Trained batch 9634 batch loss 4.79904556 epoch total loss 5.63928556\n",
      "Trained batch 9635 batch loss 6.0180335 epoch total loss 5.63932514\n",
      "Trained batch 9636 batch loss 5.75610638 epoch total loss 5.63933754\n",
      "Trained batch 9637 batch loss 5.32267094 epoch total loss 5.63930464\n",
      "Trained batch 9638 batch loss 6.23592138 epoch total loss 5.63936663\n",
      "Trained batch 9639 batch loss 5.99705458 epoch total loss 5.63940334\n",
      "Trained batch 9640 batch loss 6.21309471 epoch total loss 5.63946342\n",
      "Trained batch 9641 batch loss 5.82956505 epoch total loss 5.63948298\n",
      "Trained batch 9642 batch loss 5.78518391 epoch total loss 5.63949776\n",
      "Trained batch 9643 batch loss 6.03751612 epoch total loss 5.63953924\n",
      "Trained batch 9644 batch loss 5.84496307 epoch total loss 5.6395607\n",
      "Trained batch 9645 batch loss 5.40869808 epoch total loss 5.63953686\n",
      "Trained batch 9646 batch loss 5.53784227 epoch total loss 5.63952637\n",
      "Trained batch 9647 batch loss 5.67575121 epoch total loss 5.63953\n",
      "Trained batch 9648 batch loss 5.64678621 epoch total loss 5.63953114\n",
      "Trained batch 9649 batch loss 6.1790657 epoch total loss 5.63958693\n",
      "Trained batch 9650 batch loss 5.97473192 epoch total loss 5.63962173\n",
      "Trained batch 9651 batch loss 5.49290371 epoch total loss 5.63960648\n",
      "Trained batch 9652 batch loss 5.88225746 epoch total loss 5.63963175\n",
      "Trained batch 9653 batch loss 5.57274294 epoch total loss 5.63962507\n",
      "Trained batch 9654 batch loss 5.67585754 epoch total loss 5.63962889\n",
      "Trained batch 9655 batch loss 4.74125051 epoch total loss 5.6395359\n",
      "Trained batch 9656 batch loss 5.78975582 epoch total loss 5.63955116\n",
      "Trained batch 9657 batch loss 5.14848757 epoch total loss 5.63950062\n",
      "Trained batch 9658 batch loss 5.83024883 epoch total loss 5.63952065\n",
      "Trained batch 9659 batch loss 5.48634195 epoch total loss 5.63950491\n",
      "Trained batch 9660 batch loss 6.59162331 epoch total loss 5.63960314\n",
      "Trained batch 9661 batch loss 5.40031242 epoch total loss 5.63957834\n",
      "Trained batch 9662 batch loss 5.88400364 epoch total loss 5.63960314\n",
      "Trained batch 9663 batch loss 5.47502279 epoch total loss 5.63958645\n",
      "Trained batch 9664 batch loss 7.14212322 epoch total loss 5.6397419\n",
      "Trained batch 9665 batch loss 6.30074263 epoch total loss 5.63981\n",
      "Trained batch 9666 batch loss 4.97133923 epoch total loss 5.63974142\n",
      "Trained batch 9667 batch loss 5.73913527 epoch total loss 5.63975143\n",
      "Trained batch 9668 batch loss 5.96228218 epoch total loss 5.63978481\n",
      "Trained batch 9669 batch loss 6.18332529 epoch total loss 5.6398406\n",
      "Trained batch 9670 batch loss 5.83281898 epoch total loss 5.63986063\n",
      "Trained batch 9671 batch loss 7.11414 epoch total loss 5.64001322\n",
      "Trained batch 9672 batch loss 5.7528882 epoch total loss 5.64002466\n",
      "Trained batch 9673 batch loss 5.36725283 epoch total loss 5.63999653\n",
      "Trained batch 9674 batch loss 6.43332624 epoch total loss 5.64007854\n",
      "Trained batch 9675 batch loss 5.19136047 epoch total loss 5.64003229\n",
      "Trained batch 9676 batch loss 5.64377213 epoch total loss 5.64003277\n",
      "Trained batch 9677 batch loss 5.57496262 epoch total loss 5.64002609\n",
      "Trained batch 9678 batch loss 5.89853573 epoch total loss 5.6400528\n",
      "Trained batch 9679 batch loss 6.11166859 epoch total loss 5.64010143\n",
      "Trained batch 9680 batch loss 5.60424519 epoch total loss 5.64009809\n",
      "Trained batch 9681 batch loss 5.27656746 epoch total loss 5.64006042\n",
      "Trained batch 9682 batch loss 5.46523905 epoch total loss 5.6400423\n",
      "Trained batch 9683 batch loss 5.727458 epoch total loss 5.64005136\n",
      "Trained batch 9684 batch loss 5.16612816 epoch total loss 5.64000273\n",
      "Trained batch 9685 batch loss 5.92563534 epoch total loss 5.64003229\n",
      "Trained batch 9686 batch loss 4.75765228 epoch total loss 5.63994122\n",
      "Trained batch 9687 batch loss 5.52147388 epoch total loss 5.63992882\n",
      "Trained batch 9688 batch loss 5.38096142 epoch total loss 5.63990211\n",
      "Trained batch 9689 batch loss 6.02080822 epoch total loss 5.63994122\n",
      "Trained batch 9690 batch loss 6.66318941 epoch total loss 5.64004707\n",
      "Trained batch 9691 batch loss 5.71320677 epoch total loss 5.6400547\n",
      "Trained batch 9692 batch loss 6.36331463 epoch total loss 5.64012909\n",
      "Trained batch 9693 batch loss 4.86888695 epoch total loss 5.64004946\n",
      "Trained batch 9694 batch loss 5.12175179 epoch total loss 5.63999605\n",
      "Trained batch 9695 batch loss 5.24089813 epoch total loss 5.63995504\n",
      "Trained batch 9696 batch loss 5.86914301 epoch total loss 5.63997889\n",
      "Trained batch 9697 batch loss 6.92253923 epoch total loss 5.64011097\n",
      "Trained batch 9698 batch loss 6.82532024 epoch total loss 5.64023304\n",
      "Trained batch 9699 batch loss 5.73768616 epoch total loss 5.64024305\n",
      "Trained batch 9700 batch loss 6.29996586 epoch total loss 5.64031124\n",
      "Trained batch 9701 batch loss 5.63321495 epoch total loss 5.64031029\n",
      "Trained batch 9702 batch loss 4.65944719 epoch total loss 5.64020967\n",
      "Trained batch 9703 batch loss 4.49821091 epoch total loss 5.6400919\n",
      "Trained batch 9704 batch loss 6.77983189 epoch total loss 5.64020967\n",
      "Trained batch 9705 batch loss 6.49960518 epoch total loss 5.64029837\n",
      "Trained batch 9706 batch loss 5.82511234 epoch total loss 5.64031696\n",
      "Trained batch 9707 batch loss 4.76136875 epoch total loss 5.64022684\n",
      "Trained batch 9708 batch loss 6.17105865 epoch total loss 5.6402812\n",
      "Trained batch 9709 batch loss 5.73865891 epoch total loss 5.64029169\n",
      "Trained batch 9710 batch loss 5.51523685 epoch total loss 5.64027882\n",
      "Trained batch 9711 batch loss 5.93788195 epoch total loss 5.64030933\n",
      "Trained batch 9712 batch loss 5.9959 epoch total loss 5.64034605\n",
      "Trained batch 9713 batch loss 4.46570396 epoch total loss 5.64022493\n",
      "Trained batch 9714 batch loss 5.30613708 epoch total loss 5.64019\n",
      "Trained batch 9715 batch loss 5.24574709 epoch total loss 5.64014959\n",
      "Trained batch 9716 batch loss 6.27938223 epoch total loss 5.64021587\n",
      "Trained batch 9717 batch loss 5.96565723 epoch total loss 5.64024925\n",
      "Trained batch 9718 batch loss 5.02772903 epoch total loss 5.64018583\n",
      "Trained batch 9719 batch loss 6.0814867 epoch total loss 5.64023161\n",
      "Trained batch 9720 batch loss 5.29040337 epoch total loss 5.64019537\n",
      "Trained batch 9721 batch loss 5.00011396 epoch total loss 5.64012957\n",
      "Trained batch 9722 batch loss 4.4263258 epoch total loss 5.64000463\n",
      "Trained batch 9723 batch loss 5.34941959 epoch total loss 5.63997459\n",
      "Trained batch 9724 batch loss 4.79340124 epoch total loss 5.63988733\n",
      "Trained batch 9725 batch loss 4.61303902 epoch total loss 5.63978195\n",
      "Trained batch 9726 batch loss 6.10732079 epoch total loss 5.63982964\n",
      "Trained batch 9727 batch loss 5.68097258 epoch total loss 5.63983393\n",
      "Trained batch 9728 batch loss 3.84603596 epoch total loss 5.63965\n",
      "Trained batch 9729 batch loss 5.19235134 epoch total loss 5.63960361\n",
      "Trained batch 9730 batch loss 4.76781082 epoch total loss 5.63951397\n",
      "Trained batch 9731 batch loss 4.34168148 epoch total loss 5.63938046\n",
      "Trained batch 9732 batch loss 4.73001766 epoch total loss 5.63928699\n",
      "Trained batch 9733 batch loss 4.86285543 epoch total loss 5.63920736\n",
      "Trained batch 9734 batch loss 4.5686655 epoch total loss 5.63909769\n",
      "Trained batch 9735 batch loss 5.77949429 epoch total loss 5.63911247\n",
      "Trained batch 9736 batch loss 4.42026806 epoch total loss 5.63898706\n",
      "Trained batch 9737 batch loss 5.55063152 epoch total loss 5.638978\n",
      "Trained batch 9738 batch loss 6.41895771 epoch total loss 5.63905811\n",
      "Trained batch 9739 batch loss 6.39934683 epoch total loss 5.63913631\n",
      "Trained batch 9740 batch loss 5.36189651 epoch total loss 5.6391077\n",
      "Trained batch 9741 batch loss 3.86740637 epoch total loss 5.63892603\n",
      "Trained batch 9742 batch loss 6.03673553 epoch total loss 5.63896656\n",
      "Trained batch 9743 batch loss 5.66987467 epoch total loss 5.63896942\n",
      "Trained batch 9744 batch loss 6.08266068 epoch total loss 5.6390152\n",
      "Trained batch 9745 batch loss 4.40996647 epoch total loss 5.63888884\n",
      "Trained batch 9746 batch loss 5.90235233 epoch total loss 5.63891602\n",
      "Trained batch 9747 batch loss 6.50564623 epoch total loss 5.63900471\n",
      "Trained batch 9748 batch loss 5.74023151 epoch total loss 5.63901472\n",
      "Trained batch 9749 batch loss 5.41601801 epoch total loss 5.63899231\n",
      "Trained batch 9750 batch loss 5.28962421 epoch total loss 5.63895655\n",
      "Trained batch 9751 batch loss 5.84715 epoch total loss 5.63897753\n",
      "Trained batch 9752 batch loss 5.78201056 epoch total loss 5.63899231\n",
      "Trained batch 9753 batch loss 5.43480158 epoch total loss 5.63897133\n",
      "Trained batch 9754 batch loss 5.03208256 epoch total loss 5.63890886\n",
      "Trained batch 9755 batch loss 5.97464371 epoch total loss 5.63894367\n",
      "Trained batch 9756 batch loss 5.3904047 epoch total loss 5.63891792\n",
      "Trained batch 9757 batch loss 5.50700665 epoch total loss 5.63890457\n",
      "Trained batch 9758 batch loss 4.87015343 epoch total loss 5.63882589\n",
      "Trained batch 9759 batch loss 5.06335258 epoch total loss 5.63876677\n",
      "Trained batch 9760 batch loss 4.79895 epoch total loss 5.63868093\n",
      "Trained batch 9761 batch loss 5.53076553 epoch total loss 5.63867\n",
      "Trained batch 9762 batch loss 4.8584938 epoch total loss 5.63859034\n",
      "Trained batch 9763 batch loss 5.00203419 epoch total loss 5.63852501\n",
      "Trained batch 9764 batch loss 6.09375381 epoch total loss 5.63857174\n",
      "Trained batch 9765 batch loss 5.56134796 epoch total loss 5.63856411\n",
      "Trained batch 9766 batch loss 5.86880445 epoch total loss 5.63858747\n",
      "Trained batch 9767 batch loss 6.67431259 epoch total loss 5.63869381\n",
      "Trained batch 9768 batch loss 4.23107338 epoch total loss 5.63854933\n",
      "Trained batch 9769 batch loss 4.84451818 epoch total loss 5.63846827\n",
      "Trained batch 9770 batch loss 5.51596546 epoch total loss 5.63845539\n",
      "Trained batch 9771 batch loss 4.8257184 epoch total loss 5.63837242\n",
      "Trained batch 9772 batch loss 5.87939072 epoch total loss 5.63839674\n",
      "Trained batch 9773 batch loss 6.18095207 epoch total loss 5.63845205\n",
      "Trained batch 9774 batch loss 6.19206715 epoch total loss 5.6385088\n",
      "Trained batch 9775 batch loss 5.24952602 epoch total loss 5.63846922\n",
      "Trained batch 9776 batch loss 5.98699 epoch total loss 5.63850498\n",
      "Trained batch 9777 batch loss 5.93202782 epoch total loss 5.63853502\n",
      "Trained batch 9778 batch loss 5.81517553 epoch total loss 5.63855314\n",
      "Trained batch 9779 batch loss 4.97284603 epoch total loss 5.63848495\n",
      "Trained batch 9780 batch loss 6.19148874 epoch total loss 5.6385417\n",
      "Trained batch 9781 batch loss 5.8906846 epoch total loss 5.63856745\n",
      "Trained batch 9782 batch loss 5.35898161 epoch total loss 5.63853884\n",
      "Trained batch 9783 batch loss 6.6240778 epoch total loss 5.63864\n",
      "Trained batch 9784 batch loss 7.24706078 epoch total loss 5.63880396\n",
      "Trained batch 9785 batch loss 5.46181 epoch total loss 5.63878584\n",
      "Trained batch 9786 batch loss 6.6425581 epoch total loss 5.63888836\n",
      "Trained batch 9787 batch loss 5.70455265 epoch total loss 5.63889456\n",
      "Trained batch 9788 batch loss 5.22254276 epoch total loss 5.63885212\n",
      "Trained batch 9789 batch loss 5.44909763 epoch total loss 5.63883305\n",
      "Trained batch 9790 batch loss 5.96005106 epoch total loss 5.63886595\n",
      "Trained batch 9791 batch loss 5.34197712 epoch total loss 5.63883543\n",
      "Trained batch 9792 batch loss 6.2094717 epoch total loss 5.63889408\n",
      "Trained batch 9793 batch loss 6.23468494 epoch total loss 5.63895512\n",
      "Trained batch 9794 batch loss 5.78251839 epoch total loss 5.63896942\n",
      "Trained batch 9795 batch loss 6.19834089 epoch total loss 5.63902664\n",
      "Trained batch 9796 batch loss 6.07782 epoch total loss 5.63907146\n",
      "Trained batch 9797 batch loss 5.6688652 epoch total loss 5.63907433\n",
      "Trained batch 9798 batch loss 6.33233166 epoch total loss 5.6391449\n",
      "Trained batch 9799 batch loss 4.29352665 epoch total loss 5.63900757\n",
      "Trained batch 9800 batch loss 4.4876194 epoch total loss 5.63889027\n",
      "Trained batch 9801 batch loss 5.1694684 epoch total loss 5.63884211\n",
      "Trained batch 9802 batch loss 5.01628876 epoch total loss 5.63877869\n",
      "Trained batch 9803 batch loss 5.64072609 epoch total loss 5.63877869\n",
      "Trained batch 9804 batch loss 4.93317604 epoch total loss 5.63870716\n",
      "Trained batch 9805 batch loss 4.88634872 epoch total loss 5.63863039\n",
      "Trained batch 9806 batch loss 5.80954599 epoch total loss 5.63864756\n",
      "Trained batch 9807 batch loss 5.75062847 epoch total loss 5.638659\n",
      "Trained batch 9808 batch loss 5.53671646 epoch total loss 5.63864851\n",
      "Trained batch 9809 batch loss 6.06221628 epoch total loss 5.63869143\n",
      "Trained batch 9810 batch loss 5.2753067 epoch total loss 5.63865423\n",
      "Trained batch 9811 batch loss 5.5691824 epoch total loss 5.63864756\n",
      "Trained batch 9812 batch loss 5.73634243 epoch total loss 5.63865757\n",
      "Trained batch 9813 batch loss 5.91061926 epoch total loss 5.63868523\n",
      "Trained batch 9814 batch loss 5.82174206 epoch total loss 5.63870382\n",
      "Trained batch 9815 batch loss 4.368855 epoch total loss 5.63857412\n",
      "Trained batch 9816 batch loss 4.53839397 epoch total loss 5.63846207\n",
      "Trained batch 9817 batch loss 3.87250566 epoch total loss 5.6382823\n",
      "Trained batch 9818 batch loss 5.67135715 epoch total loss 5.63828564\n",
      "Trained batch 9819 batch loss 6.20204735 epoch total loss 5.63834286\n",
      "Trained batch 9820 batch loss 6.46923685 epoch total loss 5.63842773\n",
      "Trained batch 9821 batch loss 6.29126263 epoch total loss 5.63849449\n",
      "Trained batch 9822 batch loss 6.22459364 epoch total loss 5.63855362\n",
      "Trained batch 9823 batch loss 5.3347106 epoch total loss 5.6385231\n",
      "Trained batch 9824 batch loss 5.70996714 epoch total loss 5.63853025\n",
      "Trained batch 9825 batch loss 5.69424391 epoch total loss 5.63853598\n",
      "Trained batch 9826 batch loss 5.27154493 epoch total loss 5.63849878\n",
      "Trained batch 9827 batch loss 5.5702219 epoch total loss 5.63849211\n",
      "Trained batch 9828 batch loss 5.32920265 epoch total loss 5.63846064\n",
      "Trained batch 9829 batch loss 5.27581644 epoch total loss 5.63842392\n",
      "Trained batch 9830 batch loss 5.00897503 epoch total loss 5.63835955\n",
      "Trained batch 9831 batch loss 5.53897 epoch total loss 5.63834953\n",
      "Trained batch 9832 batch loss 5.27487087 epoch total loss 5.63831234\n",
      "Trained batch 9833 batch loss 6.7307806 epoch total loss 5.63842344\n",
      "Trained batch 9834 batch loss 5.96719456 epoch total loss 5.63845682\n",
      "Trained batch 9835 batch loss 5.49431515 epoch total loss 5.63844252\n",
      "Trained batch 9836 batch loss 5.95022202 epoch total loss 5.63847399\n",
      "Trained batch 9837 batch loss 4.71639252 epoch total loss 5.63838\n",
      "Trained batch 9838 batch loss 4.71851778 epoch total loss 5.63828659\n",
      "Trained batch 9839 batch loss 5.24848652 epoch total loss 5.63824749\n",
      "Trained batch 9840 batch loss 5.65607834 epoch total loss 5.63824892\n",
      "Trained batch 9841 batch loss 5.80767918 epoch total loss 5.63826656\n",
      "Trained batch 9842 batch loss 6.12543488 epoch total loss 5.63831568\n",
      "Trained batch 9843 batch loss 4.66156673 epoch total loss 5.6382165\n",
      "Trained batch 9844 batch loss 5.71009159 epoch total loss 5.63822365\n",
      "Trained batch 9845 batch loss 5.36420345 epoch total loss 5.63819599\n",
      "Trained batch 9846 batch loss 4.29552174 epoch total loss 5.63805962\n",
      "Trained batch 9847 batch loss 5.08388948 epoch total loss 5.63800335\n",
      "Trained batch 9848 batch loss 5.50093555 epoch total loss 5.63798904\n",
      "Trained batch 9849 batch loss 5.97622633 epoch total loss 5.63802338\n",
      "Trained batch 9850 batch loss 5.80936146 epoch total loss 5.63804102\n",
      "Trained batch 9851 batch loss 5.74934483 epoch total loss 5.63805246\n",
      "Trained batch 9852 batch loss 5.22807407 epoch total loss 5.6380105\n",
      "Trained batch 9853 batch loss 5.94228077 epoch total loss 5.6380415\n",
      "Trained batch 9854 batch loss 6.06682682 epoch total loss 5.63808489\n",
      "Trained batch 9855 batch loss 5.93959141 epoch total loss 5.63811541\n",
      "Trained batch 9856 batch loss 5.68385029 epoch total loss 5.63812\n",
      "Trained batch 9857 batch loss 5.89876461 epoch total loss 5.6381464\n",
      "Trained batch 9858 batch loss 5.35163403 epoch total loss 5.63811731\n",
      "Trained batch 9859 batch loss 6.64675379 epoch total loss 5.63822\n",
      "Trained batch 9860 batch loss 6.70383549 epoch total loss 5.63832808\n",
      "Trained batch 9861 batch loss 5.38153458 epoch total loss 5.63830185\n",
      "Trained batch 9862 batch loss 5.60616732 epoch total loss 5.63829899\n",
      "Trained batch 9863 batch loss 5.02249432 epoch total loss 5.63823652\n",
      "Trained batch 9864 batch loss 6.00798702 epoch total loss 5.63827372\n",
      "Trained batch 9865 batch loss 5.67704153 epoch total loss 5.63827753\n",
      "Trained batch 9866 batch loss 5.30230045 epoch total loss 5.63824368\n",
      "Trained batch 9867 batch loss 4.70721912 epoch total loss 5.63814926\n",
      "Trained batch 9868 batch loss 6.03853893 epoch total loss 5.63819\n",
      "Trained batch 9869 batch loss 7.04403687 epoch total loss 5.63833189\n",
      "Trained batch 9870 batch loss 7.10167599 epoch total loss 5.63848\n",
      "Trained batch 9871 batch loss 6.44725323 epoch total loss 5.6385622\n",
      "Trained batch 9872 batch loss 5.74552155 epoch total loss 5.63857317\n",
      "Trained batch 9873 batch loss 5.1388073 epoch total loss 5.63852262\n",
      "Trained batch 9874 batch loss 5.82004642 epoch total loss 5.63854074\n",
      "Trained batch 9875 batch loss 5.26408052 epoch total loss 5.63850307\n",
      "Trained batch 9876 batch loss 5.17224026 epoch total loss 5.63845587\n",
      "Trained batch 9877 batch loss 5.66143322 epoch total loss 5.63845825\n",
      "Trained batch 9878 batch loss 5.65370607 epoch total loss 5.63845968\n",
      "Trained batch 9879 batch loss 5.70471 epoch total loss 5.63846588\n",
      "Trained batch 9880 batch loss 5.77781868 epoch total loss 5.63848\n",
      "Trained batch 9881 batch loss 5.65479183 epoch total loss 5.63848209\n",
      "Trained batch 9882 batch loss 5.6542635 epoch total loss 5.63848352\n",
      "Trained batch 9883 batch loss 4.31737137 epoch total loss 5.63834953\n",
      "Trained batch 9884 batch loss 6.08863258 epoch total loss 5.63839531\n",
      "Trained batch 9885 batch loss 6.55759335 epoch total loss 5.63848829\n",
      "Trained batch 9886 batch loss 6.55794811 epoch total loss 5.63858128\n",
      "Trained batch 9887 batch loss 5.49992704 epoch total loss 5.63856745\n",
      "Trained batch 9888 batch loss 6.33565712 epoch total loss 5.63863802\n",
      "Trained batch 9889 batch loss 4.36759281 epoch total loss 5.63850927\n",
      "Trained batch 9890 batch loss 5.19531107 epoch total loss 5.63846445\n",
      "Trained batch 9891 batch loss 6.61505604 epoch total loss 5.63856316\n",
      "Trained batch 9892 batch loss 5.08875704 epoch total loss 5.63850737\n",
      "Trained batch 9893 batch loss 5.50975037 epoch total loss 5.63849449\n",
      "Trained batch 9894 batch loss 5.75171 epoch total loss 5.63850546\n",
      "Trained batch 9895 batch loss 5.60156965 epoch total loss 5.63850212\n",
      "Trained batch 9896 batch loss 5.01279736 epoch total loss 5.6384387\n",
      "Trained batch 9897 batch loss 6.97281 epoch total loss 5.63857317\n",
      "Trained batch 9898 batch loss 6.15159035 epoch total loss 5.63862514\n",
      "Trained batch 9899 batch loss 5.45782804 epoch total loss 5.63860703\n",
      "Trained batch 9900 batch loss 6.55705929 epoch total loss 5.6387\n",
      "Trained batch 9901 batch loss 5.09651 epoch total loss 5.63864517\n",
      "Trained batch 9902 batch loss 6.86293364 epoch total loss 5.63876915\n",
      "Trained batch 9903 batch loss 4.87758684 epoch total loss 5.63869238\n",
      "Trained batch 9904 batch loss 5.45833254 epoch total loss 5.63867378\n",
      "Trained batch 9905 batch loss 5.44895029 epoch total loss 5.63865471\n",
      "Trained batch 9906 batch loss 4.41502 epoch total loss 5.63853121\n",
      "Trained batch 9907 batch loss 4.78518677 epoch total loss 5.6384449\n",
      "Trained batch 9908 batch loss 4.40865421 epoch total loss 5.63832092\n",
      "Trained batch 9909 batch loss 5.01894808 epoch total loss 5.63825846\n",
      "Trained batch 9910 batch loss 5.57163239 epoch total loss 5.63825178\n",
      "Trained batch 9911 batch loss 5.41271973 epoch total loss 5.63822889\n",
      "Trained batch 9912 batch loss 5.28030777 epoch total loss 5.63819313\n",
      "Trained batch 9913 batch loss 5.18251228 epoch total loss 5.63814735\n",
      "Trained batch 9914 batch loss 5.56047 epoch total loss 5.63813925\n",
      "Trained batch 9915 batch loss 5.83478069 epoch total loss 5.63815928\n",
      "Trained batch 9916 batch loss 5.61157465 epoch total loss 5.63815641\n",
      "Trained batch 9917 batch loss 5.92976284 epoch total loss 5.63818598\n",
      "Trained batch 9918 batch loss 5.38846588 epoch total loss 5.63816071\n",
      "Trained batch 9919 batch loss 5.30577612 epoch total loss 5.63812685\n",
      "Trained batch 9920 batch loss 6.01765108 epoch total loss 5.63816547\n",
      "Trained batch 9921 batch loss 5.31415844 epoch total loss 5.63813257\n",
      "Trained batch 9922 batch loss 5.34023952 epoch total loss 5.63810253\n",
      "Trained batch 9923 batch loss 5.5486865 epoch total loss 5.63809347\n",
      "Trained batch 9924 batch loss 5.38130331 epoch total loss 5.63806772\n",
      "Trained batch 9925 batch loss 5.70140553 epoch total loss 5.6380744\n",
      "Trained batch 9926 batch loss 5.302917 epoch total loss 5.63804054\n",
      "Trained batch 9927 batch loss 5.074543 epoch total loss 5.6379838\n",
      "Trained batch 9928 batch loss 5.53918791 epoch total loss 5.63797379\n",
      "Trained batch 9929 batch loss 4.17137051 epoch total loss 5.63782644\n",
      "Trained batch 9930 batch loss 4.51346397 epoch total loss 5.63771296\n",
      "Trained batch 9931 batch loss 4.52946758 epoch total loss 5.63760138\n",
      "Trained batch 9932 batch loss 4.21536732 epoch total loss 5.63745832\n",
      "Trained batch 9933 batch loss 4.31788874 epoch total loss 5.63732529\n",
      "Trained batch 9934 batch loss 4.52788162 epoch total loss 5.63721323\n",
      "Trained batch 9935 batch loss 3.91431618 epoch total loss 5.63704\n",
      "Trained batch 9936 batch loss 3.73025131 epoch total loss 5.63684797\n",
      "Trained batch 9937 batch loss 4.99711514 epoch total loss 5.6367836\n",
      "Trained batch 9938 batch loss 4.74630547 epoch total loss 5.63669395\n",
      "Trained batch 9939 batch loss 4.59039354 epoch total loss 5.63658857\n",
      "Trained batch 9940 batch loss 4.68022251 epoch total loss 5.63649225\n",
      "Trained batch 9941 batch loss 4.27642965 epoch total loss 5.63635588\n",
      "Trained batch 9942 batch loss 4.45241404 epoch total loss 5.63623667\n",
      "Trained batch 9943 batch loss 5.08473063 epoch total loss 5.63618135\n",
      "Trained batch 9944 batch loss 5.49830103 epoch total loss 5.63616753\n",
      "Trained batch 9945 batch loss 6.16909361 epoch total loss 5.63622093\n",
      "Trained batch 9946 batch loss 5.24947 epoch total loss 5.63618231\n",
      "Trained batch 9947 batch loss 5.01535511 epoch total loss 5.63612\n",
      "Trained batch 9948 batch loss 6.00327206 epoch total loss 5.63615704\n",
      "Trained batch 9949 batch loss 5.00440693 epoch total loss 5.63609314\n",
      "Trained batch 9950 batch loss 7.01338482 epoch total loss 5.63623142\n",
      "Trained batch 9951 batch loss 6.08060551 epoch total loss 5.63627625\n",
      "Trained batch 9952 batch loss 4.07710695 epoch total loss 5.63612\n",
      "Trained batch 9953 batch loss 5.70314264 epoch total loss 5.63612652\n",
      "Trained batch 9954 batch loss 5.90271759 epoch total loss 5.63615322\n",
      "Trained batch 9955 batch loss 5.99284363 epoch total loss 5.63618898\n",
      "Trained batch 9956 batch loss 5.54235077 epoch total loss 5.63617945\n",
      "Trained batch 9957 batch loss 3.54203892 epoch total loss 5.63596964\n",
      "Trained batch 9958 batch loss 5.15710735 epoch total loss 5.63592148\n",
      "Trained batch 9959 batch loss 6.33097601 epoch total loss 5.6359911\n",
      "Trained batch 9960 batch loss 6.12533379 epoch total loss 5.63604\n",
      "Trained batch 9961 batch loss 5.79693651 epoch total loss 5.63605642\n",
      "Trained batch 9962 batch loss 5.17560911 epoch total loss 5.63601\n",
      "Trained batch 9963 batch loss 4.20938206 epoch total loss 5.63586712\n",
      "Trained batch 9964 batch loss 6.00645161 epoch total loss 5.63590431\n",
      "Trained batch 9965 batch loss 4.88849545 epoch total loss 5.63582945\n",
      "Trained batch 9966 batch loss 5.75536346 epoch total loss 5.63584137\n",
      "Trained batch 9967 batch loss 6.2027688 epoch total loss 5.63589811\n",
      "Trained batch 9968 batch loss 5.27069092 epoch total loss 5.6358614\n",
      "Trained batch 9969 batch loss 5.9338479 epoch total loss 5.63589096\n",
      "Trained batch 9970 batch loss 6.00028944 epoch total loss 5.63592768\n",
      "Trained batch 9971 batch loss 5.80564785 epoch total loss 5.63594484\n",
      "Trained batch 9972 batch loss 5.07050896 epoch total loss 5.6358881\n",
      "Trained batch 9973 batch loss 5.26570129 epoch total loss 5.63585091\n",
      "Trained batch 9974 batch loss 5.26459694 epoch total loss 5.63581371\n",
      "Trained batch 9975 batch loss 5.43545723 epoch total loss 5.63579321\n",
      "Trained batch 9976 batch loss 5.13519 epoch total loss 5.63574314\n",
      "Trained batch 9977 batch loss 5.80160666 epoch total loss 5.63576\n",
      "Trained batch 9978 batch loss 6.05011463 epoch total loss 5.63580132\n",
      "Trained batch 9979 batch loss 6.46867132 epoch total loss 5.63588476\n",
      "Trained batch 9980 batch loss 5.83823538 epoch total loss 5.63590527\n",
      "Trained batch 9981 batch loss 6.00533152 epoch total loss 5.63594246\n",
      "Trained batch 9982 batch loss 5.03393459 epoch total loss 5.6358819\n",
      "Trained batch 9983 batch loss 6.27125263 epoch total loss 5.6359458\n",
      "Trained batch 9984 batch loss 6.20117378 epoch total loss 5.63600254\n",
      "Trained batch 9985 batch loss 6.13781118 epoch total loss 5.63605261\n",
      "Trained batch 9986 batch loss 4.33127975 epoch total loss 5.63592196\n",
      "Trained batch 9987 batch loss 5.51623 epoch total loss 5.63591\n",
      "Trained batch 9988 batch loss 5.33321953 epoch total loss 5.63587952\n",
      "Trained batch 9989 batch loss 4.80904675 epoch total loss 5.63579655\n",
      "Trained batch 9990 batch loss 5.41241646 epoch total loss 5.63577461\n",
      "Trained batch 9991 batch loss 5.63095856 epoch total loss 5.63577414\n",
      "Trained batch 9992 batch loss 5.63848591 epoch total loss 5.63577414\n",
      "Trained batch 9993 batch loss 5.42034674 epoch total loss 5.63575268\n",
      "Trained batch 9994 batch loss 5.12562418 epoch total loss 5.63570166\n",
      "Trained batch 9995 batch loss 5.68065929 epoch total loss 5.63570595\n",
      "Trained batch 9996 batch loss 5.79999161 epoch total loss 5.63572264\n",
      "Trained batch 9997 batch loss 5.71022367 epoch total loss 5.63573027\n",
      "Trained batch 9998 batch loss 5.35672665 epoch total loss 5.63570213\n",
      "Trained batch 9999 batch loss 6.036798 epoch total loss 5.63574219\n",
      "Trained batch 10000 batch loss 5.52562046 epoch total loss 5.63573122\n",
      "Trained batch 10001 batch loss 4.7370863 epoch total loss 5.63564157\n",
      "Trained batch 10002 batch loss 5.1529932 epoch total loss 5.63559341\n",
      "Trained batch 10003 batch loss 5.36882401 epoch total loss 5.63556623\n",
      "Trained batch 10004 batch loss 5.80421448 epoch total loss 5.6355834\n",
      "Trained batch 10005 batch loss 5.43209553 epoch total loss 5.6355629\n",
      "Trained batch 10006 batch loss 5.57808304 epoch total loss 5.63555717\n",
      "Trained batch 10007 batch loss 5.57515287 epoch total loss 5.63555098\n",
      "Trained batch 10008 batch loss 5.37406063 epoch total loss 5.63552523\n",
      "Trained batch 10009 batch loss 5.73598623 epoch total loss 5.63553524\n",
      "Trained batch 10010 batch loss 5.37412643 epoch total loss 5.63550901\n",
      "Trained batch 10011 batch loss 5.28354359 epoch total loss 5.63547421\n",
      "Trained batch 10012 batch loss 5.84652758 epoch total loss 5.63549519\n",
      "Trained batch 10013 batch loss 5.20200539 epoch total loss 5.63545227\n",
      "Trained batch 10014 batch loss 5.93063879 epoch total loss 5.63548136\n",
      "Trained batch 10015 batch loss 5.23647738 epoch total loss 5.63544178\n",
      "Trained batch 10016 batch loss 5.37041378 epoch total loss 5.63541555\n",
      "Trained batch 10017 batch loss 5.77540684 epoch total loss 5.63542938\n",
      "Trained batch 10018 batch loss 5.70537758 epoch total loss 5.63543653\n",
      "Trained batch 10019 batch loss 7.20721436 epoch total loss 5.63559341\n",
      "Trained batch 10020 batch loss 6.26254 epoch total loss 5.63565588\n",
      "Trained batch 10021 batch loss 5.55985641 epoch total loss 5.63564825\n",
      "Trained batch 10022 batch loss 6.60958672 epoch total loss 5.63574553\n",
      "Trained batch 10023 batch loss 6.24557 epoch total loss 5.63580656\n",
      "Trained batch 10024 batch loss 5.69549417 epoch total loss 5.63581228\n",
      "Trained batch 10025 batch loss 5.34135246 epoch total loss 5.63578272\n",
      "Trained batch 10026 batch loss 5.83639 epoch total loss 5.63580275\n",
      "Trained batch 10027 batch loss 3.38753772 epoch total loss 5.63557863\n",
      "Trained batch 10028 batch loss 3.62590504 epoch total loss 5.63537788\n",
      "Trained batch 10029 batch loss 4.5563488 epoch total loss 5.63527\n",
      "Trained batch 10030 batch loss 3.53430367 epoch total loss 5.63506079\n",
      "Trained batch 10031 batch loss 4.10092545 epoch total loss 5.63490772\n",
      "Trained batch 10032 batch loss 4.87789917 epoch total loss 5.63483238\n",
      "Trained batch 10033 batch loss 3.98070598 epoch total loss 5.63466787\n",
      "Trained batch 10034 batch loss 4.5131793 epoch total loss 5.63455582\n",
      "Trained batch 10035 batch loss 3.59885883 epoch total loss 5.63435268\n",
      "Trained batch 10036 batch loss 3.79996514 epoch total loss 5.63417\n",
      "Trained batch 10037 batch loss 3.48697305 epoch total loss 5.63395643\n",
      "Trained batch 10038 batch loss 4.39396763 epoch total loss 5.63383293\n",
      "Trained batch 10039 batch loss 4.00394726 epoch total loss 5.63367033\n",
      "Trained batch 10040 batch loss 3.51612616 epoch total loss 5.63345957\n",
      "Trained batch 10041 batch loss 3.43620753 epoch total loss 5.6332407\n",
      "Trained batch 10042 batch loss 3.42062449 epoch total loss 5.6330204\n",
      "Trained batch 10043 batch loss 3.62182522 epoch total loss 5.63282\n",
      "Trained batch 10044 batch loss 3.82098627 epoch total loss 5.63264\n",
      "Trained batch 10045 batch loss 4.37694168 epoch total loss 5.63251448\n",
      "Trained batch 10046 batch loss 5.89092255 epoch total loss 5.63254\n",
      "Trained batch 10047 batch loss 5.31934834 epoch total loss 5.63250923\n",
      "Trained batch 10048 batch loss 4.8205905 epoch total loss 5.63242817\n",
      "Trained batch 10049 batch loss 5.90219498 epoch total loss 5.63245535\n",
      "Trained batch 10050 batch loss 6.0148592 epoch total loss 5.6324935\n",
      "Trained batch 10051 batch loss 6.20848751 epoch total loss 5.63255072\n",
      "Trained batch 10052 batch loss 6.71419716 epoch total loss 5.632658\n",
      "Trained batch 10053 batch loss 4.97132874 epoch total loss 5.63259268\n",
      "Trained batch 10054 batch loss 6.16476631 epoch total loss 5.63264561\n",
      "Trained batch 10055 batch loss 6.27676487 epoch total loss 5.6327095\n",
      "Trained batch 10056 batch loss 6.19091368 epoch total loss 5.63276529\n",
      "Trained batch 10057 batch loss 6.28216839 epoch total loss 5.63282967\n",
      "Trained batch 10058 batch loss 6.26202106 epoch total loss 5.63289213\n",
      "Trained batch 10059 batch loss 6.50285721 epoch total loss 5.63297892\n",
      "Trained batch 10060 batch loss 6.42712 epoch total loss 5.63305759\n",
      "Trained batch 10061 batch loss 6.27433062 epoch total loss 5.63312101\n",
      "Trained batch 10062 batch loss 6.66338444 epoch total loss 5.63322353\n",
      "Trained batch 10063 batch loss 6.58752346 epoch total loss 5.63331842\n",
      "Trained batch 10064 batch loss 5.77227688 epoch total loss 5.63333225\n",
      "Trained batch 10065 batch loss 6.57137918 epoch total loss 5.63342524\n",
      "Trained batch 10066 batch loss 6.46462536 epoch total loss 5.63350773\n",
      "Trained batch 10067 batch loss 6.21417761 epoch total loss 5.63356543\n",
      "Trained batch 10068 batch loss 7.02808189 epoch total loss 5.63370419\n",
      "Trained batch 10069 batch loss 6.45227051 epoch total loss 5.63378525\n",
      "Trained batch 10070 batch loss 6.03239107 epoch total loss 5.63382483\n",
      "Trained batch 10071 batch loss 6.27098751 epoch total loss 5.63388824\n",
      "Trained batch 10072 batch loss 5.93482971 epoch total loss 5.63391781\n",
      "Trained batch 10073 batch loss 6.48543358 epoch total loss 5.63400221\n",
      "Trained batch 10074 batch loss 5.94550562 epoch total loss 5.6340332\n",
      "Trained batch 10075 batch loss 6.12630367 epoch total loss 5.63408184\n",
      "Trained batch 10076 batch loss 5.95809937 epoch total loss 5.63411379\n",
      "Trained batch 10077 batch loss 6.17454147 epoch total loss 5.63416767\n",
      "Trained batch 10078 batch loss 6.10303402 epoch total loss 5.63421392\n",
      "Trained batch 10079 batch loss 6.27717638 epoch total loss 5.63427782\n",
      "Trained batch 10080 batch loss 5.81659794 epoch total loss 5.63429594\n",
      "Trained batch 10081 batch loss 5.87395859 epoch total loss 5.63432\n",
      "Trained batch 10082 batch loss 5.98633 epoch total loss 5.63435507\n",
      "Trained batch 10083 batch loss 5.34503412 epoch total loss 5.63432598\n",
      "Trained batch 10084 batch loss 6.00321198 epoch total loss 5.6343627\n",
      "Trained batch 10085 batch loss 5.3653512 epoch total loss 5.63433647\n",
      "Trained batch 10086 batch loss 6.09561157 epoch total loss 5.63438177\n",
      "Trained batch 10087 batch loss 5.53941202 epoch total loss 5.63437223\n",
      "Trained batch 10088 batch loss 5.39764833 epoch total loss 5.63434887\n",
      "Trained batch 10089 batch loss 6.08191442 epoch total loss 5.63439322\n",
      "Trained batch 10090 batch loss 5.04005432 epoch total loss 5.63433456\n",
      "Trained batch 10091 batch loss 5.85439634 epoch total loss 5.6343565\n",
      "Trained batch 10092 batch loss 5.82818222 epoch total loss 5.63437557\n",
      "Trained batch 10093 batch loss 5.76171684 epoch total loss 5.63438797\n",
      "Trained batch 10094 batch loss 6.90982 epoch total loss 5.63451433\n",
      "Trained batch 10095 batch loss 5.04241085 epoch total loss 5.63445568\n",
      "Trained batch 10096 batch loss 5.64570665 epoch total loss 5.63445663\n",
      "Trained batch 10097 batch loss 5.97717094 epoch total loss 5.63449097\n",
      "Trained batch 10098 batch loss 5.72982311 epoch total loss 5.6345\n",
      "Trained batch 10099 batch loss 5.91143656 epoch total loss 5.63452768\n",
      "Trained batch 10100 batch loss 6.82373714 epoch total loss 5.63464546\n",
      "Trained batch 10101 batch loss 5.45044088 epoch total loss 5.63462687\n",
      "Trained batch 10102 batch loss 4.79936171 epoch total loss 5.63454437\n",
      "Trained batch 10103 batch loss 5.12071 epoch total loss 5.63449383\n",
      "Trained batch 10104 batch loss 6.34821033 epoch total loss 5.6345644\n",
      "Trained batch 10105 batch loss 5.28268814 epoch total loss 5.63452911\n",
      "Trained batch 10106 batch loss 4.64597082 epoch total loss 5.63443136\n",
      "Trained batch 10107 batch loss 5.30234146 epoch total loss 5.63439846\n",
      "Trained batch 10108 batch loss 5.77831078 epoch total loss 5.63441229\n",
      "Trained batch 10109 batch loss 5.13774061 epoch total loss 5.63436317\n",
      "Trained batch 10110 batch loss 5.40464115 epoch total loss 5.63434076\n",
      "Trained batch 10111 batch loss 5.51332664 epoch total loss 5.63432837\n",
      "Trained batch 10112 batch loss 5.58130693 epoch total loss 5.63432312\n",
      "Trained batch 10113 batch loss 5.7168169 epoch total loss 5.6343317\n",
      "Trained batch 10114 batch loss 6.09166193 epoch total loss 5.63437653\n",
      "Trained batch 10115 batch loss 5.9927597 epoch total loss 5.63441229\n",
      "Trained batch 10116 batch loss 4.52364969 epoch total loss 5.63430214\n",
      "Trained batch 10117 batch loss 4.61486912 epoch total loss 5.63420153\n",
      "Trained batch 10118 batch loss 5.59951305 epoch total loss 5.63419771\n",
      "Trained batch 10119 batch loss 5.48153543 epoch total loss 5.63418245\n",
      "Trained batch 10120 batch loss 6.1948595 epoch total loss 5.63423777\n",
      "Trained batch 10121 batch loss 5.60395813 epoch total loss 5.63423491\n",
      "Trained batch 10122 batch loss 5.7413826 epoch total loss 5.63424587\n",
      "Trained batch 10123 batch loss 4.85359716 epoch total loss 5.6341691\n",
      "Trained batch 10124 batch loss 4.65261269 epoch total loss 5.63407183\n",
      "Trained batch 10125 batch loss 5.49474764 epoch total loss 5.63405848\n",
      "Trained batch 10126 batch loss 4.08005524 epoch total loss 5.63390446\n",
      "Trained batch 10127 batch loss 5.20536852 epoch total loss 5.6338625\n",
      "Trained batch 10128 batch loss 5.19604254 epoch total loss 5.6338191\n",
      "Trained batch 10129 batch loss 5.43924713 epoch total loss 5.63379955\n",
      "Trained batch 10130 batch loss 6.21148109 epoch total loss 5.63385677\n",
      "Trained batch 10131 batch loss 5.41514587 epoch total loss 5.63383484\n",
      "Trained batch 10132 batch loss 5.15990353 epoch total loss 5.63378811\n",
      "Trained batch 10133 batch loss 5.81933403 epoch total loss 5.63380671\n",
      "Trained batch 10134 batch loss 6.14073467 epoch total loss 5.63385677\n",
      "Trained batch 10135 batch loss 5.41229105 epoch total loss 5.63383484\n",
      "Trained batch 10136 batch loss 5.08814716 epoch total loss 5.63378143\n",
      "Trained batch 10137 batch loss 5.56229115 epoch total loss 5.63377428\n",
      "Trained batch 10138 batch loss 5.01673603 epoch total loss 5.63371325\n",
      "Trained batch 10139 batch loss 4.90490913 epoch total loss 5.63364172\n",
      "Trained batch 10140 batch loss 5.07557631 epoch total loss 5.63358641\n",
      "Trained batch 10141 batch loss 4.98106623 epoch total loss 5.63352203\n",
      "Trained batch 10142 batch loss 5.01335478 epoch total loss 5.63346052\n",
      "Trained batch 10143 batch loss 5.04939842 epoch total loss 5.6334033\n",
      "Trained batch 10144 batch loss 5.65517616 epoch total loss 5.63340569\n",
      "Trained batch 10145 batch loss 5.92391205 epoch total loss 5.6334343\n",
      "Trained batch 10146 batch loss 6.05820751 epoch total loss 5.63347626\n",
      "Trained batch 10147 batch loss 5.5344348 epoch total loss 5.63346672\n",
      "Trained batch 10148 batch loss 5.83860683 epoch total loss 5.63348675\n",
      "Trained batch 10149 batch loss 3.6780293 epoch total loss 5.63329458\n",
      "Trained batch 10150 batch loss 5.61462164 epoch total loss 5.6332922\n",
      "Trained batch 10151 batch loss 6.01523685 epoch total loss 5.63333\n",
      "Trained batch 10152 batch loss 5.46691704 epoch total loss 5.63331366\n",
      "Trained batch 10153 batch loss 6.27635479 epoch total loss 5.63337708\n",
      "Trained batch 10154 batch loss 6.36407566 epoch total loss 5.63344908\n",
      "Trained batch 10155 batch loss 4.70976543 epoch total loss 5.63335848\n",
      "Trained batch 10156 batch loss 4.81824112 epoch total loss 5.63327789\n",
      "Trained batch 10157 batch loss 6.14087963 epoch total loss 5.63332796\n",
      "Trained batch 10158 batch loss 5.21837807 epoch total loss 5.63328695\n",
      "Trained batch 10159 batch loss 5.1162653 epoch total loss 5.63323641\n",
      "Trained batch 10160 batch loss 5.65096283 epoch total loss 5.63323832\n",
      "Trained batch 10161 batch loss 5.58492851 epoch total loss 5.63323355\n",
      "Trained batch 10162 batch loss 5.78935909 epoch total loss 5.63324881\n",
      "Trained batch 10163 batch loss 5.90015221 epoch total loss 5.63327503\n",
      "Trained batch 10164 batch loss 5.7822423 epoch total loss 5.63328934\n",
      "Trained batch 10165 batch loss 5.42665482 epoch total loss 5.63326883\n",
      "Trained batch 10166 batch loss 5.05689144 epoch total loss 5.63321257\n",
      "Trained batch 10167 batch loss 6.0182209 epoch total loss 5.63325071\n",
      "Trained batch 10168 batch loss 5.55318689 epoch total loss 5.63324261\n",
      "Trained batch 10169 batch loss 5.76096582 epoch total loss 5.63325548\n",
      "Trained batch 10170 batch loss 5.75422382 epoch total loss 5.6332674\n",
      "Trained batch 10171 batch loss 5.15776062 epoch total loss 5.63322\n",
      "Trained batch 10172 batch loss 4.72115564 epoch total loss 5.63313103\n",
      "Trained batch 10173 batch loss 5.60376167 epoch total loss 5.63312817\n",
      "Trained batch 10174 batch loss 5.19002819 epoch total loss 5.63308477\n",
      "Trained batch 10175 batch loss 5.96686 epoch total loss 5.63311768\n",
      "Trained batch 10176 batch loss 6.00925684 epoch total loss 5.63315439\n",
      "Trained batch 10177 batch loss 4.53434515 epoch total loss 5.63304663\n",
      "Trained batch 10178 batch loss 5.10225344 epoch total loss 5.63299465\n",
      "Trained batch 10179 batch loss 4.94355488 epoch total loss 5.63292694\n",
      "Trained batch 10180 batch loss 5.08193064 epoch total loss 5.63287258\n",
      "Trained batch 10181 batch loss 5.047822 epoch total loss 5.63281536\n",
      "Trained batch 10182 batch loss 6.01269245 epoch total loss 5.63285255\n",
      "Trained batch 10183 batch loss 6.65093327 epoch total loss 5.63295269\n",
      "Trained batch 10184 batch loss 5.54488373 epoch total loss 5.63294363\n",
      "Trained batch 10185 batch loss 5.70023346 epoch total loss 5.63295031\n",
      "Trained batch 10186 batch loss 5.72438717 epoch total loss 5.63295889\n",
      "Trained batch 10187 batch loss 5.75079584 epoch total loss 5.63297033\n",
      "Trained batch 10188 batch loss 5.79563427 epoch total loss 5.63298655\n",
      "Trained batch 10189 batch loss 5.96124029 epoch total loss 5.63301897\n",
      "Trained batch 10190 batch loss 5.2833972 epoch total loss 5.63298464\n",
      "Trained batch 10191 batch loss 6.08321667 epoch total loss 5.63302851\n",
      "Trained batch 10192 batch loss 5.56085491 epoch total loss 5.63302183\n",
      "Trained batch 10193 batch loss 5.63379 epoch total loss 5.63302183\n",
      "Trained batch 10194 batch loss 5.75904942 epoch total loss 5.63303375\n",
      "Trained batch 10195 batch loss 6.47073841 epoch total loss 5.63311625\n",
      "Trained batch 10196 batch loss 6.57300568 epoch total loss 5.63320875\n",
      "Trained batch 10197 batch loss 5.84361887 epoch total loss 5.63322926\n",
      "Trained batch 10198 batch loss 6.16779852 epoch total loss 5.63328171\n",
      "Trained batch 10199 batch loss 5.85122681 epoch total loss 5.63330317\n",
      "Trained batch 10200 batch loss 6.0443 epoch total loss 5.63334322\n",
      "Trained batch 10201 batch loss 5.87932873 epoch total loss 5.63336754\n",
      "Trained batch 10202 batch loss 6.06909275 epoch total loss 5.63341\n",
      "Trained batch 10203 batch loss 6.05675364 epoch total loss 5.63345194\n",
      "Trained batch 10204 batch loss 4.77216911 epoch total loss 5.63336754\n",
      "Trained batch 10205 batch loss 5.33750725 epoch total loss 5.63333845\n",
      "Trained batch 10206 batch loss 5.29731083 epoch total loss 5.63330555\n",
      "Trained batch 10207 batch loss 6.52841 epoch total loss 5.63339329\n",
      "Trained batch 10208 batch loss 6.61194611 epoch total loss 5.63348913\n",
      "Trained batch 10209 batch loss 5.8648386 epoch total loss 5.63351154\n",
      "Trained batch 10210 batch loss 5.78560495 epoch total loss 5.63352633\n",
      "Trained batch 10211 batch loss 6.32962036 epoch total loss 5.63359451\n",
      "Trained batch 10212 batch loss 6.20896387 epoch total loss 5.63365078\n",
      "Trained batch 10213 batch loss 6.85127926 epoch total loss 5.63377\n",
      "Trained batch 10214 batch loss 6.6136179 epoch total loss 5.63386583\n",
      "Trained batch 10215 batch loss 6.1343689 epoch total loss 5.63391447\n",
      "Trained batch 10216 batch loss 5.57002544 epoch total loss 5.63390827\n",
      "Trained batch 10217 batch loss 5.31013346 epoch total loss 5.63387632\n",
      "Trained batch 10218 batch loss 5.61625719 epoch total loss 5.63387489\n",
      "Trained batch 10219 batch loss 5.80850887 epoch total loss 5.63389206\n",
      "Trained batch 10220 batch loss 6.2382164 epoch total loss 5.63395119\n",
      "Trained batch 10221 batch loss 6.13490772 epoch total loss 5.6340003\n",
      "Trained batch 10222 batch loss 5.07478333 epoch total loss 5.63394547\n",
      "Trained batch 10223 batch loss 5.08280754 epoch total loss 5.63389158\n",
      "Trained batch 10224 batch loss 5.44367504 epoch total loss 5.63387299\n",
      "Trained batch 10225 batch loss 4.69570446 epoch total loss 5.63378143\n",
      "Trained batch 10226 batch loss 5.97312498 epoch total loss 5.63381433\n",
      "Trained batch 10227 batch loss 5.82004786 epoch total loss 5.63383293\n",
      "Trained batch 10228 batch loss 6.40367794 epoch total loss 5.63390779\n",
      "Trained batch 10229 batch loss 5.76895905 epoch total loss 5.63392115\n",
      "Trained batch 10230 batch loss 5.35094833 epoch total loss 5.63389349\n",
      "Trained batch 10231 batch loss 5.12209892 epoch total loss 5.63384342\n",
      "Trained batch 10232 batch loss 5.755445 epoch total loss 5.63385534\n",
      "Trained batch 10233 batch loss 5.52379036 epoch total loss 5.63384438\n",
      "Trained batch 10234 batch loss 5.97649527 epoch total loss 5.63387775\n",
      "Trained batch 10235 batch loss 5.16929245 epoch total loss 5.63383245\n",
      "Trained batch 10236 batch loss 5.90324831 epoch total loss 5.63385868\n",
      "Trained batch 10237 batch loss 5.81991 epoch total loss 5.6338768\n",
      "Trained batch 10238 batch loss 5.00898552 epoch total loss 5.63381577\n",
      "Trained batch 10239 batch loss 6.03642178 epoch total loss 5.63385487\n",
      "Trained batch 10240 batch loss 6.13804531 epoch total loss 5.63390398\n",
      "Trained batch 10241 batch loss 6.04204559 epoch total loss 5.63394403\n",
      "Trained batch 10242 batch loss 6.15319872 epoch total loss 5.63399458\n",
      "Trained batch 10243 batch loss 6.14882946 epoch total loss 5.63404465\n",
      "Trained batch 10244 batch loss 5.44357443 epoch total loss 5.63402605\n",
      "Trained batch 10245 batch loss 6.18838358 epoch total loss 5.63408041\n",
      "Trained batch 10246 batch loss 5.58629322 epoch total loss 5.63407564\n",
      "Trained batch 10247 batch loss 5.68613148 epoch total loss 5.63408089\n",
      "Trained batch 10248 batch loss 5.90623426 epoch total loss 5.63410711\n",
      "Trained batch 10249 batch loss 5.30939436 epoch total loss 5.63407564\n",
      "Trained batch 10250 batch loss 5.4345479 epoch total loss 5.63405609\n",
      "Trained batch 10251 batch loss 6.11574936 epoch total loss 5.6341033\n",
      "Trained batch 10252 batch loss 6.33314753 epoch total loss 5.63417101\n",
      "Trained batch 10253 batch loss 5.29549646 epoch total loss 5.63413811\n",
      "Trained batch 10254 batch loss 5.65694 epoch total loss 5.63414049\n",
      "Trained batch 10255 batch loss 4.87058353 epoch total loss 5.6340661\n",
      "Trained batch 10256 batch loss 5.32810974 epoch total loss 5.63403606\n",
      "Trained batch 10257 batch loss 6.34297085 epoch total loss 5.63410521\n",
      "Trained batch 10258 batch loss 6.31788778 epoch total loss 5.63417196\n",
      "Trained batch 10259 batch loss 5.43665314 epoch total loss 5.63415289\n",
      "Trained batch 10260 batch loss 6.53819323 epoch total loss 5.6342411\n",
      "Trained batch 10261 batch loss 5.28143644 epoch total loss 5.63420677\n",
      "Trained batch 10262 batch loss 5.10830879 epoch total loss 5.63415527\n",
      "Trained batch 10263 batch loss 5.99915218 epoch total loss 5.63419104\n",
      "Trained batch 10264 batch loss 5.11693382 epoch total loss 5.63414049\n",
      "Trained batch 10265 batch loss 6.38271618 epoch total loss 5.63421345\n",
      "Trained batch 10266 batch loss 6.78036118 epoch total loss 5.6343255\n",
      "Trained batch 10267 batch loss 5.24025536 epoch total loss 5.63428736\n",
      "Trained batch 10268 batch loss 5.85598564 epoch total loss 5.63430882\n",
      "Trained batch 10269 batch loss 5.90230465 epoch total loss 5.63433504\n",
      "Trained batch 10270 batch loss 4.59740639 epoch total loss 5.63423395\n",
      "Trained batch 10271 batch loss 4.88610268 epoch total loss 5.634161\n",
      "Trained batch 10272 batch loss 4.69348335 epoch total loss 5.63407\n",
      "Trained batch 10273 batch loss 5.44304895 epoch total loss 5.63405085\n",
      "Trained batch 10274 batch loss 5.91783714 epoch total loss 5.6340785\n",
      "Trained batch 10275 batch loss 4.84367466 epoch total loss 5.63400173\n",
      "Trained batch 10276 batch loss 5.42984867 epoch total loss 5.6339817\n",
      "Trained batch 10277 batch loss 4.27456856 epoch total loss 5.63384962\n",
      "Trained batch 10278 batch loss 5.07537317 epoch total loss 5.63379478\n",
      "Trained batch 10279 batch loss 5.24563217 epoch total loss 5.63375711\n",
      "Trained batch 10280 batch loss 5.31054544 epoch total loss 5.63372564\n",
      "Trained batch 10281 batch loss 5.27089405 epoch total loss 5.63369036\n",
      "Trained batch 10282 batch loss 5.4846096 epoch total loss 5.63367558\n",
      "Trained batch 10283 batch loss 3.44695234 epoch total loss 5.63346291\n",
      "Trained batch 10284 batch loss 4.57916546 epoch total loss 5.63336039\n",
      "Trained batch 10285 batch loss 4.77843475 epoch total loss 5.63327694\n",
      "Trained batch 10286 batch loss 3.89808464 epoch total loss 5.63310814\n",
      "Trained batch 10287 batch loss 4.73377132 epoch total loss 5.63302088\n",
      "Trained batch 10288 batch loss 5.31115913 epoch total loss 5.63299\n",
      "Trained batch 10289 batch loss 5.13713264 epoch total loss 5.63294172\n",
      "Trained batch 10290 batch loss 4.21951485 epoch total loss 5.63280392\n",
      "Trained batch 10291 batch loss 3.81212807 epoch total loss 5.63262749\n",
      "Trained batch 10292 batch loss 3.68850875 epoch total loss 5.63243818\n",
      "Trained batch 10293 batch loss 4.80926132 epoch total loss 5.63235807\n",
      "Trained batch 10294 batch loss 5.39747381 epoch total loss 5.63233566\n",
      "Trained batch 10295 batch loss 4.67292261 epoch total loss 5.6322422\n",
      "Trained batch 10296 batch loss 4.61835384 epoch total loss 5.6321435\n",
      "Trained batch 10297 batch loss 5.16100311 epoch total loss 5.63209772\n",
      "Trained batch 10298 batch loss 5.44007 epoch total loss 5.63207912\n",
      "Trained batch 10299 batch loss 5.29142952 epoch total loss 5.63204622\n",
      "Trained batch 10300 batch loss 4.64350748 epoch total loss 5.63195038\n",
      "Trained batch 10301 batch loss 4.76025295 epoch total loss 5.63186598\n",
      "Trained batch 10302 batch loss 6.8591404 epoch total loss 5.63198519\n",
      "Trained batch 10303 batch loss 5.78318882 epoch total loss 5.63199949\n",
      "Trained batch 10304 batch loss 7.26702404 epoch total loss 5.63215828\n",
      "Trained batch 10305 batch loss 6.93789911 epoch total loss 5.63228464\n",
      "Trained batch 10306 batch loss 7.15088892 epoch total loss 5.63243246\n",
      "Trained batch 10307 batch loss 7.18515205 epoch total loss 5.63258266\n",
      "Trained batch 10308 batch loss 7.4159 epoch total loss 5.63275576\n",
      "Trained batch 10309 batch loss 6.9992 epoch total loss 5.63288832\n",
      "Trained batch 10310 batch loss 7.07240582 epoch total loss 5.63302803\n",
      "Trained batch 10311 batch loss 7.08203077 epoch total loss 5.6331687\n",
      "Trained batch 10312 batch loss 6.51293468 epoch total loss 5.63325357\n",
      "Trained batch 10313 batch loss 5.07083225 epoch total loss 5.63319921\n",
      "Trained batch 10314 batch loss 6.45254564 epoch total loss 5.63327885\n",
      "Trained batch 10315 batch loss 5.3811655 epoch total loss 5.63325453\n",
      "Trained batch 10316 batch loss 5.52697945 epoch total loss 5.63324404\n",
      "Trained batch 10317 batch loss 6.53207159 epoch total loss 5.6333313\n",
      "Trained batch 10318 batch loss 5.67367411 epoch total loss 5.63333511\n",
      "Trained batch 10319 batch loss 5.95208359 epoch total loss 5.63336611\n",
      "Trained batch 10320 batch loss 5.25519848 epoch total loss 5.63332939\n",
      "Trained batch 10321 batch loss 5.5533886 epoch total loss 5.63332176\n",
      "Trained batch 10322 batch loss 5.90598774 epoch total loss 5.63334799\n",
      "Trained batch 10323 batch loss 5.83565187 epoch total loss 5.63336754\n",
      "Trained batch 10324 batch loss 5.84393883 epoch total loss 5.63338804\n",
      "Trained batch 10325 batch loss 5.81364632 epoch total loss 5.63340521\n",
      "Trained batch 10326 batch loss 5.7656765 epoch total loss 5.63341808\n",
      "Trained batch 10327 batch loss 5.53198051 epoch total loss 5.63340807\n",
      "Trained batch 10328 batch loss 5.65471697 epoch total loss 5.63341045\n",
      "Trained batch 10329 batch loss 5.25975227 epoch total loss 5.63337421\n",
      "Trained batch 10330 batch loss 6.06948328 epoch total loss 5.63341618\n",
      "Trained batch 10331 batch loss 6.02779531 epoch total loss 5.63345432\n",
      "Trained batch 10332 batch loss 5.62019062 epoch total loss 5.63345337\n",
      "Trained batch 10333 batch loss 5.73029852 epoch total loss 5.63346291\n",
      "Trained batch 10334 batch loss 5.64869499 epoch total loss 5.63346434\n",
      "Trained batch 10335 batch loss 5.72017384 epoch total loss 5.63347244\n",
      "Trained batch 10336 batch loss 6.6242094 epoch total loss 5.63356829\n",
      "Trained batch 10337 batch loss 5.27717686 epoch total loss 5.63353395\n",
      "Trained batch 10338 batch loss 5.95181274 epoch total loss 5.63356495\n",
      "Trained batch 10339 batch loss 6.69509888 epoch total loss 5.63366747\n",
      "Trained batch 10340 batch loss 6.64884377 epoch total loss 5.6337657\n",
      "Trained batch 10341 batch loss 6.42050838 epoch total loss 5.63384199\n",
      "Trained batch 10342 batch loss 4.87810087 epoch total loss 5.63376904\n",
      "Trained batch 10343 batch loss 5.94725847 epoch total loss 5.63379908\n",
      "Trained batch 10344 batch loss 6.3247366 epoch total loss 5.63386583\n",
      "Trained batch 10345 batch loss 5.63943672 epoch total loss 5.63386631\n",
      "Trained batch 10346 batch loss 5.95144463 epoch total loss 5.6338973\n",
      "Trained batch 10347 batch loss 5.02816248 epoch total loss 5.63383865\n",
      "Trained batch 10348 batch loss 4.50071 epoch total loss 5.63372898\n",
      "Trained batch 10349 batch loss 5.69995356 epoch total loss 5.63373518\n",
      "Trained batch 10350 batch loss 4.85484314 epoch total loss 5.63366032\n",
      "Trained batch 10351 batch loss 5.79245663 epoch total loss 5.63367558\n",
      "Trained batch 10352 batch loss 5.72573 epoch total loss 5.63368464\n",
      "Trained batch 10353 batch loss 6.18944836 epoch total loss 5.63373804\n",
      "Trained batch 10354 batch loss 5.96042442 epoch total loss 5.63376951\n",
      "Trained batch 10355 batch loss 6.27445126 epoch total loss 5.6338315\n",
      "Trained batch 10356 batch loss 5.44594812 epoch total loss 5.63381338\n",
      "Trained batch 10357 batch loss 4.64607191 epoch total loss 5.63371754\n",
      "Trained batch 10358 batch loss 5.27194357 epoch total loss 5.63368273\n",
      "Trained batch 10359 batch loss 5.38314867 epoch total loss 5.63365889\n",
      "Trained batch 10360 batch loss 5.23644257 epoch total loss 5.63362074\n",
      "Trained batch 10361 batch loss 5.15997887 epoch total loss 5.63357496\n",
      "Trained batch 10362 batch loss 5.86728239 epoch total loss 5.63359737\n",
      "Trained batch 10363 batch loss 5.30063772 epoch total loss 5.63356543\n",
      "Trained batch 10364 batch loss 5.88591623 epoch total loss 5.63358974\n",
      "Trained batch 10365 batch loss 6.23321819 epoch total loss 5.63364744\n",
      "Trained batch 10366 batch loss 6.37655544 epoch total loss 5.63371897\n",
      "Trained batch 10367 batch loss 5.90209389 epoch total loss 5.63374519\n",
      "Trained batch 10368 batch loss 5.9431448 epoch total loss 5.63377476\n",
      "Trained batch 10369 batch loss 5.51144838 epoch total loss 5.63376284\n",
      "Trained batch 10370 batch loss 6.49004936 epoch total loss 5.63384533\n",
      "Trained batch 10371 batch loss 5.64508533 epoch total loss 5.63384628\n",
      "Trained batch 10372 batch loss 5.809062 epoch total loss 5.63386345\n",
      "Trained batch 10373 batch loss 5.38681221 epoch total loss 5.63383961\n",
      "Trained batch 10374 batch loss 5.08075 epoch total loss 5.6337862\n",
      "Trained batch 10375 batch loss 5.32645893 epoch total loss 5.63375664\n",
      "Trained batch 10376 batch loss 7.04451323 epoch total loss 5.63389254\n",
      "Trained batch 10377 batch loss 5.02992058 epoch total loss 5.63383436\n",
      "Trained batch 10378 batch loss 5.35658789 epoch total loss 5.63380766\n",
      "Trained batch 10379 batch loss 5.59146261 epoch total loss 5.63380337\n",
      "Trained batch 10380 batch loss 5.148808 epoch total loss 5.63375664\n",
      "Trained batch 10381 batch loss 4.95213127 epoch total loss 5.63369131\n",
      "Trained batch 10382 batch loss 5.76684046 epoch total loss 5.63370371\n",
      "Trained batch 10383 batch loss 5.45550251 epoch total loss 5.63368702\n",
      "Trained batch 10384 batch loss 6.33065271 epoch total loss 5.63375425\n",
      "Trained batch 10385 batch loss 6.49611568 epoch total loss 5.63383722\n",
      "Trained batch 10386 batch loss 5.93734932 epoch total loss 5.63386631\n",
      "Trained batch 10387 batch loss 5.43850422 epoch total loss 5.63384724\n",
      "Trained batch 10388 batch loss 5.37722206 epoch total loss 5.63382292\n",
      "Trained batch 10389 batch loss 6.07307529 epoch total loss 5.63386536\n",
      "Trained batch 10390 batch loss 5.76862192 epoch total loss 5.63387823\n",
      "Trained batch 10391 batch loss 6.34729481 epoch total loss 5.6339469\n",
      "Trained batch 10392 batch loss 5.00982904 epoch total loss 5.63388729\n",
      "Trained batch 10393 batch loss 5.52658367 epoch total loss 5.6338768\n",
      "Trained batch 10394 batch loss 5.46165276 epoch total loss 5.63386\n",
      "Trained batch 10395 batch loss 5.69389105 epoch total loss 5.63386631\n",
      "Trained batch 10396 batch loss 5.62556839 epoch total loss 5.63386536\n",
      "Trained batch 10397 batch loss 6.7739625 epoch total loss 5.63397503\n",
      "Trained batch 10398 batch loss 5.76268101 epoch total loss 5.63398743\n",
      "Trained batch 10399 batch loss 6.7627387 epoch total loss 5.63409567\n",
      "Trained batch 10400 batch loss 6.47161818 epoch total loss 5.63417625\n",
      "Trained batch 10401 batch loss 5.49726486 epoch total loss 5.6341629\n",
      "Trained batch 10402 batch loss 5.40538597 epoch total loss 5.63414097\n",
      "Trained batch 10403 batch loss 5.71355724 epoch total loss 5.63414907\n",
      "Trained batch 10404 batch loss 5.86990356 epoch total loss 5.63417149\n",
      "Trained batch 10405 batch loss 5.4085331 epoch total loss 5.63415\n",
      "Trained batch 10406 batch loss 4.12486839 epoch total loss 5.63400507\n",
      "Trained batch 10407 batch loss 5.50070763 epoch total loss 5.6339922\n",
      "Trained batch 10408 batch loss 6.07479048 epoch total loss 5.63403463\n",
      "Trained batch 10409 batch loss 5.02366924 epoch total loss 5.63397598\n",
      "Trained batch 10410 batch loss 4.70763254 epoch total loss 5.63388681\n",
      "Trained batch 10411 batch loss 5.33701611 epoch total loss 5.6338582\n",
      "Trained batch 10412 batch loss 5.52236557 epoch total loss 5.63384771\n",
      "Trained batch 10413 batch loss 5.99190378 epoch total loss 5.63388205\n",
      "Trained batch 10414 batch loss 5.32230473 epoch total loss 5.63385248\n",
      "Trained batch 10415 batch loss 6.1261487 epoch total loss 5.63389921\n",
      "Trained batch 10416 batch loss 5.67394209 epoch total loss 5.6339035\n",
      "Trained batch 10417 batch loss 6.16178226 epoch total loss 5.63395405\n",
      "Trained batch 10418 batch loss 5.48321581 epoch total loss 5.63393974\n",
      "Trained batch 10419 batch loss 5.76601648 epoch total loss 5.63395214\n",
      "Trained batch 10420 batch loss 5.7137146 epoch total loss 5.63396\n",
      "Trained batch 10421 batch loss 5.01326704 epoch total loss 5.6339\n",
      "Trained batch 10422 batch loss 5.54641247 epoch total loss 5.63389206\n",
      "Trained batch 10423 batch loss 5.64672756 epoch total loss 5.63389349\n",
      "Trained batch 10424 batch loss 5.361269 epoch total loss 5.63386679\n",
      "Trained batch 10425 batch loss 6.16118813 epoch total loss 5.63391733\n",
      "Trained batch 10426 batch loss 5.49419498 epoch total loss 5.63390446\n",
      "Trained batch 10427 batch loss 5.03877735 epoch total loss 5.63384724\n",
      "Trained batch 10428 batch loss 5.24457169 epoch total loss 5.63381\n",
      "Trained batch 10429 batch loss 5.61897707 epoch total loss 5.63380861\n",
      "Trained batch 10430 batch loss 5.16253519 epoch total loss 5.63376331\n",
      "Trained batch 10431 batch loss 5.1995244 epoch total loss 5.63372183\n",
      "Trained batch 10432 batch loss 5.52441406 epoch total loss 5.63371134\n",
      "Trained batch 10433 batch loss 5.39544964 epoch total loss 5.63368845\n",
      "Trained batch 10434 batch loss 5.77761841 epoch total loss 5.6337018\n",
      "Trained batch 10435 batch loss 5.01865959 epoch total loss 5.63364315\n",
      "Trained batch 10436 batch loss 5.04396629 epoch total loss 5.63358641\n",
      "Trained batch 10437 batch loss 5.25623465 epoch total loss 5.63355064\n",
      "Trained batch 10438 batch loss 5.04808617 epoch total loss 5.63349438\n",
      "Trained batch 10439 batch loss 5.41948748 epoch total loss 5.63347387\n",
      "Trained batch 10440 batch loss 5.18799162 epoch total loss 5.63343096\n",
      "Trained batch 10441 batch loss 5.12571049 epoch total loss 5.63338232\n",
      "Trained batch 10442 batch loss 5.08789539 epoch total loss 5.63333035\n",
      "Trained batch 10443 batch loss 5.20756149 epoch total loss 5.63328934\n",
      "Trained batch 10444 batch loss 5.38419867 epoch total loss 5.6332655\n",
      "Trained batch 10445 batch loss 5.97326756 epoch total loss 5.63329792\n",
      "Trained batch 10446 batch loss 5.22705555 epoch total loss 5.63325882\n",
      "Trained batch 10447 batch loss 4.56268358 epoch total loss 5.6331563\n",
      "Trained batch 10448 batch loss 4.88524961 epoch total loss 5.63308525\n",
      "Trained batch 10449 batch loss 5.07174158 epoch total loss 5.63303137\n",
      "Trained batch 10450 batch loss 5.94311714 epoch total loss 5.63306093\n",
      "Trained batch 10451 batch loss 5.58342171 epoch total loss 5.63305569\n",
      "Trained batch 10452 batch loss 5.82762718 epoch total loss 5.63307428\n",
      "Trained batch 10453 batch loss 5.21098185 epoch total loss 5.63303423\n",
      "Trained batch 10454 batch loss 6.33264971 epoch total loss 5.63310099\n",
      "Trained batch 10455 batch loss 4.75113773 epoch total loss 5.63301659\n",
      "Trained batch 10456 batch loss 5.37381 epoch total loss 5.63299179\n",
      "Trained batch 10457 batch loss 5.25718498 epoch total loss 5.63295603\n",
      "Trained batch 10458 batch loss 5.09497 epoch total loss 5.63290453\n",
      "Trained batch 10459 batch loss 4.89575386 epoch total loss 5.63283396\n",
      "Trained batch 10460 batch loss 5.53210878 epoch total loss 5.63282394\n",
      "Trained batch 10461 batch loss 5.88604736 epoch total loss 5.63284826\n",
      "Trained batch 10462 batch loss 5.87220621 epoch total loss 5.63287115\n",
      "Trained batch 10463 batch loss 6.09982586 epoch total loss 5.63291597\n",
      "Trained batch 10464 batch loss 5.73680878 epoch total loss 5.63292599\n",
      "Trained batch 10465 batch loss 6.20566368 epoch total loss 5.63298082\n",
      "Trained batch 10466 batch loss 5.56383133 epoch total loss 5.63297415\n",
      "Trained batch 10467 batch loss 6.47430515 epoch total loss 5.63305426\n",
      "Trained batch 10468 batch loss 4.02337742 epoch total loss 5.63290071\n",
      "Trained batch 10469 batch loss 6.91573763 epoch total loss 5.63302279\n",
      "Trained batch 10470 batch loss 6.12022161 epoch total loss 5.63306952\n",
      "Trained batch 10471 batch loss 5.134552 epoch total loss 5.63302183\n",
      "Trained batch 10472 batch loss 5.80395508 epoch total loss 5.63303804\n",
      "Trained batch 10473 batch loss 5.51866341 epoch total loss 5.63302755\n",
      "Trained batch 10474 batch loss 5.55387545 epoch total loss 5.63302\n",
      "Trained batch 10475 batch loss 5.23794413 epoch total loss 5.63298225\n",
      "Trained batch 10476 batch loss 5.53439903 epoch total loss 5.63297272\n",
      "Trained batch 10477 batch loss 5.39990902 epoch total loss 5.63295031\n",
      "Trained batch 10478 batch loss 5.07561111 epoch total loss 5.6328969\n",
      "Trained batch 10479 batch loss 5.83515167 epoch total loss 5.63291645\n",
      "Trained batch 10480 batch loss 5.27712965 epoch total loss 5.6328826\n",
      "Trained batch 10481 batch loss 6.31699944 epoch total loss 5.63294792\n",
      "Trained batch 10482 batch loss 6.3101368 epoch total loss 5.63301229\n",
      "Trained batch 10483 batch loss 6.29873276 epoch total loss 5.63307571\n",
      "Trained batch 10484 batch loss 6.02473497 epoch total loss 5.63311291\n",
      "Trained batch 10485 batch loss 6.10474396 epoch total loss 5.63315773\n",
      "Trained batch 10486 batch loss 5.53396511 epoch total loss 5.63314867\n",
      "Trained batch 10487 batch loss 5.42950106 epoch total loss 5.63312912\n",
      "Trained batch 10488 batch loss 5.96155167 epoch total loss 5.63316059\n",
      "Trained batch 10489 batch loss 5.09772825 epoch total loss 5.63310909\n",
      "Trained batch 10490 batch loss 4.77193069 epoch total loss 5.63302755\n",
      "Trained batch 10491 batch loss 4.75737667 epoch total loss 5.63294411\n",
      "Trained batch 10492 batch loss 5.97706699 epoch total loss 5.63297653\n",
      "Trained batch 10493 batch loss 6.1181922 epoch total loss 5.63302279\n",
      "Trained batch 10494 batch loss 4.86132431 epoch total loss 5.63294888\n",
      "Trained batch 10495 batch loss 5.17137814 epoch total loss 5.63290501\n",
      "Trained batch 10496 batch loss 6.86373043 epoch total loss 5.63302231\n",
      "Trained batch 10497 batch loss 7.24860764 epoch total loss 5.63317633\n",
      "Trained batch 10498 batch loss 5.70274162 epoch total loss 5.633183\n",
      "Trained batch 10499 batch loss 4.81670094 epoch total loss 5.63310528\n",
      "Trained batch 10500 batch loss 5.10337067 epoch total loss 5.63305473\n",
      "Trained batch 10501 batch loss 5.07832623 epoch total loss 5.6330018\n",
      "Trained batch 10502 batch loss 4.67742968 epoch total loss 5.63291073\n",
      "Trained batch 10503 batch loss 5.39027214 epoch total loss 5.63288784\n",
      "Trained batch 10504 batch loss 5.24749565 epoch total loss 5.63285065\n",
      "Trained batch 10505 batch loss 6.31789398 epoch total loss 5.63291597\n",
      "Trained batch 10506 batch loss 6.54962349 epoch total loss 5.63300323\n",
      "Trained batch 10507 batch loss 6.44503975 epoch total loss 5.63308048\n",
      "Trained batch 10508 batch loss 6.39500141 epoch total loss 5.63315296\n",
      "Trained batch 10509 batch loss 5.70642567 epoch total loss 5.63316\n",
      "Trained batch 10510 batch loss 6.22165346 epoch total loss 5.6332159\n",
      "Trained batch 10511 batch loss 6.37563705 epoch total loss 5.63328648\n",
      "Trained batch 10512 batch loss 6.70353889 epoch total loss 5.63338852\n",
      "Trained batch 10513 batch loss 5.86868286 epoch total loss 5.63341093\n",
      "Trained batch 10514 batch loss 6.07226133 epoch total loss 5.63345242\n",
      "Trained batch 10515 batch loss 5.27530241 epoch total loss 5.63341808\n",
      "Trained batch 10516 batch loss 5.16696548 epoch total loss 5.63337374\n",
      "Trained batch 10517 batch loss 6.03014517 epoch total loss 5.63341141\n",
      "Trained batch 10518 batch loss 6.31539917 epoch total loss 5.63347673\n",
      "Trained batch 10519 batch loss 5.77634 epoch total loss 5.63349\n",
      "Trained batch 10520 batch loss 5.81829786 epoch total loss 5.63350773\n",
      "Trained batch 10521 batch loss 5.13664055 epoch total loss 5.63346052\n",
      "Trained batch 10522 batch loss 6.07450819 epoch total loss 5.63350248\n",
      "Trained batch 10523 batch loss 5.6467452 epoch total loss 5.63350391\n",
      "Trained batch 10524 batch loss 5.8117075 epoch total loss 5.6335206\n",
      "Trained batch 10525 batch loss 5.64316177 epoch total loss 5.63352156\n",
      "Trained batch 10526 batch loss 5.98462105 epoch total loss 5.63355494\n",
      "Trained batch 10527 batch loss 5.14606857 epoch total loss 5.63350868\n",
      "Trained batch 10528 batch loss 5.76421165 epoch total loss 5.63352108\n",
      "Trained batch 10529 batch loss 5.413517 epoch total loss 5.6335\n",
      "Trained batch 10530 batch loss 5.52932072 epoch total loss 5.63349056\n",
      "Trained batch 10531 batch loss 5.8277626 epoch total loss 5.63350916\n",
      "Trained batch 10532 batch loss 6.07095242 epoch total loss 5.63355064\n",
      "Trained batch 10533 batch loss 5.80101681 epoch total loss 5.63356638\n",
      "Trained batch 10534 batch loss 5.56101942 epoch total loss 5.6335597\n",
      "Trained batch 10535 batch loss 6.22325 epoch total loss 5.63361549\n",
      "Trained batch 10536 batch loss 5.37219143 epoch total loss 5.6335907\n",
      "Trained batch 10537 batch loss 5.76631832 epoch total loss 5.6336031\n",
      "Trained batch 10538 batch loss 6.01803875 epoch total loss 5.63364\n",
      "Trained batch 10539 batch loss 5.9552784 epoch total loss 5.63367033\n",
      "Trained batch 10540 batch loss 5.97618055 epoch total loss 5.63370323\n",
      "Trained batch 10541 batch loss 5.96340752 epoch total loss 5.6337347\n",
      "Trained batch 10542 batch loss 5.71585226 epoch total loss 5.63374233\n",
      "Trained batch 10543 batch loss 5.80031395 epoch total loss 5.63375807\n",
      "Trained batch 10544 batch loss 6.06783342 epoch total loss 5.63379908\n",
      "Trained batch 10545 batch loss 5.91259289 epoch total loss 5.63382578\n",
      "Trained batch 10546 batch loss 5.67697525 epoch total loss 5.63382959\n",
      "Trained batch 10547 batch loss 6.01342058 epoch total loss 5.63386536\n",
      "Trained batch 10548 batch loss 5.57494116 epoch total loss 5.63385963\n",
      "Trained batch 10549 batch loss 5.25118065 epoch total loss 5.63382339\n",
      "Trained batch 10550 batch loss 5.80613565 epoch total loss 5.63383961\n",
      "Trained batch 10551 batch loss 6.01800537 epoch total loss 5.63387632\n",
      "Trained batch 10552 batch loss 6.3981781 epoch total loss 5.6339488\n",
      "Trained batch 10553 batch loss 6.04275894 epoch total loss 5.63398743\n",
      "Trained batch 10554 batch loss 5.93576765 epoch total loss 5.63401604\n",
      "Trained batch 10555 batch loss 5.38486385 epoch total loss 5.63399267\n",
      "Trained batch 10556 batch loss 5.60760117 epoch total loss 5.63399029\n",
      "Trained batch 10557 batch loss 4.89474 epoch total loss 5.63392\n",
      "Trained batch 10558 batch loss 5.65246773 epoch total loss 5.6339221\n",
      "Trained batch 10559 batch loss 5.54995728 epoch total loss 5.63391399\n",
      "Trained batch 10560 batch loss 5.24825478 epoch total loss 5.63387775\n",
      "Trained batch 10561 batch loss 5.43091583 epoch total loss 5.63385868\n",
      "Trained batch 10562 batch loss 6.07878971 epoch total loss 5.63390064\n",
      "Trained batch 10563 batch loss 5.44945145 epoch total loss 5.633883\n",
      "Trained batch 10564 batch loss 5.55320501 epoch total loss 5.63387537\n",
      "Trained batch 10565 batch loss 6.50657082 epoch total loss 5.63395834\n",
      "Trained batch 10566 batch loss 5.13926792 epoch total loss 5.63391161\n",
      "Trained batch 10567 batch loss 6.13566637 epoch total loss 5.63395929\n",
      "Trained batch 10568 batch loss 6.26704216 epoch total loss 5.6340189\n",
      "Trained batch 10569 batch loss 5.98399639 epoch total loss 5.63405228\n",
      "Trained batch 10570 batch loss 6.48106956 epoch total loss 5.63413239\n",
      "Trained batch 10571 batch loss 5.84708548 epoch total loss 5.63415241\n",
      "Trained batch 10572 batch loss 6.56855917 epoch total loss 5.6342411\n",
      "Trained batch 10573 batch loss 6.39244938 epoch total loss 5.63431263\n",
      "Trained batch 10574 batch loss 6.24018097 epoch total loss 5.63436937\n",
      "Trained batch 10575 batch loss 5.77081776 epoch total loss 5.63438225\n",
      "Trained batch 10576 batch loss 6.35605 epoch total loss 5.63445044\n",
      "Trained batch 10577 batch loss 6.53096 epoch total loss 5.63453531\n",
      "Trained batch 10578 batch loss 6.25495243 epoch total loss 5.63459396\n",
      "Trained batch 10579 batch loss 5.93856335 epoch total loss 5.63462257\n",
      "Trained batch 10580 batch loss 6.32965374 epoch total loss 5.6346879\n",
      "Trained batch 10581 batch loss 6.15569448 epoch total loss 5.63473749\n",
      "Trained batch 10582 batch loss 6.43628073 epoch total loss 5.63481331\n",
      "Trained batch 10583 batch loss 6.31720352 epoch total loss 5.63487768\n",
      "Trained batch 10584 batch loss 6.24965668 epoch total loss 5.63493586\n",
      "Trained batch 10585 batch loss 6.46773529 epoch total loss 5.63501453\n",
      "Trained batch 10586 batch loss 5.39172745 epoch total loss 5.63499165\n",
      "Trained batch 10587 batch loss 5.66122246 epoch total loss 5.63499403\n",
      "Trained batch 10588 batch loss 6.15287209 epoch total loss 5.63504267\n",
      "Trained batch 10589 batch loss 6.06634951 epoch total loss 5.6350832\n",
      "Trained batch 10590 batch loss 6.08398342 epoch total loss 5.63512564\n",
      "Trained batch 10591 batch loss 5.56401396 epoch total loss 5.63511896\n",
      "Trained batch 10592 batch loss 5.59109879 epoch total loss 5.63511467\n",
      "Trained batch 10593 batch loss 5.09137678 epoch total loss 5.63506317\n",
      "Trained batch 10594 batch loss 4.76781464 epoch total loss 5.63498116\n",
      "Trained batch 10595 batch loss 6.02078962 epoch total loss 5.6350174\n",
      "Trained batch 10596 batch loss 5.82121754 epoch total loss 5.63503504\n",
      "Trained batch 10597 batch loss 5.43720055 epoch total loss 5.63501644\n",
      "Trained batch 10598 batch loss 5.30697489 epoch total loss 5.63498545\n",
      "Trained batch 10599 batch loss 5.45931053 epoch total loss 5.63496923\n",
      "Trained batch 10600 batch loss 5.4976716 epoch total loss 5.63495636\n",
      "Trained batch 10601 batch loss 6.33197069 epoch total loss 5.63502169\n",
      "Trained batch 10602 batch loss 5.31299543 epoch total loss 5.63499165\n",
      "Trained batch 10603 batch loss 6.44138336 epoch total loss 5.63506746\n",
      "Trained batch 10604 batch loss 6.41173506 epoch total loss 5.63514042\n",
      "Trained batch 10605 batch loss 5.53819084 epoch total loss 5.63513136\n",
      "Trained batch 10606 batch loss 5.33300209 epoch total loss 5.63510323\n",
      "Trained batch 10607 batch loss 5.18152666 epoch total loss 5.63506\n",
      "Trained batch 10608 batch loss 5.65242481 epoch total loss 5.63506174\n",
      "Trained batch 10609 batch loss 5.52720356 epoch total loss 5.63505173\n",
      "Trained batch 10610 batch loss 5.25567865 epoch total loss 5.63501549\n",
      "Trained batch 10611 batch loss 4.74878597 epoch total loss 5.63493204\n",
      "Trained batch 10612 batch loss 6.0367856 epoch total loss 5.63496971\n",
      "Trained batch 10613 batch loss 5.33201742 epoch total loss 5.63494158\n",
      "Trained batch 10614 batch loss 6.09002876 epoch total loss 5.63498402\n",
      "Trained batch 10615 batch loss 5.28559589 epoch total loss 5.63495111\n",
      "Trained batch 10616 batch loss 5.71269894 epoch total loss 5.63495827\n",
      "Trained batch 10617 batch loss 5.44998264 epoch total loss 5.6349411\n",
      "Trained batch 10618 batch loss 5.56953287 epoch total loss 5.6349349\n",
      "Trained batch 10619 batch loss 6.92631054 epoch total loss 5.6350565\n",
      "Trained batch 10620 batch loss 5.55656815 epoch total loss 5.63504887\n",
      "Trained batch 10621 batch loss 5.65704155 epoch total loss 5.63505077\n",
      "Trained batch 10622 batch loss 5.77987957 epoch total loss 5.6350646\n",
      "Trained batch 10623 batch loss 5.7308383 epoch total loss 5.63507366\n",
      "Trained batch 10624 batch loss 5.98480892 epoch total loss 5.63510656\n",
      "Trained batch 10625 batch loss 6.16231537 epoch total loss 5.63515615\n",
      "Trained batch 10626 batch loss 5.52790737 epoch total loss 5.63514614\n",
      "Trained batch 10627 batch loss 5.08326817 epoch total loss 5.63509417\n",
      "Trained batch 10628 batch loss 4.53412676 epoch total loss 5.63499069\n",
      "Trained batch 10629 batch loss 6.09366608 epoch total loss 5.63503361\n",
      "Trained batch 10630 batch loss 6.0853672 epoch total loss 5.63507605\n",
      "Trained batch 10631 batch loss 4.78526783 epoch total loss 5.63499641\n",
      "Trained batch 10632 batch loss 5.40966606 epoch total loss 5.63497496\n",
      "Trained batch 10633 batch loss 6.1443224 epoch total loss 5.63502312\n",
      "Trained batch 10634 batch loss 5.78719 epoch total loss 5.63503742\n",
      "Trained batch 10635 batch loss 5.80948925 epoch total loss 5.63505363\n",
      "Trained batch 10636 batch loss 5.98884964 epoch total loss 5.63508701\n",
      "Trained batch 10637 batch loss 6.57278347 epoch total loss 5.63517523\n",
      "Trained batch 10638 batch loss 6.47632504 epoch total loss 5.63525438\n",
      "Trained batch 10639 batch loss 6.71720314 epoch total loss 5.63535643\n",
      "Trained batch 10640 batch loss 6.62628365 epoch total loss 5.63544941\n",
      "Trained batch 10641 batch loss 6.59466839 epoch total loss 5.63553905\n",
      "Trained batch 10642 batch loss 6.0191493 epoch total loss 5.63557529\n",
      "Trained batch 10643 batch loss 6.21956253 epoch total loss 5.63563\n",
      "Trained batch 10644 batch loss 6.43997049 epoch total loss 5.63570595\n",
      "Trained batch 10645 batch loss 6.34146643 epoch total loss 5.63577223\n",
      "Trained batch 10646 batch loss 6.05956841 epoch total loss 5.63581181\n",
      "Trained batch 10647 batch loss 5.82588387 epoch total loss 5.63582945\n",
      "Trained batch 10648 batch loss 5.70675278 epoch total loss 5.63583612\n",
      "Trained batch 10649 batch loss 6.13053608 epoch total loss 5.63588238\n",
      "Trained batch 10650 batch loss 6.44234657 epoch total loss 5.63595819\n",
      "Trained batch 10651 batch loss 6.8742485 epoch total loss 5.63607454\n",
      "Trained batch 10652 batch loss 5.76949787 epoch total loss 5.63608694\n",
      "Trained batch 10653 batch loss 6.67599201 epoch total loss 5.63618469\n",
      "Trained batch 10654 batch loss 6.64237309 epoch total loss 5.63627863\n",
      "Trained batch 10655 batch loss 6.6649437 epoch total loss 5.63637543\n",
      "Trained batch 10656 batch loss 6.60880518 epoch total loss 5.6364665\n",
      "Trained batch 10657 batch loss 6.60596657 epoch total loss 5.63655758\n",
      "Trained batch 10658 batch loss 6.58647346 epoch total loss 5.63664675\n",
      "Trained batch 10659 batch loss 6.35642338 epoch total loss 5.63671398\n",
      "Trained batch 10660 batch loss 6.8305707 epoch total loss 5.63682604\n",
      "Trained batch 10661 batch loss 6.13249779 epoch total loss 5.63687277\n",
      "Trained batch 10662 batch loss 6.58337212 epoch total loss 5.63696146\n",
      "Trained batch 10663 batch loss 5.12798309 epoch total loss 5.63691378\n",
      "Trained batch 10664 batch loss 6.04593229 epoch total loss 5.63695192\n",
      "Trained batch 10665 batch loss 5.17577219 epoch total loss 5.63690901\n",
      "Trained batch 10666 batch loss 4.92811155 epoch total loss 5.63684273\n",
      "Trained batch 10667 batch loss 4.90560436 epoch total loss 5.63677406\n",
      "Trained batch 10668 batch loss 3.44248676 epoch total loss 5.63656807\n",
      "Trained batch 10669 batch loss 5.26876 epoch total loss 5.63653374\n",
      "Trained batch 10670 batch loss 4.33096313 epoch total loss 5.63641167\n",
      "Trained batch 10671 batch loss 4.67512035 epoch total loss 5.63632154\n",
      "Trained batch 10672 batch loss 4.74207258 epoch total loss 5.63623762\n",
      "Trained batch 10673 batch loss 4.28985405 epoch total loss 5.63611174\n",
      "Trained batch 10674 batch loss 4.76031399 epoch total loss 5.63602972\n",
      "Trained batch 10675 batch loss 4.68335533 epoch total loss 5.63594055\n",
      "Trained batch 10676 batch loss 4.90341663 epoch total loss 5.63587189\n",
      "Trained batch 10677 batch loss 5.19041824 epoch total loss 5.63583\n",
      "Trained batch 10678 batch loss 6.07441378 epoch total loss 5.63587093\n",
      "Trained batch 10679 batch loss 5.96141148 epoch total loss 5.63590145\n",
      "Trained batch 10680 batch loss 5.67893 epoch total loss 5.63590574\n",
      "Trained batch 10681 batch loss 6.16005373 epoch total loss 5.63595486\n",
      "Trained batch 10682 batch loss 6.03515625 epoch total loss 5.63599205\n",
      "Trained batch 10683 batch loss 5.28655624 epoch total loss 5.63595915\n",
      "Trained batch 10684 batch loss 6.30996895 epoch total loss 5.63602209\n",
      "Trained batch 10685 batch loss 5.77376747 epoch total loss 5.63603497\n",
      "Trained batch 10686 batch loss 5.56297779 epoch total loss 5.63602829\n",
      "Trained batch 10687 batch loss 5.76568031 epoch total loss 5.63604\n",
      "Trained batch 10688 batch loss 5.02510262 epoch total loss 5.63598299\n",
      "Trained batch 10689 batch loss 6.01795912 epoch total loss 5.63601875\n",
      "Trained batch 10690 batch loss 5.40670872 epoch total loss 5.6359973\n",
      "Trained batch 10691 batch loss 5.87852764 epoch total loss 5.63602\n",
      "Trained batch 10692 batch loss 5.21990395 epoch total loss 5.63598108\n",
      "Trained batch 10693 batch loss 5.61817741 epoch total loss 5.63597918\n",
      "Trained batch 10694 batch loss 5.47939777 epoch total loss 5.63596487\n",
      "Trained batch 10695 batch loss 3.8082993 epoch total loss 5.63579416\n",
      "Trained batch 10696 batch loss 5.32585526 epoch total loss 5.6357646\n",
      "Trained batch 10697 batch loss 5.24625874 epoch total loss 5.63572836\n",
      "Trained batch 10698 batch loss 4.95662212 epoch total loss 5.63566494\n",
      "Trained batch 10699 batch loss 5.65734386 epoch total loss 5.63566685\n",
      "Trained batch 10700 batch loss 5.67943859 epoch total loss 5.63567114\n",
      "Trained batch 10701 batch loss 5.12384224 epoch total loss 5.63562346\n",
      "Trained batch 10702 batch loss 4.99647331 epoch total loss 5.63556337\n",
      "Trained batch 10703 batch loss 5.63614464 epoch total loss 5.63556385\n",
      "Trained batch 10704 batch loss 5.23271561 epoch total loss 5.63552618\n",
      "Trained batch 10705 batch loss 5.40493298 epoch total loss 5.63550472\n",
      "Trained batch 10706 batch loss 5.28431511 epoch total loss 5.63547182\n",
      "Trained batch 10707 batch loss 6.44831467 epoch total loss 5.63554811\n",
      "Trained batch 10708 batch loss 5.12969637 epoch total loss 5.63550091\n",
      "Trained batch 10709 batch loss 5.19127846 epoch total loss 5.63545942\n",
      "Trained batch 10710 batch loss 5.16754293 epoch total loss 5.63541555\n",
      "Trained batch 10711 batch loss 5.067976 epoch total loss 5.63536263\n",
      "Trained batch 10712 batch loss 5.27027893 epoch total loss 5.63532829\n",
      "Trained batch 10713 batch loss 5.12303686 epoch total loss 5.63528\n",
      "Trained batch 10714 batch loss 5.52403736 epoch total loss 5.63526964\n",
      "Trained batch 10715 batch loss 5.45615196 epoch total loss 5.63525343\n",
      "Trained batch 10716 batch loss 4.29361153 epoch total loss 5.63512802\n",
      "Trained batch 10717 batch loss 5.45745087 epoch total loss 5.63511133\n",
      "Trained batch 10718 batch loss 5.63359165 epoch total loss 5.63511133\n",
      "Trained batch 10719 batch loss 4.93247128 epoch total loss 5.63504553\n",
      "Trained batch 10720 batch loss 5.64276361 epoch total loss 5.63504648\n",
      "Trained batch 10721 batch loss 5.22008 epoch total loss 5.63500786\n",
      "Trained batch 10722 batch loss 6.48577547 epoch total loss 5.63508701\n",
      "Trained batch 10723 batch loss 5.34114 epoch total loss 5.63505936\n",
      "Trained batch 10724 batch loss 5.2046175 epoch total loss 5.6350193\n",
      "Trained batch 10725 batch loss 5.17845726 epoch total loss 5.63497686\n",
      "Trained batch 10726 batch loss 4.83192444 epoch total loss 5.634902\n",
      "Trained batch 10727 batch loss 5.58356619 epoch total loss 5.63489676\n",
      "Trained batch 10728 batch loss 5.21187067 epoch total loss 5.63485718\n",
      "Trained batch 10729 batch loss 4.51272964 epoch total loss 5.63475275\n",
      "Trained batch 10730 batch loss 4.91188526 epoch total loss 5.63468504\n",
      "Trained batch 10731 batch loss 5.50736427 epoch total loss 5.63467312\n",
      "Trained batch 10732 batch loss 4.98806858 epoch total loss 5.63461304\n",
      "Trained batch 10733 batch loss 4.92469501 epoch total loss 5.63454723\n",
      "Trained batch 10734 batch loss 5.26412439 epoch total loss 5.6345129\n",
      "Trained batch 10735 batch loss 5.19961929 epoch total loss 5.63447237\n",
      "Trained batch 10736 batch loss 5.11000633 epoch total loss 5.63442326\n",
      "Trained batch 10737 batch loss 5.03145838 epoch total loss 5.63436699\n",
      "Trained batch 10738 batch loss 4.26318884 epoch total loss 5.6342392\n",
      "Trained batch 10739 batch loss 5.00730324 epoch total loss 5.63418102\n",
      "Trained batch 10740 batch loss 5.68637943 epoch total loss 5.63418579\n",
      "Trained batch 10741 batch loss 5.48450041 epoch total loss 5.63417196\n",
      "Trained batch 10742 batch loss 5.44857693 epoch total loss 5.6341548\n",
      "Trained batch 10743 batch loss 4.68693733 epoch total loss 5.63406658\n",
      "Trained batch 10744 batch loss 5.01880789 epoch total loss 5.63400936\n",
      "Trained batch 10745 batch loss 5.50225925 epoch total loss 5.63399744\n",
      "Trained batch 10746 batch loss 5.73610401 epoch total loss 5.6340065\n",
      "Trained batch 10747 batch loss 5.97670889 epoch total loss 5.63403845\n",
      "Trained batch 10748 batch loss 6.54386044 epoch total loss 5.63412285\n",
      "Trained batch 10749 batch loss 5.84269714 epoch total loss 5.6341424\n",
      "Trained batch 10750 batch loss 5.60260963 epoch total loss 5.63413954\n",
      "Trained batch 10751 batch loss 5.365242 epoch total loss 5.63411474\n",
      "Trained batch 10752 batch loss 5.74022 epoch total loss 5.63412428\n",
      "Trained batch 10753 batch loss 6.17519045 epoch total loss 5.63417482\n",
      "Trained batch 10754 batch loss 5.97614336 epoch total loss 5.63420677\n",
      "Trained batch 10755 batch loss 5.68769503 epoch total loss 5.63421154\n",
      "Trained batch 10756 batch loss 5.85973835 epoch total loss 5.63423252\n",
      "Trained batch 10757 batch loss 5.20270061 epoch total loss 5.63419247\n",
      "Trained batch 10758 batch loss 5.74831533 epoch total loss 5.63420296\n",
      "Trained batch 10759 batch loss 4.72838545 epoch total loss 5.63411903\n",
      "Trained batch 10760 batch loss 5.57912874 epoch total loss 5.63411379\n",
      "Trained batch 10761 batch loss 5.45182848 epoch total loss 5.63409662\n",
      "Trained batch 10762 batch loss 5.12238789 epoch total loss 5.63404894\n",
      "Trained batch 10763 batch loss 5.46504354 epoch total loss 5.6340332\n",
      "Trained batch 10764 batch loss 4.90615177 epoch total loss 5.63396597\n",
      "Trained batch 10765 batch loss 5.08747053 epoch total loss 5.63391495\n",
      "Trained batch 10766 batch loss 5.42411184 epoch total loss 5.6338954\n",
      "Trained batch 10767 batch loss 5.49043465 epoch total loss 5.63388252\n",
      "Trained batch 10768 batch loss 5.32265472 epoch total loss 5.63385344\n",
      "Trained batch 10769 batch loss 5.2560873 epoch total loss 5.63381863\n",
      "Trained batch 10770 batch loss 5.62349081 epoch total loss 5.63381767\n",
      "Trained batch 10771 batch loss 5.50778 epoch total loss 5.63380623\n",
      "Trained batch 10772 batch loss 6.21626234 epoch total loss 5.63386\n",
      "Trained batch 10773 batch loss 4.19448185 epoch total loss 5.6337266\n",
      "Trained batch 10774 batch loss 5.27138329 epoch total loss 5.63369274\n",
      "Trained batch 10775 batch loss 5.39131927 epoch total loss 5.63367033\n",
      "Trained batch 10776 batch loss 5.28430843 epoch total loss 5.63363791\n",
      "Trained batch 10777 batch loss 5.60395 epoch total loss 5.63363552\n",
      "Trained batch 10778 batch loss 5.6176486 epoch total loss 5.63363361\n",
      "Trained batch 10779 batch loss 5.14974546 epoch total loss 5.63358879\n",
      "Trained batch 10780 batch loss 5.80478239 epoch total loss 5.63360453\n",
      "Trained batch 10781 batch loss 5.91067791 epoch total loss 5.63363028\n",
      "Trained batch 10782 batch loss 5.68656254 epoch total loss 5.63363504\n",
      "Trained batch 10783 batch loss 6.80271435 epoch total loss 5.63374329\n",
      "Trained batch 10784 batch loss 5.92615604 epoch total loss 5.63377047\n",
      "Trained batch 10785 batch loss 4.89568329 epoch total loss 5.63370228\n",
      "Trained batch 10786 batch loss 6.69368267 epoch total loss 5.63380051\n",
      "Trained batch 10787 batch loss 6.06439 epoch total loss 5.63384\n",
      "Trained batch 10788 batch loss 6.26796103 epoch total loss 5.63389921\n",
      "Trained batch 10789 batch loss 6.09345531 epoch total loss 5.63394165\n",
      "Trained batch 10790 batch loss 5.40114975 epoch total loss 5.63392\n",
      "Trained batch 10791 batch loss 5.76843 epoch total loss 5.63393307\n",
      "Trained batch 10792 batch loss 5.34184265 epoch total loss 5.63390589\n",
      "Trained batch 10793 batch loss 5.30274343 epoch total loss 5.63387537\n",
      "Trained batch 10794 batch loss 4.41967583 epoch total loss 5.63376284\n",
      "Trained batch 10795 batch loss 4.24851227 epoch total loss 5.63363457\n",
      "Trained batch 10796 batch loss 5.79134941 epoch total loss 5.63364935\n",
      "Trained batch 10797 batch loss 5.6472621 epoch total loss 5.63365078\n",
      "Trained batch 10798 batch loss 5.72143555 epoch total loss 5.63365889\n",
      "Trained batch 10799 batch loss 5.75553226 epoch total loss 5.63367033\n",
      "Trained batch 10800 batch loss 6.07609272 epoch total loss 5.63371086\n",
      "Trained batch 10801 batch loss 6.30027056 epoch total loss 5.63377285\n",
      "Trained batch 10802 batch loss 6.14413643 epoch total loss 5.63382\n",
      "Trained batch 10803 batch loss 6.06980705 epoch total loss 5.63386059\n",
      "Trained batch 10804 batch loss 5.64184284 epoch total loss 5.63386106\n",
      "Trained batch 10805 batch loss 6.46803141 epoch total loss 5.63393831\n",
      "Trained batch 10806 batch loss 4.60179234 epoch total loss 5.63384295\n",
      "Trained batch 10807 batch loss 5.59623909 epoch total loss 5.63383961\n",
      "Trained batch 10808 batch loss 5.93124056 epoch total loss 5.63386679\n",
      "Trained batch 10809 batch loss 6.05809593 epoch total loss 5.63390589\n",
      "Trained batch 10810 batch loss 6.06330872 epoch total loss 5.63394594\n",
      "Trained batch 10811 batch loss 5.86860132 epoch total loss 5.6339674\n",
      "Trained batch 10812 batch loss 5.35878277 epoch total loss 5.63394165\n",
      "Trained batch 10813 batch loss 5.39809752 epoch total loss 5.63392\n",
      "Trained batch 10814 batch loss 5.31553888 epoch total loss 5.63389063\n",
      "Trained batch 10815 batch loss 6.02856874 epoch total loss 5.63392735\n",
      "Trained batch 10816 batch loss 5.20912361 epoch total loss 5.63388824\n",
      "Trained batch 10817 batch loss 5.96544743 epoch total loss 5.63391876\n",
      "Trained batch 10818 batch loss 6.15218163 epoch total loss 5.63396645\n",
      "Trained batch 10819 batch loss 5.91485786 epoch total loss 5.6339922\n",
      "Trained batch 10820 batch loss 6.29789305 epoch total loss 5.63405371\n",
      "Trained batch 10821 batch loss 5.47999287 epoch total loss 5.6340394\n",
      "Trained batch 10822 batch loss 5.87804 epoch total loss 5.63406229\n",
      "Trained batch 10823 batch loss 6.0629859 epoch total loss 5.63410187\n",
      "Trained batch 10824 batch loss 6.4868474 epoch total loss 5.63418055\n",
      "Trained batch 10825 batch loss 6.09105921 epoch total loss 5.63422251\n",
      "Trained batch 10826 batch loss 5.55897427 epoch total loss 5.63421583\n",
      "Trained batch 10827 batch loss 5.55329084 epoch total loss 5.6342082\n",
      "Trained batch 10828 batch loss 5.42477179 epoch total loss 5.63418913\n",
      "Trained batch 10829 batch loss 6.24624538 epoch total loss 5.63424587\n",
      "Trained batch 10830 batch loss 5.37633896 epoch total loss 5.63422155\n",
      "Trained batch 10831 batch loss 5.42457962 epoch total loss 5.63420248\n",
      "Trained batch 10832 batch loss 5.418 epoch total loss 5.63418245\n",
      "Trained batch 10833 batch loss 5.73419237 epoch total loss 5.63419151\n",
      "Trained batch 10834 batch loss 5.27655602 epoch total loss 5.63415861\n",
      "Trained batch 10835 batch loss 5.85713243 epoch total loss 5.63417912\n",
      "Trained batch 10836 batch loss 5.45996904 epoch total loss 5.63416338\n",
      "Trained batch 10837 batch loss 5.26268959 epoch total loss 5.63412905\n",
      "Trained batch 10838 batch loss 5.57188511 epoch total loss 5.63412285\n",
      "Trained batch 10839 batch loss 5.76058912 epoch total loss 5.63413477\n",
      "Trained batch 10840 batch loss 5.42939186 epoch total loss 5.6341157\n",
      "Trained batch 10841 batch loss 5.84246254 epoch total loss 5.63413525\n",
      "Trained batch 10842 batch loss 4.96726513 epoch total loss 5.63407373\n",
      "Trained batch 10843 batch loss 5.55364704 epoch total loss 5.63406658\n",
      "Trained batch 10844 batch loss 5.51820469 epoch total loss 5.63405609\n",
      "Trained batch 10845 batch loss 5.73147392 epoch total loss 5.63406467\n",
      "Trained batch 10846 batch loss 4.92397404 epoch total loss 5.63399935\n",
      "Trained batch 10847 batch loss 5.18918276 epoch total loss 5.63395834\n",
      "Trained batch 10848 batch loss 5.26388454 epoch total loss 5.63392448\n",
      "Trained batch 10849 batch loss 4.75244379 epoch total loss 5.63384342\n",
      "Trained batch 10850 batch loss 5.22175026 epoch total loss 5.63380527\n",
      "Trained batch 10851 batch loss 5.89994049 epoch total loss 5.63382959\n",
      "Trained batch 10852 batch loss 4.61034203 epoch total loss 5.63373566\n",
      "Trained batch 10853 batch loss 3.98585963 epoch total loss 5.63358355\n",
      "Trained batch 10854 batch loss 5.41399384 epoch total loss 5.63356304\n",
      "Trained batch 10855 batch loss 4.69549561 epoch total loss 5.63347673\n",
      "Trained batch 10856 batch loss 5.70999956 epoch total loss 5.63348389\n",
      "Trained batch 10857 batch loss 6.57712841 epoch total loss 5.63357115\n",
      "Trained batch 10858 batch loss 5.5846653 epoch total loss 5.63356638\n",
      "Trained batch 10859 batch loss 5.63385582 epoch total loss 5.63356638\n",
      "Trained batch 10860 batch loss 6.22191191 epoch total loss 5.63362074\n",
      "Trained batch 10861 batch loss 6.443223 epoch total loss 5.63369513\n",
      "Trained batch 10862 batch loss 5.75273228 epoch total loss 5.63370609\n",
      "Trained batch 10863 batch loss 6.01846218 epoch total loss 5.63374186\n",
      "Trained batch 10864 batch loss 5.43660498 epoch total loss 5.63372374\n",
      "Trained batch 10865 batch loss 5.60933399 epoch total loss 5.63372135\n",
      "Trained batch 10866 batch loss 5.78938675 epoch total loss 5.63373566\n",
      "Trained batch 10867 batch loss 5.88967 epoch total loss 5.6337595\n",
      "Trained batch 10868 batch loss 4.95965672 epoch total loss 5.63369751\n",
      "Trained batch 10869 batch loss 6.35406256 epoch total loss 5.63376379\n",
      "Trained batch 10870 batch loss 6.24952555 epoch total loss 5.63382053\n",
      "Trained batch 10871 batch loss 6.38406086 epoch total loss 5.6338892\n",
      "Trained batch 10872 batch loss 6.36056423 epoch total loss 5.63395596\n",
      "Trained batch 10873 batch loss 6.58299541 epoch total loss 5.63404322\n",
      "Trained batch 10874 batch loss 6.3874197 epoch total loss 5.63411236\n",
      "Trained batch 10875 batch loss 5.40255165 epoch total loss 5.63409138\n",
      "Trained batch 10876 batch loss 5.31925201 epoch total loss 5.63406229\n",
      "Trained batch 10877 batch loss 4.36440945 epoch total loss 5.63394547\n",
      "Trained batch 10878 batch loss 5.05337381 epoch total loss 5.63389254\n",
      "Trained batch 10879 batch loss 5.17846966 epoch total loss 5.63385057\n",
      "Trained batch 10880 batch loss 4.65524101 epoch total loss 5.63376093\n",
      "Trained batch 10881 batch loss 5.77243376 epoch total loss 5.63377333\n",
      "Trained batch 10882 batch loss 6.0539484 epoch total loss 5.63381243\n",
      "Trained batch 10883 batch loss 6.30635071 epoch total loss 5.63387394\n",
      "Trained batch 10884 batch loss 5.9326 epoch total loss 5.6339016\n",
      "Trained batch 10885 batch loss 5.64556026 epoch total loss 5.63390255\n",
      "Trained batch 10886 batch loss 5.71181393 epoch total loss 5.63390923\n",
      "Trained batch 10887 batch loss 4.96818495 epoch total loss 5.63384819\n",
      "Trained batch 10888 batch loss 6.08627892 epoch total loss 5.63388968\n",
      "Trained batch 10889 batch loss 5.46397877 epoch total loss 5.63387442\n",
      "Trained batch 10890 batch loss 6.27255297 epoch total loss 5.63393307\n",
      "Trained batch 10891 batch loss 5.63686657 epoch total loss 5.63393354\n",
      "Trained batch 10892 batch loss 5.30711269 epoch total loss 5.6339035\n",
      "Trained batch 10893 batch loss 5.47267532 epoch total loss 5.63388872\n",
      "Trained batch 10894 batch loss 5.66387796 epoch total loss 5.63389158\n",
      "Trained batch 10895 batch loss 6.31419373 epoch total loss 5.63395357\n",
      "Trained batch 10896 batch loss 5.62547684 epoch total loss 5.63395309\n",
      "Trained batch 10897 batch loss 6.38764143 epoch total loss 5.63402176\n",
      "Trained batch 10898 batch loss 6.53713608 epoch total loss 5.63410521\n",
      "Trained batch 10899 batch loss 5.65635109 epoch total loss 5.63410711\n",
      "Trained batch 10900 batch loss 5.37563419 epoch total loss 5.63408327\n",
      "Trained batch 10901 batch loss 4.72742081 epoch total loss 5.634\n",
      "Trained batch 10902 batch loss 5.38715601 epoch total loss 5.63397741\n",
      "Trained batch 10903 batch loss 5.82705593 epoch total loss 5.63399506\n",
      "Trained batch 10904 batch loss 5.68834639 epoch total loss 5.6340003\n",
      "Trained batch 10905 batch loss 6.23046732 epoch total loss 5.63405466\n",
      "Trained batch 10906 batch loss 6.15987396 epoch total loss 5.63410282\n",
      "Trained batch 10907 batch loss 6.66431904 epoch total loss 5.63419724\n",
      "Trained batch 10908 batch loss 6.57631207 epoch total loss 5.63428402\n",
      "Trained batch 10909 batch loss 5.53166103 epoch total loss 5.63427448\n",
      "Trained batch 10910 batch loss 5.69224405 epoch total loss 5.63427973\n",
      "Trained batch 10911 batch loss 4.76224184 epoch total loss 5.63419962\n",
      "Trained batch 10912 batch loss 4.79722691 epoch total loss 5.63412285\n",
      "Trained batch 10913 batch loss 4.86137 epoch total loss 5.63405228\n",
      "Trained batch 10914 batch loss 4.09616852 epoch total loss 5.63391161\n",
      "Trained batch 10915 batch loss 4.55714 epoch total loss 5.6338129\n",
      "Trained batch 10916 batch loss 4.48467302 epoch total loss 5.633708\n",
      "Trained batch 10917 batch loss 4.54141903 epoch total loss 5.63360786\n",
      "Trained batch 10918 batch loss 4.10712433 epoch total loss 5.63346815\n",
      "Trained batch 10919 batch loss 4.61121941 epoch total loss 5.63337421\n",
      "Trained batch 10920 batch loss 4.94836 epoch total loss 5.63331175\n",
      "Trained batch 10921 batch loss 6.05105686 epoch total loss 5.63335\n",
      "Trained batch 10922 batch loss 5.72061825 epoch total loss 5.63335752\n",
      "Trained batch 10923 batch loss 5.48181438 epoch total loss 5.6333437\n",
      "Trained batch 10924 batch loss 5.47013378 epoch total loss 5.63332844\n",
      "Trained batch 10925 batch loss 5.91529751 epoch total loss 5.63335419\n",
      "Trained batch 10926 batch loss 5.47474194 epoch total loss 5.63334\n",
      "Trained batch 10927 batch loss 5.80916214 epoch total loss 5.63335609\n",
      "Trained batch 10928 batch loss 5.98039722 epoch total loss 5.63338757\n",
      "Trained batch 10929 batch loss 6.01106071 epoch total loss 5.63342237\n",
      "Trained batch 10930 batch loss 5.52059793 epoch total loss 5.63341188\n",
      "Trained batch 10931 batch loss 6.52175856 epoch total loss 5.63349342\n",
      "Trained batch 10932 batch loss 5.11588097 epoch total loss 5.63344622\n",
      "Trained batch 10933 batch loss 5.83704567 epoch total loss 5.63346434\n",
      "Trained batch 10934 batch loss 6.03475094 epoch total loss 5.63350153\n",
      "Trained batch 10935 batch loss 5.90503502 epoch total loss 5.63352633\n",
      "Trained batch 10936 batch loss 6.14392757 epoch total loss 5.63357306\n",
      "Trained batch 10937 batch loss 6.36670589 epoch total loss 5.63364\n",
      "Trained batch 10938 batch loss 5.59592819 epoch total loss 5.63363695\n",
      "Trained batch 10939 batch loss 5.61745453 epoch total loss 5.63363504\n",
      "Trained batch 10940 batch loss 5.99778271 epoch total loss 5.63366842\n",
      "Trained batch 10941 batch loss 5.72099972 epoch total loss 5.63367653\n",
      "Trained batch 10942 batch loss 5.71240616 epoch total loss 5.63368368\n",
      "Trained batch 10943 batch loss 5.86876 epoch total loss 5.63370514\n",
      "Trained batch 10944 batch loss 5.61039257 epoch total loss 5.63370275\n",
      "Trained batch 10945 batch loss 5.7876606 epoch total loss 5.63371706\n",
      "Trained batch 10946 batch loss 5.63690567 epoch total loss 5.63371706\n",
      "Trained batch 10947 batch loss 5.42803812 epoch total loss 5.63369846\n",
      "Trained batch 10948 batch loss 5.6159277 epoch total loss 5.63369703\n",
      "Trained batch 10949 batch loss 5.22663403 epoch total loss 5.63366\n",
      "Trained batch 10950 batch loss 5.87375641 epoch total loss 5.63368177\n",
      "Trained batch 10951 batch loss 5.70449448 epoch total loss 5.63368797\n",
      "Trained batch 10952 batch loss 5.4068861 epoch total loss 5.63366747\n",
      "Trained batch 10953 batch loss 4.51513767 epoch total loss 5.63356543\n",
      "Trained batch 10954 batch loss 4.23957443 epoch total loss 5.63343811\n",
      "Trained batch 10955 batch loss 5.43732738 epoch total loss 5.63342\n",
      "Trained batch 10956 batch loss 6.64522552 epoch total loss 5.6335125\n",
      "Trained batch 10957 batch loss 5.98623371 epoch total loss 5.63354445\n",
      "Trained batch 10958 batch loss 6.04914284 epoch total loss 5.63358259\n",
      "Trained batch 10959 batch loss 6.36498785 epoch total loss 5.63364887\n",
      "Trained batch 10960 batch loss 6.38098812 epoch total loss 5.63371754\n",
      "Trained batch 10961 batch loss 5.89371157 epoch total loss 5.63374138\n",
      "Trained batch 10962 batch loss 6.10426617 epoch total loss 5.63378429\n",
      "Trained batch 10963 batch loss 5.70860672 epoch total loss 5.63379097\n",
      "Trained batch 10964 batch loss 6.30803728 epoch total loss 5.63385248\n",
      "Trained batch 10965 batch loss 4.64266491 epoch total loss 5.63376236\n",
      "Trained batch 10966 batch loss 4.17657757 epoch total loss 5.63362932\n",
      "Trained batch 10967 batch loss 4.5766592 epoch total loss 5.633533\n",
      "Trained batch 10968 batch loss 4.26450443 epoch total loss 5.63340855\n",
      "Trained batch 10969 batch loss 4.73442698 epoch total loss 5.63332653\n",
      "Trained batch 10970 batch loss 5.03939199 epoch total loss 5.63327217\n",
      "Trained batch 10971 batch loss 6.03312826 epoch total loss 5.63330841\n",
      "Trained batch 10972 batch loss 5.66076422 epoch total loss 5.63331079\n",
      "Trained batch 10973 batch loss 5.65628958 epoch total loss 5.63331318\n",
      "Trained batch 10974 batch loss 5.93485355 epoch total loss 5.63334036\n",
      "Trained batch 10975 batch loss 5.83538818 epoch total loss 5.63335896\n",
      "Trained batch 10976 batch loss 5.77291822 epoch total loss 5.63337183\n",
      "Trained batch 10977 batch loss 6.06798458 epoch total loss 5.63341093\n",
      "Trained batch 10978 batch loss 5.34681892 epoch total loss 5.63338518\n",
      "Trained batch 10979 batch loss 5.92627716 epoch total loss 5.63341188\n",
      "Trained batch 10980 batch loss 5.89061069 epoch total loss 5.63343525\n",
      "Trained batch 10981 batch loss 5.6218214 epoch total loss 5.63343382\n",
      "Trained batch 10982 batch loss 6.04486656 epoch total loss 5.63347101\n",
      "Trained batch 10983 batch loss 5.51286554 epoch total loss 5.63346\n",
      "Trained batch 10984 batch loss 5.64519644 epoch total loss 5.633461\n",
      "Trained batch 10985 batch loss 6.54191971 epoch total loss 5.63354397\n",
      "Trained batch 10986 batch loss 6.05282688 epoch total loss 5.63358212\n",
      "Trained batch 10987 batch loss 6.38830948 epoch total loss 5.63365078\n",
      "Trained batch 10988 batch loss 5.47708511 epoch total loss 5.63363647\n",
      "Trained batch 10989 batch loss 5.55580759 epoch total loss 5.63362932\n",
      "Trained batch 10990 batch loss 4.61141396 epoch total loss 5.63353634\n",
      "Trained batch 10991 batch loss 4.74191189 epoch total loss 5.63345528\n",
      "Trained batch 10992 batch loss 4.46537495 epoch total loss 5.63334894\n",
      "Trained batch 10993 batch loss 4.93952608 epoch total loss 5.633286\n",
      "Trained batch 10994 batch loss 3.15470982 epoch total loss 5.63306093\n",
      "Trained batch 10995 batch loss 4.8600297 epoch total loss 5.63299036\n",
      "Trained batch 10996 batch loss 3.94510174 epoch total loss 5.63283682\n",
      "Trained batch 10997 batch loss 3.93196964 epoch total loss 5.63268232\n",
      "Trained batch 10998 batch loss 4.1628828 epoch total loss 5.63254881\n",
      "Trained batch 10999 batch loss 5.54355669 epoch total loss 5.6325407\n",
      "Trained batch 11000 batch loss 6.32810783 epoch total loss 5.63260412\n",
      "Trained batch 11001 batch loss 6.72414494 epoch total loss 5.6327033\n",
      "Trained batch 11002 batch loss 6.43211699 epoch total loss 5.63277578\n",
      "Trained batch 11003 batch loss 6.3977747 epoch total loss 5.6328454\n",
      "Trained batch 11004 batch loss 6.57935619 epoch total loss 5.63293123\n",
      "Trained batch 11005 batch loss 6.33271313 epoch total loss 5.63299513\n",
      "Trained batch 11006 batch loss 5.84406185 epoch total loss 5.6330142\n",
      "Trained batch 11007 batch loss 4.55729 epoch total loss 5.63291645\n",
      "Trained batch 11008 batch loss 6.12739 epoch total loss 5.63296175\n",
      "Trained batch 11009 batch loss 5.95921326 epoch total loss 5.63299131\n",
      "Trained batch 11010 batch loss 6.22942448 epoch total loss 5.63304567\n",
      "Trained batch 11011 batch loss 5.47973585 epoch total loss 5.63303185\n",
      "Trained batch 11012 batch loss 5.95532322 epoch total loss 5.63306093\n",
      "Trained batch 11013 batch loss 5.82359076 epoch total loss 5.63307858\n",
      "Trained batch 11014 batch loss 5.7445755 epoch total loss 5.63308859\n",
      "Trained batch 11015 batch loss 5.81657124 epoch total loss 5.63310528\n",
      "Trained batch 11016 batch loss 5.31318521 epoch total loss 5.63307619\n",
      "Trained batch 11017 batch loss 6.08104897 epoch total loss 5.6331172\n",
      "Trained batch 11018 batch loss 5.58495808 epoch total loss 5.63311291\n",
      "Trained batch 11019 batch loss 6.24994 epoch total loss 5.6331687\n",
      "Trained batch 11020 batch loss 5.71769714 epoch total loss 5.63317633\n",
      "Trained batch 11021 batch loss 5.34918976 epoch total loss 5.63315058\n",
      "Trained batch 11022 batch loss 5.26087141 epoch total loss 5.63311672\n",
      "Trained batch 11023 batch loss 5.28494215 epoch total loss 5.63308525\n",
      "Trained batch 11024 batch loss 5.16939831 epoch total loss 5.63304329\n",
      "Trained batch 11025 batch loss 5.46066666 epoch total loss 5.63302755\n",
      "Trained batch 11026 batch loss 5.46584463 epoch total loss 5.63301229\n",
      "Trained batch 11027 batch loss 5.75387 epoch total loss 5.63302326\n",
      "Trained batch 11028 batch loss 5.35736656 epoch total loss 5.63299799\n",
      "Trained batch 11029 batch loss 5.14833927 epoch total loss 5.63295412\n",
      "Trained batch 11030 batch loss 5.02318239 epoch total loss 5.63289881\n",
      "Trained batch 11031 batch loss 5.20470858 epoch total loss 5.63286\n",
      "Trained batch 11032 batch loss 5.15357256 epoch total loss 5.63281631\n",
      "Trained batch 11033 batch loss 5.30896711 epoch total loss 5.63278723\n",
      "Trained batch 11034 batch loss 5.15820885 epoch total loss 5.63274431\n",
      "Trained batch 11035 batch loss 5.47434235 epoch total loss 5.63272953\n",
      "Trained batch 11036 batch loss 5.37072706 epoch total loss 5.63270617\n",
      "Trained batch 11037 batch loss 4.86296129 epoch total loss 5.63263607\n",
      "Trained batch 11038 batch loss 4.62745237 epoch total loss 5.63254547\n",
      "Trained batch 11039 batch loss 5.01871204 epoch total loss 5.63248968\n",
      "Trained batch 11040 batch loss 5.05439663 epoch total loss 5.63243723\n",
      "Trained batch 11041 batch loss 5.52863741 epoch total loss 5.63242769\n",
      "Trained batch 11042 batch loss 5.18740082 epoch total loss 5.63238764\n",
      "Trained batch 11043 batch loss 5.19753456 epoch total loss 5.63234854\n",
      "Trained batch 11044 batch loss 5.37419033 epoch total loss 5.63232517\n",
      "Trained batch 11045 batch loss 5.56710148 epoch total loss 5.63231897\n",
      "Trained batch 11046 batch loss 5.24685526 epoch total loss 5.63228416\n",
      "Trained batch 11047 batch loss 5.79733181 epoch total loss 5.63229895\n",
      "Trained batch 11048 batch loss 5.25681973 epoch total loss 5.63226509\n",
      "Trained batch 11049 batch loss 5.3495779 epoch total loss 5.63223934\n",
      "Trained batch 11050 batch loss 5.29674 epoch total loss 5.6322093\n",
      "Trained batch 11051 batch loss 4.67814159 epoch total loss 5.63212299\n",
      "Trained batch 11052 batch loss 5.24960136 epoch total loss 5.63208818\n",
      "Trained batch 11053 batch loss 5.21762085 epoch total loss 5.63205099\n",
      "Trained batch 11054 batch loss 4.8448987 epoch total loss 5.63197947\n",
      "Trained batch 11055 batch loss 4.8299408 epoch total loss 5.63190699\n",
      "Trained batch 11056 batch loss 5.31247854 epoch total loss 5.6318779\n",
      "Trained batch 11057 batch loss 5.28632116 epoch total loss 5.63184643\n",
      "Trained batch 11058 batch loss 4.86815357 epoch total loss 5.63177729\n",
      "Trained batch 11059 batch loss 4.88083172 epoch total loss 5.63170958\n",
      "Trained batch 11060 batch loss 5.46496153 epoch total loss 5.63169432\n",
      "Trained batch 11061 batch loss 5.69721317 epoch total loss 5.6317\n",
      "Trained batch 11062 batch loss 4.89647484 epoch total loss 5.63163328\n",
      "Trained batch 11063 batch loss 4.29745388 epoch total loss 5.63151264\n",
      "Trained batch 11064 batch loss 5.63945 epoch total loss 5.6315136\n",
      "Trained batch 11065 batch loss 5.25944233 epoch total loss 5.63147974\n",
      "Trained batch 11066 batch loss 5.59084702 epoch total loss 5.63147593\n",
      "Trained batch 11067 batch loss 5.07882309 epoch total loss 5.63142586\n",
      "Trained batch 11068 batch loss 4.09806824 epoch total loss 5.63128757\n",
      "Trained batch 11069 batch loss 5.15315151 epoch total loss 5.63124418\n",
      "Trained batch 11070 batch loss 3.59931326 epoch total loss 5.6310606\n",
      "Trained batch 11071 batch loss 5.70527887 epoch total loss 5.63106728\n",
      "Trained batch 11072 batch loss 5.57816696 epoch total loss 5.63106251\n",
      "Trained batch 11073 batch loss 5.46960545 epoch total loss 5.63104773\n",
      "Trained batch 11074 batch loss 5.6864481 epoch total loss 5.63105297\n",
      "Trained batch 11075 batch loss 5.29372072 epoch total loss 5.63102245\n",
      "Trained batch 11076 batch loss 5.40949774 epoch total loss 5.63100243\n",
      "Trained batch 11077 batch loss 4.29167843 epoch total loss 5.63088179\n",
      "Trained batch 11078 batch loss 5.42960072 epoch total loss 5.63086367\n",
      "Trained batch 11079 batch loss 5.5677309 epoch total loss 5.63085794\n",
      "Trained batch 11080 batch loss 4.89972305 epoch total loss 5.63079166\n",
      "Trained batch 11081 batch loss 5.33631706 epoch total loss 5.63076496\n",
      "Trained batch 11082 batch loss 4.9829073 epoch total loss 5.63070679\n",
      "Trained batch 11083 batch loss 5.32083464 epoch total loss 5.63067865\n",
      "Trained batch 11084 batch loss 5.88540173 epoch total loss 5.63070202\n",
      "Trained batch 11085 batch loss 6.18240595 epoch total loss 5.63075161\n",
      "Trained batch 11086 batch loss 5.2629137 epoch total loss 5.63071823\n",
      "Trained batch 11087 batch loss 5.81969929 epoch total loss 5.6307354\n",
      "Trained batch 11088 batch loss 5.02904415 epoch total loss 5.63068104\n",
      "Trained batch 11089 batch loss 5.01531935 epoch total loss 5.63062572\n",
      "Trained batch 11090 batch loss 5.36291885 epoch total loss 5.63060141\n",
      "Trained batch 11091 batch loss 5.44684601 epoch total loss 5.63058472\n",
      "Trained batch 11092 batch loss 5.75539589 epoch total loss 5.63059616\n",
      "Trained batch 11093 batch loss 5.03021717 epoch total loss 5.6305418\n",
      "Trained batch 11094 batch loss 5.80356026 epoch total loss 5.63055754\n",
      "Trained batch 11095 batch loss 5.65859509 epoch total loss 5.6305604\n",
      "Trained batch 11096 batch loss 6.12077618 epoch total loss 5.63060427\n",
      "Trained batch 11097 batch loss 5.53485298 epoch total loss 5.63059568\n",
      "Trained batch 11098 batch loss 5.14616394 epoch total loss 5.63055229\n",
      "Trained batch 11099 batch loss 5.31679487 epoch total loss 5.63052368\n",
      "Trained batch 11100 batch loss 5.78372288 epoch total loss 5.63053751\n",
      "Trained batch 11101 batch loss 5.87143898 epoch total loss 5.63055944\n",
      "Trained batch 11102 batch loss 6.25117 epoch total loss 5.63061523\n",
      "Trained batch 11103 batch loss 5.94383812 epoch total loss 5.63064337\n",
      "Trained batch 11104 batch loss 5.4870944 epoch total loss 5.63063049\n",
      "Trained batch 11105 batch loss 5.64057827 epoch total loss 5.63063145\n",
      "Trained batch 11106 batch loss 5.45475197 epoch total loss 5.63061571\n",
      "Trained batch 11107 batch loss 5.43972 epoch total loss 5.63059855\n",
      "Trained batch 11108 batch loss 5.23738289 epoch total loss 5.63056326\n",
      "Trained batch 11109 batch loss 5.41613 epoch total loss 5.63054419\n",
      "Trained batch 11110 batch loss 5.68129873 epoch total loss 5.63054848\n",
      "Trained batch 11111 batch loss 5.73264122 epoch total loss 5.63055801\n",
      "Trained batch 11112 batch loss 5.22726488 epoch total loss 5.63052177\n",
      "Trained batch 11113 batch loss 4.42512321 epoch total loss 5.63041306\n",
      "Trained batch 11114 batch loss 5.43970251 epoch total loss 5.63039637\n",
      "Trained batch 11115 batch loss 6.18919754 epoch total loss 5.63044643\n",
      "Trained batch 11116 batch loss 4.96909 epoch total loss 5.63038683\n",
      "Trained batch 11117 batch loss 5.93093872 epoch total loss 5.63041353\n",
      "Trained batch 11118 batch loss 4.92461681 epoch total loss 5.63035\n",
      "Trained batch 11119 batch loss 5.69216251 epoch total loss 5.63035583\n",
      "Trained batch 11120 batch loss 5.12446117 epoch total loss 5.63031054\n",
      "Trained batch 11121 batch loss 5.77504826 epoch total loss 5.63032341\n",
      "Trained batch 11122 batch loss 4.57497168 epoch total loss 5.63022804\n",
      "Trained batch 11123 batch loss 5.05224323 epoch total loss 5.63017607\n",
      "Epoch 3 train loss 5.630176067352295\n",
      "Validated batch 1 batch loss 5.05865097\n",
      "Validated batch 2 batch loss 6.32362843\n",
      "Validated batch 3 batch loss 7.03541517\n",
      "Validated batch 4 batch loss 5.75680399\n",
      "Validated batch 5 batch loss 5.2039156\n",
      "Validated batch 6 batch loss 6.72413921\n",
      "Validated batch 7 batch loss 6.37336254\n",
      "Validated batch 8 batch loss 6.14874411\n",
      "Validated batch 9 batch loss 6.22354603\n",
      "Validated batch 10 batch loss 6.04268742\n",
      "Validated batch 11 batch loss 5.59554291\n",
      "Validated batch 12 batch loss 5.84123421\n",
      "Validated batch 13 batch loss 6.5821681\n",
      "Validated batch 14 batch loss 5.91993332\n",
      "Validated batch 15 batch loss 5.82822943\n",
      "Validated batch 16 batch loss 5.50174236\n",
      "Validated batch 17 batch loss 5.91011429\n",
      "Validated batch 18 batch loss 6.05083561\n",
      "Validated batch 19 batch loss 6.986866\n",
      "Validated batch 20 batch loss 6.03333759\n",
      "Validated batch 21 batch loss 6.50975513\n",
      "Validated batch 22 batch loss 4.41197\n",
      "Validated batch 23 batch loss 5.97128534\n",
      "Validated batch 24 batch loss 4.88404\n",
      "Validated batch 25 batch loss 5.53026295\n",
      "Validated batch 26 batch loss 7.20463467\n",
      "Validated batch 27 batch loss 6.28063\n",
      "Validated batch 28 batch loss 7.15431213\n",
      "Validated batch 29 batch loss 5.91370106\n",
      "Validated batch 30 batch loss 7.33717299\n",
      "Validated batch 31 batch loss 7.55507946\n",
      "Validated batch 32 batch loss 5.71025085\n",
      "Validated batch 33 batch loss 5.92023468\n",
      "Validated batch 34 batch loss 5.9101696\n",
      "Validated batch 35 batch loss 5.91398239\n",
      "Validated batch 36 batch loss 6.60334396\n",
      "Validated batch 37 batch loss 5.91976643\n",
      "Validated batch 38 batch loss 6.54200029\n",
      "Validated batch 39 batch loss 6.70266485\n",
      "Validated batch 40 batch loss 5.72341728\n",
      "Validated batch 41 batch loss 5.96345377\n",
      "Validated batch 42 batch loss 6.03967333\n",
      "Validated batch 43 batch loss 5.48987\n",
      "Validated batch 44 batch loss 6.37275505\n",
      "Validated batch 45 batch loss 5.48102713\n",
      "Validated batch 46 batch loss 6.49026966\n",
      "Validated batch 47 batch loss 6.51326942\n",
      "Validated batch 48 batch loss 6.47155714\n",
      "Validated batch 49 batch loss 6.24002\n",
      "Validated batch 50 batch loss 6.3412261\n",
      "Validated batch 51 batch loss 6.27015495\n",
      "Validated batch 52 batch loss 5.99680424\n",
      "Validated batch 53 batch loss 6.29294872\n",
      "Validated batch 54 batch loss 6.27333355\n",
      "Validated batch 55 batch loss 6.57084\n",
      "Validated batch 56 batch loss 6.98320723\n",
      "Validated batch 57 batch loss 4.69004917\n",
      "Validated batch 58 batch loss 5.87168837\n",
      "Validated batch 59 batch loss 6.86102962\n",
      "Validated batch 60 batch loss 6.76995\n",
      "Validated batch 61 batch loss 6.85218334\n",
      "Validated batch 62 batch loss 5.67591095\n",
      "Validated batch 63 batch loss 6.04792547\n",
      "Validated batch 64 batch loss 5.80769253\n",
      "Validated batch 65 batch loss 6.75679255\n",
      "Validated batch 66 batch loss 5.44882774\n",
      "Validated batch 67 batch loss 7.71063375\n",
      "Validated batch 68 batch loss 6.05169\n",
      "Validated batch 69 batch loss 6.37980461\n",
      "Validated batch 70 batch loss 5.93741417\n",
      "Validated batch 71 batch loss 5.72287178\n",
      "Validated batch 72 batch loss 6.13073444\n",
      "Validated batch 73 batch loss 6.17669916\n",
      "Validated batch 74 batch loss 6.03186417\n",
      "Validated batch 75 batch loss 5.94728851\n",
      "Validated batch 76 batch loss 6.66763973\n",
      "Validated batch 77 batch loss 5.94680119\n",
      "Validated batch 78 batch loss 5.92903757\n",
      "Validated batch 79 batch loss 6.75974274\n",
      "Validated batch 80 batch loss 6.88059521\n",
      "Validated batch 81 batch loss 6.19698238\n",
      "Validated batch 82 batch loss 6.27253485\n",
      "Validated batch 83 batch loss 6.60525322\n",
      "Validated batch 84 batch loss 5.89102459\n",
      "Validated batch 85 batch loss 6.98728561\n",
      "Validated batch 86 batch loss 6.37831116\n",
      "Validated batch 87 batch loss 7.27629566\n",
      "Validated batch 88 batch loss 6.68399715\n",
      "Validated batch 89 batch loss 5.81605\n",
      "Validated batch 90 batch loss 7.59624386\n",
      "Validated batch 91 batch loss 5.87383747\n",
      "Validated batch 92 batch loss 5.8855176\n",
      "Validated batch 93 batch loss 6.38780403\n",
      "Validated batch 94 batch loss 5.83400345\n",
      "Validated batch 95 batch loss 5.60070038\n",
      "Validated batch 96 batch loss 5.91395092\n",
      "Validated batch 97 batch loss 5.18474197\n",
      "Validated batch 98 batch loss 5.69459915\n",
      "Validated batch 99 batch loss 5.7653594\n",
      "Validated batch 100 batch loss 6.90616608\n",
      "Validated batch 101 batch loss 7.03393412\n",
      "Validated batch 102 batch loss 6.17327213\n",
      "Validated batch 103 batch loss 6.02084589\n",
      "Validated batch 104 batch loss 6.05522728\n",
      "Validated batch 105 batch loss 6.92723751\n",
      "Validated batch 106 batch loss 7.31165552\n",
      "Validated batch 107 batch loss 5.76987\n",
      "Validated batch 108 batch loss 5.90526\n",
      "Validated batch 109 batch loss 6.6232152\n",
      "Validated batch 110 batch loss 6.27838135\n",
      "Validated batch 111 batch loss 6.39875317\n",
      "Validated batch 112 batch loss 6.09516048\n",
      "Validated batch 113 batch loss 6.67623806\n",
      "Validated batch 114 batch loss 5.75956964\n",
      "Validated batch 115 batch loss 5.85322046\n",
      "Validated batch 116 batch loss 6.52827883\n",
      "Validated batch 117 batch loss 5.703\n",
      "Validated batch 118 batch loss 5.6303463\n",
      "Validated batch 119 batch loss 6.21150637\n",
      "Validated batch 120 batch loss 6.67517233\n",
      "Validated batch 121 batch loss 5.47068405\n",
      "Validated batch 122 batch loss 5.90947247\n",
      "Validated batch 123 batch loss 6.62282372\n",
      "Validated batch 124 batch loss 6.91772604\n",
      "Validated batch 125 batch loss 5.79160118\n",
      "Validated batch 126 batch loss 6.88144827\n",
      "Validated batch 127 batch loss 5.15364122\n",
      "Validated batch 128 batch loss 6.22707605\n",
      "Validated batch 129 batch loss 5.80722713\n",
      "Validated batch 130 batch loss 6.82814455\n",
      "Validated batch 131 batch loss 6.43565607\n",
      "Validated batch 132 batch loss 6.25131083\n",
      "Validated batch 133 batch loss 7.52669\n",
      "Validated batch 134 batch loss 7.46563101\n",
      "Validated batch 135 batch loss 6.00978088\n",
      "Validated batch 136 batch loss 5.52633667\n",
      "Validated batch 137 batch loss 5.28435421\n",
      "Validated batch 138 batch loss 5.57798576\n",
      "Validated batch 139 batch loss 5.30921888\n",
      "Validated batch 140 batch loss 5.35219049\n",
      "Validated batch 141 batch loss 5.7186718\n",
      "Validated batch 142 batch loss 5.12651873\n",
      "Validated batch 143 batch loss 5.43019199\n",
      "Validated batch 144 batch loss 6.62948895\n",
      "Validated batch 145 batch loss 6.85129642\n",
      "Validated batch 146 batch loss 6.07997\n",
      "Validated batch 147 batch loss 4.48831558\n",
      "Validated batch 148 batch loss 6.23033094\n",
      "Validated batch 149 batch loss 5.96012402\n",
      "Validated batch 150 batch loss 6.7312746\n",
      "Validated batch 151 batch loss 6.62882137\n",
      "Validated batch 152 batch loss 6.57508\n",
      "Validated batch 153 batch loss 5.40647602\n",
      "Validated batch 154 batch loss 6.13742208\n",
      "Validated batch 155 batch loss 5.51900959\n",
      "Validated batch 156 batch loss 5.6735\n",
      "Validated batch 157 batch loss 6.04489946\n",
      "Validated batch 158 batch loss 5.91130829\n",
      "Validated batch 159 batch loss 5.61156893\n",
      "Validated batch 160 batch loss 5.98218107\n",
      "Validated batch 161 batch loss 5.53406525\n",
      "Validated batch 162 batch loss 5.42516708\n",
      "Validated batch 163 batch loss 5.69897604\n",
      "Validated batch 164 batch loss 6.10160971\n",
      "Validated batch 165 batch loss 6.25267315\n",
      "Validated batch 166 batch loss 5.88227654\n",
      "Validated batch 167 batch loss 6.73866081\n",
      "Validated batch 168 batch loss 6.78487968\n",
      "Validated batch 169 batch loss 7.4198904\n",
      "Validated batch 170 batch loss 5.72589302\n",
      "Validated batch 171 batch loss 6.92378616\n",
      "Validated batch 172 batch loss 6.83510303\n",
      "Validated batch 173 batch loss 6.35619783\n",
      "Validated batch 174 batch loss 6.9610405\n",
      "Validated batch 175 batch loss 6.46917534\n",
      "Validated batch 176 batch loss 5.71654034\n",
      "Validated batch 177 batch loss 5.37021351\n",
      "Validated batch 178 batch loss 5.28746176\n",
      "Validated batch 179 batch loss 5.42586899\n",
      "Validated batch 180 batch loss 5.47033691\n",
      "Validated batch 181 batch loss 5.63788176\n",
      "Validated batch 182 batch loss 6.11370897\n",
      "Validated batch 183 batch loss 6.65264606\n",
      "Validated batch 184 batch loss 5.46911144\n",
      "Validated batch 185 batch loss 6.58575296\n",
      "Validated batch 186 batch loss 6.20060062\n",
      "Validated batch 187 batch loss 5.51637602\n",
      "Validated batch 188 batch loss 6.2106638\n",
      "Validated batch 189 batch loss 6.24746323\n",
      "Validated batch 190 batch loss 6.70584249\n",
      "Validated batch 191 batch loss 5.74174166\n",
      "Validated batch 192 batch loss 7.28845119\n",
      "Validated batch 193 batch loss 7.41457796\n",
      "Validated batch 194 batch loss 6.87576103\n",
      "Validated batch 195 batch loss 6.47788525\n",
      "Validated batch 196 batch loss 6.16877317\n",
      "Validated batch 197 batch loss 5.75379181\n",
      "Validated batch 198 batch loss 7.20961523\n",
      "Validated batch 199 batch loss 5.76087666\n",
      "Validated batch 200 batch loss 5.81049824\n",
      "Validated batch 201 batch loss 5.27100801\n",
      "Validated batch 202 batch loss 4.85383081\n",
      "Validated batch 203 batch loss 5.57231712\n",
      "Validated batch 204 batch loss 6.59309864\n",
      "Validated batch 205 batch loss 6.57574654\n",
      "Validated batch 206 batch loss 7.24283743\n",
      "Validated batch 207 batch loss 4.66422\n",
      "Validated batch 208 batch loss 6.35898352\n",
      "Validated batch 209 batch loss 5.35139704\n",
      "Validated batch 210 batch loss 6.05566\n",
      "Validated batch 211 batch loss 5.87143946\n",
      "Validated batch 212 batch loss 4.13098\n",
      "Validated batch 213 batch loss 4.9093504\n",
      "Validated batch 214 batch loss 6.09540462\n",
      "Validated batch 215 batch loss 4.93782139\n",
      "Validated batch 216 batch loss 6.39494705\n",
      "Validated batch 217 batch loss 5.66799355\n",
      "Validated batch 218 batch loss 5.72798634\n",
      "Validated batch 219 batch loss 6.20843315\n",
      "Validated batch 220 batch loss 6.57267141\n",
      "Validated batch 221 batch loss 6.53625441\n",
      "Validated batch 222 batch loss 7.02026367\n",
      "Validated batch 223 batch loss 5.93600464\n",
      "Validated batch 224 batch loss 4.24416351\n",
      "Validated batch 225 batch loss 5.30535841\n",
      "Validated batch 226 batch loss 6.04162693\n",
      "Validated batch 227 batch loss 6.50580835\n",
      "Validated batch 228 batch loss 6.40519047\n",
      "Validated batch 229 batch loss 6.41943\n",
      "Validated batch 230 batch loss 6.60152769\n",
      "Validated batch 231 batch loss 5.85090685\n",
      "Validated batch 232 batch loss 5.99190617\n",
      "Validated batch 233 batch loss 4.90123844\n",
      "Validated batch 234 batch loss 5.88666534\n",
      "Validated batch 235 batch loss 6.27605391\n",
      "Validated batch 236 batch loss 6.39549351\n",
      "Validated batch 237 batch loss 5.14612532\n",
      "Validated batch 238 batch loss 6.15723848\n",
      "Validated batch 239 batch loss 5.03937721\n",
      "Validated batch 240 batch loss 5.57451868\n",
      "Validated batch 241 batch loss 6.31233406\n",
      "Validated batch 242 batch loss 4.89237595\n",
      "Validated batch 243 batch loss 5.33274126\n",
      "Validated batch 244 batch loss 6.95577526\n",
      "Validated batch 245 batch loss 6.5796566\n",
      "Validated batch 246 batch loss 6.26316738\n",
      "Validated batch 247 batch loss 6.12266254\n",
      "Validated batch 248 batch loss 5.25979376\n",
      "Validated batch 249 batch loss 4.73436356\n",
      "Validated batch 250 batch loss 4.81951904\n",
      "Validated batch 251 batch loss 5.59660578\n",
      "Validated batch 252 batch loss 6.60025549\n",
      "Validated batch 253 batch loss 6.05781746\n",
      "Validated batch 254 batch loss 6.73307133\n",
      "Validated batch 255 batch loss 6.35550976\n",
      "Validated batch 256 batch loss 7.28977728\n",
      "Validated batch 257 batch loss 8.02736855\n",
      "Validated batch 258 batch loss 6.09238434\n",
      "Validated batch 259 batch loss 5.25423908\n",
      "Validated batch 260 batch loss 6.43063068\n",
      "Validated batch 261 batch loss 6.44074583\n",
      "Validated batch 262 batch loss 6.1472187\n",
      "Validated batch 263 batch loss 6.71770859\n",
      "Validated batch 264 batch loss 5.57059813\n",
      "Validated batch 265 batch loss 5.91115141\n",
      "Validated batch 266 batch loss 6.16928577\n",
      "Validated batch 267 batch loss 5.39041376\n",
      "Validated batch 268 batch loss 6.30828285\n",
      "Validated batch 269 batch loss 5.88683128\n",
      "Validated batch 270 batch loss 4.64357281\n",
      "Validated batch 271 batch loss 5.9077158\n",
      "Validated batch 272 batch loss 6.09825563\n",
      "Validated batch 273 batch loss 5.78048086\n",
      "Validated batch 274 batch loss 5.37246084\n",
      "Validated batch 275 batch loss 5.75461054\n",
      "Validated batch 276 batch loss 6.0187397\n",
      "Validated batch 277 batch loss 5.60493374\n",
      "Validated batch 278 batch loss 6.08545\n",
      "Validated batch 279 batch loss 3.78805923\n",
      "Validated batch 280 batch loss 6.75976801\n",
      "Validated batch 281 batch loss 5.45885515\n",
      "Validated batch 282 batch loss 6.57782412\n",
      "Validated batch 283 batch loss 5.50557137\n",
      "Validated batch 284 batch loss 5.83800125\n",
      "Validated batch 285 batch loss 4.73811626\n",
      "Validated batch 286 batch loss 6.21263504\n",
      "Validated batch 287 batch loss 7.41541576\n",
      "Validated batch 288 batch loss 6.33636713\n",
      "Validated batch 289 batch loss 6.22265482\n",
      "Validated batch 290 batch loss 7.19792652\n",
      "Validated batch 291 batch loss 5.73752642\n",
      "Validated batch 292 batch loss 7.60962486\n",
      "Validated batch 293 batch loss 6.75854778\n",
      "Validated batch 294 batch loss 5.88828945\n",
      "Validated batch 295 batch loss 6.17421627\n",
      "Validated batch 296 batch loss 4.64596\n",
      "Validated batch 297 batch loss 6.03306866\n",
      "Validated batch 298 batch loss 5.86686659\n",
      "Validated batch 299 batch loss 6.23173\n",
      "Validated batch 300 batch loss 6.86186838\n",
      "Validated batch 301 batch loss 5.8685894\n",
      "Validated batch 302 batch loss 6.08288956\n",
      "Validated batch 303 batch loss 6.30606556\n",
      "Validated batch 304 batch loss 5.85175037\n",
      "Validated batch 305 batch loss 6.3436656\n",
      "Validated batch 306 batch loss 6.88595486\n",
      "Validated batch 307 batch loss 5.55410194\n",
      "Validated batch 308 batch loss 6.08055782\n",
      "Validated batch 309 batch loss 6.11080742\n",
      "Validated batch 310 batch loss 6.87935686\n",
      "Validated batch 311 batch loss 6.52126408\n",
      "Validated batch 312 batch loss 5.5231905\n",
      "Validated batch 313 batch loss 5.96878719\n",
      "Validated batch 314 batch loss 5.68438244\n",
      "Validated batch 315 batch loss 5.40030193\n",
      "Validated batch 316 batch loss 5.94833279\n",
      "Validated batch 317 batch loss 6.08336735\n",
      "Validated batch 318 batch loss 6.43297148\n",
      "Validated batch 319 batch loss 6.58604622\n",
      "Validated batch 320 batch loss 5.86294937\n",
      "Validated batch 321 batch loss 6.69100046\n",
      "Validated batch 322 batch loss 6.99394035\n",
      "Validated batch 323 batch loss 6.91402435\n",
      "Validated batch 324 batch loss 6.00173855\n",
      "Validated batch 325 batch loss 6.14748573\n",
      "Validated batch 326 batch loss 6.47648907\n",
      "Validated batch 327 batch loss 6.38543177\n",
      "Validated batch 328 batch loss 5.84168959\n",
      "Validated batch 329 batch loss 5.30307579\n",
      "Validated batch 330 batch loss 5.29725075\n",
      "Validated batch 331 batch loss 5.94287872\n",
      "Validated batch 332 batch loss 5.92872858\n",
      "Validated batch 333 batch loss 6.35818\n",
      "Validated batch 334 batch loss 6.39690447\n",
      "Validated batch 335 batch loss 6.27544451\n",
      "Validated batch 336 batch loss 6.74498034\n",
      "Validated batch 337 batch loss 7.45608616\n",
      "Validated batch 338 batch loss 6.64555883\n",
      "Validated batch 339 batch loss 6.59296513\n",
      "Validated batch 340 batch loss 5.34343529\n",
      "Validated batch 341 batch loss 4.4934516\n",
      "Validated batch 342 batch loss 6.56475735\n",
      "Validated batch 343 batch loss 5.47204781\n",
      "Validated batch 344 batch loss 5.62109852\n",
      "Validated batch 345 batch loss 6.36415386\n",
      "Validated batch 346 batch loss 6.9455\n",
      "Validated batch 347 batch loss 6.28063154\n",
      "Validated batch 348 batch loss 7.3121357\n",
      "Validated batch 349 batch loss 6.30724335\n",
      "Validated batch 350 batch loss 6.25749493\n",
      "Validated batch 351 batch loss 5.24565601\n",
      "Validated batch 352 batch loss 5.73199272\n",
      "Validated batch 353 batch loss 5.57118368\n",
      "Validated batch 354 batch loss 4.92959881\n",
      "Validated batch 355 batch loss 5.58455467\n",
      "Validated batch 356 batch loss 4.44047689\n",
      "Validated batch 357 batch loss 4.84807\n",
      "Validated batch 358 batch loss 5.02317572\n",
      "Validated batch 359 batch loss 4.92956638\n",
      "Validated batch 360 batch loss 5.88250065\n",
      "Validated batch 361 batch loss 5.74698496\n",
      "Validated batch 362 batch loss 6.21543646\n",
      "Validated batch 363 batch loss 5.16044617\n",
      "Validated batch 364 batch loss 5.90622711\n",
      "Validated batch 365 batch loss 6.37408638\n",
      "Validated batch 366 batch loss 7.34512424\n",
      "Validated batch 367 batch loss 7.24791718\n",
      "Validated batch 368 batch loss 5.92616701\n",
      "Validated batch 369 batch loss 6.62108707\n",
      "Validated batch 370 batch loss 6.42132425\n",
      "Validated batch 371 batch loss 5.74823189\n",
      "Validated batch 372 batch loss 6.48549843\n",
      "Validated batch 373 batch loss 6.44072819\n",
      "Validated batch 374 batch loss 6.12810802\n",
      "Validated batch 375 batch loss 6.19050312\n",
      "Validated batch 376 batch loss 5.87146616\n",
      "Validated batch 377 batch loss 5.64765\n",
      "Validated batch 378 batch loss 6.71062\n",
      "Validated batch 379 batch loss 6.05401373\n",
      "Validated batch 380 batch loss 5.55534649\n",
      "Validated batch 381 batch loss 5.60924482\n",
      "Validated batch 382 batch loss 5.3030653\n",
      "Validated batch 383 batch loss 6.52169943\n",
      "Validated batch 384 batch loss 6.30750179\n",
      "Validated batch 385 batch loss 6.23231506\n",
      "Validated batch 386 batch loss 5.66435623\n",
      "Validated batch 387 batch loss 6.52733374\n",
      "Validated batch 388 batch loss 5.97790146\n",
      "Validated batch 389 batch loss 5.84015846\n",
      "Validated batch 390 batch loss 5.08579254\n",
      "Validated batch 391 batch loss 5.71469975\n",
      "Validated batch 392 batch loss 5.17851686\n",
      "Validated batch 393 batch loss 5.61034441\n",
      "Validated batch 394 batch loss 5.81232882\n",
      "Validated batch 395 batch loss 6.19593525\n",
      "Validated batch 396 batch loss 5.78820658\n",
      "Validated batch 397 batch loss 6.45686769\n",
      "Validated batch 398 batch loss 5.24205399\n",
      "Validated batch 399 batch loss 5.32131386\n",
      "Validated batch 400 batch loss 6.20579529\n",
      "Validated batch 401 batch loss 5.63106728\n",
      "Validated batch 402 batch loss 5.81256866\n",
      "Validated batch 403 batch loss 6.32746601\n",
      "Validated batch 404 batch loss 5.74204063\n",
      "Validated batch 405 batch loss 5.55228424\n",
      "Validated batch 406 batch loss 6.00940943\n",
      "Validated batch 407 batch loss 6.73089218\n",
      "Validated batch 408 batch loss 6.54919672\n",
      "Validated batch 409 batch loss 6.04291582\n",
      "Validated batch 410 batch loss 6.13341284\n",
      "Validated batch 411 batch loss 5.80419636\n",
      "Validated batch 412 batch loss 5.1054821\n",
      "Validated batch 413 batch loss 6.52603722\n",
      "Validated batch 414 batch loss 6.05264091\n",
      "Validated batch 415 batch loss 6.38488913\n",
      "Validated batch 416 batch loss 6.27343702\n",
      "Validated batch 417 batch loss 6.71137476\n",
      "Validated batch 418 batch loss 5.97172165\n",
      "Validated batch 419 batch loss 5.70696163\n",
      "Validated batch 420 batch loss 5.69702911\n",
      "Validated batch 421 batch loss 4.58460236\n",
      "Validated batch 422 batch loss 6.46522665\n",
      "Validated batch 423 batch loss 7.77129841\n",
      "Validated batch 424 batch loss 6.23461866\n",
      "Validated batch 425 batch loss 5.10661221\n",
      "Validated batch 426 batch loss 6.49777126\n",
      "Validated batch 427 batch loss 6.32559109\n",
      "Validated batch 428 batch loss 5.81921959\n",
      "Validated batch 429 batch loss 5.95015621\n",
      "Validated batch 430 batch loss 5.84232044\n",
      "Validated batch 431 batch loss 6.34284\n",
      "Validated batch 432 batch loss 5.50307417\n",
      "Validated batch 433 batch loss 6.40757751\n",
      "Validated batch 434 batch loss 6.76630878\n",
      "Validated batch 435 batch loss 5.60004425\n",
      "Validated batch 436 batch loss 6.36454868\n",
      "Validated batch 437 batch loss 5.76313591\n",
      "Validated batch 438 batch loss 6.37870884\n",
      "Validated batch 439 batch loss 4.59896421\n",
      "Validated batch 440 batch loss 5.70829105\n",
      "Validated batch 441 batch loss 6.44116783\n",
      "Validated batch 442 batch loss 5.89121771\n",
      "Validated batch 443 batch loss 5.89150429\n",
      "Validated batch 444 batch loss 5.76836586\n",
      "Validated batch 445 batch loss 5.28427601\n",
      "Validated batch 446 batch loss 6.43653059\n",
      "Validated batch 447 batch loss 6.75593\n",
      "Validated batch 448 batch loss 7.38149834\n",
      "Validated batch 449 batch loss 7.68715\n",
      "Validated batch 450 batch loss 6.33969259\n",
      "Validated batch 451 batch loss 5.91471481\n",
      "Validated batch 452 batch loss 6.22526169\n",
      "Validated batch 453 batch loss 6.30456257\n",
      "Validated batch 454 batch loss 6.09873295\n",
      "Validated batch 455 batch loss 5.64194345\n",
      "Validated batch 456 batch loss 5.56404686\n",
      "Validated batch 457 batch loss 6.4445076\n",
      "Validated batch 458 batch loss 6.90306664\n",
      "Validated batch 459 batch loss 7.30384922\n",
      "Validated batch 460 batch loss 5.9561882\n",
      "Validated batch 461 batch loss 5.35898161\n",
      "Validated batch 462 batch loss 5.71264076\n",
      "Validated batch 463 batch loss 5.63822746\n",
      "Validated batch 464 batch loss 6.22111225\n",
      "Validated batch 465 batch loss 5.39767742\n",
      "Validated batch 466 batch loss 6.29140377\n",
      "Validated batch 467 batch loss 4.728302\n",
      "Validated batch 468 batch loss 6.33182\n",
      "Validated batch 469 batch loss 6.07022047\n",
      "Validated batch 470 batch loss 5.86413097\n",
      "Validated batch 471 batch loss 7.28830147\n",
      "Validated batch 472 batch loss 6.3291235\n",
      "Validated batch 473 batch loss 6.16994143\n",
      "Validated batch 474 batch loss 6.08913565\n",
      "Validated batch 475 batch loss 5.40267277\n",
      "Validated batch 476 batch loss 5.53310823\n",
      "Validated batch 477 batch loss 5.78644323\n",
      "Validated batch 478 batch loss 6.20396233\n",
      "Validated batch 479 batch loss 5.9083786\n",
      "Validated batch 480 batch loss 6.12834597\n",
      "Validated batch 481 batch loss 6.43109179\n",
      "Validated batch 482 batch loss 6.46380568\n",
      "Validated batch 483 batch loss 6.03783703\n",
      "Validated batch 484 batch loss 5.66075802\n",
      "Validated batch 485 batch loss 6.36055136\n",
      "Validated batch 486 batch loss 5.88197041\n",
      "Validated batch 487 batch loss 6.36631489\n",
      "Validated batch 488 batch loss 5.81830263\n",
      "Validated batch 489 batch loss 6.34803391\n",
      "Validated batch 490 batch loss 6.5427947\n",
      "Validated batch 491 batch loss 5.06815338\n",
      "Validated batch 492 batch loss 5.66440296\n",
      "Validated batch 493 batch loss 6.2265873\n",
      "Validated batch 494 batch loss 5.35120773\n",
      "Validated batch 495 batch loss 6.58897209\n",
      "Validated batch 496 batch loss 6.81902313\n",
      "Validated batch 497 batch loss 6.68146896\n",
      "Validated batch 498 batch loss 6.91472912\n",
      "Validated batch 499 batch loss 6.34878206\n",
      "Validated batch 500 batch loss 5.88081503\n",
      "Validated batch 501 batch loss 6.18808174\n",
      "Validated batch 502 batch loss 6.63377094\n",
      "Validated batch 503 batch loss 6.58726215\n",
      "Validated batch 504 batch loss 5.41464138\n",
      "Validated batch 505 batch loss 6.27053547\n",
      "Validated batch 506 batch loss 4.73673916\n",
      "Validated batch 507 batch loss 5.05649185\n",
      "Validated batch 508 batch loss 6.06445312\n",
      "Validated batch 509 batch loss 6.34886456\n",
      "Validated batch 510 batch loss 6.89739323\n",
      "Validated batch 511 batch loss 7.22758293\n",
      "Validated batch 512 batch loss 7.23082829\n",
      "Validated batch 513 batch loss 5.95605707\n",
      "Validated batch 514 batch loss 6.34156036\n",
      "Validated batch 515 batch loss 6.31572056\n",
      "Validated batch 516 batch loss 5.82460451\n",
      "Validated batch 517 batch loss 6.09150362\n",
      "Validated batch 518 batch loss 6.03641796\n",
      "Validated batch 519 batch loss 3.8256669\n",
      "Validated batch 520 batch loss 3.96043181\n",
      "Validated batch 521 batch loss 6.6114397\n",
      "Validated batch 522 batch loss 6.77218\n",
      "Validated batch 523 batch loss 6.72781372\n",
      "Validated batch 524 batch loss 5.62587261\n",
      "Validated batch 525 batch loss 5.28085709\n",
      "Validated batch 526 batch loss 7.18593025\n",
      "Validated batch 527 batch loss 6.69140291\n",
      "Validated batch 528 batch loss 5.79661846\n",
      "Validated batch 529 batch loss 6.06739187\n",
      "Validated batch 530 batch loss 4.89078379\n",
      "Validated batch 531 batch loss 5.66667175\n",
      "Validated batch 532 batch loss 6.61185455\n",
      "Validated batch 533 batch loss 6.46483564\n",
      "Validated batch 534 batch loss 5.69822168\n",
      "Validated batch 535 batch loss 6.32694483\n",
      "Validated batch 536 batch loss 5.58178711\n",
      "Validated batch 537 batch loss 5.95123291\n",
      "Validated batch 538 batch loss 6.49129152\n",
      "Validated batch 539 batch loss 7.27110863\n",
      "Validated batch 540 batch loss 6.38968801\n",
      "Validated batch 541 batch loss 6.2100172\n",
      "Validated batch 542 batch loss 5.26062965\n",
      "Validated batch 543 batch loss 5.04798651\n",
      "Validated batch 544 batch loss 5.06826687\n",
      "Validated batch 545 batch loss 5.90740538\n",
      "Validated batch 546 batch loss 6.09872293\n",
      "Validated batch 547 batch loss 6.200665\n",
      "Validated batch 548 batch loss 6.38394928\n",
      "Validated batch 549 batch loss 5.16673279\n",
      "Validated batch 550 batch loss 7.37718105\n",
      "Validated batch 551 batch loss 6.03517818\n",
      "Validated batch 552 batch loss 6.6091156\n",
      "Validated batch 553 batch loss 6.64706755\n",
      "Validated batch 554 batch loss 5.29607534\n",
      "Validated batch 555 batch loss 6.42649698\n",
      "Validated batch 556 batch loss 6.07447624\n",
      "Validated batch 557 batch loss 5.70818901\n",
      "Validated batch 558 batch loss 6.56109\n",
      "Validated batch 559 batch loss 4.81439257\n",
      "Validated batch 560 batch loss 6.53578329\n",
      "Validated batch 561 batch loss 6.71960497\n",
      "Validated batch 562 batch loss 6.1412015\n",
      "Validated batch 563 batch loss 5.15343285\n",
      "Validated batch 564 batch loss 4.96780205\n",
      "Validated batch 565 batch loss 5.46738625\n",
      "Validated batch 566 batch loss 5.59971809\n",
      "Validated batch 567 batch loss 5.35152626\n",
      "Validated batch 568 batch loss 6.22841692\n",
      "Validated batch 569 batch loss 5.84878349\n",
      "Validated batch 570 batch loss 5.61160088\n",
      "Validated batch 571 batch loss 6.38110542\n",
      "Validated batch 572 batch loss 5.81717873\n",
      "Validated batch 573 batch loss 6.37619686\n",
      "Validated batch 574 batch loss 6.47952461\n",
      "Validated batch 575 batch loss 5.53471565\n",
      "Validated batch 576 batch loss 6.15087414\n",
      "Validated batch 577 batch loss 5.6898365\n",
      "Validated batch 578 batch loss 5.66572094\n",
      "Validated batch 579 batch loss 6.10526657\n",
      "Validated batch 580 batch loss 5.66051674\n",
      "Validated batch 581 batch loss 5.96329212\n",
      "Validated batch 582 batch loss 6.21846819\n",
      "Validated batch 583 batch loss 4.9565649\n",
      "Validated batch 584 batch loss 5.73403311\n",
      "Validated batch 585 batch loss 6.27097702\n",
      "Validated batch 586 batch loss 6.16225243\n",
      "Validated batch 587 batch loss 6.2482543\n",
      "Validated batch 588 batch loss 5.68026829\n",
      "Validated batch 589 batch loss 5.57940149\n",
      "Validated batch 590 batch loss 5.40898752\n",
      "Validated batch 591 batch loss 4.53799391\n",
      "Validated batch 592 batch loss 6.83668327\n",
      "Validated batch 593 batch loss 6.66937256\n",
      "Validated batch 594 batch loss 5.59788227\n",
      "Validated batch 595 batch loss 5.90672302\n",
      "Validated batch 596 batch loss 6.8519659\n",
      "Validated batch 597 batch loss 7.08063793\n",
      "Validated batch 598 batch loss 6.40806627\n",
      "Validated batch 599 batch loss 6.39528894\n",
      "Validated batch 600 batch loss 6.12950516\n",
      "Validated batch 601 batch loss 6.0424614\n",
      "Validated batch 602 batch loss 5.43670559\n",
      "Validated batch 603 batch loss 5.61164188\n",
      "Validated batch 604 batch loss 5.63365221\n",
      "Validated batch 605 batch loss 7.24190569\n",
      "Validated batch 606 batch loss 6.00022221\n",
      "Validated batch 607 batch loss 6.32051039\n",
      "Validated batch 608 batch loss 6.39459133\n",
      "Validated batch 609 batch loss 5.76987457\n",
      "Validated batch 610 batch loss 5.86868334\n",
      "Validated batch 611 batch loss 5.61454964\n",
      "Validated batch 612 batch loss 6.09043407\n",
      "Validated batch 613 batch loss 6.04962778\n",
      "Validated batch 614 batch loss 6.64900637\n",
      "Validated batch 615 batch loss 6.7724781\n",
      "Validated batch 616 batch loss 7.70855\n",
      "Validated batch 617 batch loss 7.06218243\n",
      "Validated batch 618 batch loss 6.32258558\n",
      "Validated batch 619 batch loss 6.11809635\n",
      "Validated batch 620 batch loss 6.03829575\n",
      "Validated batch 621 batch loss 5.5199852\n",
      "Validated batch 622 batch loss 6.4212\n",
      "Validated batch 623 batch loss 5.84578228\n",
      "Validated batch 624 batch loss 6.73281\n",
      "Validated batch 625 batch loss 7.05447769\n",
      "Validated batch 626 batch loss 7.39853764\n",
      "Validated batch 627 batch loss 6.59180593\n",
      "Validated batch 628 batch loss 6.07234764\n",
      "Validated batch 629 batch loss 5.90520811\n",
      "Validated batch 630 batch loss 5.81803322\n",
      "Validated batch 631 batch loss 5.66265392\n",
      "Validated batch 632 batch loss 5.18803072\n",
      "Validated batch 633 batch loss 6.63933802\n",
      "Validated batch 634 batch loss 6.35427856\n",
      "Validated batch 635 batch loss 6.3894043\n",
      "Validated batch 636 batch loss 5.18236637\n",
      "Validated batch 637 batch loss 7.07875156\n",
      "Validated batch 638 batch loss 5.50073862\n",
      "Validated batch 639 batch loss 5.49493504\n",
      "Validated batch 640 batch loss 5.46708918\n",
      "Validated batch 641 batch loss 6.93166637\n",
      "Validated batch 642 batch loss 5.97383308\n",
      "Validated batch 643 batch loss 6.0127058\n",
      "Validated batch 644 batch loss 5.93555164\n",
      "Validated batch 645 batch loss 7.03789854\n",
      "Validated batch 646 batch loss 7.22949\n",
      "Validated batch 647 batch loss 6.754704\n",
      "Validated batch 648 batch loss 5.45864677\n",
      "Validated batch 649 batch loss 5.44226074\n",
      "Validated batch 650 batch loss 6.18517923\n",
      "Validated batch 651 batch loss 6.65677261\n",
      "Validated batch 652 batch loss 6.41167259\n",
      "Validated batch 653 batch loss 5.87124538\n",
      "Validated batch 654 batch loss 6.0919733\n",
      "Validated batch 655 batch loss 5.15382099\n",
      "Validated batch 656 batch loss 6.090168\n",
      "Validated batch 657 batch loss 7.0265274\n",
      "Validated batch 658 batch loss 6.61143923\n",
      "Validated batch 659 batch loss 6.73255825\n",
      "Validated batch 660 batch loss 6.37325668\n",
      "Validated batch 661 batch loss 6.03696394\n",
      "Validated batch 662 batch loss 6.02043104\n",
      "Validated batch 663 batch loss 5.9869895\n",
      "Validated batch 664 batch loss 6.37232685\n",
      "Validated batch 665 batch loss 7.17719173\n",
      "Validated batch 666 batch loss 6.38276672\n",
      "Validated batch 667 batch loss 6.19076633\n",
      "Validated batch 668 batch loss 6.52101803\n",
      "Validated batch 669 batch loss 6.10900402\n",
      "Validated batch 670 batch loss 5.91726065\n",
      "Validated batch 671 batch loss 6.1658473\n",
      "Validated batch 672 batch loss 6.31750393\n",
      "Validated batch 673 batch loss 5.53433418\n",
      "Validated batch 674 batch loss 6.65869093\n",
      "Validated batch 675 batch loss 6.46538591\n",
      "Validated batch 676 batch loss 5.85190392\n",
      "Validated batch 677 batch loss 6.24303246\n",
      "Validated batch 678 batch loss 6.37259054\n",
      "Validated batch 679 batch loss 5.94753456\n",
      "Validated batch 680 batch loss 5.46315479\n",
      "Validated batch 681 batch loss 5.74594307\n",
      "Validated batch 682 batch loss 6.6401186\n",
      "Validated batch 683 batch loss 6.5391345\n",
      "Validated batch 684 batch loss 6.35395288\n",
      "Validated batch 685 batch loss 5.54292\n",
      "Validated batch 686 batch loss 5.80609369\n",
      "Validated batch 687 batch loss 6.19980907\n",
      "Validated batch 688 batch loss 6.07412291\n",
      "Validated batch 689 batch loss 6.74705887\n",
      "Validated batch 690 batch loss 5.89551497\n",
      "Validated batch 691 batch loss 6.53735304\n",
      "Validated batch 692 batch loss 5.60002804\n",
      "Validated batch 693 batch loss 4.1687\n",
      "Validated batch 694 batch loss 5.00771475\n",
      "Validated batch 695 batch loss 5.42579365\n",
      "Validated batch 696 batch loss 6.20388222\n",
      "Validated batch 697 batch loss 6.11818504\n",
      "Validated batch 698 batch loss 5.62363625\n",
      "Validated batch 699 batch loss 6.30323219\n",
      "Validated batch 700 batch loss 5.86283493\n",
      "Validated batch 701 batch loss 5.65924692\n",
      "Validated batch 702 batch loss 6.01142502\n",
      "Validated batch 703 batch loss 6.15499592\n",
      "Validated batch 704 batch loss 5.78652811\n",
      "Validated batch 705 batch loss 5.97858715\n",
      "Validated batch 706 batch loss 5.71920824\n",
      "Validated batch 707 batch loss 6.13497353\n",
      "Validated batch 708 batch loss 5.96156263\n",
      "Validated batch 709 batch loss 6.0544734\n",
      "Validated batch 710 batch loss 6.12803411\n",
      "Validated batch 711 batch loss 6.44330597\n",
      "Validated batch 712 batch loss 6.30689812\n",
      "Validated batch 713 batch loss 6.12175941\n",
      "Validated batch 714 batch loss 6.23545837\n",
      "Validated batch 715 batch loss 7.05007505\n",
      "Validated batch 716 batch loss 5.89420033\n",
      "Validated batch 717 batch loss 5.64592266\n",
      "Validated batch 718 batch loss 6.39901161\n",
      "Validated batch 719 batch loss 5.27375889\n",
      "Validated batch 720 batch loss 6.1179781\n",
      "Validated batch 721 batch loss 6.08118105\n",
      "Validated batch 722 batch loss 5.81990433\n",
      "Validated batch 723 batch loss 6.19705\n",
      "Validated batch 724 batch loss 6.10191\n",
      "Validated batch 725 batch loss 6.37850189\n",
      "Validated batch 726 batch loss 6.39074\n",
      "Validated batch 727 batch loss 5.77602\n",
      "Validated batch 728 batch loss 6.10236645\n",
      "Validated batch 729 batch loss 7.08286667\n",
      "Validated batch 730 batch loss 6.16850471\n",
      "Validated batch 731 batch loss 5.50547171\n",
      "Validated batch 732 batch loss 6.14436674\n",
      "Validated batch 733 batch loss 5.60212803\n",
      "Validated batch 734 batch loss 5.38645506\n",
      "Validated batch 735 batch loss 5.20492363\n",
      "Validated batch 736 batch loss 5.55958223\n",
      "Validated batch 737 batch loss 6.67739153\n",
      "Validated batch 738 batch loss 6.45238161\n",
      "Validated batch 739 batch loss 5.85387135\n",
      "Validated batch 740 batch loss 6.82676888\n",
      "Validated batch 741 batch loss 5.75372028\n",
      "Validated batch 742 batch loss 5.22572708\n",
      "Validated batch 743 batch loss 4.56793356\n",
      "Validated batch 744 batch loss 5.04515028\n",
      "Validated batch 745 batch loss 6.09565926\n",
      "Validated batch 746 batch loss 6.72288609\n",
      "Validated batch 747 batch loss 6.46000528\n",
      "Validated batch 748 batch loss 5.59876919\n",
      "Validated batch 749 batch loss 6.82154226\n",
      "Validated batch 750 batch loss 7.42505455\n",
      "Validated batch 751 batch loss 6.30389833\n",
      "Validated batch 752 batch loss 5.35677385\n",
      "Validated batch 753 batch loss 6.2249279\n",
      "Validated batch 754 batch loss 6.33130407\n",
      "Validated batch 755 batch loss 6.79155779\n",
      "Validated batch 756 batch loss 6.94223785\n",
      "Validated batch 757 batch loss 5.81228924\n",
      "Validated batch 758 batch loss 6.01960945\n",
      "Validated batch 759 batch loss 5.55146885\n",
      "Validated batch 760 batch loss 5.64470387\n",
      "Validated batch 761 batch loss 6.1357708\n",
      "Validated batch 762 batch loss 5.97219563\n",
      "Validated batch 763 batch loss 6.79201651\n",
      "Validated batch 764 batch loss 6.52854824\n",
      "Validated batch 765 batch loss 6.60251904\n",
      "Validated batch 766 batch loss 6.19240284\n",
      "Validated batch 767 batch loss 4.85393333\n",
      "Validated batch 768 batch loss 6.09782791\n",
      "Validated batch 769 batch loss 5.67697334\n",
      "Validated batch 770 batch loss 6.55393744\n",
      "Validated batch 771 batch loss 6.4355278\n",
      "Validated batch 772 batch loss 5.42400599\n",
      "Validated batch 773 batch loss 6.47430325\n",
      "Validated batch 774 batch loss 6.46175575\n",
      "Validated batch 775 batch loss 6.59181\n",
      "Validated batch 776 batch loss 5.91546917\n",
      "Validated batch 777 batch loss 5.99286556\n",
      "Validated batch 778 batch loss 6.47607899\n",
      "Validated batch 779 batch loss 7.81342554\n",
      "Validated batch 780 batch loss 6.81622\n",
      "Validated batch 781 batch loss 6.73320532\n",
      "Validated batch 782 batch loss 6.35754681\n",
      "Validated batch 783 batch loss 6.50120974\n",
      "Validated batch 784 batch loss 6.22708511\n",
      "Validated batch 785 batch loss 5.99897289\n",
      "Validated batch 786 batch loss 6.24648762\n",
      "Validated batch 787 batch loss 5.8440218\n",
      "Validated batch 788 batch loss 6.56302929\n",
      "Validated batch 789 batch loss 4.22069359\n",
      "Validated batch 790 batch loss 5.49617195\n",
      "Validated batch 791 batch loss 5.91459942\n",
      "Validated batch 792 batch loss 6.18165588\n",
      "Validated batch 793 batch loss 6.17336178\n",
      "Validated batch 794 batch loss 6.84655094\n",
      "Validated batch 795 batch loss 6.47118425\n",
      "Validated batch 796 batch loss 6.24902964\n",
      "Validated batch 797 batch loss 5.92322445\n",
      "Validated batch 798 batch loss 6.18411922\n",
      "Validated batch 799 batch loss 6.95986748\n",
      "Validated batch 800 batch loss 6.16963673\n",
      "Validated batch 801 batch loss 6.16261292\n",
      "Validated batch 802 batch loss 6.24465\n",
      "Validated batch 803 batch loss 6.16204262\n",
      "Validated batch 804 batch loss 5.90857935\n",
      "Validated batch 805 batch loss 6.21486187\n",
      "Validated batch 806 batch loss 4.54274273\n",
      "Validated batch 807 batch loss 5.14670086\n",
      "Validated batch 808 batch loss 5.63028574\n",
      "Validated batch 809 batch loss 6.35655069\n",
      "Validated batch 810 batch loss 6.50854826\n",
      "Validated batch 811 batch loss 6.14095402\n",
      "Validated batch 812 batch loss 6.03676748\n",
      "Validated batch 813 batch loss 6.90125275\n",
      "Validated batch 814 batch loss 6.32390308\n",
      "Validated batch 815 batch loss 5.5081358\n",
      "Validated batch 816 batch loss 6.41850138\n",
      "Validated batch 817 batch loss 6.21952343\n",
      "Validated batch 818 batch loss 6.31678057\n",
      "Validated batch 819 batch loss 5.51463795\n",
      "Validated batch 820 batch loss 6.92259693\n",
      "Validated batch 821 batch loss 5.40967274\n",
      "Validated batch 822 batch loss 6.4892478\n",
      "Validated batch 823 batch loss 7.02941895\n",
      "Validated batch 824 batch loss 6.67921829\n",
      "Validated batch 825 batch loss 5.88420582\n",
      "Validated batch 826 batch loss 5.37995291\n",
      "Validated batch 827 batch loss 5.45857763\n",
      "Validated batch 828 batch loss 6.2510519\n",
      "Validated batch 829 batch loss 5.47653246\n",
      "Validated batch 830 batch loss 5.66735792\n",
      "Validated batch 831 batch loss 5.3453145\n",
      "Validated batch 832 batch loss 5.56511\n",
      "Validated batch 833 batch loss 5.00473404\n",
      "Validated batch 834 batch loss 4.6377182\n",
      "Validated batch 835 batch loss 4.63244772\n",
      "Validated batch 836 batch loss 4.70575809\n",
      "Validated batch 837 batch loss 4.45765209\n",
      "Validated batch 838 batch loss 5.7101841\n",
      "Validated batch 839 batch loss 6.42036152\n",
      "Validated batch 840 batch loss 7.31251383\n",
      "Validated batch 841 batch loss 6.0900445\n",
      "Validated batch 842 batch loss 5.68531752\n",
      "Validated batch 843 batch loss 6.41692448\n",
      "Validated batch 844 batch loss 6.19778919\n",
      "Validated batch 845 batch loss 6.05923748\n",
      "Validated batch 846 batch loss 5.47277451\n",
      "Validated batch 847 batch loss 5.935009\n",
      "Validated batch 848 batch loss 6.48450184\n",
      "Validated batch 849 batch loss 6.24782228\n",
      "Validated batch 850 batch loss 6.39437151\n",
      "Validated batch 851 batch loss 5.87046289\n",
      "Validated batch 852 batch loss 5.93844223\n",
      "Validated batch 853 batch loss 6.38864613\n",
      "Validated batch 854 batch loss 5.66002417\n",
      "Validated batch 855 batch loss 5.80572271\n",
      "Validated batch 856 batch loss 6.37553406\n",
      "Validated batch 857 batch loss 6.34332514\n",
      "Validated batch 858 batch loss 5.84330082\n",
      "Validated batch 859 batch loss 5.55997181\n",
      "Validated batch 860 batch loss 6.61308193\n",
      "Validated batch 861 batch loss 5.08733749\n",
      "Validated batch 862 batch loss 6.09858608\n",
      "Validated batch 863 batch loss 6.30650139\n",
      "Validated batch 864 batch loss 6.11538696\n",
      "Validated batch 865 batch loss 4.7506361\n",
      "Validated batch 866 batch loss 7.23357964\n",
      "Validated batch 867 batch loss 6.51298809\n",
      "Validated batch 868 batch loss 6.11588955\n",
      "Validated batch 869 batch loss 6.13245249\n",
      "Validated batch 870 batch loss 6.29883671\n",
      "Validated batch 871 batch loss 6.20715141\n",
      "Validated batch 872 batch loss 5.81792355\n",
      "Validated batch 873 batch loss 5.63440895\n",
      "Validated batch 874 batch loss 6.47160101\n",
      "Validated batch 875 batch loss 6.93045855\n",
      "Validated batch 876 batch loss 6.07006836\n",
      "Validated batch 877 batch loss 6.12114906\n",
      "Validated batch 878 batch loss 5.78786469\n",
      "Validated batch 879 batch loss 6.03546095\n",
      "Validated batch 880 batch loss 6.44479322\n",
      "Validated batch 881 batch loss 5.62827492\n",
      "Validated batch 882 batch loss 5.99631548\n",
      "Validated batch 883 batch loss 7.73336506\n",
      "Validated batch 884 batch loss 6.34453964\n",
      "Validated batch 885 batch loss 6.18470478\n",
      "Validated batch 886 batch loss 5.9970355\n",
      "Validated batch 887 batch loss 5.43803406\n",
      "Validated batch 888 batch loss 5.74490833\n",
      "Validated batch 889 batch loss 6.24956274\n",
      "Validated batch 890 batch loss 6.06164694\n",
      "Validated batch 891 batch loss 6.81155109\n",
      "Validated batch 892 batch loss 6.74582\n",
      "Validated batch 893 batch loss 6.17587757\n",
      "Validated batch 894 batch loss 7.02479076\n",
      "Validated batch 895 batch loss 5.99046707\n",
      "Validated batch 896 batch loss 6.02264786\n",
      "Validated batch 897 batch loss 5.67602205\n",
      "Validated batch 898 batch loss 6.05962515\n",
      "Validated batch 899 batch loss 6.14647341\n",
      "Validated batch 900 batch loss 6.57894135\n",
      "Validated batch 901 batch loss 5.8989296\n",
      "Validated batch 902 batch loss 5.99470234\n",
      "Validated batch 903 batch loss 6.32055092\n",
      "Validated batch 904 batch loss 6.15776587\n",
      "Validated batch 905 batch loss 6.7540617\n",
      "Validated batch 906 batch loss 6.59676886\n",
      "Validated batch 907 batch loss 6.07859421\n",
      "Validated batch 908 batch loss 5.85394335\n",
      "Validated batch 909 batch loss 6.77599955\n",
      "Validated batch 910 batch loss 5.62269545\n",
      "Validated batch 911 batch loss 5.39254951\n",
      "Validated batch 912 batch loss 6.43383741\n",
      "Validated batch 913 batch loss 5.66695213\n",
      "Validated batch 914 batch loss 6.07967663\n",
      "Validated batch 915 batch loss 5.30491638\n",
      "Validated batch 916 batch loss 5.2011\n",
      "Validated batch 917 batch loss 5.42578411\n",
      "Validated batch 918 batch loss 5.64299536\n",
      "Validated batch 919 batch loss 6.11744499\n",
      "Validated batch 920 batch loss 6.26694298\n",
      "Validated batch 921 batch loss 6.57025623\n",
      "Validated batch 922 batch loss 6.24627\n",
      "Validated batch 923 batch loss 5.91711283\n",
      "Validated batch 924 batch loss 6.2362113\n",
      "Validated batch 925 batch loss 6.50845814\n",
      "Validated batch 926 batch loss 7.34567356\n",
      "Validated batch 927 batch loss 6.40640259\n",
      "Validated batch 928 batch loss 6.62928677\n",
      "Validated batch 929 batch loss 6.89818907\n",
      "Validated batch 930 batch loss 5.60606718\n",
      "Validated batch 931 batch loss 5.71294117\n",
      "Validated batch 932 batch loss 6.18801212\n",
      "Validated batch 933 batch loss 5.89736938\n",
      "Validated batch 934 batch loss 5.86257219\n",
      "Validated batch 935 batch loss 5.84683084\n",
      "Validated batch 936 batch loss 7.05404282\n",
      "Validated batch 937 batch loss 6.69079971\n",
      "Validated batch 938 batch loss 6.81494236\n",
      "Validated batch 939 batch loss 5.80852652\n",
      "Validated batch 940 batch loss 5.89565086\n",
      "Validated batch 941 batch loss 7.29517937\n",
      "Validated batch 942 batch loss 5.63954639\n",
      "Validated batch 943 batch loss 5.28567457\n",
      "Validated batch 944 batch loss 5.42824459\n",
      "Validated batch 945 batch loss 6.3773632\n",
      "Validated batch 946 batch loss 6.16254711\n",
      "Validated batch 947 batch loss 5.75169277\n",
      "Validated batch 948 batch loss 5.31332397\n",
      "Validated batch 949 batch loss 5.30910301\n",
      "Validated batch 950 batch loss 6.25571537\n",
      "Validated batch 951 batch loss 6.86102772\n",
      "Validated batch 952 batch loss 6.06534\n",
      "Validated batch 953 batch loss 6.18673658\n",
      "Validated batch 954 batch loss 5.83299494\n",
      "Validated batch 955 batch loss 6.28314495\n",
      "Validated batch 956 batch loss 5.70655155\n",
      "Validated batch 957 batch loss 5.44041967\n",
      "Validated batch 958 batch loss 6.27079821\n",
      "Validated batch 959 batch loss 5.21578741\n",
      "Validated batch 960 batch loss 6.57428265\n",
      "Validated batch 961 batch loss 6.10670948\n",
      "Validated batch 962 batch loss 6.25684\n",
      "Validated batch 963 batch loss 6.6341095\n",
      "Validated batch 964 batch loss 7.18006611\n",
      "Validated batch 965 batch loss 6.56897259\n",
      "Validated batch 966 batch loss 6.11967945\n",
      "Validated batch 967 batch loss 5.55185127\n",
      "Validated batch 968 batch loss 6.17175055\n",
      "Validated batch 969 batch loss 6.56795692\n",
      "Validated batch 970 batch loss 5.33318472\n",
      "Validated batch 971 batch loss 5.86438656\n",
      "Validated batch 972 batch loss 6.5363121\n",
      "Validated batch 973 batch loss 5.96785259\n",
      "Validated batch 974 batch loss 6.43970346\n",
      "Validated batch 975 batch loss 6.27505302\n",
      "Validated batch 976 batch loss 5.18268251\n",
      "Validated batch 977 batch loss 6.07380962\n",
      "Validated batch 978 batch loss 6.38657284\n",
      "Validated batch 979 batch loss 5.89266396\n",
      "Validated batch 980 batch loss 5.23568296\n",
      "Validated batch 981 batch loss 5.70290375\n",
      "Validated batch 982 batch loss 5.74663973\n",
      "Validated batch 983 batch loss 5.67078209\n",
      "Validated batch 984 batch loss 5.50753117\n",
      "Validated batch 985 batch loss 5.74936676\n",
      "Validated batch 986 batch loss 5.8509717\n",
      "Validated batch 987 batch loss 5.74073029\n",
      "Validated batch 988 batch loss 5.7627573\n",
      "Validated batch 989 batch loss 5.77882719\n",
      "Validated batch 990 batch loss 5.29522419\n",
      "Validated batch 991 batch loss 6.04884052\n",
      "Validated batch 992 batch loss 6.1265564\n",
      "Validated batch 993 batch loss 5.88699389\n",
      "Validated batch 994 batch loss 7.25003958\n",
      "Validated batch 995 batch loss 5.69507504\n",
      "Validated batch 996 batch loss 5.85685492\n",
      "Validated batch 997 batch loss 5.96197796\n",
      "Validated batch 998 batch loss 7.16850662\n",
      "Validated batch 999 batch loss 4.6541934\n",
      "Validated batch 1000 batch loss 5.35032\n",
      "Validated batch 1001 batch loss 5.88998079\n",
      "Validated batch 1002 batch loss 6.28452778\n",
      "Validated batch 1003 batch loss 6.00711632\n",
      "Validated batch 1004 batch loss 6.16830635\n",
      "Validated batch 1005 batch loss 6.31955338\n",
      "Validated batch 1006 batch loss 5.4892149\n",
      "Validated batch 1007 batch loss 6.50657034\n",
      "Validated batch 1008 batch loss 6.4492774\n",
      "Validated batch 1009 batch loss 6.11935949\n",
      "Validated batch 1010 batch loss 5.78137589\n",
      "Validated batch 1011 batch loss 6.40540743\n",
      "Validated batch 1012 batch loss 6.35322285\n",
      "Validated batch 1013 batch loss 5.79611826\n",
      "Validated batch 1014 batch loss 5.58584595\n",
      "Validated batch 1015 batch loss 6.68899202\n",
      "Validated batch 1016 batch loss 5.58774614\n",
      "Validated batch 1017 batch loss 6.17596531\n",
      "Validated batch 1018 batch loss 4.70143\n",
      "Validated batch 1019 batch loss 6.07777834\n",
      "Validated batch 1020 batch loss 6.53925419\n",
      "Validated batch 1021 batch loss 6.06112766\n",
      "Validated batch 1022 batch loss 6.73225069\n",
      "Validated batch 1023 batch loss 5.63386488\n",
      "Validated batch 1024 batch loss 5.71634388\n",
      "Validated batch 1025 batch loss 5.35364628\n",
      "Validated batch 1026 batch loss 5.62685204\n",
      "Validated batch 1027 batch loss 6.48734808\n",
      "Validated batch 1028 batch loss 4.98918533\n",
      "Validated batch 1029 batch loss 5.96509552\n",
      "Validated batch 1030 batch loss 6.13104391\n",
      "Validated batch 1031 batch loss 5.55507469\n",
      "Validated batch 1032 batch loss 5.61321115\n",
      "Validated batch 1033 batch loss 5.28960419\n",
      "Validated batch 1034 batch loss 6.39320755\n",
      "Validated batch 1035 batch loss 6.98785496\n",
      "Validated batch 1036 batch loss 5.89290524\n",
      "Validated batch 1037 batch loss 6.21809292\n",
      "Validated batch 1038 batch loss 6.16847801\n",
      "Validated batch 1039 batch loss 5.60899353\n",
      "Validated batch 1040 batch loss 5.41922665\n",
      "Validated batch 1041 batch loss 5.4734478\n",
      "Validated batch 1042 batch loss 6.36240864\n",
      "Validated batch 1043 batch loss 6.21875381\n",
      "Validated batch 1044 batch loss 6.73811\n",
      "Validated batch 1045 batch loss 6.13474655\n",
      "Validated batch 1046 batch loss 5.71487951\n",
      "Validated batch 1047 batch loss 5.94729233\n",
      "Validated batch 1048 batch loss 6.24343586\n",
      "Validated batch 1049 batch loss 5.97168684\n",
      "Validated batch 1050 batch loss 5.62517262\n",
      "Validated batch 1051 batch loss 5.91858292\n",
      "Validated batch 1052 batch loss 5.63983345\n",
      "Validated batch 1053 batch loss 5.97867441\n",
      "Validated batch 1054 batch loss 7.33576488\n",
      "Validated batch 1055 batch loss 5.57336473\n",
      "Validated batch 1056 batch loss 6.29366684\n",
      "Validated batch 1057 batch loss 6.40879822\n",
      "Validated batch 1058 batch loss 6.645648\n",
      "Validated batch 1059 batch loss 4.91045856\n",
      "Validated batch 1060 batch loss 7.13126\n",
      "Validated batch 1061 batch loss 6.82970142\n",
      "Validated batch 1062 batch loss 6.71702576\n",
      "Validated batch 1063 batch loss 6.36459255\n",
      "Validated batch 1064 batch loss 6.02783298\n",
      "Validated batch 1065 batch loss 6.27265835\n",
      "Validated batch 1066 batch loss 6.83592224\n",
      "Validated batch 1067 batch loss 6.36613274\n",
      "Validated batch 1068 batch loss 6.34578228\n",
      "Validated batch 1069 batch loss 6.12346649\n",
      "Validated batch 1070 batch loss 7.03731537\n",
      "Validated batch 1071 batch loss 6.59049416\n",
      "Validated batch 1072 batch loss 7.23543882\n",
      "Validated batch 1073 batch loss 6.2441287\n",
      "Validated batch 1074 batch loss 7.32514524\n",
      "Validated batch 1075 batch loss 6.98020554\n",
      "Validated batch 1076 batch loss 7.35958767\n",
      "Validated batch 1077 batch loss 6.24659586\n",
      "Validated batch 1078 batch loss 7.89472771\n",
      "Validated batch 1079 batch loss 7.24010658\n",
      "Validated batch 1080 batch loss 7.64785957\n",
      "Validated batch 1081 batch loss 7.17806911\n",
      "Validated batch 1082 batch loss 7.19156075\n",
      "Validated batch 1083 batch loss 6.45781898\n",
      "Validated batch 1084 batch loss 7.66402626\n",
      "Validated batch 1085 batch loss 7.18372488\n",
      "Validated batch 1086 batch loss 6.312644\n",
      "Validated batch 1087 batch loss 6.03189898\n",
      "Validated batch 1088 batch loss 6.4608469\n",
      "Validated batch 1089 batch loss 6.01746321\n",
      "Validated batch 1090 batch loss 6.22094822\n",
      "Validated batch 1091 batch loss 5.86601162\n",
      "Validated batch 1092 batch loss 5.54471588\n",
      "Validated batch 1093 batch loss 6.53351593\n",
      "Validated batch 1094 batch loss 6.57559395\n",
      "Validated batch 1095 batch loss 6.53900623\n",
      "Validated batch 1096 batch loss 6.19426394\n",
      "Validated batch 1097 batch loss 6.28176689\n",
      "Validated batch 1098 batch loss 5.93839645\n",
      "Validated batch 1099 batch loss 5.27537775\n",
      "Validated batch 1100 batch loss 5.22211933\n",
      "Validated batch 1101 batch loss 5.80359745\n",
      "Validated batch 1102 batch loss 5.39060974\n",
      "Validated batch 1103 batch loss 6.1125288\n",
      "Validated batch 1104 batch loss 5.82435703\n",
      "Validated batch 1105 batch loss 6.43946266\n",
      "Validated batch 1106 batch loss 5.20505524\n",
      "Validated batch 1107 batch loss 5.06217\n",
      "Validated batch 1108 batch loss 5.45945\n",
      "Validated batch 1109 batch loss 5.37460709\n",
      "Validated batch 1110 batch loss 6.06004906\n",
      "Validated batch 1111 batch loss 6.29814339\n",
      "Validated batch 1112 batch loss 6.03571224\n",
      "Validated batch 1113 batch loss 5.54569578\n",
      "Validated batch 1114 batch loss 6.29657745\n",
      "Validated batch 1115 batch loss 5.98343229\n",
      "Validated batch 1116 batch loss 6.38105488\n",
      "Validated batch 1117 batch loss 5.93278599\n",
      "Validated batch 1118 batch loss 6.68938923\n",
      "Validated batch 1119 batch loss 7.15420818\n",
      "Validated batch 1120 batch loss 5.33297825\n",
      "Validated batch 1121 batch loss 6.04175377\n",
      "Validated batch 1122 batch loss 5.95683\n",
      "Validated batch 1123 batch loss 5.51355648\n",
      "Validated batch 1124 batch loss 6.69420242\n",
      "Validated batch 1125 batch loss 5.83242321\n",
      "Validated batch 1126 batch loss 6.37068892\n",
      "Validated batch 1127 batch loss 5.00286579\n",
      "Validated batch 1128 batch loss 6.1230669\n",
      "Validated batch 1129 batch loss 5.83972836\n",
      "Validated batch 1130 batch loss 4.8729372\n",
      "Validated batch 1131 batch loss 4.60581875\n",
      "Validated batch 1132 batch loss 5.26505947\n",
      "Validated batch 1133 batch loss 5.28546238\n",
      "Validated batch 1134 batch loss 5.59699106\n",
      "Validated batch 1135 batch loss 5.88787746\n",
      "Validated batch 1136 batch loss 6.52106857\n",
      "Validated batch 1137 batch loss 6.09232807\n",
      "Validated batch 1138 batch loss 6.30608\n",
      "Validated batch 1139 batch loss 6.22301674\n",
      "Validated batch 1140 batch loss 5.77011\n",
      "Validated batch 1141 batch loss 6.90121841\n",
      "Validated batch 1142 batch loss 6.38543224\n",
      "Validated batch 1143 batch loss 5.2803936\n",
      "Validated batch 1144 batch loss 4.99108505\n",
      "Validated batch 1145 batch loss 4.77176142\n",
      "Validated batch 1146 batch loss 6.91298485\n",
      "Validated batch 1147 batch loss 6.50436211\n",
      "Validated batch 1148 batch loss 6.88509655\n",
      "Validated batch 1149 batch loss 6.63723755\n",
      "Validated batch 1150 batch loss 8.32937622\n",
      "Validated batch 1151 batch loss 6.36602402\n",
      "Validated batch 1152 batch loss 5.93661594\n",
      "Validated batch 1153 batch loss 5.48281574\n",
      "Validated batch 1154 batch loss 4.9547205\n",
      "Validated batch 1155 batch loss 5.41399431\n",
      "Validated batch 1156 batch loss 6.15962029\n",
      "Validated batch 1157 batch loss 6.7283268\n",
      "Validated batch 1158 batch loss 6.34641314\n",
      "Validated batch 1159 batch loss 5.11720181\n",
      "Validated batch 1160 batch loss 5.56329918\n",
      "Validated batch 1161 batch loss 6.14123487\n",
      "Validated batch 1162 batch loss 6.75733709\n",
      "Validated batch 1163 batch loss 6.24762201\n",
      "Validated batch 1164 batch loss 5.77181816\n",
      "Validated batch 1165 batch loss 6.24510813\n",
      "Validated batch 1166 batch loss 5.01872158\n",
      "Validated batch 1167 batch loss 5.52718544\n",
      "Validated batch 1168 batch loss 5.75658035\n",
      "Validated batch 1169 batch loss 6.14639187\n",
      "Validated batch 1170 batch loss 6.04759789\n",
      "Validated batch 1171 batch loss 6.03909397\n",
      "Validated batch 1172 batch loss 5.84964848\n",
      "Validated batch 1173 batch loss 5.5293541\n",
      "Validated batch 1174 batch loss 6.06354189\n",
      "Validated batch 1175 batch loss 6.35207558\n",
      "Validated batch 1176 batch loss 6.01049423\n",
      "Validated batch 1177 batch loss 5.85776043\n",
      "Validated batch 1178 batch loss 6.10934734\n",
      "Validated batch 1179 batch loss 5.58123589\n",
      "Validated batch 1180 batch loss 6.8671031\n",
      "Validated batch 1181 batch loss 5.30527592\n",
      "Validated batch 1182 batch loss 6.43440247\n",
      "Validated batch 1183 batch loss 5.9974823\n",
      "Validated batch 1184 batch loss 6.55670452\n",
      "Validated batch 1185 batch loss 5.40208817\n",
      "Validated batch 1186 batch loss 6.30204725\n",
      "Validated batch 1187 batch loss 6.25001717\n",
      "Validated batch 1188 batch loss 6.13333941\n",
      "Validated batch 1189 batch loss 6.33760738\n",
      "Validated batch 1190 batch loss 6.05195332\n",
      "Validated batch 1191 batch loss 5.45139217\n",
      "Validated batch 1192 batch loss 5.57356\n",
      "Validated batch 1193 batch loss 5.52459049\n",
      "Validated batch 1194 batch loss 5.33501482\n",
      "Validated batch 1195 batch loss 5.8409071\n",
      "Validated batch 1196 batch loss 6.92867851\n",
      "Validated batch 1197 batch loss 6.53275585\n",
      "Validated batch 1198 batch loss 7.62440062\n",
      "Validated batch 1199 batch loss 5.54941177\n",
      "Validated batch 1200 batch loss 6.17351913\n",
      "Validated batch 1201 batch loss 4.66468668\n",
      "Validated batch 1202 batch loss 5.15830612\n",
      "Validated batch 1203 batch loss 6.04732513\n",
      "Validated batch 1204 batch loss 6.46155357\n",
      "Validated batch 1205 batch loss 6.21364832\n",
      "Validated batch 1206 batch loss 5.36709356\n",
      "Validated batch 1207 batch loss 4.67980957\n",
      "Validated batch 1208 batch loss 5.9862113\n",
      "Validated batch 1209 batch loss 7.41522217\n",
      "Validated batch 1210 batch loss 5.9729104\n",
      "Validated batch 1211 batch loss 6.26577\n",
      "Validated batch 1212 batch loss 6.12543726\n",
      "Validated batch 1213 batch loss 4.98953342\n",
      "Validated batch 1214 batch loss 5.26158905\n",
      "Validated batch 1215 batch loss 6.10872841\n",
      "Validated batch 1216 batch loss 6.30789423\n",
      "Validated batch 1217 batch loss 6.06330252\n",
      "Validated batch 1218 batch loss 6.33602905\n",
      "Validated batch 1219 batch loss 5.93648911\n",
      "Validated batch 1220 batch loss 6.99660778\n",
      "Validated batch 1221 batch loss 5.71466303\n",
      "Validated batch 1222 batch loss 6.37933111\n",
      "Validated batch 1223 batch loss 6.05145407\n",
      "Validated batch 1224 batch loss 6.19491673\n",
      "Validated batch 1225 batch loss 5.71351624\n",
      "Validated batch 1226 batch loss 7.55393696\n",
      "Validated batch 1227 batch loss 6.50700283\n",
      "Validated batch 1228 batch loss 7.15980339\n",
      "Validated batch 1229 batch loss 6.42571783\n",
      "Validated batch 1230 batch loss 5.99995565\n",
      "Validated batch 1231 batch loss 7.31004477\n",
      "Validated batch 1232 batch loss 6.58612251\n",
      "Validated batch 1233 batch loss 6.45005369\n",
      "Validated batch 1234 batch loss 5.62227249\n",
      "Validated batch 1235 batch loss 5.92837429\n",
      "Validated batch 1236 batch loss 6.24755907\n",
      "Validated batch 1237 batch loss 6.58833122\n",
      "Validated batch 1238 batch loss 5.95328903\n",
      "Validated batch 1239 batch loss 5.29982042\n",
      "Validated batch 1240 batch loss 5.14802551\n",
      "Validated batch 1241 batch loss 5.27223396\n",
      "Validated batch 1242 batch loss 6.67758369\n",
      "Validated batch 1243 batch loss 7.34841728\n",
      "Validated batch 1244 batch loss 7.02986431\n",
      "Validated batch 1245 batch loss 6.3678546\n",
      "Validated batch 1246 batch loss 6.4024334\n",
      "Validated batch 1247 batch loss 7.01517677\n",
      "Validated batch 1248 batch loss 5.64398575\n",
      "Validated batch 1249 batch loss 5.10514641\n",
      "Validated batch 1250 batch loss 7.55803871\n",
      "Validated batch 1251 batch loss 6.2794\n",
      "Validated batch 1252 batch loss 5.38891506\n",
      "Validated batch 1253 batch loss 5.85573339\n",
      "Validated batch 1254 batch loss 5.40225458\n",
      "Validated batch 1255 batch loss 5.4740243\n",
      "Validated batch 1256 batch loss 5.64666\n",
      "Validated batch 1257 batch loss 5.00871515\n",
      "Validated batch 1258 batch loss 5.71239567\n",
      "Validated batch 1259 batch loss 6.55257034\n",
      "Validated batch 1260 batch loss 4.96355104\n",
      "Validated batch 1261 batch loss 5.52190161\n",
      "Validated batch 1262 batch loss 5.74825382\n",
      "Validated batch 1263 batch loss 5.66529846\n",
      "Validated batch 1264 batch loss 6.1045742\n",
      "Validated batch 1265 batch loss 6.0934763\n",
      "Validated batch 1266 batch loss 5.83133459\n",
      "Validated batch 1267 batch loss 6.47516966\n",
      "Validated batch 1268 batch loss 6.05549526\n",
      "Validated batch 1269 batch loss 5.37739182\n",
      "Validated batch 1270 batch loss 5.72746944\n",
      "Validated batch 1271 batch loss 5.75451279\n",
      "Validated batch 1272 batch loss 5.24648237\n",
      "Validated batch 1273 batch loss 6.01817036\n",
      "Validated batch 1274 batch loss 5.70444632\n",
      "Validated batch 1275 batch loss 5.67551279\n",
      "Validated batch 1276 batch loss 5.73640633\n",
      "Validated batch 1277 batch loss 6.08460426\n",
      "Validated batch 1278 batch loss 6.24425554\n",
      "Validated batch 1279 batch loss 6.09526348\n",
      "Validated batch 1280 batch loss 6.35033798\n",
      "Validated batch 1281 batch loss 6.29908323\n",
      "Validated batch 1282 batch loss 6.00655556\n",
      "Validated batch 1283 batch loss 6.84329414\n",
      "Validated batch 1284 batch loss 6.5529685\n",
      "Validated batch 1285 batch loss 7.09398\n",
      "Validated batch 1286 batch loss 6.88452101\n",
      "Validated batch 1287 batch loss 6.13074732\n",
      "Validated batch 1288 batch loss 5.92174435\n",
      "Validated batch 1289 batch loss 6.67323303\n",
      "Validated batch 1290 batch loss 6.61167145\n",
      "Validated batch 1291 batch loss 5.83666563\n",
      "Validated batch 1292 batch loss 6.39136219\n",
      "Validated batch 1293 batch loss 6.02715492\n",
      "Validated batch 1294 batch loss 5.77506\n",
      "Validated batch 1295 batch loss 6.30794287\n",
      "Validated batch 1296 batch loss 6.04977942\n",
      "Validated batch 1297 batch loss 5.88241959\n",
      "Validated batch 1298 batch loss 6.86853456\n",
      "Validated batch 1299 batch loss 6.00169659\n",
      "Validated batch 1300 batch loss 6.71590805\n",
      "Validated batch 1301 batch loss 6.99280453\n",
      "Validated batch 1302 batch loss 5.70942116\n",
      "Validated batch 1303 batch loss 5.93705654\n",
      "Validated batch 1304 batch loss 5.70918369\n",
      "Validated batch 1305 batch loss 6.44364691\n",
      "Validated batch 1306 batch loss 5.69477415\n",
      "Validated batch 1307 batch loss 5.51617813\n",
      "Validated batch 1308 batch loss 7.09898281\n",
      "Validated batch 1309 batch loss 6.04955387\n",
      "Validated batch 1310 batch loss 7.22170258\n",
      "Validated batch 1311 batch loss 5.51319599\n",
      "Validated batch 1312 batch loss 6.19921398\n",
      "Validated batch 1313 batch loss 5.56769562\n",
      "Validated batch 1314 batch loss 5.04138851\n",
      "Validated batch 1315 batch loss 6.14047241\n",
      "Validated batch 1316 batch loss 5.65667105\n",
      "Validated batch 1317 batch loss 6.4283638\n",
      "Validated batch 1318 batch loss 5.6561408\n",
      "Validated batch 1319 batch loss 5.90545654\n",
      "Validated batch 1320 batch loss 4.93183517\n",
      "Validated batch 1321 batch loss 5.97191238\n",
      "Validated batch 1322 batch loss 4.99277782\n",
      "Validated batch 1323 batch loss 6.4768486\n",
      "Validated batch 1324 batch loss 6.10863686\n",
      "Validated batch 1325 batch loss 5.65993261\n",
      "Validated batch 1326 batch loss 6.17527914\n",
      "Validated batch 1327 batch loss 5.16293907\n",
      "Validated batch 1328 batch loss 6.41093874\n",
      "Validated batch 1329 batch loss 6.02124643\n",
      "Validated batch 1330 batch loss 5.64027882\n",
      "Validated batch 1331 batch loss 6.33233547\n",
      "Validated batch 1332 batch loss 6.04683\n",
      "Validated batch 1333 batch loss 5.35299873\n",
      "Validated batch 1334 batch loss 6.88710356\n",
      "Validated batch 1335 batch loss 7.17834091\n",
      "Validated batch 1336 batch loss 7.2391367\n",
      "Validated batch 1337 batch loss 6.27537441\n",
      "Validated batch 1338 batch loss 6.77661228\n",
      "Validated batch 1339 batch loss 5.62554073\n",
      "Validated batch 1340 batch loss 5.56990623\n",
      "Validated batch 1341 batch loss 5.74745941\n",
      "Validated batch 1342 batch loss 6.16028738\n",
      "Validated batch 1343 batch loss 5.50645208\n",
      "Validated batch 1344 batch loss 5.33596706\n",
      "Validated batch 1345 batch loss 6.19419336\n",
      "Validated batch 1346 batch loss 6.56547117\n",
      "Validated batch 1347 batch loss 6.29987526\n",
      "Validated batch 1348 batch loss 5.74859715\n",
      "Validated batch 1349 batch loss 5.94267273\n",
      "Validated batch 1350 batch loss 6.27984285\n",
      "Validated batch 1351 batch loss 5.32792473\n",
      "Validated batch 1352 batch loss 5.48092747\n",
      "Validated batch 1353 batch loss 6.57933903\n",
      "Validated batch 1354 batch loss 6.60852528\n",
      "Validated batch 1355 batch loss 6.21385384\n",
      "Validated batch 1356 batch loss 6.79507637\n",
      "Validated batch 1357 batch loss 6.21194077\n",
      "Validated batch 1358 batch loss 6.12999487\n",
      "Validated batch 1359 batch loss 5.65677357\n",
      "Validated batch 1360 batch loss 5.7703371\n",
      "Validated batch 1361 batch loss 5.785923\n",
      "Validated batch 1362 batch loss 6.3274188\n",
      "Validated batch 1363 batch loss 5.42509222\n",
      "Validated batch 1364 batch loss 6.8647728\n",
      "Validated batch 1365 batch loss 6.68530798\n",
      "Validated batch 1366 batch loss 6.60766792\n",
      "Validated batch 1367 batch loss 6.37504959\n",
      "Validated batch 1368 batch loss 7.01600075\n",
      "Validated batch 1369 batch loss 4.84838057\n",
      "Validated batch 1370 batch loss 5.91080189\n",
      "Validated batch 1371 batch loss 6.13698816\n",
      "Validated batch 1372 batch loss 6.13370895\n",
      "Validated batch 1373 batch loss 5.91269\n",
      "Validated batch 1374 batch loss 6.18401146\n",
      "Validated batch 1375 batch loss 5.22338486\n",
      "Validated batch 1376 batch loss 5.5215559\n",
      "Validated batch 1377 batch loss 6.46372223\n",
      "Validated batch 1378 batch loss 5.71621609\n",
      "Validated batch 1379 batch loss 5.63942432\n",
      "Validated batch 1380 batch loss 6.36881399\n",
      "Validated batch 1381 batch loss 6.17849398\n",
      "Validated batch 1382 batch loss 6.20469046\n",
      "Validated batch 1383 batch loss 5.57970238\n",
      "Validated batch 1384 batch loss 5.65818691\n",
      "Validated batch 1385 batch loss 6.27526569\n",
      "Validated batch 1386 batch loss 5.47545671\n",
      "Validated batch 1387 batch loss 5.35589409\n",
      "Validated batch 1388 batch loss 4.9927454\n",
      "Validated batch 1389 batch loss 5.09831429\n",
      "Validated batch 1390 batch loss 6.04275036\n",
      "Validated batch 1391 batch loss 5.96960449\n",
      "Validated batch 1392 batch loss 5.91354847\n",
      "Validated batch 1393 batch loss 5.65196323\n",
      "Validated batch 1394 batch loss 6.73941517\n",
      "Validated batch 1395 batch loss 7.74842548\n",
      "Validated batch 1396 batch loss 6.26919794\n",
      "Validated batch 1397 batch loss 7.65928268\n",
      "Validated batch 1398 batch loss 7.39991093\n",
      "Validated batch 1399 batch loss 7.32418728\n",
      "Validated batch 1400 batch loss 6.65900373\n",
      "Validated batch 1401 batch loss 6.30918026\n",
      "Validated batch 1402 batch loss 5.60868168\n",
      "Validated batch 1403 batch loss 4.50844955\n",
      "Validated batch 1404 batch loss 6.3812561\n",
      "Validated batch 1405 batch loss 5.74634266\n",
      "Validated batch 1406 batch loss 4.66176414\n",
      "Validated batch 1407 batch loss 6.04695654\n",
      "Validated batch 1408 batch loss 6.423666\n",
      "Validated batch 1409 batch loss 6.66209602\n",
      "Validated batch 1410 batch loss 5.67336559\n",
      "Validated batch 1411 batch loss 6.60404253\n",
      "Validated batch 1412 batch loss 6.7576685\n",
      "Validated batch 1413 batch loss 5.60065889\n",
      "Validated batch 1414 batch loss 5.34622383\n",
      "Validated batch 1415 batch loss 5.23918533\n",
      "Validated batch 1416 batch loss 6.16362476\n",
      "Validated batch 1417 batch loss 5.8730526\n",
      "Validated batch 1418 batch loss 6.02630424\n",
      "Validated batch 1419 batch loss 6.66091394\n",
      "Validated batch 1420 batch loss 6.4230566\n",
      "Validated batch 1421 batch loss 5.95561457\n",
      "Validated batch 1422 batch loss 5.40603876\n",
      "Validated batch 1423 batch loss 6.93028069\n",
      "Validated batch 1424 batch loss 6.02468681\n",
      "Validated batch 1425 batch loss 6.39785957\n",
      "Validated batch 1426 batch loss 5.55673265\n",
      "Validated batch 1427 batch loss 6.96023273\n",
      "Validated batch 1428 batch loss 5.43097448\n",
      "Validated batch 1429 batch loss 6.15284634\n",
      "Validated batch 1430 batch loss 6.60668373\n",
      "Validated batch 1431 batch loss 6.60499859\n",
      "Validated batch 1432 batch loss 6.78701878\n",
      "Validated batch 1433 batch loss 5.55578613\n",
      "Validated batch 1434 batch loss 4.70320415\n",
      "Validated batch 1435 batch loss 4.55204868\n",
      "Validated batch 1436 batch loss 5.20137453\n",
      "Validated batch 1437 batch loss 5.16085434\n",
      "Validated batch 1438 batch loss 5.38740253\n",
      "Validated batch 1439 batch loss 6.08296871\n",
      "Validated batch 1440 batch loss 5.61325216\n",
      "Validated batch 1441 batch loss 6.12327766\n",
      "Validated batch 1442 batch loss 5.89936161\n",
      "Validated batch 1443 batch loss 5.15459442\n",
      "Validated batch 1444 batch loss 5.50827742\n",
      "Validated batch 1445 batch loss 6.3806262\n",
      "Validated batch 1446 batch loss 5.81388283\n",
      "Validated batch 1447 batch loss 6.20426\n",
      "Validated batch 1448 batch loss 6.21962738\n",
      "Validated batch 1449 batch loss 6.0777154\n",
      "Validated batch 1450 batch loss 5.96742105\n",
      "Validated batch 1451 batch loss 5.99066162\n",
      "Validated batch 1452 batch loss 5.83408\n",
      "Validated batch 1453 batch loss 6.07501\n",
      "Validated batch 1454 batch loss 5.48593521\n",
      "Validated batch 1455 batch loss 6.46989346\n",
      "Validated batch 1456 batch loss 5.39859\n",
      "Validated batch 1457 batch loss 6.69296455\n",
      "Validated batch 1458 batch loss 6.10359669\n",
      "Validated batch 1459 batch loss 5.294384\n",
      "Validated batch 1460 batch loss 4.50678158\n",
      "Validated batch 1461 batch loss 5.98652077\n",
      "Validated batch 1462 batch loss 6.45123672\n",
      "Validated batch 1463 batch loss 6.2307806\n",
      "Validated batch 1464 batch loss 6.07658863\n",
      "Validated batch 1465 batch loss 5.68291569\n",
      "Validated batch 1466 batch loss 5.40461254\n",
      "Validated batch 1467 batch loss 5.06117439\n",
      "Validated batch 1468 batch loss 7.40673208\n",
      "Validated batch 1469 batch loss 6.71808815\n",
      "Validated batch 1470 batch loss 6.60247612\n",
      "Validated batch 1471 batch loss 6.28682804\n",
      "Validated batch 1472 batch loss 6.18559456\n",
      "Validated batch 1473 batch loss 5.04108095\n",
      "Validated batch 1474 batch loss 6.74563074\n",
      "Validated batch 1475 batch loss 5.50954294\n",
      "Validated batch 1476 batch loss 5.9778223\n",
      "Validated batch 1477 batch loss 6.3165226\n",
      "Validated batch 1478 batch loss 6.87358952\n",
      "Validated batch 1479 batch loss 6.7846117\n",
      "Epoch 3 val loss 6.071439743041992\n",
      "Model ./models/model-v0.0.1-epoch-3-loss-6.0714.h5 saved.\n"
     ]
    }
   ],
   "source": [
    "from mpii_gtuV0hd.train import train\n",
    "\n",
    "\n",
    "train_tfrecords = os.path.join(TFRECORD_PATH, 'train*')\n",
    "val_tfrecords = os.path.join(TFRECORD_PATH, 'val*')\n",
    "epochs = 3\n",
    "batch_size = 2\n",
    "num_heatmap = 16\n",
    "learning_rate = 0.0007\n",
    "\n",
    "best_model_file = train(epochs, learning_rate,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hourglass 모델의 예측:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplebaseline 모델의 예측:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzYAAAIzCAYAAADS9YxGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAADp0klEQVR4nOz9eZRmZ3neC9/7HeuteerqudXdUmtCgEACMRkJ22CISUJI8ESS4y/HDjaOEzKREPKdJXN8pIATm7PCMTZOgp34wyaJj2OWBwx2GIwZBAIxaB5a6kE9Vtdc9c77+6ODpK7r94hdCKF+xfVbS3/0rb338+xn3PvddV13lud5HsYYY4wxxhgzwJSe6QoYY4wxxhhjzFPFLzbGGGOMMcaYgccvNsYYY4wxxpiBxy82xhhjjDHGmIHHLzbGGGOMMcaYgccvNsYYY4wxxpiBxy82xhhjjDHGmIHHLzbGGGOMMcaYgccvNsYYY4wxxpiBxy82xhhjjDHGmIHnaXux+dVf/dU4cOBADA0NxXXXXRd/8Rd/8XQVZYwxxhhjjPkep/J0XPTDH/5wvO1tb4tf/dVfjZe//OXx67/+6/G6170u7rrrrti3b9+Tntvv9+PRRx+NsbGxyLLs6aieMcYYY4wxZgDI8zxWVlZi165dUSo9+TeZLM/z/DtdgRtuuCFe+MIXxvvf//7HYldddVW84Q1viFtvvfVJzz127Fjs3bv3O10lY4wxxhhjzIBy9OjR2LNnz5Me8x3/YtNut+P222+Pf/kv/+UF8de85jXx2c9+Vo5vtVrRarUe+/c337P+w3/7YgwPjz5+YEmrOjwyJrGsWsN65aW+xHqZxnL4SFSGVhofqUtsKPESOQzxE4cflNjpo2ckdt89d0ls74FLJHboqkNY9sEr9AtZr6rHLbW0LVajqwcOafu2E/dN4R7E6Lscv21rHUuJv6bsw7FVOFaP4vr06cAEOdQevz4W/Emhn8N9J36x2EI1C1GGWOqnELxFOLYEx9HdlOE4GLoRwQsZ1h1iT/W7cAb9wwdqSTRWkESj9wtWPuvr+evr6xIbHhrRcxNjLcfZDGUX/fK+hZ/Yil6z6HFlGAXUZhERdTiWuqcNA7AHY6UGoz9LtAWd3yv42yTtbbWSVjL1W2dO45fmd8GOrECF+omFlvqx6G+yOS04QD+168BzAlEqOH6zvLgCgPqMapMXfJbZStlF507x38ahjql17Smsyk/1L31Kmc4JWmc7MBef6v6SwcTHNoI60nFdmE80j88fq+t5l9Yb2A+6eOfwvJXo7wwesnu9C+uzvrwSP3bgQIyN6XP/Zr7jLzZnz56NXq8X27dvvyC+ffv2OHnypBx/6623xi/8wi9IfHh49MIXF3ixGRkdl9hTfbGhAVyBp6nRLbzYjEB8GV7Khof1QWNoqCGxRkMfPkZGubPHxrWN6MWmBy82UfDFpjUgLzb0AEH1wXr7xeYxno4XGypnUF5sSt+FF5vkA8BTeLEpVeDHInixSY+17+yLTeph/ild0y82j+EXmyfHLzabjvWLzWPQOtv+HnixoZe3Z+rF5rFjC/Tv02YesLnwPM+xQu94xztiaWnpsf+OHj36dFXJGGOMMcYY8yzlO/7FZnZ2NsrlsnydOX36tHzFiYio1+tRr+vXjzOnTkZjePWxf49PTskxOfwk0XzCn7U9kR78ulgf0t9/6VfIoYYeV21pnVdba1j2vUcPS2xjYUGv2de38KsO6p+dnV44JbGP/Nfbsezp7bMSu/ra50ls3+VXSGx2x6TEVuAlutdN/MoGb/Y5/CJBL7z0I3EXfkHtZfxWX4VfNFpwfgV+zaPfFPDLTuqHqu+w58VWfoGiOtGvTfQjJn2V2ooALy/2Az5/2YFxUXqKv7w9FfHgVn55o18X6Vdr+jpDv7Jtpb/pqwKdT7+UVav0/avYL7/n/8e3qt3/OgzKpq9A1D6lRKsX/rW+YNn0Jxf0xe/8sYWKTn7Z3Ax9cckSJxf9OkNfdkoZ3Df8opv6QkcfLqh3MlpcEl+/5NytjP2CX3FojuCXzkTRRb/E4Ln0dW8L59OXO2xeOvmpqqeh3ZJrwXeBp/LVbmvlaIyKqdB8onlHH1wS45zmbfGv01of+quH1JdJGqtlWAvor2TweY3GfuKLLH2p3dy3W+nr7/gXm1qtFtddd118/OMfvyD+8Y9/PF72spd9p4szxhhjjDHGmKfH7vmf/JN/En/n7/yduP766+OlL31pfOADH4gjR47Ez/zMzzwdxRljjDHGGGO+x3laXmx+9Ed/NObn5+Nd73pXnDhxIq655pr44z/+47jkEv2zKmOMMcYYY4x5qjwtLzYREW9961vjrW9969N1eWOMMcYYY4x5jKftxeap0u/1ot973G6422rKMfXqkMTKYFkXEVEj68P1Db1mGYSGa4sSO9dZlVhz5SyW3cg7Etve0KZvlFXy1IH7ntim1s5zDU5YdPLsvMTuv+0LEjty130SGwOzh0PPV+OB6d07sOwyOG/nIChbautxzVyDWUXlvHWwK41go4A6qC6bJC4msR6o0VJaNhSMPgWR41YEqEUF2CT0J8EdCym59HJRa10oiMrOu1rJUpn7uwfy2R5ZHMN46cOaUYFyUqJLqhEZCmTUNyD4RIVuwpmB7EFpdJShnBL52JOZRsqik8S8YNpB9039RW1BVuf/66JadkGjABTokukGlxw9qCc1ERli0JjMyO45YVHch3GAgnm4bzKQQBOHRH+XipoCfKc956O4rTmadtB9g+1xqtoo3iZ7ZSyHVge9Xi+5olOKAxr8MIbIRKRgvSMiyjCX0bwCxgXPsS04NhR0PqB7fKomA6RvJxE+mRlVaN3fQn2ozakp6Jo0hmjtTZmilMFyuQL9iIYjfdjHcPxxf9PSsvkey9QJCZ42u2djjDHGGGOM+W7hFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMBz0ZoH1PN21PPHxa35xrIc03qCucA3GR4exuuVuqBQz/X8PhgKtKHsXntFz93QWEREY7yux66q+UAnIRiVsjsqIK0nTBP2zo5LrFxrSGyjrSKztZOnJHbXymck1koI4aZ37dZgTYfcnkMHJTaze1Zi6z0tp9XRPoyIqFW1nJVmS2KNIe2bVlevWSbRekqPCAI5Et4W1TPmlAE6kckbhYog9iuDgr9PwmY6NyE/pOGLWZdBnFnFNgMheqLN+qAuzkm8WNBQoNvT+ZTyRiCxKYmyyyAsJUF3mfohceMpQwOhsHiWMl9v4ZLUuXDflL26T/LtRNmUTR4FtdiPZHAA54J5RUREF/aNKqw3JJZmIbGW0eulTBOKGiTAuCpqFLCFdQ2NCwqaiGwF6m+CzFOKQpnkIyL6fdhjoB+Kzm/0pklUG8c0XJMe5EjAT/sQZayPCHRTAG8l9I9gg4MtZI7HfayYKQoNga34WWRksEBlF/0uQPedmCNozgCQqQq1GRmyJM1BsM+Krd0daPQu1JHmSAT3z+YmT+39hL/YGGOMMcYYYwYev9gYY4wxxhhjBh6/2BhjjDHGGGMGHr/YGGOMMcYYYwYev9gYY4wxxhhjBp6L1hWt0l6IavlxF6tK1OSYUr4usU434dgE7jidpjqTRa8jITDyiqGqvhPWRyax7NaauqXV4KKtljqy9TpaH3JK2Vhbw7LJMade1vN3Tk5LbK2lTnLr4Mi22tY6RkQc+fJX9FhwGrr905+X2L4rD0nsla95lcQaE0NY9vx6U2Kjw3psC9yHGhXtG/JH2opLR1HDJzJLyaGglPEQxcmoqpTrHdVg6mTgNpaB20kEuwp12npsE5wHh8ZGJNag+iR+iunRnOiSXQ9ZEkE5cL0OzMXzx0JFwaWwAmXXwJGthP3NN57len4PHGrIVa1PoxoNfHiwoVMbjKs+1L0PVnJVaIuUg09W0jlKx5JrEl2yD+sS1TFVTqWidUeXQXT/g+MS4zwnpzW4n3JRZzJye4I9I3Us7S85jAGG6phyeyzm7oSOd+QsBmXAUnf+fArmxdy4yIWMFunUHKMxSOsI7iUQZTfNLcxvOLSwM+MW5jdXSEM01th5sLh7HzrrFZ1O0F/ozpncQ/XYLuwlNO9oYcN7TLpcFnPb64NDbB/6oQQutiWceREZjOnNd1jZguOhv9gYY4wxxhhjBh6/2BhjjDHGGGMGHr/YGGOMMcYYYwYev9gYY4wxxhhjBp6L1jxg4fTRaA49Ueyt72DD4xMSAx1mRESMDTckViVVdV8FwvWoSqwGQqt8oyWxiIiRmp5PavIKHFdrqOC9CyLmXZOTWHYFhPCtTldi5ZKKuqan9ZonTp/WcxNi0amdcxLrZHqPZ1fUBCJWVGD+2T/6qMSe/9IXY9k7Du7SS4JRAAllm9C+JGxOQZpLEhWWQYTHgk+tIwn9I1gbH2AWQUM/wGCjBPOukhAfnju7JLHF+UWJrS2rmcY8iMGnxsYl1gFDi4iIxpjO78aoxs4snJPY2ISWMzGha0utVseywU8jmk01r+iDkLNa1/YtQX/lKWUzCT7Les0eiDZZug312YLYlMSqPKZJJKuDspQSNpPhA4pV9cCNDe0b2l8qlZRhAxgNwCDo9XSdrVZJQK3lpMT/RUX0ObU5HEfrWikhbCb6sGbQmprRxgxGCOjsECwSL5FInKpO6x95iCSeHWj8ZgWNIUirn9NcTPU3GUNAfVhLTuMC+iZpNFFsDJWh0alryV0hJaLH2rCzjh6H5inF9+8+3CPFyCilAh3RhXuslPnRu6ghBtaHTCUKPotE8LzNYb3poYEEPKOQKQqY3UQk2nxTP5a34DPhLzbGGGOMMcaYgccvNsYYY4wxxpiBxy82xhhjjDHGmIHHLzbGGGOMMcaYgeeiNQ/odZrRfYLOqFzVqp44dlhiJPqNiKj0hyWWgZi8BGLINonRQPxfQkV2RANEx6NDGquDInwYBGE5ZTXfAAF+RLThHjcg8/vo+KTEOmCGUG1rOdPDanAQEZHVIOMzCOYbZe2bJoiLj544JbH3/h/vwrJvev1fkdhLf/AmiU1uU+F4C8Zau6tC4JTokn4toAzUzSaYTcB918FwYWF+Acsuh7bvCPRPFe6xMaSxXhtE0WA+EREx3NWy11p6/q7RKYltm9BYB0TeSy0Sfkf0mtoeCyd1vOQwv9eX1yR29sFjEtt94BIsO6vWJLa2tiqxoSHth16m5zYpWXRK2QxxykxOInrKsN0DMW6flcAoYiZhKo39CphFbKzTfEhk6C7pnCCjgaUlNbQgA4oOGIbU62wWUQNTARJvdzs6Vvfu2ykxarP1dV7PaQxRFnIS9ZORDImLU2YRRI8Ey3R+4QzoxUX0RA59k8O+jALzxDUrYLBA5hVkCER7RGI6MdhuJMrWw8ow58GHJgktOX16HgHzi+KFPLXf1mltoecj9B1IGFX0oXPJ5ACNISBWB/F/D+ZnRCQqSqZHdN9QR7gceMucPx/MENBkA26cDIVyXAgSbQ6xzbUBD5Ak/mJjjDHGGGOMGXj8YmOMMcYYY4wZePxiY4wxxhhjjBl4/GJjjDHGGGOMGXguWvOAvN+JvPe46KoPSraZaRV+N2oqxo2I2FjTbOckIu01NUaZdTPIHrxj+zYsuz6kQtduV8vJQNlXr+t9ry+pMLnMasbogqqQMswunjsrsUoNROdgcEBGChHnDSAEEJ6PVFWkO17XrPF75q6S2KEDe7Dsuw4/LLEPve/9Envu9ddJ7MrnP0diO3bvkFgloWbrg7Z+cVFFzOtn5iU23ND7npzScdVpqtg5IqK9oUL4zqKOlz5l4wZhMgkSR4a0jhERDRALTs/tklgVmq1H5gwgdp6a4zl2+uSjEltYVZOMBoiL6zDOe00Vby8dVUOBiIhzy8sSG5ualNjMnt1azor2zQYYJPRgvYmImNympgs5GEh0Ya0r13RdqoGpxAaYOERE5JkeW4b1ptvRsqs1PbcKa+/qiq7bERErYLyxAf29sab92GmBSQGQEheTILwE+xM0ZfTXdB0ghe7+/fux7DpkLCchMpkzkG63BW1Ra7BpArUGPUBszhgekcqoDicnnAtA14z90CcBP1yvX9jMgDOec/707/zvxCT2J5OM4rrqYn1z/n/oVckwhEqnTPL0iLKlOUZlY8Z6aB84LmXiQNt6H56ZcEWGdYDuMXXfOdS9Ag2Xk3MBnEsC/mTZcGwP5hOOfWjMnAwOeqmy9R43l1MDE4YU/mJjjDHGGGOMGXj8YmOMMcYYY4wZePxiY4wxxhhjjBl4/GJjjDHGGGOMGXguWvOALPoXiNJJwD8zAVnjE8LQs+fOSWxsZERis7vn9JrrKsheA8HwyTOnsewoqeB5HcwMpscmJLbRVlH1/LwKUE+cOIFFLyyoQH3fvr0SGx3XthgaVpF4taqC45FM6x0R0eqqUGx5VduyAtfMIJXt+IwKpceHdQxERLz0OVdKbHVD2/LYIyo6/8hnv6h1BCHc2NgYln3o0BVaz3Gt5/CwitbrVRVALx7XPmwkzCLq0OaUmTwgG3cT5g7pKzeGhrHsDOo0PKzHViCje7etZVd0WMTSqortIyL6LRWJ10HI3mmqEL7Z1jYfAgXp+mnth4iIekfbMu/oNc+uLkqsBy283tG2aEzwWNsOBiq1KvTDuPYDWX4sb4DQtZlK+67zaXVF18W1RV17m6u6hs1O6jpy8uGHseQSCF1371Cjio1MJb4tiNH83NjQPoyIWIUxmMNYq4cO4Efvu1diC8u6F5w9zkYVL3zh9RJrg7nD6rLWkQTZjzx6VGITM9NY9hXPUfOWGhh8kHkACaA7cByL01MmOCRa13PZj6C4AJ/O30IS9KdEB0XisN5AivkSGKWQ6LxK+0OwWUQfzi9BA1WLthA6SETANoYmGWSaROO8Rm2RqFKxFk89PIMAH+pTT7S52qdw2cUsHPheUvdNkEEC1YeOo/aplRMmV0XqUrJ5gDHGGGOMMeZ7CL/YGGOMMcYYYwYev9gYY4wxxhhjBh6/2BhjjDHGGGMGHr/YGGOMMcYYYwaei9YVrdEYjsbQ465RQ0M1OWYdHHia4HoUEZGV1AtiEZzNxicnJTY0MiqxPnhD9FqJsqvazCPj6gDULamLzvwSOOasqLPY6TV1hYqIOLOo7kPZiDokve4Gddv5+je+IbFOpyOxnQm3ijNnFiQ2PTMjsWPHj0is3VNnqF3tnRIbG53Estst9Qjp9rUfsqbez0uuVke1Pbt2S+yRw4ex7N3QvkNVbaMeOHllG+ovkvfBhyRhbVLOijmHkLtdypFoM52VRS4bxu/GmrozrYLDYZlWoqrWZ2NDx35ERLervir9rpZTyrXh+rme22rrXM57OlYiImqZVr7bUketVXAH68FPS+2e1mfhpLpXRUQ8cOfXJFapqutcp61+O2vrWscXXv8SiY2MsCPbCqwtnTXtn6yrbdkHJ7pjxx6RWAXcsCIi8q7Ok+MLZyU2Oqprdw/GygMP3SMxckqLiNi+fbvEzs6fkdjako6XGuwFsw3d284+/CCW/akT6pZWAWey1UXd2yamJiVWH9W16v5HtR8iItZX9ZrX3fBivSbYGVIv0nqTw9oQEVFF5y2FTMTAYDPhFsVl015P600N7pt+Oc5C1+jUypv1tZxyWc/vw7Lfz3WOlLbgBodxtpgTqH270LfklhcR0Ye1lsZVC9qnBptJF2qUJe6cVnnanni0KLQj805S3LEMHmcDtja8w1S9c3L/K+gIWPRFYitjrdPfNH43//tJ8BcbY4wxxhhjzMDjFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMBz0ZoHVCqlqFRKT/g3iK9LIEbLSVIYsX37NokdPXpcYvfcc5/EZmamJDbSACFbmyVh6yC4Hx1RYWr1CWYJ32SopO+eU1UVm47v1PuLiNi1ckBibRAS98padnlIRcPjMyNaCNQnIqLZmZfY4qqKi2e275DYuSUVWt97/0MSu+qqq7Ds4bqKhtfALGLfJfslNjmpxg7lsgqgr33eQSx7bU2F0c0NFdGXq9q3vQzGNMxSEq9GRAzBGCqVQcxLCsI+iHl7euD4qJYREUEeB92ujv0qyBfJ82Adxkr0eY71Wmo2sbio5hUNEMKvr6uJQxUMMaqJcd4Go4AMVJekuy1Dm5XBzGAD7iUiogPmIuvQZ48efVRiQ2Byke/dI7F2je+7Bh3egnqWSekK4uD2qt5LDUwuIiJyGKuLy2pmsLqg5zeb2t+lsl5vpaNzNiKis6ZrUwvGX6+nYtc6GAUMj+lecPUlu7Ds+XNqUnDm9MMSG4F2a57Vvuk3dT0/dMkhLBs063HfN+6S2Owurfv2Od2fMpBKp8TkFTh2bWFRYiceVpONE4+q4UK7q/21bRfvoavrOi43mro2DQ9pWzbB1Kev2y+aYURE7Llkn8T2HdgvsVEwPQoQ2xf0h4mICKoRbhtgUpDD+kcmDikpex9K/+M/+5jElld1jj73+c+T2GUHLpdYj9T2EVECMxjosuiByUsFjAvaIHqvJAyXCGq2HOZDCTYY2pMric8ZZLDQh1gPHh5y+EZCzw6p8Uf3U9307AteQkn8xcYYY4wxxhgz8PjFxhhjjDHGGDPw+MXGGGOMMcYYM/D4xcYYY4wxxhgz8GR5nlBQPUMsLy/HxMRE/NL/919F4wlC6Cpkbh8ZUbFeSlTd6+ltLi+BODkv5qfQGFZh3sKCikojIkaHVaS7c+dOveaICt5XV1V8uL6hYmUSr0ZELC+poHZmUs0QpiG2DoJ3Mj1obaiwPoIF1COjDYmRCO/wkQckRpmds0RmchLR79l3icQeOaLC0lOnTklsfEzrvXBOzREiIrbPqgiVhMSd0Pumeq+CKHpikrPBr67oeKFxFSCQvOtONc7ogSHGFZexuJjEgh0wzliD7PQVUDROTulYm5rScRrBY20SBLU9UFOOgKEA6GEjp9TMEZiymbOYF4v1wCCB2jEiolIGMfqw9veu7SrozkFsury8KLGNDVgnI6LZ1Hm/tqJjtd3TtakM/VUG4TiNlYiILmWhLumYpu2tDEYB3bbeSwmMWyIi2k29H9qLaBCVwARnB+wFqyCKjohotrQ9VlfUFKA+VKwtzoChwPgsGxfM7blMYiMTcxKb2KH304bxu31uVmL3361mBBERnXXtnyoIju/68le1PhO6jpTrOgZOzuu6HxFxbmVRYuPjOseWwMxgYlSNaLbPgEkBmGFERPShz0bGdb3aaKq8fQxMcOZmtM33J9bz7bv3Sqw8pPsgLYt9WNdW1nVM3/H1r2HZX/7ilyT28CNqHnQGnrlmwajiLT/7cxLbu1efByIiOvAISftLHYwCaJ+H5Sb5nFqu6jXp2DLs37T/Ur1TdKHP0NoB9g3aS8gzpp0wyejBc1x3k0nByvJyPG9uZywtLcX4uM7pC8p+0v9rjDHGGGOMMQOAX2yMMcYYY4wxA49fbIwxxhhjjDEDj19sjDHGGGOMMQNPMaX8M8BIoxKNJwggSVRN0qZOj3LERvRBtDQKBgC9XjGxVR/Eq8MjnJF9bFxFfG0QhJVaKpAcHtZrkpFCp1PHssuQJXZkRAWAjWE9Pwdx+8K5sxKbnVVB4vk6qWCUtGyttpohjI+qQHICYkOJNqcs5K2Otu+5Jb2fB8G4YOfOHRJ7zQ/9IJZ9/PhxiY2Nad37fW3fhw8/KLEWiK9PnlGTgIiIEydOSmxiZlpi17/k5RK7oqa/c5x4VK93fFFjEREL8yrkzED8eumll0rsr73hDRL7xp0qBN6/VzNxR8QFRiPfZH1Vx1WvpW1OBh3Vqs6HPqVxjoheR+NlSNNequhy2+lo35bg96aUCJRMQ7Kynv/gwzquSJS6tq7zhsZuRMR1L3yRxM6cVQH2Aw/cK7EOZH7fWNf+CriXiIgWrJVrG1p3+u2O+mZjTYXNrYRpwo657RJbAdMFMpvo9XRNPHbsqMSmpyex7KGGmkU0m7AWZLS3gYEJnLt06lEsu9zXtpy5SkW8VTCQePg+FX7fAWvLtpkZLLtW0z5b7+oY+Bt/5QckdgLupwWGNZdcoqYHERG1UTX/GQXxcqel45fWlt0790hs/rTuQxER99x1t8Qm6joGqrCOTIB6+8yDurcduUfnZ0TE3B5da7O6Pjvs2K33Mzare065qvX+k//xh1j2CJSzZ0pNAaptXXvv/IoaEnzp038hsQM/ruYIERFlWHM6bR0vKysrElubX5RYEwycMjI/iYi8pOOcjFLI2KQC57Zaut6kTFHIwIfKboO5DRlIbNuh62RjjEX/VTCGqtcujPUT9Sb8xcYYY4wxxhgz8PjFxhhjjDHGGDPw+MXGGGOMMcYYM/D4xcYYY4wxxhgz8Fy05gHjE40YbjwuCKaM45SVOg8V0UVEVPt6q4sLyxJbXlRB2BgJniDT6vQ0i+grkG06z1WM1gXjg35XYyR+7bb5vsfHNEPyyIiKIQPqMzujbT4DotZqVYVfEdw/FDtzRgXHLRBijjZAIL7Ogm7K0D2ea1v0+pCxeVzLmd2hYshOn9s8r4CpBYzLUlnFnTt26Rg6VdK+SQm69x5UIWcG429yRttidEIFm+NTGjtxnMXFwyNaztTkpMSufs5zJbbR1nn32tepEPjw4cNY9olHj0ls354DEut3tc0pI/s6ZDpPeAdEH0TZ/dBY1v/W2ZUjIqKnx9WGIbN9RMyCiJmMD4j1dRU2T4canYyMcNmLi4sSawzr2rL/oPbDfQ/eJ7FtezTjPRlxRESsgfnA6JTW/dTJMxLrgWnCxqquF3OzLGQ/c0avWYH04hUwiyBTk/37NQP6xASLbB85ouOfzGQ2wBSgtaFjmkx5phJlnzmhffG5z3xOYr3Q/aBa0XExPKRr2H19FrLX6tq+x08ckdhv/ef/JLFDV1wmsT0HtM1f8X3fh2Vf8+IbJHbuzGmJ0TpCe3WtpvNzbruO/YiI3/u935PYSZgTBy/ZL7Ex2OcnJ3UfW1pmk4xjMHeOnda9+mHYD278QTXW+d/+95+S2BWHrsSy847uy/fd9XWJ3fO1b0isB89Ct31SzQO+74aXYtlDDV3vTp3S++7AfMrBZGACTBM2C+O/SROuqWfzWNNIBDyKRK1GV4zog7FOLYNyYO3NMy1o6RFdgx6F57qIiAzafHj8wvVhdVUNXlL4i40xxhhjjDFm4PGLjTHGGGOMMWbg8YuNMcYYY4wxZuDxi40xxhhjjDFm4LlozQPW1s5Fv/+4yK7bU4HbMAhVhxqcofvswqLE6nDsFTv3S2xlWUVL4+OTEmsmBPxnTmmG5R07NJP9xJgKOc+cUpHi8rIKresgUIuI2D6r2XqHa3rfHRCOdZva5rMgqJ1f0IzzERE1yGR7dm1RYnXQ0Y2PTmp9IGt3PyHgL4Mw/9w57YfpSRWt7Zi7WmJ79qu489jR+7FsEoTnufZPt6v3swFtvmuvCj7rdRaI90BC2AZB48K8iu1B/xcby/NadpnbfA7qOTOlsWNH7tKTu5oFulHW9nn0kUew7FZTxfr33K1i06kxHb/lkvZNp60izkqZ5xgJJ8lQoAsGHc223mMVxJ2VhOBzta0i230gRKZfsCamtS3qdRU779ipGaQjIvImiPDbKg7dAIOO8Vk1yRibULOSccgQHxFRGtV522xqn3VA/Do6oSYDs9shS3aizUuh/Q3JyqMDAuge3M/YmN7LyhoLZU+d1v2gAQYAlx28VGIbG7q2HDl2VK83MollN8F04fTSgsQqNb2fdleFxO3WWYmtN1lc/Kaf+FsSe/6NL5fYMpgzVGs6pvfv3yexuw+rGUFExHt/7T9K7L771PyCxh8Z6yxBJvqbbroJy776yqs0WFNDl+tufKXESGy9a8dOia2sat+cR58TDnV07V+HNexlL1cjhodh7b7qat1rIyIeul/bd9+B/RKbGFEDirvvVEOBSkfn7K+86xYs+6f+3v8usSpM8KFM2yeHdaAFYzJPrC1Dte/sI3kGxiLtFq8tVTIKgGeZcl/XgdV1vcfDjzwosVPndM5HROzYqwYz2aZn2vUNXhsIf7ExxhhjjDHGDDx+sTHGGGOMMcYMPH6xMcYYY4wxxgw8frExxhhjjDHGDDx+sTHGGGOMMcYMPBetK1q724xy53GXhqGGukgcO64uJp22uhFFROQldYcYH5uUWAZOFyOj6r42Oq7OL7U22HtFxKmTZ7Q+uTpQNJvqTtLpqgtJva5tMdxQp5SIiDa4mNQ62u2Vkt53t6XnHj2qziZ5cJuXq1rOtjl1Pmq1tH0rFT13CNx/zpw5hWXXK9oeQ8PqJLayog5zfbgf6psG9ENExNKqOoQsgbMeXbNc1n5YWtXfH0ZyHX/nr6nOPKWSnl/JdT7Q2B8Hp76JcR5ro0Pavq2WOjHNTKuTzeWH9kvsa1+5Q2KNBt93paFzr9fT+1leUQe/UXKBKqkbzAI4xEXwuMxhPm1A3/RgHSiF3svaBjvZdHp6/sZD6ii0tqz98NCD5Oqn9/385z4Hy54E58KNFjgsVXSsbXTUWWfxxHGJDQ3zWKvnGj+3qA5dqy110llb07YYqmibz0yqe1pExNrqsp4Pa0G3q65oQ0N63Jl5dQqqgKNkRMTMjDrZkfvVI+BANTWlay+t0c2ujtOIiKlZnbc/+9afktgXb/+axB56WPv2qqteILEz57QPIyJe89pXS+ySyw5KLAc3QnIha4AV5+c+9zksu7mh/bhr5x6JZVmx34nPDKmzHbmnRUTs2aPl0Jy49oXaltPT6kh5Glz16uCyFhExP6/r3SUH1b2q3tD1792/9G8lVi7rPdKYjIh43tXqBvdIBn17Uu/n/nt1/ZsDF8bLLrsMy/7spz8psQfu17VybU2fHUYb+iwzC+6Ty4vsJNsGZ90y7N8lcKxrg8tqq6NzOdXm66t6P7TW7dqlrpu7d85J7OQpfT4/C2t0REQZ5s7IxOSF9YP9M4W/2BhjjDHGGGMGHr/YGGOMMcYYYwYev9gYY4wxxhhjBh6/2BhjjDHGGGMGnovWPKDTy6LyBPFvqaICtX6uQsxHjj6M1yuXVcC6MKyiy4N7VRxHAsvlVRXJNpsqMoyImNm2Q2IbLRXp1oa0jkMj43ruita7WmMhewnE6CREzlSDHB0wD+jmKvolwXoEixx7IO4cm1CR7gaI+ms17Yd+n40LtoFwcmpmUstpqZCORIFkCNBuc3+TMUR7Q9uo29Mx0NXmjX6uY21hkcXkFehvErdX4TcNErCOToBYvw+DJSLWwaiiXgNRbK73fddd35BYVtJy8jxhDgJtPglC62pJDQ5KmY6rKkynlVUWPq6sqkB9bELH1SgYMXRB/H/4yMMS+8JtX8Syz55blBi1RQUEqN22ik2Xl/V6n/rEn2PZP/P33yKxSy+9VGLHT52UWBUauAfjqtPUOkZE9Ls6Dl760pdKbOmsinTv/IaK28fqOi4Wz6rpS0TEdde/UGKfwDYCI4bnP19idVi7T5zUNotgM5nxiVGJVcAsp5frerVtblJiWUXrHRFRqej5C+eOSez661T4ffkh3Vdvv+3rEnv5y2/Esu/58pck9uhhFYkvrYCxA6x/05O6NnRX1VQiIuL1P/Qaid17j4rJy2VdR/bvv0SPq+gaPTqqdYyIqIFgft/ObRL7xm2fl9jk5KTE1td1L9mxYyeWXQYzmS99+hNaH7jHcVj2Z6a1Pqmyd8NePVm7XGLbx/T5aGpSYxNTGquDgURExLFjOqb/5BMfldiBS/ZL7M0//uMSu+qKKyV2DgxDIiL++3/9kMTIhOShBw5LrAXPa9e+UNeb1/zwD2LZ/+P3f1/LOa4mJBNz+mx1HEycvn7nPXruNBsXdMEM4ZL9F64Za+u6z6bwFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMDjFxtjjDHGGGPMwHPRmge0m70oPSED/NEjJ+SYZlPFUsPDnJm801YR3ulTKuC6ZNd+iW1AhviVUyosBS14RERMT6vYL++rWKoPAqoaCEt7kOH93DnOZNvtqfCsAlnRA8S4o6MqSiUh5uo6iy7JKKAJgvvWWe2HWl2H5saG3ncbMutGRCyvLEpseEzPL0NbkLiz11ERczmRHbzS1rrnOVyzq2LpOoiY+zCuTp9m8eHoiBo21CGreruvLgVksNGDDPEZiEojOAN6aVxFm80VHS9kxDA+ppnOafxFRIyNqwEFZW+vgrnCwoK2ZQeE7EMj2jcREaU2tFFJx36nozEy2Dh3TuvzlS/fjmVv26ZrSwmMArbv1GzRa2vaD/t2qdD1619XkXdExEkQuFcqOvZPgHnAth2aqboCv7NNz6rIOyJivaXzvg3r0E4op7OuQvYP//9+W2KXXaqZ7SMiJiZ0XI6P61o5PKJj9fte+XKJnTml2dPve+BeLPv0aW3LCRBlX32l9iOZosyfU9FvrabrQEREeVT31lZLTVVOHNd+uPKK50ps6lUvk9iphGnC+JS2+eH7VORNBigju1SgvrakRiC7tvFYG61pP+6Da26s6/MIeDjEBJg9LC7MY9nUP+WyzpMhMONogQlEFwTmp47p801ExM6deo+7t09qHU9qhvkf+D418uj3tN6nYOxHRDx0jwrPab2ivWhuVut4LRh+nJ3nsit17bR3/h//SmLl0OPa0L5fvF2NX3bt3I5lX3JQjRi6XX32mIax+spXvlJivZ6ee9vtt2HZO3Zpna6+Ro1A5ud1rC4t6Tqwe5+usydP6XiOiDgLz4Cbn2nXwYApxUX7YmOMMcZ8J2jceXfUjhyL9r49EQ12kDTGGDP4+MXGGGPMs5a5/+c3Yva3P/zYv/PX/1Dc9+NvfAZrZIwx5unCGhtjjDHPShp33n3BS01ExME//NOYgDwQxhhjBh+/2BhjjHlWUjuiOoyIiJET/LfexhhjBpuL9k/Rzp1biHr98b+FXlxclGMaNRXzjo6qWDkiogci8bExFRyvr6lAicTx3VBxcQmEixERD8PmOjs7q/XpUJZ3FRw3Gio4zoIzsjfXVUDYbKlYkDLRnzqlhg0z26DekyrsjIhoQdZwqvtGU9t3ZUXrmIGKns6NiBidGJYYaKqDdPA1EEC3mzouutA3ESxkX1xWcV0fBOptyAbf7ep9jw6rADUiYmZaswLnuV6zD8YO3TbcY0sFkmtNzgBMZggLPRXpbiyryUC3qwL8Lhg2JJo8mhtqPrBr1x6tY6bljE7qvBuq6zj96ldZRJ9l2kbL89rfzbYKS2mOtMGsZAfMu4iIS/fvl9jP/PTfl9jufXsl9t8//F8l9mcf/RMtGzKYR0Sswppc3avljJR1XWwu6Lnnzmnszq99Bcte29Ax9JznPEcPLEXshvN3vPyGmH7O48LY4Z/6e3LMsSMPY9lkHvBXX/9XJEZGFytLixKbndU5+/f//k9h2aUyrBk9HdO/+7u/K7ExMKq45hpts2qVzQOIBw4/JLGTJ9RY5/rrrpPYAhjwLJ3TPSciYqOl5jhktLIEJjoTY7oXzM7qmFxeZDH5/Fmdy8Ow/k7APkhZ49sber3IeU0daeimlfd1zSiFjoHTJ3XtnZyE56OMNWd33a0i8zEwdBkdm5TY2rLWMTKdD3UwPYiIaIBB0skT+hxFhjW79qjpwcqiitMfPXYUy77qKhXMk1HKVZfrcbd/SdusCvN7mvohInZ8/40Sa4MBFD0Pk+nH4cNq7LBvjxrJRERMjGmdyJyGnntOwVy++qprJPalL30JywYvrRiB9aoo/mJjjDHmWcnSZQfiwde/5oLY8v/nzdF+7tXPUI2MMcY8nVy0X2yMMcaYp8p9P/bGOHX9C2Lk5KlY27E99r3hh5/pKhljjHma8IuNMcaYZzVLlx2Ipcs0r4IxxphnF/5TNGOMMcYYY8zAc9F+sVk6txS12uMCP8poTYqjNohxIyLKoByvg/nAAmT9ziDT7zAIxCcSItuV5Ue0PpmKQJcXVexHb54vfKFm0Z0/o+KtiIgTJ1Ug1wFRNrVvDqYJS6uavXqmzgLASlVrv2+fCrope/DCgopAex0VX0emwrqIiDxXgVsPRLZ9MCRYX1fxdqmidVxbYOOCJhgADA2pgLXd1rr34NwSiNNHR3X8RUTkYAqwAgLLcq73vQqCz5UVvceRETC5iIgW3M/qsvbZ8vKyxEaH9ZrLKyqoXV15FMsuwfzeALH+8LAKWIeHtW++9ojOm2aTx9quXSrGXFlblNj6KsynJY2Njagw+ZorLseyZ8dV8LlvVrNSz4zoPb7xh14tsR++SbNXP/CgCsQjIm77sgr7x8dU8Nna0LI3Z5WOiCiXdY7t2sEZup//Aq37/Q9otvKvfPkLErv99r+U2NVXq+bmDX/zr2HZy+d0naa5TPfzFx/7lMRGGjomM9gfIiLWWzperrjiColNQD8swv6yt7JDYv1c18mIiD/90z+V2O7das3wouuuldjn/vKTEus09R5vfNWrsOwNMLxpg6lKGXbM1VU9t9eDNZ6cZCJiclL7p1aDclbUaa9W0321Utb1vNvhvaRW03uswXPLGcjonsMaX67r2pIHZ3TfsVP3mKUV3UtKZTUU2LFTBfP9ntY7K/EjaBZa98aoztHxUS375Glti+VlHfv9LjxPRMTxo/q8thdMUTaa+ix02SH9Krwwr2vd+oaeGxGxbYeK9e+//36JPe/5z5fY0aO6Z83NzUnsxdddj2W3NsA4C54dSjmMXzDJ+MgffURir3yl7i8REVUw3lpZvdBko5cXf13xFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMCzpRebW2+9NV70ohfF2NhYzM3NxRve8Ia49957Lzgmz/O4+eabY9euXdFoNOKmm26KO++88ztaaWOMMcYYY4x5Ilme56xSBF772tfGj/3Yj8WLXvSi6Ha78c53vjO+/vWvx1133fWYoPjd7353/F//1/8Vv/mbvxmXX355/OIv/mJ8+tOfjnvvvRez1m5meXk5JiYm4kde/YqoVR8XC02CML9RUdE6CdEjWFzcA6F1taLHTYMYNwdBd7nK4qaRURX4diB76/iItg9lV6YuIwHz+fO1jb7+tTskRoJuyrBNxgMjI5whtgaCsB07VKxKfbsKJgUnjquob2ycy55fVMHeCGSgJtOEs/PzEuvCLFlvcrboNRDhUT9mfb3oGogzqyA2nZqawrJPndIMySREJsH88IiKZMtVrXeKjVVtDxqrZD6QZTrv5qEf2k0WfD7RaOSb7LtE58T+S1QE+o1vfENiw3VtH1pDIiJ27tRM1+OQjfuBBw5L7NOf+rzETp3RsbtnlkX0L79WjUSuumSfxBowhqqwNnzx6/oj1H/7g9/Hstswfocho/a+Kw5JbA368arnPEdiDz6kwtmIiCsvv1Rie/aqULYHAuGVFc38Xi1rW7zs5d+HZc9u13HVbuqc/6M/VPHsy16swt177rxDYl+87bNYNq1XV11xpcRWFvUeazW9x9EpXeNHRnlNJcOH/QdULD0xoWvT5z8PGdlLkIm+rrGIiBpkoifjl/mzKhKfmFARfAWeHbZv5zlGbU5lP/KIzm9aMxqjuv5ddZUaQEREVMB84J5775LYiePHJDY1MymxGhhVbP5x+puQic7zYb2Zm9P5UKupScHuXfsldv/9D2LZXTAK2nuJGg+RgVQD9jZ6bpmHORIRsQb72HNe8AKJ5R2d8xk8Pz5yv7YvzcUI3v8bDR0vu/celFhzTevdBAMoaouIiAz2ahL1L63S2qLjlOZsE9bJiIijR3TurG9qi7X1ZvzVn/wXsbS0FONgmvNEtuSK9tGPfvSCf3/wgx+Mubm5uP322+OVr3xl5Hke733ve+Od73xnvPGNb4yIiN/6rd+K7du3x4c+9KF4y1vespXijDHGGGOMMaYQT0ljs7R0/s1tevq8td/hw4fj5MmT8ZrXvOaxY+r1etx4443x2c/yL1DGGGOMMcYY81T5tvPY5Hke/+Sf/JN4xSteEddcc01ERJw8ef7PYDZ/0t2+fXs88oj+GVFERKvVilbr8c+O9CdRxhhjjDHGGPNkfNtfbP7BP/gH8bWvfS1+53d+R/7fZp1LnudJ7cutt94aExMTj/1HiZCMMcYYY4wx5sn4tl5sfv7nfz4+8pGPxCc+8YnYs+dxQdc3heHf/HLzTU6fPp0U5r3jHe+IpaWlx/6j7KnGGGOMMcYY82Rs6U/R8jyPn//5n4/f//3fj09+8pNxYJMjyoEDB2LHjh3x8Y9/PF7wv1wk2u12fOpTn4p3v/vdeM16vY6OUaWsGqXsCdUD9wt8L1OzkoiI6HTaEiPXj+h3JbSxqs4SI+PkJsNODceOqWPJ9jl1UhoqgwPFwqLEmm11DLnrLnVKiYh40YvUhef4o+qcdeLEcYldd911EpuamZZYmT/GRaWs/XP6xAmJ9cEhbmlJ3W3WmtoPUeYOP3X6tMSaR9SRgxzvlsCRbXJanX5K4JYTEZHBfaO7XVfvm/4UcxeMlWqZyybXsKGGOrCMgANftarOWY1RPW5hQfsmIuLUKW3z4Ya64zTbet+jMHf6PW3HcoVdkzo9ve81cImZnZ2V2LXXXiuxsWF1ollbg/EX7BZ15OjDEltdVcebgwf3S2xkWNv8Y3/4x1j2qfvUceen3/y3JdYAx5v3//qvS2wCfoD6N+96F5b9ZXCTu/+Y/snxErjwXHn11RIbg7EyWddYRMSdn1GXreV9Wvdtu3XejoNbVKWqY+3k6VNY9uiszsd26Np94qyuIx/6b+ow191Q97/XvfY1EouI2LtHHahoHaF9gxwKV1s6R4bBtSvicS3tEyHHMHKdu+GGF0lsbk4dMs+CI2BExNmzZyU2PjUpsR279X7IgaoKrmgpp6W77lanwEZN1/PtO3RtefjhhyR2bvmM1rHO5rTkXFgf1rHWGFcnsCFwuezlul/u2qPjOcXxE/ossw7zO+/rfPril2+XWKejYzciotvV57Uv3vEFiZXhmakMDpC0/9LYjeB1/s67vyyxmW06H+bntW+nYb159Jg+b0VETE3q80i1pHU/dlzPP3CJOqWR0+nk3DYsO3o6Nk6delRii4s6F7vw3Dw2rusIORRGRBw4oH+pdf8DF+5tnUg8aAJberH5uZ/7ufjQhz4Uf/AHfxBjY2OPfZmZmJiIRqMRWZbF2972trjlllvi0KFDcejQobjllltieHg4fuInfmIrRRljjDHGGGNMYbb0YvP+978/IiJuuummC+If/OAH4yd/8icjIuLtb397bGxsxFvf+tZYWFiIG264IT72sY8VymFjjDHGGGOMMd8OW/5TtG9FlmVx8803x8033/zt1skYY4wxxhhjtsRTymNjjDHGGGOMMRcD33Yem6eb+fmFqFaeIALr69eifESFZw0Q2z1V2m0Vsg3D16ulZRZV5yDKIiEx/bletapddNc9d0vs3OIilk0u2yTMn942J7ESlF2tqlhveEjFbRERHTA5aGXaFseOHcHzNzMyomK0Y4+yCG9iYlJiw8MqclxfV7FpvaaCu3JJ2yLP+HeBSknbaH1djQtIoF6vNyR2/30PSuzKqy7HsqdmVHy4vq5iyEcfVROHOsydrKxi3kZD6xgRsWP7HomRGUJ7Q+dTpwKmB8Mq5m2u67yJiKhk2j8n4R6/9EUVge7YqWP/8OHDWA5xHISc99+n599z7wMSu/yQiuj/2dv+qcS2JUSX//WD/0liZxdU3Ll/x24t5+1vl9gYCLIrCdHm7Z8HAT8IiXdcepnETj+iotR2VefD1ZNa74iIvbu13ZYW1LxiaVnHS7+spjFncxW8n14Gc5mIuOZlPySxfEP3oq/fq+Pvf/7xH0js7/74X5HYJz/1OSz7ykP7NQjCXVqn27Ae5yXtW1oTI1iA3e1q2XTNVlfHRa+ne2g/57FG97OxofWka34zkfgTqQ2p2J6uFxExNqLrXa2i9aE6Viq6R5BZzle+egeW3YFjm23Ys6Bvnpgf8JuUQTBPYvuIiFJJ697raX2OnVTBPJoC5FrOEPRDBD9zjUA/dDo6v8l4IOC5o1bT/oqI6LT0fDIXuffBrxSq4/0Pan+ljCpWN3S/7Xd1Tgwd13Ju/8pnJDY8rO1bBnOjiIhJ2GNOnlIDlVF4DqN9nkyGnvOc52DZ27apocHMtgufZeqrvB4T/mJjjDHGGGOMGXj8YmOMMcYYY4wZePxiY4wxxhhjjBl4/GJjjDHGGGOMGXiyvIiH83eR5eXlmJiYiB+8/nkXmAeMNFQElYOw7pJLNINpREQNMrUvLGrG5+ipUKvTVRHe1JRmtF7bYHFTtaLCvqlZFUuVQaBWBSH7iRMnJfaC667DsknYT8K8sUkVjnU6et8kVJ2Z0baIiCjB0GpBpuuHHtLszKdPQxZ7yKJbAcFmRMT+Sy6RWLevAtYvf1nF5Nt3aEbsrKLCxxLEIliEWgJxO029dhMMDsBoIiXgj1CRZJZpOVmmdSdh6MKyCm+3zWqG94iICmTzXl/XOTExqiYZ7Z6OyRyExD0akxFRq+qxkxOatZ7G9MaGmis87/nXSGwHjIuIiPU1NYboazdEDebyUFXn557tKpgfn9Qs1xERd/7FX0js2ANqXDAGa9ClBzRT9ee/8CWJtcG4JSLid3//IxLbcfBSiR0GAWpzXfthGAT4z93B6/nfeskrJLa2rsLbExWdOyuQUu1MT+fs637872LZz3/JqyW2uKTj980/+nckNlrRgfEPfupHJXbb5z6GZY8N67wdH9O+XVxU4S6J20kgToLjiIhuS/sHhnm0etq3tH4GrEH9RHJxqmcTzBCqNa77ZkgwX6ux8VCvo3se3Tntq7Sm0i2S+D8iIi/p3CNDASoHTQHA2KFPi1WwEQNRhjWsArFOh/Ym7nCqE+2XlZLeIxk2lKt6XBf2nIiITkvX83IF9qKe7tVUR4pt5b5Lud5PKXQuUzlk7tFu6/1F8Byr17WcblfrSGOt1yazJn5GprLLm8wdms1O/B+3/l4sLS0lzRceu96T/l9jjDHGGGOMGQD8YmOMMcYYY4wZePxiY4wxxhhjjBl4/GJjjDHGGGOMGXhU0XyR0Oq1oxdPECR19B1sZnpSYpVhFeieB0RZkHm2WlYB4VRdhbskbiehVkREu61ZWRdXNLZtTsXJzbZmEW8MqSi6BWLciIgpyCZ79dWatfv0vJZz//2aHbxaVtHbyooKzCMiqgUzF6+sqHi7Xldx/NkzYPaQYHVxRWI7dmv7zk6ricPqstZnbk6z09cbPNa6bb3HvOBvCHmm5zZGtQ8XFxfxfBKwjo6ooHZqXBXU5ZLOkea6ljN/VgXZEdxnU9MzElte1fGyuKTC7yEwviihXDni7Kr2d7etZV92mQrm1+lc1T2iSUBExAuue6HEFs7ofFpf0nHVb2p/PXzn1yU2f1LXm4iIMTJV6Wg9v3HHV7WOR49L7PSj2g/b9x/Ast/0ph+X2J9++i8ltrwAGeI39L4nczBK+er9WPadi3qPe/erqcVr3/zDevLBnRJ6uKljYP/Bq7DsCN0jqkMam5zQ8Xf43jsl9o5//X9K7G0//5NY8itf+jyJLS2qOcP6hs7RTkfbfG1Nx+TKirZFREQM6xpGRimR62NFu6MTCveCda1PREQOovehMc2Ante07GYTzD26ENvg+yahN63mLBLXWBmMZJqtVFZ1WO8gc3y5BCJvEtuDiVI3IWSn+6ZYFxqjC8L6oHaEZ4Tz/wPKBhOTToCRArRZDkL2FCU0kaA20hiZVNE9pp4VSyUwGSJHjYImBWXwpKiP6PNjRES5XMzkoFLROqLpAfTDaBecWyIiz7UfNz/LZGDgkMJfbIwxxhhjjDEDj19sjDHGGGOMMQOPX2yMMcYYY4wxA49fbIwxxhhjjDEDz0VrHnDFVVdE7Qni/mueq4L3g5fslxhloo2I+NxfaobuIw9rhu4aZOheoQzJIHwcG2NhFOgH4wyI9U9Bhu49+zTz9qHLrpTY0BAL2UkIescdd2jZZ89IbGlJs1cvnFUBfw4CvoiIUuiN3/jK75MYidFI8NlpqkFCq8WmCQ89+LDERiFb7ca6igqzsgoxd+zYU7jsdktFc+uQzbgC5hUZTMmTZ1Q4fvykjpWIiCqM/1JorNPU8UeZySch4/2ZhInD9h0qSuxA1u5Hoe6zM2qQ0IX2XVlnge/KkppxVCAb8qc+peL2H/3RN0ms2VIR89SUisEjIu69+x6J5bA+lEC/+sj9D0ps3w41uciCxab333eXxMZ62o+1FRWOnz2twvzOmtZ7bq+uNxER9VmtZ7z4lRL6we/TNfV/fPj/1XNP63ozBRnMIyI60N/1lhqB/MXv/YnEhq5RM4Q7VvV6/+z7/yqWncMcBZ+LGB5VcfuZczp3Du6ekthff8Mbsez11ZMSq7VVeF6CjOHtDT1uGITE0zOzWHYHjEnaLchsDkLgLswHWvdXoY4REfNgUPPIo8ckNgJ7bR/2oUpNf9OtgHA7IqIEInEShOegbUaRONQxB2F8REQfhOP9vrblKmSTz0iY3wJDH6pQsFg/p5sEyOyBRPAkeI/gfYzMJrJSMRE9kTQuANhAAsT2VDa0Y73O61oGY5DqWUIjBrgeHJdsHyoHrtkGYwgqpweGAhmYT0VE1OCZqz50YVtUqsVfV/zFxhhjjDHGGDPw+MXGGGOMMcYYM/D4xcYYY4wxxhgz8PjFxhhjjDHGGDPwXLTmATe+8uUx3Hg8k3mno0LiJRDgLy+qyDAi4pHDD0usDOLihUUSVWsm2v1790ssJQgbqmt28LEJFbLf/cB9Erv33nslNj+v2cH37df6REQ8//nPl1gNhKWUQfr0iUclRkK29YTg84df91qJ9foqpux0VRS4COLgDRCvLieyZB87cUJiWUX7uzGsAt+du3dJ7Bt3qkgbs25HxPS0Cu7LYBRQr6nhw87dalKwA+qTMos4fuSoxEhgWQchXrcL2cFzMEjItB8iIhrDes2spP09BWN/+6wKv9fXQRSdyJJ99qQaLKwsqwFABzKg/97v/b7ErrjiUoldul9F5xERR46roJuErldfdY3E/soLrpXY8cMPSezAIS77zINHJDa9rv29sK7j/MjtOqYfOvyIxP7y7t/Fsk/2tG+XYU3dd81zJbYz0/mw1IWxlhBVd3Id/1PjOu/ydV1H/scHPiyxF//jt2ghFV23IyKaG1rPOrgHvO51r5FYa1X3l2sO6Zz/yEf+B5a9vKzjvLWh9zh/Tg06alXtm6Ul3S/HxnSsRETMgakAranPueZ5EpvdPiexFTCnqQ/zHnr5NWoeVP3qVyR2H5hxTE2pOUOzrWt3ak2tN3T/JwE2ifAbQw2Jkf5+KLGuUTlkRFPuax1prQPfgeiTwUFE9DMVf/dgPlLWefITysBFKSVkJ4OO4UT/SNFkMgBlV2Dsnj9W4ySOr1b0WagM4ngyQkBTieDnoz7EyjXtb7pH6ptUm3dyPbacgxkRPAP24RmOTDfIMCQiotvVYze30cYGP3cQ/mJjjDHGGGOMGXj8YmOMMcYYY4wZeC7aP0UzxhhjLiq+cFuU77o3+pcdivzFL36ma2OMMWYT/mJjjDHGfCv+5b+KeNkrovZT/3sM3fTKqPzrdz7TNTLGGLMJv9gYY4wxT8YXbov4pX97Qaj6y/8usttue4YqZIwxhrho/xStXOpGufy4q0cPHHiip+4O7RY7dF168BKJzczMSOz48eMSW15W1xlyvzh5Ul3EIiKOHDkmsXpD3VIWl9Xha2FxUWKPHFPnq6lZdQSKiMhDnS7IKYNcP8jd68orr5TYzKy6zkREDA2pc8fKsrrwkBvN3Jy66Nx1552F6hgRsWePOg0dOaIOUuQadw6c9Urg5rF/524sew2c2hYW1AHo4KXqvLUKDnPkvtYGh7iIiMawtiW5v9TAVYXcZNotdYAaHWXXpJkZHQcPPaQOXzPkGgduWnv36pytlnnJ2rdnn8ROnlRnvFNn1FVq+w51nZub2yGxEyfU/SwiInJ1+JqCe3z0tJZ9+LjO5eER7Zs/+zg/QK89oGvL9eM6LmeOqkPcjra25dxOHZM5jJWIiPawrmEn17Wcz375GxLbDWNorqLXmx1mZ7JrLzkkse3jumbMbNspsYOwpv7NN/2EFvLENfH++7EetYceivxlL4+vf1XXpssu1fG7e6fWcddujS2taB0jInJYu7fv0nvcvkNdBsHMKHJwXBoZ0X6IiNi7d6/ExsZG9Zqwl2Qwv+dhTVxLOGzed+SwxE6DOyitQWtrOiardXI6AyuviOhDvA2uVqsw9hdgvyMq4JoZwesiOUvRcVgOHNeosRMde2cp9DzxVGIR/DzSzWHP6+n55E5LTl7kGhcR0e9pOeQk1oHqgAFalMEdrAHPfxER1SrcNzzn5qF1pzqWylv5dqHHlkpanz5Y61EsoA/bbe7vcmjDbb6fvPCI9BcbY4wx5sm5XF+kIiLyyy//LlfEGGPMk+EXG2OMMebJuOGGiLf/swtC/X/+9ogX3/AMVcgYYwxx0f4pmjHGGHPR8G9ujfgbb4j+3Q+c/1LjlxpjjLno8IuNMcYYU4Qbboj8upc+07UwxhiT4KJ9sRkZyWJk+HHx0fiO7XJMa0PVW5ceVNF4BAvUl5ZU5H3V1fq31KdOn5UY6Qy7XRZGHT/6iMQWoOz6sAoxGyNjeM3NkNg+IqLZVDFmv6/CSRKM7tx5jcQmZrQ+JFqLiDh2Qo0YSnAoif1aIFofAsHdffc9gGXfdOP3SeyVr3ylxB568GGJje6ZlFgfRHT1Bouqx8a0jQ5edpnEdoHolwT89zxwr8TIJCAiIkDEV8m17nv2qsB8aUmFriurq3o9EN5GRHS6Oh/JoKMP97gTjBiOHdHxU62y6HJmWteHk6fmJXbwgM5vmiMbTTDYKHHZG+2mlv2gmgKMTY9L7My8GhLM7tA2m55RMXhExMgJEEavqLB0uqzjZXxGDRIO36si+VUwNYmImJ3VOjV6WvY111wrscqQtmUO47Td1HUgImK4pueXVrXP7jyrY+CHf+pn9NyDByTWSfylNmhi49zpUxK766tfllivqf21fW5WYuVMx0pExMSY9mO3p+YiFaj6xoaW3Yf+6nS4zR+6X9faNhzbAqOftabWkXYNEuBHRGQ1fVQZH9f9cmND6zMMBhSdvh6XlVKPQzquanU9dmJoQmLloWKPWP2EMJpE77RH9LsaIwF+D/qb9tqICJiOaFKQQR27ubYZCdlTe0nWByE83M8QGB/QMwa1xfBIwjQBnmcolkE5ZEBRhgPTGhAQ8GdaTzI+IAF/qarXo7ZI1qqvx2LdIVgta99mGZtc0JjeXHJWtnmAMcYYY4wx5nsIv9gYY4wxxhhjBh6/2BhjjDHGGGMGHr/YGGOMMcYYYwaei9Y8YH3tXET/cdFUu6Ui5ujre9mZMyrkjYiYm1FxPWVfr0IG4KuvPiixbkeFTHS9iIiXv+xaDUKK2h5kXyVKIHI8e1YNDiIiHnkEjAvOPSqxiQkVPp4+qwLoo8dVBLptm4qdIyL2QDZ4otnUPuv3VNB42SHNiv7CF74Qrzk5oeLbdlvF7XPbVbh74qQKursgZrz66iux7AkwDyAR38MPazbt5VUVao+PqPh1N5hpRESsLC1LbP60CqhpXJBA8tDlV0iMhKEREU0QoW7bpgLf9XXt76PHT0jsq1/VjPWjo3q9iIgzZzWLeR/6jATC/Vzn3eVXXC0xEiFHRJxb1DmxfZeK0ReXtR9yEIbed6+Oi9oSZ2SvPKhzeVtoG2V9zche7uiYvAxMHGJGxeARwam3M22jpXk1pWiMgRC0pGtvbZjNU5YXdT9Yr6nwvLR9UmL7XvdqvSCIottdzkw+UtE+e/gBNV34zCf/p8R+6AfV1GR1Wcfu/Fk1zoiImBjX9s0D+ifTGAvR9R4bde2HiIhOW+c3Ccz7GYjth/SaLRg/Y5Pc33mmdW9CfTIQanfA3KM2pHt1pcb7b7+v90NZ0DtgmpCB8UAPls+VFTbooLLJbKdW0fYliXgFxnkVYhFkmcDQGMig9A70d7fPawuN1WZLz19e1LlDv9anBfNKuazH0t5YhjFJ5go1cPKoVBKP3jnVXmNUnx4MrHaHn4eJXo+uCS5ZRUdGv9jzbESqfy4sp9nk9ZjwFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMBz0WpsjDHGmO8EY/Mno7G6EBujqjUy5qkwd/h0TJ5airOzo3HqACfSNcZ897hoX2zmz56O9ScI/EhMRmL9xcVzeL2Tj6ogl8Tk1aqK8IaGNNvzxoYKmXbu1EzyERHtrgrkzsyr2H8Gsotv26axvXv3Smx8grvykkt0I3/FK54nsYUFEF+DcIyEiynjguWVMxKbm9Ns5+PjKvTPZrXeWaZ9s77OouocsnFvrKmwfmJMx9D6hmY1b3e0LR68724smwSEJA7duVvb4sxpFYMvLi4WikVE3HjjjRI7dPCQxL7xDRXmH35EzSL27VfjjEeOHsGyzxzTuh8/qaYAtaoKoM9ChvhKWefdvgNqZhAR0c91Lt99950Sm5iZlth+MKX48H/7fT13fBLLzso69ygz9OiEivqHQLDcBR1xZZmzLrdXtJwpEMyPd8H4YBkKAgFqvwsmAcEmJjlkQJ8AI4/ADNQgIAVjkYiIEVinm2DQMd9bi+d8/ba44v6vPR68dTLin/7LTUXruKinMtFDV3z9jq9KbOc2vebsjJq0dPtqrjAzDW0WEbU6jBcS/ULG8HYXMsyX9LhmQtBNWuA8INM69E0P9sDGkM4HqHZERGTwP2gvarZ1PpDQutXRtui2SSjN53+zLb/vj+6IF3/insfin7/pivjUDz//sX/3wGilAsZB2yb4hRvNGcjMAOYt1TsHQxV6toqIKME10TgG6khXHIJyUkY0JTCGatR1P8iqulcT1GYdmg8R0Yfxj2MIzCvombSU6zrSBEOLCDbJyMps5qHQ5ClumkDeVSWYyyVY/8hIIYM2o36I4PaVMnrF78V/imaMMeZZydS50xe+1ERE/Idfj/jqV56ZCplnDTsemb/gpSYi4iWfvDd2HtEfaYwx3z38YmOMMeZZyeiqfg2JiAiwWjdmK0yd1b8AiIiYPsPWzcaY7w5+sTHGGPOsZHVU//QrIiL2a54hY7bCwiz/ueC5bZyLxxjz3cEvNsYYY56VLEzPxb2HNmkKf+pnIp7/gmemQuZZw8lLZuK2V12YpPlzr7oyTuzjhNXGmO8OWV5EtfNdZHl5OSYmJuKWf/b6GHpCBuQMssGS5o1EuxEsThqDLOZ0PmVf7YPOkDIhR0SMjOsvOFROCZSCQ0MqkNzYUOFZY2QEyx4b07LLIEZrwzWpzaamVOTYbLK4OAeBMBkfkJ5sGYTNE+NadqPB953lxUSk9bqKD4cbOi7OLemfHayvqUHB+XL0vhsN7ccWZAVe29Ds6Wtr2hZra3pcBPf30JAKx0dH9bidOzTr/MmTagxRAYFkRMTSCrTROo1Vbd9uVwfBiRP6t+pnz6ghRUTE5LiOgz/8w49ILCtpOTt2qInDieMXmh5csbweO9dacbxRj3vHLmzPPvQ3ZZZutXR92LNDDUde9Fw195ghFXFE7O/oInh5V+d3/0sPSGxyA9Y1EOtnIDiOiChnYJoA2cH7uYpxa92NyNqrkddGIxrn5zWZMJSGWBzcXtW2XN2nRisfHz1/P9PnzsTY6lKsjE7Eq3/r1/SCu8D4pcy/+/35H35UYr/9W78hsZ/9qb8jsYcfUdOOck3XqtU1NXOJiKhVSeELWdFh/PXgdijjdwUypUdEdMBEgvbgXkf7u1yFvqUYjKkIrmcXDAnIEAjPhQ2cs6wnRPhPiG07fComTi3F4vbxOHNw7sL69LSOfWifFCTsp/v+VnXcaiyCDQlI/E0GCdTm1L6dPj+vkTFJqaJjPwPFO5VNhj5kFHX+AnCPPXheA2E+3SOJ7XsJUT80ebTp+RP6jO4bNP1poGxqNxqTtEeUoY50vYiIPhmgbHJpaTY78a6b/99YWlpCw6knctG6ohljzMXATx4+EW869vgL3n/dNRsf3M8OiOZbUzl7X5SXHn7s3/3pyyKfu/ppLfPc9LY4N20rXvOd58yB7XHmwPbI4YHYGPPdx3+KZowxCa5YXr/gpSYi4kcePRtXrLDNuEmTNRejcu6BqD7hpSYionTugYgN/kJhjDHGbAV/sTHGmAS7NvjPS3dvtORP0kyaytn75IXmiWTt1YjwFxVjjDFPDX+xMcaYBI+CNioi4ngibpSsufikLzURcV5rY4wxxjxFLtovNju2bY/hxuMi5fWWCrXbIL6enmJ7TxI8UZbYWlljKAgD4SMJF8/XU4VnKPTKtY4kvKUMvGvr7J1/7oyKzMsVvccqGApQm505fVJiWUJk24FMzkePaf4IEuZRVmkSZJfAVCIiogrZndEYAoR97ba2+ciQiu23zaroPCIiC23LDTCWGBrSfpjeNimxOhy3dx+PcxqDIw0VYK+uqtD/rjuPS6xW0344O89/NkTizo0NnbdLh3VM7t67T2ITkzrOu20ea+WSXvNvvfHVEhsf07ao1rV9l5Yez3/yjf/xqbjm47c99u8vvuL5ceA1L45vGgb3YBBVytpuOai3R+tqejANRiCdU5z0b7Gj/f3AmVWJzXU01uhovUfATKMLBhARETmsTd2e1qfaefI/28tnLotsdCq6ICSu9diogtbuhTPaRq2OrkGrx3QNO/qgrkv//Y//CMumdfHX3v8+iS0tHJHYsZNfk9haU+div8xC9oVlOBb6oT6s/TgJxi+Li3q9co0FvuNggrO+rn1LBhKjVe0vEn632ywmp7W7DvOW9g0SvFczMNhIZEXvh/YFjb9uwmRDL6jXSz075GAaQgJsMhQoaq6QEnSjmhwgoX+no/sdPUdRHSMiYGnie4S2JDOEdlPr04RnsBTlCvQDGABQW/bAESD1zFSCOVFBcwY4vwTPlGTiEKlxquWQ8UEZhkUf1tkc1smNFu8l+Iy96fxOk9cG4qJ9sTHGmIuBr7zhxjj6/ENReuh4LMxMxKm9c9/6JPMY/Sr/yV5/9vKIse0Rw/rAbYwxxnw7+MXGGGO+BWcP7Ipz49bUfDvkQ5PRmdh/wZ+j5TOXRWy/Mn2SMcYY823gFxtjjDFPK93Zy6M3Ohelznr0q8NRm+Q/4zTGGGOeCn6xMcYY87STD01Gb2jyma6GMcaYZzF2RTPGGGOMMcYMPBftF5tts7MxMvy4w0mpou9gFYhlGTs+oAsZxLbNzkpsaEjdmehccsuJYMeHWlWvSa4s5L5CZa+uqbtNijK4i+V5MacVupeNFuf62NhQB4wcHD6GhsHJhpx+wPGm3WaXDar72po6Z5XAIQ6daMCt5OwZdgejsqt9jVGm6vmzj2odyzqmlxZ5nE+B89H6qrqJVErqCjTSoDGgLjpTU+q4FMGuSyPg8LW0omOVnMVKNZ0jV1/9Qiy7As1x/Ji6Ug0P6zXrQ9oWU5N6jwcO7sKyaxUdl+vkJNbTebe+pGOynmk/rHfYXnpiVp2qXvxqdYP72rvUtWu0r+XUwdEvlVG929VxBQZJUYH2jSq4VILT5MqqurlFRJw6tSSxpUkdazsuU7e9//vWfyOxr588JbEeuG5FRPy9n/4pidXBMWwy0/q89q++Ri8IDmjVKjs2LS/pfTebMNZgj6C9pNnRdamTcKqqQJ/RWpeDCxTugVAfclaMQCOxCBiXXXIXg+PyPuyhibHWg72I9mD6nbgDLoHNdXWKTNHtat1bsN+Se2UHHBOrVZiLCchdjNzpKEbnlsHJqwfPA6l4F56PirrBEVTHiIgGuIhmcGwf9nR6hiuBe1rCgC8Cjg1wy6UxTfUmR15cL4LHBs5vukd49u2Cw+HoKNv6U9nN5oXzZD2RU47wFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMDjFxtjjDHGGGPMwHPRmgeMj43G6BOEvvWGCg1JgNXrqWApIqJSVlFitaqx5XkVhK/BuWWKgTgugoVV+bDWc2hYxV+djfVC5UyOqHg1gu+xPqKJBrsdVWd2QbGZMkhAQPRG4rp+aDkVMDhogVi5DbEIrieLVclIAUSFIF6le4mIyDO9JtWnFFpOG8Sv1BapcU5CzmYTBLVgFkECfmoL0N1GREQL6t4F8eyO7dsKnUsC3fX1s1h2uaztOzOnc6INgsb1pgr4SSi9vs7i4k5JxZjLS3osGgrAb0stmPP1xEpNxgV3fuNzEhvbr3lj7v/k7RJ77o5LJDZUYpFt1GBSgPa7PKaVXwSh9qljpyV2/OQ8Fj0xvVtiOw49R2KfuftOjd2vsfbouMSm92gZERH/9y/eIrGTR++W2PBubYzSkIpxpyZ0nK6vsxkM6ohhngzXdS8pg8B8bHxSYtWEaUJRkThtESRsboGIvt1kgXAH9qdWS89vbui4orV3bEJFzHR/EYELXguE+bWajvPJyUmJVUs6LsbHdfxFRGRQ91YLTDvKWvbwsO7zTWgz2l8iIsiHCduoVMyYqd/VPkwJ+MlEAoX5YLSCxgVgNEVjMnU+3Q89WxFlMmYik4uIGIJ524U2b7WLmSvlOc0bnmNkmkTPI/jsCzEaP+0OGxdQSy4sXrj2r64VN93wFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMDjFxtjjDHGGGPMwHPRmgeUylmUKo+LmTCbO2bRZUEXnU/ZkAkSYGE5IKKLSGRlhfosLKhxAWWGLkN2WhJSRkQsra5IbGJqUmKQLDqWl1XA2oWMwCTKj+D2XV9XYTSxvqH3QwLSANF4BAsNSaAOOk7MQF0FkTaJMyMi1tZUjE5tQUJMKrte1bIrNc4g3WqpOG8DxIL1GmRXBiFmu6fjjwSF54EM3yDEbLW1b2kul0DwmRSbgtifBJ8ZLHkoiIUJMQTizAiey5Wyzok2iGc7MC6GRzVjfRmMHSIihmBOnGxrW6olQER/RsfvmZ6O3dkqj/MNGGvn1nR+nz55RGILML/LFW2zfS+4GsuOsoq/71s4KbF7Dj8osRkQzMeoxlpLvFaVYbz85Z9/SmK7nz8nseqUjr9mS+c87zmcyZ7WFhIc0xrWgv1lncxGIqLeGJIYjf0aCL9xLkKsjns6C9wpo3seZABABge6XqRE9CmTmCLHgR47KvCckBbRa3vUYD/oQVtQhvknPlN9k+EhFtHTcwb1Ge3BOCZhrKSgcqh/aM8qKvQnsX1ERLcFgnnYi2hfHRvRtZtE9MuLbA6ybXoG45upg8nA2JiakNDeljKAIgMAavNVMLcZGtK1oQaGFmurfN9DMO9Hxy7cd9bX2HiA8BcbY4wxxhhjzMDjFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMBz0ZoHrLY3Iq88rroiwXILxHEobgsWuK8uqZCp09ZrkqAbNGucFjrYFKACQtkTp09JrARZ7CcmJiS2ssZZ0ctVLYeE56OQebsJ4jgSo6XElSRezKrFstuiQB3aN2UAQaK3TkEhPAk5eyDeTt13SpS4mQxUhRUQKWZw3+UqT90eDX8Sq4KQPQOxX7kMAlLow/PHgsCXhJwNHX80b0mwmRLZ9kEg3IL+ziGLeGNERZfUD3mivxtw35TFuUMZnymrOawNjWEeUyU4f3FF511jVhesueeppcDJ+09LLM9Z9PvgyRMSa5JRyvSUxLY9/1qJrXe0ff7srnuw7G4PMrKD0LoD636tpkLXzqq22eSorrMRESN1LWf5hK7xV99whZZT1rbMxnT89Erc5lUQ+9PcKWpYM9TRMT0F4v+IiBzWERIcl8BgpqgAvwpzKYJF4nmvmDA6ZcSwmR5cLyKiAWOoTW47ALUPrWEpQXef9iJoy34Ge0RD1wwyXFgDsXwEi7+p7l1YkjM09dFYelzAfgvrA63nAXXs92He9Hlc4Jim/sm07isr+hxWgWc4KiMi4thpXX95TJOBlLYPtW/qGZmepeh8qg/Fmk0wGaglno2gLTc/P7Zaxc0n/MXGGGOMMcYYM/D4xcYYY4wxxhgz8PjFxhhjjDHGGDPw+MXGGGOMMcYYM/BctOYBn/rLz8VQ/fHqtTsq7uyDALrbZSEcia0aDc3eOgRi+zy0nPHJST13mDP4js1tgwrpO+U1u3dLLAOR2bn5RYnN7t6DZecgol9fV1HXOciE24QM1CRAJdFaRES3D2JM6J4SiFVLkFkXhWyZ9ldERBvqWSrrNSkbNws5iwnmUpBYsNvTMV0GESgK5kHIGxFRpqzfECPtKxkSkPA2S9w3iVq7cD4ZCuQgOCaxYKrNKcN3qax9S/dI4tcuCClJeBsR0QHxLRlIlGo6/mrQFt2yitPXE2LlLrU5GB+cqsLcmdS2qI/ruY8+/CiWvefyAxLLOtqYxzc2JPbFL3xBYktraxIjk4uIiHKA+BYE5o0xzQTeXdL1r9HVc8tdNmQpDWn/zIMRzX1fv19iz7vpWomdXAfTmDqLi/slbV9cH/q0Zug1+2USp3ObZ7AGtimzOaVaL0gbDD8iIkqwX5IIOs91LhZdp1Oi6taGrtN0TTSYydiQoCg5tHkGY4BMBti4QNeBKphuRES00WAGDABoXYTlisxyaM84fyyM1UzrWa7QGNByqlt41CXfA3r2IGMdFuDT/CwuhK9UqG+hz2CfL4NxQWo+VNERS6H2peeE6XE15UkZdJRKWs/a+IXXbDaLzyV/sTHGGGOMMcYMPH6xMcYYY4wxxgw8frExxhhjjDHGDDx+sTHGGGOMMcYMPBetecDUzEw0niDSHBpSMe7k1LjE5ubm8Ho5CKM6TRW9Dw8PS2yjqaLWhaUliR0/cRLLHgI12rnlRYmtnTguMRIkrq+pmHFjgwX8HRBjbjT1/GpV23d0dFRiJRCgUpbgiIgMjAtIeEbiQxJqZ5AJuZ8S0UO26AwExyTWT4lIN4Oi3UhlCgYBIEy/EqguWSTLZfchgy8L6zWWg6CRMjvnpAyNQMFyBUSOOdSxDxnVqyUWtWLRMA5K0N95jzJVayiD62WJcV6CMdQB44MerEE9qGMHjC9SVEBk3oKfq1o1Dc6DucfUrimJrYCoPyLiC1/6rMS6Xe1blepH1Eu6tsxNzUpsGAwgIiIqYGyyY1rPp/3g3rvultjE9LTEVhLLwD3LCxKbGtV9Y+HoWYmdO3FOYjN7td6L7UUsuwRCb5rzoC/HdaRc1XHRBYH5+WvqsZUoNkdzMr+AeqfWXvIjwDW1NCQxyjpP5iskwI+IKMO+Q+eXYa3MQHRO63FyL8E1o5ghCxmg0L202rzekPlArwPi+GLbZeExGRHRgfFC+0anX2yvJeOCSuLZgdYwNP9p0/gtWB9qjODnHjS3ob2aTA9gL8kSHZbnxfYdMg+ifmy3dY3GuRgRfdiL8k1t1GrZPMAYY4wxxhjzPYRfbIwxxhhjjDEDj19sjDHGGGOMMQOPX2yMMcYYY4wxA49fbIwxxhhjjDEDz0Xrinb9DTfEyEjjsX9vbGzIMeeW1GHm8PETeL31dfXmmZ+fl9jRo0cLnUvOJo1hdfqJiMjA2YR8Z8hZogbuXrWqxkaG1JUnIgIMLLA+ZITTB0cXvJeE00UOTiQ5OLrk5EBV0FmsAs5r58vW88nZhO6bXM0KGr+cvybEyLUm72tbBLhkkVsPubxEJJzAqM3hmujmBvVOtXnkWg673sA16+qulIHLWi/RE+g+RM56BZ15SuS2A648ERFdcB6keUK9nUObk1nPZoeYb9IC55lKTdtyA8ZaaUgLqs+OSGykcgDLvn7Hbomde+SUxMpdrU8t0zH0/KueK7FTDz6CZV8yOimxXRPqbHZu/ozEZvbtl9jQkK7dD83r/hIR0Z/bIbF7WosSe3hFnTNPHtf6NObGJNZu0miJKNdhHSnT2CAnRXK0gjU+4f7Xx10L1lmadxVy0yy+quKR6CQGcxmcGcmRje+6eC1xH4N9COsNzqkR7EpJLoxF90Z0koP1IiKiB65qdI+0t9ETDj53JJ3oaPyCkx09Y8C+kZHbGOwZERFltOCD+wFrUbwb2jdoXETgpwaqZY7PZuR6qGOgRzZrETjQ+3BH1L4lGBfk3EZOaefLprlzYazd5TWR8BcbY4wxxhhjzMDjFxtjjDHGGGPMwOMXG2OMMcYYY8zA85RebG699dbIsize9ra3PRbL8zxuvvnm2LVrVzQajbjpppvizjvvfKr1NMYYY4wxxpgk37Z5wBe/+MX4wAc+EM973vMuiL/nPe+JX/7lX47f/M3fjMsvvzx+8Rd/MV796lfHvffeG2NjKpJM8cH/8jtRqz0uzmq2WnJMB8XXDAnHq3UV4VerQxIbnlJBLYn9SOiXgrXfGmyCUKuda1uUcxbwl3MS7KmAqwRCuFJJh0cZJNAsEI8og2CUBHckYCUFNZaTKJs0iSiOh3OpnJSglqAa5SA+JAMKFF1SW/DJib4AgToZJNB9g+Y3NcqzvNg1U4JROQ7qnSXO5WsWU2LifdNQA/H1+WKKzftSSUW6/cT43QyZDEQkhNrQFj0Qm3ar2rmLsKZmmRq3RES87LlXa3C9I6H5B9VQ4LrnvEBitcVliY2vcNljfV2n54/eI7HVpgr4p2cnJdZe17IrrTUsuw57yUxDzQfOlLTNjz16WmK7rzyoZWcNiUVEBK3zfW1zMm4hgS/6DqRmOIml0dhExyoZ0RRdlyIiSnA/qPFOrItSH1jPy4myi678Jdj/qTrcFixkhyW1sCELQesNPRtFRORg8NHp6Fgj0KiHxP8J4yFq9T6ZAuD90LMZ7L+JjiXTGTJNSDzE6WFkFJBY99FUCu6bnifIxKFPA3ArnzOonjDOcfTC4C3BmErR71141dT2S3xbX2xWV1fjzW9+c/zGb/xGTE1NPV5wnsd73/veeOc73xlvfOMb45prronf+q3fivX19fjQhz707RRljDHGGGOMMd+Sb+vF5ud+7ufih3/4h+MHf/AHL4gfPnw4Tp48Ga95zWsei9Xr9bjxxhvjs5/9LF6r1WrF8vLyBf8ZY4wxxhhjzFbY8p+i/e7v/m58+ctfji9+8Yvy/06ePBkREdu3b78gvn379njkEc5HcOutt8Yv/MIvbLUaxhhjjDHGGPMYW/pic/To0fhH/+gfxW//9m/H0JD+jfM32fx3j3meJ3UY73jHO2Jpaemx/yhBpjHGGGOMMcY8GVv6YnP77bfH6dOn47rrrnss1uv14tOf/nS8733vi3vvvTcizn+52blz52PHnD59Wr7ifJN6vR71el3i/VIp+k8QK1Ybw3puVcW4yUz0JKgFNRLJwdoUhOytKR0wZYMvYSZmlWBVSByHwnoWspGgm1RzJRRywqlURlJNrveD2dfhqiUQ8JEAOiV8LCpaL4HQsKixQ0oEX1SMznUvNi5SPxQUFdjR+Th3IOM9CTEj2OQgCgo+GTIzSPwWU7Ccwu0D950S6JZR4QviWTLOIHEniE2TbVZQVF0egrJhznfL2reTO2ex6PtOPCyxhbVFrQ8kNp+bmZbY6a+pe+aBqQksO1tcl1gNbnxoVH+AGxnW/ebUwqLEGiP8413W075YWNM/oT5X0jpWZ7QtK5VxifVaLKoulWDPAgeAfkYmEAX3goSIvg/Hol8OZKwvDMzjiIgcTVW0nmSm0e5pW+ZkWJOgqMEMZWQntrK2sC8J7W3Q5nByCdalVDZ4XJrgvns9NRRot7U+tL+kxho9z2SFTYbgeausi1DSqAL25R6MaTJSoD0d7XxScwz2VvZM0rak5yN6dkh5XOHzDIS6nWLPI0VNgiJS95096b+fjC19sfmBH/iB+PrXvx533HHHY/9df/318eY3vznuuOOOOHjwYOzYsSM+/vGPP3ZOu92OT33qU/Gyl71sK0UZY4wxxhhjTGG29MVmbGwsrrnmmgtiIyMjMTMz81j8bW97W9xyyy1x6NChOHToUNxyyy0xPDwcP/ETP/Gdq7UxxhhjjDHGPIFvO49Nire//e2xsbERb33rW2NhYSFuuOGG+NjHPralHDbGGGOMMcYYsxWe8ovNJz/5yQv+nWVZ3HzzzXHzzTc/1UsbY4wxxhhjTCG+419svlPk1Urk1cerR0Ikykze7nJmXBIekUiMREzlcnHRW1GKCsdzEBpiBumEiD4lxtxMKpN9EUj8HxHRwzTvIAqE++734JrQ5qlM0zkIZcmIoWDyYBauJTLOV0AkDqFE1m6UGn6r6j1+PrYRtSXdOAj4chBDkgg5InKwhiCxP90jigdxXCU6rEzGBRoqOm+pv0oUDDbZKHrRHJWc0GYJ04SsCiJSELpSn3VaJH4Fke2Qiu0jIu654x4tp7MmsXf9m1+U2PE/07xmQ1UQ7W40sezhurbbamtDYrNz2yR2bn1FYit9FVDPJ0wyDrc0fhKOG9m+R2I7Lr1MYuvqMRC9Pm/N5TrVCcTxMBe7XT2uXAGxfT8h/ifzCziUDGsyXOvAjAAyqkdE0PCnNaOLGebJ+AXWoIQ4mfYIXJkKGhJkKP5PrGtFRdlUdXxmIlF/Yj0vaB5QLTcK1ZHWyZQgvA/1RAMoXOuKmdikoHFFlEHATyZMaHCQMqKhZwe4JM1lMpDIU2Y7BaF5UoLxgmOyV9z0CB9xNh+7hWfup3bXxhhjjDHGGHMR4BcbY4wxxhhjzMDjFxtjjDHGGGPMwOMXG2OMMcYYY8zAc/GaB8QmgV7BrKPlSg3jKFSkbPCVYhlqSVRFWY/PH6tlY5Z3AOuIOvbEO2qloEAdxHUoAIR77OWcJZsUjWR8QCpFFHnDuU81uy2rLvU4qg9l4o5g0wQyusBrFsjAG8H9EJHIQA2QWQSKRVGBn8iSDaJhumZRcSbdd0p8yOYgOseoeTBrMohXcexGcUMCrDuJO9Gogo0LeiQQBmMIEpGWMl0r866eu9psYdmXXXeNxE5940GJveeXb5XYbFvv59rhHRIbGxvCsk8tLkts+spLJHayq3U/BwYJx8HA5IF1XtdODsPaPaUmBcfXliR2aIcaCvRhzxkCUX9ERJT1vnuZGiyQCJlU8EWF9RE8dzhbOYnWFTJzSXmD9OF/FF0fihqq9Lq8dhbOqg7zjs4lwXvK/If7B55lyMUB7rsE9UmV3UsYORSpD6/TxcT/Eal9o9je1uupKUopdI7lqfsr2N889Is5TXQTZZNRBT1n8PijZ8riAv4sA3MbHGsFDamgPqm9n57Pnwr+YmOMMcYYY4wZePxiY4wxxhhjjBl4/GJjjDHGGGOMGXj8YmOMMcYYY4wZeC5a84BKpXaBwJ4ET2VQ0W9FXJxTRlhMBE5BEAV2WGxaLijKIhEyCSz5HhPCZtRngqCMRMygFqVSKAPvVuj3td1I8EnZ04sKtyOKZ2ymcjjVdEL42IMM3SC4g2GBZWdUeEqEh9Fi2bjJqIJNKbjNy3BsDmO1UmGxqtQHbjEldC36G01WLTafyAghtbbQfdOxJIAmMmqfRNk1mLcdGGtlOI5MBvpgBNJKNHk2Wteyh1Wk+9DpByQ2tPdyiX1h4ZTElk6cxrJb/bbE6qtjEtuAZju9ti6xE2sqwO+MTWLZw7sOSOy+s+ck9n1/7a9JLB/XOq6san2qMP4iIkaHCo4N3CKKzZGUphoSifP6CwtJUZOXhG8BH1tQqE2CeXyeSOwlRc1O8LgMjGigc1Ktk4GpBQnPyfAhh+z0bIS0FQMeCNI6glnnaV/kZ6aidSr6LETrWmrP6BcchCUSx+NeDc9RifW8R7cN6zn1I41z6q90yxYzgSjBcWRw0CfDLjDlOV/0t95D0cgogb/YGGOMMcYYYwYev9gYY4wxxhhjBh6/2BhjjDHGGGMGHr/YGGOMMcYYYwYev9gYY4wxxhhjBp6L1hWtMTQUtfrjDjvkApGB60LKTYMcn8jxISvotEbFVEpsH5RB4eSMQhZmKfcMOTXh6EJtxA5L5LRSzEFlK85kBLmLYN+Aq8pWXPDItYbGRUCbUTNWM3WAiogogZMNjyFqS6gOmIH0U45sRd2HCh7XgxsvJc7NqxqvVWp6HDQ69U0Z5kMODjEREWTMV3xcgUNSwl8Oy0ZHF3AmozFQsJiUH0y/15FYFZzj2l11EUOHuarGyol1bUOLjtrcjMQufekNEjtybF5iYzPqGNYaVue1iIhSQ/t2FebE816iZVdOq9Pa6tGTElvp8Fi7+oZXSOwXfvKnoT7auf/xP/2mxIZhnI6NT2HZUW5JqN/X8YdOnOSGCWtVCVyuzh9cdC8q6AK6BZcjXufRpq1Q2bSep6B50gOXLTRPhd+OsT6JsskBjaB1pAsWWxmsF11wT4vgNRmdzcDpiveX4q6muA/iHlr0mUnvpQtrYkREXvCxmMrm4xJOYE/hmlmv2DVp7Kb2UBrARccq7iV07hae1zaPja249/mLjTHGGGOMMWbg8YuNMcYYY4wxZuDxi40xxhhjjDFm4PGLjTHGGGOMMWbguWjNA0rlMguSnkC5rNUnwVtEoOIJrw/CSRK4kQCazo2IKINAM++pYI90YyRC7gWUk3hFTUi1NATNxuYMBY0QItG+VJuComoUGpYT4kMQXfZJtF7QVILE5HmfRZflrJhglLRwJDSktkDTg4jCAl8uu9h9pwSfNPfW19clRmL7Wk1NBqgfKpVE2dDfZZgnGU0Umrbk2JCCRJd0STiuBHMMRbsJwWelpmtgB8TANJc7HRUSk+FCM9EUFZh7J9oqbn/Hu94lsW07DugFMzAKSI1nEEFHReueN7U+WVXL6dGEKOuYjIigZe3MotbnX/zcP5LYj/zYmyW2Z2ZOYnd8+S+w7MWVExKr1Bt6YH9DQkUF2Tx6IzIUapPZBJwL47ffp/2BBxuJ6LFsWK96ILQm8X9qqOFcpoMphs8OehiOvwQ93NuofYqtS3mqvwuK8Gk9z+k5rFSs3ufLVmMebqJi16R9NfmsiH5LxTZhFLinzDgKggJ+GPt4br/4fWfwOIOmH5WiJk4F2ydRp273wjW1DftVCn+xMcYYY4wxxgw8frExxhhjjDHGDDx+sTHGGGOMMcYMPH6xMcYYY4wxxgw8F615QLmAeQCRzGRLQRQ8kbi4mMCyAuLViEQW8xKI40CwDDr0qIAAKyV1zkisBYKyDNoNxYcFj0vFi8ZYMEfGBcWzZGdkDAECwAz6AfMoJ8ZaD0wFCme6BoF5Br1bq/JYI1OLohmkq5AxnPsrITbFKIj6yzr2aa6TAURKfNiB0lkkSdcsdo+pcZ4SW2+mXC6YJRuE1iQYjojoNyEDNYxzEgjT2kDjIoNxERGR11SEnw8NS+xcR+fDdG0I6lh8O+rDWkuzsTwC6ywct5Vf+FbXNGP5P/2n/1hiN77iJRI7uGeH1qejNZqensSyF5bAIKGv91ivwRjqN/VcMrRICdlpnqAJCaw3fEWhVzDzego2aSk2l1Pzu+g1aYbSHOvD/E6KqmG0Fr0fogRzmcwVtkKRrPEREf0AY5zEfRetU9IAAEovDOz/VB26HxxXsK6l9lCi6D1im6ORQsocBGL0bNYDoT/u1Xq9tFkEhi+gB6Y4KfzFxhhjjDHGGDPw+MXGGGOMMcYYM/D4xcYYY4wxxhgz8PjFxhhjjDHGGDPwXLTmAf1+FzPFP5FSgBAuIbIlyoms9VqXYmJINAmIlAgf6llQ6EUisXJK+NijjNog1C4q6sdSGMo+3AcVHmnwu2BwAFq9SCXgLZw1GS7Q7xTL4pw0bCjYbmg+QAmt4fcHMns4X47WqkS/X8C4KKFxAY0LLDox/qFs6NvNWYYjAtWHtTpkp4+IMrRHr2DmY4xBvfsggo+IyPJiGZE7mKwchLeg4kyJLvtF1weY8ygcJ2Fzckkt1ubtlrYbmRnkoceVEltUCcZQmcwv8Gyl0ylmqBIR8b73/XuJHTywV2Jve+vfldgn/+KLEltbXpPY7LZpLPvhR8AMoa/17FCWbhBFd8C4ICkupjWwaMb7pyB4P38wzQkYVySAhsttKRc8mcngRbUfsClpWCUqVCq4htFxRJ6DgD9l+1LwmYDGQB8XDT23B0L0iIgsLyoULzb+tkIfron3A2slPU+QQVHKGIu9MwoaOxXd21KGDd1iZgh5QdOEokZGRekl9l/CX2yMMcYYY4wxA49fbIwxxhhjjDEDj19sjDHGGGOMMQPPRauxMcY8O9h55GxMnV2JhdmxOLFv9pmujjHGGGOepVy0LzZ5v3+B0AhFa1vIcEyiLEjQnRBvFTv3SeTkeiSJ+EBYVQEBK2WtTRkhZCUQm5L4sKhQEDMp831TE9E1ez0VhZEYnIwCUkI4EtFzlnfKhgziQRDCpUWKej6Kt2GwlTLq70QxBaHs1WXI3B5kHkBjLTHLqC9u/Ogd8ZJP3f3Yvz9/41Xx6dc9X46rVnWccvs+tY/MmDGcxK80plPjHMYvZ94uJuSkcxM69sIiUsxADQYSeQ4CfF7sYm1VM9kPD49IbNeuXXi+lANi3NR9k+D5qUyTalXv+/Yv34HHPvDAfRL7v8FQYGOjLbHd2/XF/j/90Uck9rNv/Rks+0tf+qTEstC5Q14cWVnbl8TbqXWtj2tlMRk+ZmSnckCoHxHR6+kNUXb6CqyfPJ/IkCVxL7BO017NzQYDuGCbfbdI9TfvRcXWG9rvaA1CA51Arwik6PjbCmiwQMVwECLFn5kI2jeIoqYJqbJpPnFB374BQLK/wHhjcz27YG6Qwn+KZox5Wth55OwFLzURES/51N2x88j8M1QjY4wxxjyb8YuNMeZpYfrsCsanzi5/l2tijDHGmO8F/GJjjHlaODc7hvGF2fHvck2MMcYY872AX2yMMU8LJ/bNxudvvOqC2OdvvCpO7Jt5hmpkjDHGmGczF615QL+bR/8CsSOIjkrFMqBGRJRBCNcHQWIGIiYS1hOgfT1/TdBakfiwVIGstZVi5gEVEoOfv4JEKCt1DpWkzNs5CcwSSj8SqBfNmlwuf/tZjyMiepC5uGj2axSbFhRNnv8fet8l6IcK9C1fDwTdMJ4jWGxNfUsGCTmdDH3bS7Q53eOnXntt3Hf1npg+uxLnvumKBsV0KbMzzMWArMcREd0uZbdXqOrcj99azPg4NDbgMBjTdBz4VEQk+psMH8i8As0D4JoVuu8uj/NKScf59LR+jfvYx/5MYj/w/a+V2Nzsdi072eRazx6MIVr3iS4I43/lvf8Oj/3pn/5piR15+CGJ3Xn3vRL79Kc/LbGRoYbE9u9jw4W9e/dI7IGH7tRrjgxJrAtmEVlG2bwTJhmUxRzmIwvCQUCNcx6LxjrRPkj1QfE2lQ1GChGJfYOOo+cEOpfaMTXQS7T2UzlFBd3FDAEiIjI0k9mKic6m48isKbGHFjdFKbbPJ9yeELXiiKB2A48BNkiAfSxpeoRzjBqu4DMKCv15rJQrej7uq/CgS6ZSRHqswLzdNGD4XpiL9sXGGPPs4MS+Wds8G2OMMeZpx3+KZowxxhhjjBl4/GJjjDHGGGOMGXj8YmOMMcYYY4wZePxiY4wxxhhjjBl4BsY8AF0y0hYqQtrR6ELKBV0/Njs2PFkZRd1Aki5bcpzWh5zOUscS5ChEFHVAiWD3DLKIy6AfqSnYFSXR5gXrQ85vRUm6yRR0YspSNnqboC7sd9kdrATOJtWK+rzw+NO27KKLHdPtgcMSOQ2p0U+iLZ/a7y4J3xmIkdse3EuZl0sy3MnJUYjuG2tT0AkpeGzQ3KEhyX5qenK1Vsey62WNd2EZ+vznPyuxv/7X3ojX3Ex6TSw2XnCPgLb8D//hP0jsec97Hpb83Oc+V2If+tCHJPbpz3xGYj/6pjdJ7KUvfalWMTF6Z2e3Sez+B/S4vA8Om5mO3yrtY8k1ntpSQzQfaJ3mMc3rOdWJ3NfQORPHkK6fqb2E7oecPPvgGEoOh9QPyfkNewTN+QzXymKOWCVwXjt/ZPHnqyJsxVGN4uWy1rPX0+N60A/o+rqFZxmiDx2BjqrQjKnnNapSFYK0L1drxdxTe+g2yo7ArVZLy4b2Jae/rVCkL8C4Mom/2BhjjDHGGGMGHr/YGGOMMcYYYwYev9gYY4wxxhhjBh6/2BhjjDHGGGMGnovWPKBcqUSl8nj1UDgO4rZSOSFCAhFeCQS+RSHxYJ5QN/VBwJVB2SW4HxKJk8gxJXykdntiuz5eHxApQtkk4EuRYRsVE62T4m4rhgskRisqNuXrFb/vyEFcB+X0QFhP7ZvB9VJmDx3V+uE1ixpVYDuicDuiDMLUHoh0M1LUApXKFuqNCn4wiyiqVy6DCBSMGSIiyiAG7sF8RPMKKCeH35tYAB1RJsE8lY2mHTCmydAiYXLRjbbENjbAgAIG5eLCOYkN7RyWWGqcV0pqiEHzqVTR9ul29bjpqVmJ/Z2//b9h2SsrKxI7euS4xIaqQxJ7499Q0wQy3Vha0zIiIipVNWzooaYfHTogBOMHxMERbKzT7cKaTPsY9mNxQTeaAsD5uEcUFdsnlha6Zg/nRLH1pp+DMUnCxKZUgr0aj4TaFDYj4v5mY5OCbiUALftbMk2Aeha9xxIZQCXWlpyeW2gMQLPRMwa2Lqz7ERHlgmXX6fyihlaJNqO1lsyVaoWfo4oZUp2nwDeWgkZYBa9mjDHGGGOMMRc3frExxhhjjDHGDDx+sTHGGGOMMcYMPH6xMcYYY4wxxgw8F615QGWTeQAJ7iog2k2KyUH4y8eSAIvkX8WFj5j9moRVJOSEM8uQKbicEB9inUhIR5nWAcrqm3o/LhUU+1HVMT/8FjIFbyXLMRyoMRJ+p66HQkW4bxLEFszQnTSLgO5pQ5+RyLEoKfOAHMSLLNrU48plFYPTPSalonDfOYkcyYgBROd9NHEoLrokOiASZ2UpHJYQdKf6Qo5DLbnWJwsQ5XfX8ZpZ6Pm12qjEaP285557JLZz514sh3iKia6F73/VqyQ20lDxf0TEiROnJXbu3JLEdu7cLTGatjQVH3zwQSy71dKM5Z12sbVpo9mUWLlC+11iL4BxTkYVfcoaDzdJWuCtmOCQkB19SaDeZGaQKptWSpp31OZopFBwjY9IPaNAW6JhTcHrgbHI+YOpnC3sgwVIiv8h3uno2OfnGxin5LCR2k3gOYxF+MX2tnwLovce1IlMO6icLrQPzWXaayMi+mgiUfQ5aitGAVBKgTG0lXF20b7YGGOMMcgXvhDZffdFfvnlETfc8EzXxphnJXuPnovZ+dU4OzMaR/dOP9PVMaYQfrExxhgzMJTf8a+i/G//7WP/7v/zt0f+b979DNbImGcfr/3Tb8RNn7n/sX9/8hWH4qM/dM0zWCNjimGNjTHGmIEg+8JtF7zURESUfuk9EV/4wjNUI2Oefew9eu6Cl5qIiJs+c3/sPar5p4y52PCLjTHGmIEgu/8+jt/HcWPM1pk9u8bx+dXvck2M2ToX7Z+i9aMX/Se8d3GW2OIiJsqw3AMxL4mqKXN7n4SLCSFcCUTVRQXhVB86juqYog9C5Kxg2TmokFMCwByUnHRoj8TkBUVradEl3CMeCO2LAlQYKzCmIiLKmYrz0EAC6oiCeTKVwEpGZCUYG9hGxdoSPQZS4zwHMw80ANByejj+9HpbEReTKQCtD+2uii7LOZgZJO67Wi02l3MyKaAs5MBWvB5o3sIShOLrXq8tsX4pYcgCY7pUrkusUtM1ozYEc4TGwOY6Hroc69K77GBCBK1tQe2zsd7S+iQkBR//2KckVi41JNbtkmmHXm9peUViO3fuxLJz2LIbjWGJbbT1AZSEybSvZok9tA/jl5YWNiahcuDcLQiEaT5mmLFez6Wxn5xidE0Ya9RqLLYvPplRjJ6ByJuMAqC/cWuDMs7MjGB9Tk2NRG+TED+DtsBnB7het5ta/8hcSUP47EH9jQL8RNEFnx3ILAdNKcA8IPW8RvdDz5oErsewh3bRDCuiQg4zEGLTj29ZvYh4MsMlmrcXxmjOpfAXG2OMMYPBDTdE/5/9swtCvX/+zyO/4cXPUIWMefZxdO9UfOLll10Q+5+vuCyO7rGBgLn4uWi/2BhjjDGb6d96a/Te8IbI7r8v8kOX+6XGmKeBP3nNc+IbV++MbfNrcWZm5PxLzXfYZt2YpwO/2BhjjBko8hte7BcaY55mju6Z9lcaM3D4T9GMMcYYY4wxA89F+8Wm225F6QliIRSOg8AsmVEd1bMkbAbBHVyzhNl/EyJ6zLQOmc3hfvogQKX30UpCkNgBoRgJ9sqU8RYFbsUF/CRWpVqikQIZHFA/JPqbRKgk9idhXhVNE7AYJJlNeRM4Bsog/AYFH8UiIjISemP/FBu/FKuUE8YFPY232ypG71HfkrgTxIxtnA8JsT6IQIsKELtQRxoXERFNyOhO7VaCPuM5pnWsVoubgxA4JsGAgvqhnFhbSChL429tXcXxJ04cL3Ruu6nGDhERlRpnzy4CjZXhYRXgP/DgUTyfxu/8/LzEavUZibVgrEyOT0hspMfi7XargMFCRGQZGN6ACL5cAQOJxBwjwT1dk8Xkej3qB8qUfv6apGKGi8I+j2YaUAb1a0REDvOxoGYd6ZLSOmmKAnUqFTVYKDZWKhV+DKRpnzJvkXNpPadnq8SzA40ronBTYNmJI6Ho5HPlJtAAqrgfBl8Tn6OKXZTOTR4Ldaex0QPDmwwNsorvWUWePYo+V0X4i40xxhhjjDHmWYBfbIwxxhhjjDEDj19sjDHGGGOMMQOPX2yMMcYYY4wxA89Fax5gjDHmO8Puo+di5uxKzM+OxfG9tm81xhjz7OSifbHJ8iyy/HHnB/KAyJ6i3QS5/ZCLRLujzjzk5pF06CIHK3T9KOboQh/ayIEsRa1Wkxi5WhDoBpNygwNXlqhoW+S9YvddIsebgvWO4D6rVAp+tOzrVEk5xFBb0v3082JucKUyuHsl+jvv0f0UG2vowAP0EhZx5DIDxluRl4s5qJBDUqqOOTh0kR0NOeMRGRyXXG3gJnOa3+B4R10DwyK6SZcgcpa6MPZDf3pn3PiZBx779ydedmn8yauvRqPIWqUhsVJinSVnvm67JbGd2+ckdvihByT2uc9/XmI33PBSLDuHRiqDix7VfHFhQWKlsrqsnZ0/jWWPjWkbdbprEhsf3yexoSFde/OAMZ0Y5/NnTklsdXVVYtu2q6vaRlPHX7ejc4wcPyMiej1yQ9LjaPzSvkoOhXRuRGqPAXdF6HE8Fxz4yJHyfDHFnMAKGlVFBmtVL3Hj5DBH+yWtdeg0hYt0sXU/Ba0j3S44Jm7B2BH7DAqidQCvB4cln3kKuoPSnsXuvcXLxrlDjp9QNl2Rlu6U41wJrkkOujR3ctwjij+f4+zedN99u6IZY4zZc/TcBS81ERGv+uyDsfeYPtwbY4wxg45fbIwx5lnK7Lx+SYiI2Davv/IbY4wxg45fbIwx5lnK2RlO9HhmZvS7XBNjjDHm6ccvNsYY8yzl2N7p+NQrLrsg9j9ffmkc3TP1DNXIGGOMefq4aM0DylkpKk8Q8JI4mPRXJFKMiMhBzEZiXhJ/oQivYCwiLfTeDInEKhXtIhI2bxZaPVmdSBBWLhc3Q5AyUv+DhIok2oTjyNghA6F0qs0J7DNoNxwDILij4yIi+iDGpDbvgggUrwniVRbrRWQ9iOM4x7O1aDAzyDMQFEZEuQyCRhJtkqgQRLIBJgWlSuK+c50nNH47YARCUB+mzCLKJRWeY1vgmqHXo/UiJfika24W+3/stdfEXc/ZFbNnV+Ps7Ggc2zsd1WBTEzIKSM0wNEiA+lTr2j7nFs9J7L/+tw9L7EUvugHLrpRVhL/R3JDY8JAK/cfGxiR2+sw8lMJ3vra+LLESrGs7d22Hs2G9gaMSS0s88shhiV111VUS6/b0TxBXVlVX1ajpOO/1tR0juJ5dMPjowl6dw3zKYPyk1nMS0ReFzBnyLq29qbJBEJ7qoE3Q+kktmRSTo2gdTGcK7oP8PNDGY4s+C2WwTmP7wGaQMu/J0QQCDxTQqIcMOhIU7W/aD4q2WfrRitpI97aipglUx2pV1+MI7sdKTY/t5jrn0eAADFDIcCbF5rYsOuciLuIXG2OMMd8Zju2djmO2eTbGGPMsx3+KZowxxhhjjBl4/GJjjDHGGGOMGXj8YmOMMcYYY4wZeC5ajU2v04nuEwSZGaitKiCKriQERnlB0Tq965HQi4TsKVD0TsLJglmcOZNyKmVzMVMAEvFROX24l5TJAFYTjqV+6IFgvgRKwVKqv6Ef+5zWF0LQt9BfLAxlYR+15dBwvdBxCc06Ap4L2JY0n0j0i32TqE+/r8J8avIKZIhn7SsImxMZ2UslXcrIUKMEon7qbxqnqXFOU4+OpTGJcxkarQ9Z3yPQXyGyAoYC6WvCBRNtTiYQfRhD3V5Tz60MSazVWpfY8vIilj05NadBGAM0rOpDajywvKJmBv1c6x0RcfrMoxIrg6lFqu5FSAllJybVpvuGl71YYh//sz+VWBd8M0rDOh/aHe7vLqx31N/lCs2xYoYsT2JFA0eCOBn3Er1xFoMnMrLTXKZpQuYgdD9bEf/TllXQKAD32qKFRMIoAE+HDQHHL7U5Fl1YhF/U8IbMkZLAsVQf8j2gsksg/k+5g5Rg/S36nEplZ7AuJYZ5VAqaRaE5Q8GHlNRxbKyTPem/nwx/sTHGGGOMMcYMPH6xMcYYY4wxxgw8frExxhhjjDHGDDx+sTHGGGOMMcYMPBeteUAROKNr4l0NVNUoXSSxPZ2L+v2tZAcHcTJmiAfxF5bClKvaxSTeprpT2SUQi5JIOyKijH0B4mLK4AtnkhAzJbIlYSroVzHDPPYXlZHUshXLHkxCuqIivBQor8TxWzCjNVyxD9nGIyJ6JCQGsT4LBaE+kKQ42d80IWlMh2ZSpkzMlDU5bR5QNAs0ibKLmRSkhJM0JzgLNMxlFPOCkULi9qi/M6gPibcrZRXwV6va4aurq1g2mQekMmorWu+1tRWJUb3P12lRYmS+cu7sfMH6KNS2ERGXX345lK1ry9TkjMSaG3o/IyPa5hlNvIiITM/vgSNBRgstkMPAIh168n+Q70bBdW0rWcy7lEGdzBAKivqp4llikrGA/9svm0xEUhTPbs9mE1I27EPJ9XwLJhJFoHvJEs9r+NxDaz88Y9B+R0M6dd9F+zaH9YGNh4odFxHRgX4sl2FvLGjExXtyYoLntA596zJS+IuNMcYYY4wxZuDxi40xxhhjjDFm4PGLjTHGGGOMMWbg2fKLzfHjx+Nv/+2/HTMzMzE8PBzXXntt3H777Y/9/zzP4+abb45du3ZFo9GIm266Ke68887vaKWNMcYYY4wx5olsyTxgYWEhXv7yl8erXvWq+JM/+ZOYm5uLBx98MCYnJx875j3veU/88i//cvzmb/5mXH755fGLv/iL8epXvzruvffeGBsbK1xWqVKO8hNEkUlTgE10E9qkoucX1SeRML4KovyIiFoNhONQTtHEqiwQT5wMBaEwv6DBQQUMDvJEpuoeZDZHwR6KzKCWEGsnhIvYHlhMsUbv0rnl1PQBwR4MjXaPRIFQDrRZcjxTP0L24V7v2zcuSLVZBUTHZZgTdD8Vyla+BbOIHrYlNCZkle5BOZQBOvU7UK+oAQWKUotdL3XfZYiXK/VC53c7YPaQ0ZhOlU2GLHpsldZKWEc2YI70QbgdEVGCOUbi5HanLTEyBRgeHpbY0aOPYNkTExMSO3vmjMTWN9T4oANi+0pFBbqlhIB/x/adEltZUeODubntEkMhMJSTWgaKCpHRFIXmA9xiKbGmZjAOul02MYGrQgytg7hsGFd0j2Uy4yhoypMn9jFaM6iWJRCtswh+C0YKsEdQf1dh/Bb0j0iaGfRpw0QTCGrfYo+1WzEeojHAyyKt+3BUQsDfhfWBxm8vipnb9Hp6HMUiIirYbmQOUmyfL8ONp+47j29tMtSjPkiwpRebd7/73bF379744Ac/+Fhs//79F1Tkve99b7zzne+MN77xjRER8Vu/9Vuxffv2+NCHPhRvectbtlKcMcYYY4wxxhRiS3+K9pGPfCSuv/76eNOb3hRzc3Pxghe8IH7jN37jsf9/+PDhOHnyZLzmNa95LFav1+PGG2+Mz372s3jNVqsVy8vLF/xnjDHGGGOMMVthSy82Dz30ULz//e+PQ4cOxZ/+6Z/Gz/zMz8Q//If/MP7zf/7PERFx8uTJiIjYvv3Cz+Dbt29/7P9t5tZbb42JiYnH/tu7d++3cx/GGGOMMcaY72G29GLT7/fjhS98Ydxyyy3xghe8IN7ylrfET//0T8f73//+C47b/LeLeZ4n/57xHe94RywtLT3239GjR7d4C8YYY4wxxpjvdbaksdm5c2dcffXVF8Suuuqq+L3f+72IiNixY0dEnP9ys3Pn4yLH06dPy1ecb1Kv16NeV7FrXsouyEyMAl8SWidE1ZzZXI+tVTQjNgncqvROmMiqijp4EB/2QAiHImTK5p5QfOag4uNM6VrO+vq6HgX3SJmvz0MC9eKCcC27WCwiop8QyBU5HzNVk9I1WW8QqFOW9oKZmKnNuymxKZXdKSZIpLIpo3qA0C+CszuzwJey04NgHuZnQX+D5DXLJRLmg4CaxkVCyF40sznPO4WyV6egYVmr6RrWg77p9GB+QxmJBN2JLO96HNcRhOy51ufkyUex7Ev27deyYUzT2tTptCQ2NKRt9uBDD2DZGQh3azW9yVarKTESB5N5AIlxIyI6YIYwPj4psfmzS1CO3iOtk6kpxnO5mAkJmQdksK+ScUZERPYU9g2qzxaSmHN90AAAJgrFwMylWtUxkCwH5jLuY7TGJwwSCFo/SaBOdaTRmxKOE7RWZmSQsIX7KVofukcaQ3Qcngt17IPZzfk6UX/DjIRtuai5R+q+e31dm/Jc188KjF9uMy2D7i+C94jNz1HlbvFJu6UvNi9/+cvj3nvvvSB23333xSWXXBIREQcOHIgdO3bExz/+8cf+f7vdjk996lPxspe9bCtFGWOMMcYYY0xhtvTF5h//438cL3vZy+KWW26JH/mRH4nbbrstPvCBD8QHPvCBiDj/5v62t70tbrnlljh06FAcOnQobrnllhgeHo6f+ImfeFpuwBhjjDHGGGO29GLzohe9KH7/938/3vGOd8S73vWuOHDgQLz3ve+NN7/5zY8d8/a3vz02NjbirW99aywsLMQNN9wQH/vYx7aUw8YYY4wxxhhjtsKWXmwiIl7/+tfH61//+uT/z7Isbr755rj55pufSr2MMcYYY4wxpjBb0tgYY4wxxhhjzMXIlr/YfLfIox/9J7jPoJNDptXPUk4X4HxUKen55EhUqWis11R3mm5PYxERVXB36uXqMJODgw/eTXcLTiCZOpugIQfZQNHlwK2E3DgiIvoFHXOob6kfsl7x9/Ci5dC7fRn6C9sxadFVzC0lDx0DdBw566Scqvpk94PHapBcj3rQ5rUhdTE8f0lyNIIxgOeCc0yb3IwSripFHWqgz8rlYmM6TzR6lSxdcujvgg415HqYckrLCs6xoPWP1htYWzJon/PHaozaktarSlljG80ViZ0+zfnPMnD/o5FVAufBkeERid173z0S6/XUPS0iYnxqWGIzzQmJnTpxRmIbG2sSawzp9VLuQdXqEET1Hs+dW5RYuaSuaBEbEmH3s4gKuO212uqkVKU5Si6gHdyIsGx2QC22D2Ywb9kpjdu8DGONa17MuY3WpVqN+pXpgMtlTs5btB7DuEp6e8LcIcq4txUD96sU+AxYrI6JFSxRDPUjreeFio5uR+dTao71YPjT2t/t0vgttr+gy1qwc2EJxksPHiCL7m2pdY3YvJfQ/aXwFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMDjFxtjjDHGGGPMwHPRmgf08gtF7qWC72ApPWHeUyFdB0RU7bYKRlHIDiLHOpgMRESQtrhaUbFgD0R4KDJDQRcJtyMiSwncL6QMRgwoZC+BkBJF48UFZSSwJJFZnhTrF4PFoSCmRMGnxpKCbhBqE/0+GEgUbLOUYUMPxnSJxK+gUqRyOjCu2IQh1Y96flbWsYZifTC0QGOHBKn+kXJgLhcVHEdE0OinsVoKMCGh9gWjipRwMocxVKlVtWxaW0ioSmMlT61rBYWgUPelpSWJra81JXbllVfiJfsJAexmyDyABOpf+crtEtu9Zw6veer0UYmNT+h6fvJR7ZsjR45IbHpqG5bzlID5vbGhRgFDI9o+KRE9UXSto62EziWxckRq7hXdS2h+F7/v1Hqn51NbkLmHXm9hYQGvSffdaDQkRs2Gzy1oLpMwXCpoyILPCQCVkjbBKVYfap8urKlocLClfazYGCj6fEPGLRERFTKdob6FvaRL47wP+0tiytIzQR/Op1nS66mhRbms+1BqjtH5nc6F9Wm32HCB8BcbY4wxxhhjzMDjFxtjjDHGGGPMwOMXG2OMMcYYY8zA4xcbY4wxxhhjzMBz8ZoHdPvRKz8uHiqBMJ+EY5UKv6v1ScQHGVRZOKbXQ71cQpTVAQOAZktNCkgwR84D5ZKKslLC5hykzSjCKyjGxUzpCdOEogL1SkUzWpNwnJwhUrpFEq0XzYRb1DQhSxReAiMGMnHoQUZi6oceZe1OmEXQ+O2BWLUCCkIc5wUF+OfPp7EBAkISSELZlE07JT4kc5G8oHEG5rCne0ldAO6HxMl9SitNQH+lxlpAe3TBAKWo+LVUG4YiEiJbiPX6WvbY2IzEDj/4qMS+//t/UGJ79uzHstlIhNwQtN1u+9IXJLa8ck5il4/uxrLbx9ckNjys7TY+obHjj6rxwLXPv05iWxHwk/kFrftDYCrR66thQ2qsdGG/pCFdg6FagX2s16VyeI6wZ0ix32Upu32tpntOL5WJHjfmovUptgeWSiqejojIYZaVwHwFtqeo1+sS63RhX0w8O5ARU5WMX2AQkGcH9naiaStgxIDHwtobcD/0/JcyPShq1lMt+ByWkQkOOUpFRKkE7Qv3jWY7uY4h2r+pjIiIvAIGWx16jtJzKzAu8i2YMNAQ3BzbineUv9gYY4wxxhhjBp6L9ouNMcYYYwryhS9E3HdfxOWXP9M1McaYZwx/sTHGGGMGmX/xLyN7ycsi+7s/GdlLXhYv/PCHn+kaGWPMM4JfbIwxxphB5QtfiOw9v3RB6Ll/9Mcx++CDz1CFjDHmmWOg/xQtA3VbTiq6iMhQHE8iM8isCyLkLmRkjTarm/ogEm+321pOVbuDhPVZlURZiazJINlj4RmdDcdBMdkWBOaUjRZFfKA+LIEYLSUA7Ha1fdk8AASxJOBva3+ntGwkckMzAxD4lkElSxr4rQgfKctxBYSPGYjWKZtxyiyiT0YMmOEbziUjEBhXqYzNNDZyEJZ2ejouuCeLZ6pmkwM9jgThRUXilEU8ImX6UeyamEUczFeyhKg6oGxqIhTeVnX83XDDSyRGIu+IiPW1dYnhOIdmu//++yU2MzMtsYUFNRSIiGh3tOxG6P3U68ViJKveSlb0hS98IbT2EcNHj0Rrz67H/n355QflmDvv/6KeNzKEZXebKk6mfqzRWMVFTNe/hCdKpFdbuSicCgYbYOjTS+yhGQjZU+vvZnhfBjODqhpNnAfE6FQfWGc78DxCe3XKV4TWHNqfqM2hitjm5WQmesh4X9DYidaBfl48az2VTQYSsF3i3k/PPNQWERH9TOM0x6gfazW973JF691o8PwmFhYWJLaxoYYjBD3jpkyuiuyNRQ1wIvzFxhhjjBlYugf1hSUiYnH79u9yTYwx5pnHLzbGGGPMgNJ+4Qti+a0/e0HsK6/9oThz8MAzVCNjjHnmGOg/RTPGGGO+11l+5zui+brXRuWhh6J78GB88eSJZ7pKxhjzjOAXG2OMMWbAab/wBdF+4QvO/+OP/WJjjPne5KJ9scmjf0FG5Q5kPSZDgG6fM/hWK5qFlxRuJciyXVS01uqQMDkCdOMRkKmVRGYkHEM9IqUJTh6s0D2WIDsuiSb7ZKQQ6Qy3m+m0tN1IANiFv5xM2hZA3+ahwryicrQc2refVLoWFJ5D1/RAkViFtkiJqksgmKeyae7kMPZp7KZFfMUzDcuZMP5IaNjpJDJ0FxUWwg1htmgQOyeF/iCCxqLRSIGcFPRe2h0WbGbQZ+VyQZONhNHKZsjk4nzZGqM1rA9r99TUjMQuP3SlxLqQ+ToiogNrLYliT5x4VGJHH3lYYsOjOp9WV5awbLrvXg8MYjobEjt9hl44ignEI3i92rZtm8T++l//qxL7f379vRIjgTiJpyPYkqJKJi801mB+klHFyZPaXxFs1jMzo2OoAnsWGsT0ihvw4BwtuM4SKKBGUT7PJ8ryXpR2W9dP9B+JiAzqWYb+xr2WTGdgXKTGGt13s6lrID4fQb2LPotE8LJI+/9GuyWxel2fM8nQqpd48qD5VB8C84GO3uPq6rLE+rCPLS/zukamQGQA0O1qnzVb2hbU5qk5FjC/N8+nTqu4AYQ1NsYYY4wxxpiBxy82xhhjjDHGmIHHLzbGGGOMMcaYgccvNsYYY4wxxpiB56I1D+jn/U0iO1C4gcg2paHvdCGbPAjXUGMLwTLESMgbEVEtqzCVRG8kmCuDUJAMElLCZjY+gHoWT+oq9EFMFhHRAwFhnwoqIByLiChB+6QomhkaBY1Qdk5jLVUGNAeaLhTN2AzjtL/BRhWUyZmMGGju9CFG4uLUWCOBb59Euonsw3JcoaP+17E0XmiOwrlFjQtS901l5xmsLRkZhoBBB4j/yxU2i6B1kfSZ1JYZtAaN/dQ4z0DoSmvd8tKqxK68/AVwRbhelbcoLLui68Ntn/+CxFZXVvTc2pjE+ols9zQ22GChmMD3qVKrapufOXNGYhsbamYwBIYL7Y62T0TEEPRFp6uCbsrSjh4Z0LwTExNYdrurex6tayRuR7E+mQxAvSMiSgECdejuMow/MoHokMFRj+dYvdaQWBuE4wStLaMNbd9+n0XZGdxkuw39DQsOPbfQOK0NJfaSrtZpeHgYj93M0JCO6Q0Qt+O4iOImByMj2t8smNc1tUImDBHR7em+vrioda9V9R6H6to+rZ7O+dR99+AZOaNnFDAkIDMjKiZpilLAfyfhUYX4i40xxhhjjDFm4PGLjTHGGGOMMWbg8YuNMcYYY4wxZuDxi40xxhhjjDFm4PGLjTHGGGOMMWbguWhd0fI8v8BNolwmByl11CDXjvOQ4w64kIFbRbkCzk5QQso1iayuyKWtD24p5KiRcjFh9H7Q5YiumetxXbC6SLls5NBKGTkNQfvQffegfVJOaWRwA6Zf0QP3FSILcPNI3Dc5f5BrUpX6gYqBYLvNrmhUI2rLhIEfXA/OTY1zdFor5i6GrnFwM+joFxEluGaillA2RqE+3N/kBkeOTRlYNtI4pXtJdRdWCVxrqH1pLpMzIzn9RLCLVK2uLk4rq6ckNrd9p8SqJXXWWV1bw7Knp6YldvjhwxL7xCc/LbGdu+YkRi5r7RavDZVqMdekZ5LR0VGJZTTYejoGJsfG8Zrr6wt6zaLrCDnJwVweHeWyi16zqMNhP4c1OuFUVYq6xMj1q9nW/Wl9TZ9HFhcXJbaywuN8bVXP73TIrUz3A5rzlZq6aa2usVNfFcb51LS6qpFz29r6ksTOnn5UYsMj2rYREeMTGt+5R9eMffv2SSyH/Zf2ZOqHCN4PaFxV4LjVVXWApPVzZGQEyyaob2ltImeyHN1YedLSXO53ySWzmPMbziZwyj1fuPbP5ufzPAc3wQT+YmOMMcYYY4wZePxiY4wxxhhjjBl4/GJjjDHGGGOMGXj8YmOMMcYYY4wZeC5a84BSVrpA/FYqkSBMBUvVKovRSPyVEiLLuaToBoFu6nokoidBYwdEYijKgtfRUgXqGBHlTLuYRIV0zR6IovkeE+YBoFVl3TmJQOF+oOgcxK8REb2utiUaChTU/FagHXMwV4iIKJVorIG4nUT0aPYARgpl7u8M+gf7DOregzHZ62g7poTS9aq2EQkai/6eUgHTjoR+H+dThdSdBU0BqHnJbOT8JQsKlvFUPbe3BcMGmst0PnoMFFwTaexGRGTQlnRsu6Wl799/qZ4L9R4ZHsOyN5rrEvvoRz8qMeqHsTG9Zqer4u2VlRUsuzZccI7BuBgeHsZrfqeZnZ2VGIv11WRgZfUsXrPRUOF5t9vScmChzXD/1jKSezKsV7R20/n1ugq1S2Vdl1aW2Xjo8CPHJTZ/dlFiy2s6Jk+fPi2xJhgCpNbENoi3aR0aqunau7qu9dmzR8X2cztUlB8R0RjWNmrCvLvzrnsltrGuc+fSA3slNj7Jz2sT0zouae6sgbnI8rKaIdA60Grp2I3g50oyDCGziVoNzFeobxMPHjRPyIimXErsRZuLIfOpxBTLQNhPe0S7AyZOZFQB95Iy4CHzlu4mY6det7hBi7/YGGOMMcYYYwYev9gYY4wxxhhjBh6/2BhjjDHGGGMGHr/YGGOMMcYYYwaei9Y8IIsLM6GSSDwHcWZKZEuC535fs7f24PwuZLynUkhAFcGiWDo2B0MCEvD3usWEWufLBgOArt53GYTaRTPwpgSfFcwADHVEcTtkc4fh2oFszxHclpRxl0wKqD59uF5qrBF0bK8D4w/GNF4v0eYoziPhLYxpuu8yCfAT4zwDxX2lBuJQGtM0P0HynifavFot1o8k6M7KBX/fSaRZJ3EotQW5aeCYBLMSEleeLxzWloKmKESZDFkSa0unDf3T13rOzc5J7LKDhyRGZgQJt5H40u23S+zr3/iG1gfa4vrrr5fYRz/2+xLrdFhcPBQqqqZ5R2v82MQ4XnMzSRE9maoAw8Mqvp7drv2wuq7i9hKYgERErK1rVnXMYk5mGnAYT6eUUYW2+TCYGXRA2Dx/Zklijxy5X2IPPHQEy95Y177tdrTyZJRSq2l/D81OYzkE7Rs01to9NXnZv1tNAV7/+h+WGO39ERFHjjwssc997i+1jiXdx15x44skds1zrpRYtcZll2G/JeOCteaG1ofaDMyExkYaWDb1Iz0rlmCstmHNIFMe2ttS16R1qJ3r/dSqOh/IMCn1hEFzudPV+bSxoW0+NKRll2EPTPjQ4POeHFLgmG/iLzbGGGOMMcaYgccvNsYYY4wxxpiBxy82xhhjjDHGmIHHLzbGGGOMMcaYgeeiNQ/o9/sXiORQbA+KxJToksTJvR6I6EHwlDIFKFo2qrUo6zwIhDFzbCL7OtErmE2WBInJ+yl6HAibycygDOJtFNvnYOKQKJuiqFsjcR31DYyBlHnA5oy5ERHttmabHhnRjNg4TqEPUxl8Kes3tQZlM6ZrdkF0mTIPIGkfCQ0pwzEaNqDpRmLJgrWA7jEHIwWc3yBeTYkXce5AN5TKmi2axj71YWqOdTGDNQjZYaAXXddoPEdElMtqDLGxrnN0bm6XxKpwLpHKDn7bbbdJrNFQMfDb//k/l9jtt+u5J08+KrHZOZ2fETwOMjBNIM+F48eP4jU3Q+tARES5Atm8YT7R/kLXpHHVacGcj4geme3A2K+C4UizqdccGVbBMQmgIyJKZY0fP3ZKYvff/6DEHjmqfbu6qkL0SpXF5ENDYxJrNIqtn7Qc08pN2e4j2Lio3YY1Ffback37+yN/+N8kduTYI1j2+KiO/0svu0Rir3yFGhIMDWn7rK2qicP6su6LESzWT7URnC0RWuoqMJfOl03PTHrs2LgadOQF9+rU/G63dZ602/TsUWy9yWEjSpVNjzNkpFCt6j5Ge8lW7pu6dvPzVeqZh/AXG2OMMcYYY8zA4xcbY4wxxhhjzMDjFxtjjDHGGGPMwOMXG2OMMcYYY8zAc9GaB5wXgD0uFiLdEArME0JXUolnICQmUGQLqsBKmZuTBM8VEFi2OiBKhXvMQNhMYruIiBJkMQ8SasM1ueziWc07kA2ZIMEdiVrLKIBOCABBCIeGAqmM7psg0VtR8fX5Y3UMtNraZyTW63ZUYLkV8wDsRziX+1tjjSGtY0REGcTxJIakqlM/ZiTK7/F9d2D8krCe5gP1I43dZJvDWK3XVRxfIwMKWINobUgZVXBbgrAUBLXUN/WaCqj7PR7nteqwxI4fPSex3TuvkFgOv6llEPvEJz6BZc9Ob5PYW3/2ZyV25swZif3hH31EYnv2qcFBVmZhc6+nwvPhERW3b6xT++ocQbEzmNgkISExHHbJHhV+f+OukxIbHVVRdETE2rrWswJ1r5R1XExOTEis29Vz7777ISz7vgdU4H7suJoHREnbt9FQEfzQsNYxZdDR6VH8cVH/tevNONDuxOFaJe4Y3jTvC+6r/Txh0FHS+CUH5iS2Y+c0lKPXq5S0Pq/6gRdi2WQeMDKi60Mb9qf5hUW9IBi8kFlTRERjmOaJ0mxq2VSfGjybpYxJYFjinOjA/tIF440+HJfevzU+CiZD5Yq2T6Wi+zI9D2y0+b5pXJJRAJn60F7ShXLaTS47wJxh8zPBVp49L+IXG2OMMcaYi5O3n5qPn51ffuzf758Zj/dsn3oGa2SM8Z+iGWOMMcZsgWvXmxe81ERE/Oz8cly7nvhV2hjzXcEvNsYYY4wxW+BAW3PMPFncGPPdwS82xhhjjDFb4DDqpdJxY8x3h4HR2GBmchCjpcTkpZIKo8hngAR3WDbESCx1vmwVj1UgU3UfCicBdIXEogkhO9w2Qlo2um/SV6Yysvd6+ssVZYPvwUVJJ9YBk4Fy4v5IZlYGAT/VnbL10gVTYlMSqJdB1Nru6Hgh04MeilcTv0lAmLLOc3ZlPY5E8CkhO7UHj0sygdDjyIwgRQ/Et0XHb7dgRvbUfZPhA53fhQWnTBXq6ThNiWx7MH6pb6kXSLzdyfR6vS6P8zIIo2tVME0AkwEaA0tLmpl89+7dWPa1114rscaQCpt/+7f/i8RmZiYlVqnqvbQ6bH5SJrOJjq51rbaKmMfGNIs9mQf0E8Y21ObUu3245ite8QqJfeWrn5FYr8fzLoN1rVJR0wRqtocfOiaxr37j6xI7deosll2rq4B6YkpF9P1c69gFU54O7E2pOUai5SzL46tjtfi12fH4mbOP/znar86MxB0jpfimuUC7o0YTZA5y+RWXYtlXXnVQYhNj2uaVqs6nMm2OMH42Ntaw7H6udV9aXpVYD4yLen1t306r2PNARAR99CLROu1tQ0O6BuWw1qWemaowzjtgNNBqb0is19O2yOA5NfnsAHUaGtL+zmEvaYMZEZkeZVvZV+F86ge6HzIemJycxHLW13Wt3LymUl1SDMyLjTHGGGPMxcJ7dk7Fn04Mx4FWJw5Xs7hjmB0jjTHfPfxiY4wxxhjzbfDV4Xp8dbgeOXylMMZ897HGxhhjjDHGGDPw+MXGGGOMMcYYM/D4xcYYY4wxxhgz8Fy0Gpt+ZNF/guNFBvZeGblXJcjASQyPY7MKgVwgeujFxW5IUYamh9PJiSkPdYcoJe+vmKMWOcKAIRu6tKWc6Kp1uCY4qJBBCPVDqaz1ztElKOVwg9ZmGsNL0rk8/nrQRuQkQmOgR2Vn0I7gAhbB44Uckqhv6+TuBU4kzRaXXalpPYu6oqFLG3QhHXf+ilQ2rBnUPtCNlayY01lERKOu7i/Nprq89GC8lOgmwemsk8qNAffYBoeuUqZjrQROfZ02rGspVzQYV48+elJib/uHN0ksgzVoeFQdw7bv3IFlz81uk9gHf/M/SGx17ZzEpmfUYevEmcMSq9UT7n/gNkWuhzTWyBELne0Sa0sZxjkvWNpnv/07/1liHViP67VxLJvG/9qqOkP9z0/+pcROn9J+GBnTciYn1eksIqIPjyo5OIvSc0K1pudmbXBCDHbBy6Etuz1wtMw1tmPXlMSuuPIyiV2VcEUL2OuXV7QtOz3a02H9g3shl78UdD6YlaIDaakB/ZV43irD3tjt6rpGc4z2/qyi85PcZSMicmhLco4rlbTyFSinqEPmefR+VlbUiY6e4dptHb9NiJHTaUTE0JC6V9JzSwWe9xoNdaTswcBI3XepotfMN/X3Fh73/cXGGGOMMcYYM/j4xcYYY4wxxhgz8PjFxhhjjDHGGDPw+MXGGGOMMcYYM/BctOYBRSAhcUpc3ATREom6xsdUQEXCpm6uoqyUEC4HQWMH6lMCIVyJlHlQUErYTHFqo5wU1ECPRP0J84BGQ0VqJHDjPtMYNGNkiUan+87AaICaDc0MoOx0m0OwoPCNrokC/MRY64HYvw9jKIP+pnOpH1JzLEDYjMdC2WiSQeM00ebUP+WaCjmJMhouwBxDZwcWaHZpbYJrlqDJK2QssgVIPFspa1u0WirGrZTUNIFEpRERD96vgvs3/PUfkdjePZfg+ZvpgOnBNjAJiIj4xCf/XGJ/8ZlPSGzfJdsltrKq4usyGJN0Oiwmr5ZVKEvrWqsFAnMyyYDxV4X+imBjktPzZyX27977SxI7cuQhiT33uVfo9c7q9SIilhcWJXbbbV+U2Mqa9uPUtPYjLQ3dxNJCy2efHFAoBEYreWgde33tr4iIrKSV2janxgeXX/Fcie3cMSuxRl3n99q6jsmIiF4XnjNoL4J1OgNDixL8lp1az9Pr/KZyaDOCLYsMBZLXxOeMYgYJZDzQ79PzAD+3BDwnVKvalr0cTFrgkpUyiPVhvflfhUuEnh9p/65U9L7JKCDVqzmYmNAzaaulhiFFn+vIUCCC6765v9mIiPEXG2OMMcYYY8zAM9BfbIwxxhji4On5uHrhXMzPTMXxvWwbbYwx5tmFX2yMMcY8q/hbt90Rf+Xr9zz278+84rr4s9e84hmskTHGmO8G/lM0Y4wxzxoOnp6/4KUmIuIVn7k9dh/V5KHGGGOeXVy0X2zyXh75E5TqJLwlsXMvpaoGVRdpkXogyiINHWXybvc4O3giv61EUAdfUHSeFgAWOzYDYR8JtUloHSDMi4horpN4FjJdN4YkRn1DYlESrZ0/H7IcQ2NijMoGyV0v0eYZiOgx8zsaHIB4sFfMaOL8+SAOhe4hsTQaLsC8Seiak+YZUh+qOhlDwIEJ74DogYg0ByVyBtmiCWrztHEB9DdkoA4Y+2jkAatyn1TREdFpabxSUwOAJgjhh4bBFKCr7dhFU4mIQ5ddKbE3venHJJZD57baujaUYE387//9w1j27/zOb0ts/4HdERFx4FF+gRl59FisTtcjDyi7DMYOiTm2vr4usampGYkNNbTN77nvXriilr3R0kznERFrq009dkPFvC+6/iUSo7F2z10PSuzUiUex7K997WsSGx0dk1hjeFRiyUTrm0msqT3YW7MKjFXITh+ZFl6B/t6+awrLPnjpXont2q1mCLUaiNZB/N/pan+RcDsiot3S/kYRfVXLJhORnIxSEoLuHJ+l6Fh6lin2mznV8X9VSq8JG0wFDGL63WLmSusbOo9TZVObU/u0oexWR/s7Be3/FTKVgv27C5OsAuMiSxoXKEXNf9pFJzgZfkREt/2tn4W67aKLiL/YGGOMeRZxZmoC42en1cnKGGPMswu/2BhjjHnWcGTnXPz5i553QezPb3hOHNnF1tHGGGOePVy0f4pmjDHGfDv80StviK8fOhBzS2fj7PS4X2qMMeZ7BL/YGGOMedZxZOdcHNvLf5ZmjDHm2clF+2LT6/Wi13tcPNQH8RcKbyk9fUSUIE5a4LWmCr1qIMZ9qn/ERwJzEhBWKiREp3thYTPdd6OhmbO7vWKCu35X65gqu1vw2NUVFfGRyJvEgwl9L4vwoZqVgtlst9Tm5D4AUB3JXIGyGWcJkW0Z7qda0/4uej/oZ5FoMzYPoEzrxbJfU/ukvDSoaBJT0rhC04SiTggRUQExJtW9BAYddByZoiTLhrWJ2nJoCDJQw1irltXIo9nm+lDW71JofVpgXFCvaTknT5yQ2B/8wR9g2bt2a16akVFa17QtypDlOjK9R8o2HhFRAfeMHqx1MzP6leiRw8ckduedd0rsmue8EMveWNe23L/voMQWFpcl9iu/8isSOzevRguLi+ew7Jlt2ubNporbezkZb0D7wsLW6auxQ0REDvEc+nZ0TA0xpqfnJLZ3j8a2b1cDiIiIxrCOlx7Up7Whhg80l0mInhpr9OyB+06uc7EDYmvcFxO56EnYj89caDJEV9T77nTYcImW33Zbx1p3bVUPBHF7BrdIpi/nzwfzAHoOg20wg+ctarPUGk9jowJGNH0wxim6b/TA0CKCxwaNy6EhMHtCEycwOEg8t7AJ1OZ+4HMJa2yMMcYYY4wxA49fbIwxxhhjjDEDj19sjDHGGGOMMQOPX2yMMcYYY4wxA89Fax6Q53lSnP1NSGiFWc0jkZUVRKTlChkFkNCahEwpcZMKz0i0TveDImQSpyfum9qwA2Jgyl5NbUYZjqmOESw8yyGDeo/KIfUhiegTQv0eCEupnq2C4s4ylJOD4UJERL+kZbMIVMumLNA9KgcEjhERnY62ZWtDRZc1EG9zfyldEINHsOATDRJgDJCQswzzLpWpmhJdV8h1AX7LyUlZCnM5tR4VNbUgbWcJzAwIMh6IiGiB+JbGWqen44LGwNqqrgPlkmaSj4jISjRe9MbrVS2H+oEy2//AD3w/ln3i9BGNnVBhft7XsdrtqfCbhLenT6uwPiKCpmOrpddcW1Fh88I5FfW/7rV/Q2LXPOd6LHt6alpiTRADt1p6P3UwEdlo6bnTsyyiP7d4Vsvp6n2PZirgpzlP83tsQs+NiNize7fE9h/YK7HhYR1rFOu2wYwgWMjebOn6WYI5T8YtaNICYx8zvEci4z2ts7A50jWpH8hgKCKiB3sJPQuRSVAX1s9SlyqeWP9gf8M9gtoX1tQqPOvVKmqoEhFRBnMQ8hno9XS8dMCQpaiBTgQ/k2awC5PhA12yC/Uh8f/5cggtZ3hY5+gCmkCAGUFi/67ANTc7PlQq1LaMv9gYY4wxxhhjBh6/2BhjjDHGGGMGHr/YGGOMMcYYYwYev9gYY4wxxhhjBp6L1jwgy7ILhMecpR0EVAlRdWFhPor1igmJU1nR6e2RykkkdC9U9rcyWnginJlcBWWYpRhuMlU2lVOtqmCvAWWjmBJEganMxdQbRQ0FcFzhfXPJJBglM4Q+qP2yXrH6pAqnchqNES0H1JCYVRqE9UnxISXEprEK16T69EBsmhpr9brWqXAm5oKmB1FOTFBYc4rOR24fEBcnrkeGD0XnDpmDwBSJ8fEJLPsbX71fYv/23/2yxL7/+9UAYGZmSmKr6ysSe/jIYSz7zz/1UYnNnzslseWFBT0Zxh+14wZkko+IGBpRM4VmUwXm43AcCfh/5Ve0zdotFsqePrsosd/93Q9LbHlpXWLVum73ExNjEju3fAbLHhrR80fralRBfTs8osft27NHYtvm9NyIiEqVdldtIxr73Y6OK9qscxR+sykAXYDWDHzGAAE1PQ9ERHRhf+v3wXQGYviMAsWk1ioyaim6rtHaQoYLtbrOu/PlkIlOMaMfavMKmK/UG7yP9cEMgfosq0Df9sngQO87tTdVwPhlbU3XoX43ZVR1IUXHZAQ/t6yv67rW72l9cO+HYiplMOeKQGOofn7hGCqViz/j+ouNMcYYY4wxZuDxi40xxhhjjDFm4PGLjTHGGGOMMWbg2dKLTbfbjX/9r/91HDhwIBqNRhw8eDDe9a53XfB3rXmex8033xy7du2KRqMRN910U9x5553f8YobY4wxxhhjzDfZ0ovNu9/97vi1X/u1eN/73hd33313vOc974lf+qVfin//7//9Y8e85z3viV/+5V+O973vffHFL34xduzYEa9+9atjZQUEfMYYY4wxxhjzHSDLt2Cn9frXvz62b98e//E//sfHYn/zb/7NGB4ejv/yX/5L5Hkeu3btire97W3xL/7Fv4iIiFarFdu3b493v/vd8Za3vOVblrG8vBwTExPxE3/v5VGrPe5kge5V4JqUcpsA06Xog2tDUZeNfoCzU6IlK3DNSgmcUcBlg8io7ITTRQ43Sc4x7NIGDnFQDLownf8/ifiFkCMRQUM15SZDTjZF3eT64I6TgSNWqt40BsklhsCyYfCmpi2Ng3pdneho/BHU38ljyVKr4DVbrZYeB2O3UmEjR3KoQUp0zWJjJdXfOTaSXrPT//+3d+5BdlT3nf91933MQzMjhIzE8JQSEfEK5p0NskkcmwrP9abKLsAGasmuHzyM7CwI28FmUwUCbGMSiCF2ubKksBdqd7HLSVUSyw6WzVIBShIGjCtiY1k8xYAR8557b98++4dgzMzvc6AVguZe8f1UqSjO7e5z+vx+55zuuff7Pd7hKObiOJ+YIRvFu9lswoGQQ4m/n0rqXbJeeWka656Z8o0afXnClT311FOujNKv1fI58MLIc1h3FQyN6uD6td+yJa5s0aI+f26fv2DM/W++W4+Z2eDAYle2ZLEvG1rkXb9+9atRV/aj+/4v1n3wIb/hygYW+XucmfQ5EGCBevTxh13ZqsMPxbpXrT7YldVqPpADg975LTGYe3M/HhozPn/MzAo4P4ktuPNo5dAX6GrKg4zWGJrje3v8PPtWHUzZ3dPXTVNveWdGfmaiNZRdREu6lcEkFmsjlZPjJ0HPheTwVqnwfD494ee7Nj17wBxWqZDjnT8udt8UW36+8hfNMH9p3ERc0eCG+Hm6ZF7AqTEX22qPj8XExNy5oNnI7W9uf8BGR0dtcHAQrxNv4RuwZs0a++EPf2hbt241M7Of/vSndv/999sZZ5xhZmbbtm2zHTt22GmnnTZ7Tr1et1NPPdUeeOCB3alKCCGEEEIIIUqzW/vYrFu3zkZHR2316tWWZZm122277rrr7LzzzjMzsx07dpiZ2bJly+act2zZMtu+fTtes9FozPlr7djY2G7dgBBCCCGEEELs1jc299xzj91111327W9/2zZv3mx33nmnffnLX7Y777xzznHzv5YKIUS//lq/fr0NDQ3N/jvooIN28xaEEEIIIYQQ73R268XmyiuvtKuvvtrOPfdcO/roo+2CCy6wT3/607Z+/XozM1u+fLmZ/fqbm9cYGRlx3+K8xmc/+1kbHR2d/ff000//W+5DCCGEEEII8Q5mt36KNjU15URhWZbNiptWrFhhy5cvtw0bNtixxx5rZruErBs3brQbb7wRr1mv11HcnOf5HNEVivBAnBQTFxdwcAKqLhJ5k3grq3qhVU/N38erF4Dzy4n1UTgGurO4mByMCyJ95M/137KF3RCjkfCRoHtEcSeK6LkOahPeD9bjhWxtEAzHDAHK3g9B9xOCv14shlQPCgBB2YdxBEFhVPjY8v1B4SkrzE8Tf49JGqnbWAA7nwL6Ms9hfIKhQNwAAowC2tAX0EY07YBbbOV8fzQ9ZBnkRurLUsjzHGL4q1dexLoXL/J/rOpZ5O9naF8/Lz733DOubHCg35Wtee/xWHdvf83Xs8QLSvddOuTKSCzdhryITWs4nmhM5P4CkzPeKKCv38fmxJPejXWPjU+5siX7+n579JlfurKZhhdF1/p8G486ZlWk7l/5a7agL1LfnjSB+QbGSDViBtMmgToIwim2FTAMaeM8W17QTWYILLQuuxaU/xtzgedDspLjCBq8cN2tJvU5nF3SZCCFNlYihiw5GK3gcTAnt6EsSfgZk0hhbe3v9YYYedubUrQhp2k+bsBY3FXuDVTqdTDjgOc6gnOXSWBMpBU/z1ZTHzMyBSgghjnMs2ZmGa1j1cqbHhNjt15szj77bLvuuuvs4IMPtiOPPNK2bNliN998s1188cVmtiuZ165da9dff72tWrXKVq1aZddff7319fXZ+eefvztVCSGEEEIIIURpduvF5tZbb7VrrrnGLrnkEhsZGbHh4WH7+Mc/bl/4whdmj7nqqqtsenraLrnkEtu5c6edfPLJ9v3vf98GBryNqBBCCCGEEEL8e7BbLzYDAwN2yy232C233BI9JkkSu/baa+3aa699i00TQgghhBBCiHLslnmAEEIIIYQQQnQiu/WNzZ4kL3JLXicWqoOIicSDccB8AFRUvWAAABugWwGiwJy2WjWzvl5/zayksJ61lP6+eXdas7SkeUBZwXsKuz0Xkd3TMxAno7i9KGfYkID4OkTEpkUBYmkSOaIO09edghAz1ucG/RE3WJhLgOOykkYIZoYJQ8K+JogcKS9S2Ek5RqXmxyhBO4YnUE9CAy8ybig+vLs47K4MIkc2YYiYRdAYy3xfJDBf4U7TZNAROM/JeIXjAGYILdpNm/psBut++eWnXNnIyIgrm2lMurI17znGla1ceagrI8MFM7PCIBaJL2u1vUiXBNAGcWhFxMXtlh/3uBt80/cl7XaeWY8rGxryAnwzs9Hxl13Z40887MqmZrzJQCv3wuRjj1vtytoFx7sChhr1um87zU1kDpKQqL/pBdlmPL5J/F2D3GfzlHKmPGZmCaz1uNN6bD2Yf9xulJY1waF5MYBRABnwxJaSap360s+VGXRcCg9X9MxTgFnJrkbRcwbcD8yLWeZzDc/FicAsgVybmPJzWJLCcwKsv80cjBAidZNRAMa2rBkRrKG4rHI1mBsF3E8Oa2MBF4w9t1AfFfPWrPn//0boGxshhBBCCCFE16MXGyGEEEIIIUTXoxcbIYQQQgghRNejFxshhBBCCCFE19Ox5gFJkswRGpF2FrSHlkUMBVIQbeYgKKM3Pbwi7ORd6/FCXjOzBgg5Z0D4SMIzEuaxOA6rxt1fSXgW67dy9UR2i8bdvEFQRjsSg5CdiInoaAdqPBbElESRg6g68meBDIwuSOhKIj7KAYN7weN2feCKqhBbOh93Kd4N4wLKIaonpOXMFchkICbQrcB4pJ23GzN+PLQptrRrd0RdnCZgbALtIZOMQLs9w7ScVTlPaQ5rTYPAF9pD5gwk8j7i8N/CuqnfDj/iN309EO++Pi86b7W8aL2IGLIUYCrQhPP5XA+ZMMCwMzOe7Wg3+CrGjOY/6J9+3z9mZisOPciV7T+8nyuj+XOffYZc2XTTi6JbIBA3M6vXB10ZjcfR0VdcWd7yxgXL9l3syshcwcxsBs6nmMV2k59PIDF5ZF5LMWZgfEDPE7F5uiRsokNGKZTVZETjj6Ixb8b9EYLP6QD5QmtJWcH7rmvC4KM1goyLoB7a8T5EjGhoTiYoNhTuvr5FrqwZMcmYHPfjkdZVNK+ALgvwkEK5u+tYX1aQuQ3cN67LsL6An8Wr9ZB5wFzKjm0zfWMjhBBCCCGE2AvQi40QQgghhBCi69GLjRBCCCGEEKLr0YuNEEIIIYQQouvpWPMAS5M57gAkTiIRckxc3JzxIigUzJMiHFwKChCjTU2yIKwAVVbZjY9JxJzQuRFhHom/ygoa6Zq0K3rseng+7W4Lu/CSGBfF5JH7RiEodRxQVuSYRYwH2iTOgziiCJnyHO6lHWljFXdiLifqJ0MMFpBy3SRwp/MziCNlPw3FFgjWzbjfKmDiYCA6T1IQ/4NINpY9IXjxbbNB4mTfvzSe0OOCLUysWvVtR3MQPN/HpgnmCtUqm6LQLtloAgGxmQGDA7rv2G7TKdxPDeKY577uNuyS3YC8qlRYwF+p+Hvsq8OO4yAQnm54EXyWgQEEiOXNWJw8NNjnykjI3i78NfOmL2tB/5iZjc2MU4tcyW+tOtIfBtd8avsvXNnQIOdaP8xhjYY3i0jBNIHikMKajkJ0M0thfaLzqYzGd5KTKwULuhMSf9PaBoOH5oEUZrFGcxrrJrE2rSUZtB3XMXhmymAsmZW/R+o3eibIIE9bLTbJqGSwbsCzQ97y/UO+G1nmxxiZK5jxGkrGLzT/zbR8ngdod+x5rd2imJV7DjMYOzFzBgSeW5J5dZPZTQx9YyOEEEIIIYToevRiI4QQQgghhOh69GIjhBBCCCGE6Hr0YiOEEEIIIYToejrWPKAoghWvEyQVBYhN4bUsi2wHjyI+3O2c2zKfJogho6J8EJnTsdSevE0Cfi88q0TqrvWUDDHowZo57AYLCuqePi9eNTMLwfcRCRJpl2zeuRiMECosqiaxX4Eix8j24vMvB8nGuz2z6J1im1IZnEs7IScR4wKCBIC0EzNB9SQRE4Y25CqJ9QOID3PINdodPAp0eqPhr5kXYOKQefE/ia9pHJtxbFvNcnnV0+MF6gmIkGdmpvD8RqOcoLKW+TG2eHAfV9bX5/snJnSt1coZF9B203013+c0D1TgODOzxrQXjs/MeJFub48Xo09O+t29A82pZC5jZovgmjQ3Vas+tkuXesMFElpPT7Ogu6fH9/nMjO+L6YY/v9Hwcy/l38wrr2DdRx5+hCtrtcCUB4TNz4/scGW9vb4v2iisNyuCH8uUL5Mz/r5pre3v8WsW9aOZWQbGL9Wqz8sWrCUhMnbKQqObyjIyeSl5xWpkXqv2+v5FQwIa8nTb0I9FxJKF1kYyLijA9IPaSEL2Cpi5mPF4TOAmKQdo3I69MurrjsxrRBsMUDIyoqE1iwwtIusqeQJgX5K5DZoMQXMiz6n0XIgGEiXRNzZCCCGEEEKIrkcvNkIIIYQQQoiuRy82QgghhBBCiK5HLzZCCCGEEEKIrkcvNkIIIYQQQoiup2Nd0ZKQznGiIqegFJyQYkYKFXJtAOeONhiEJFBPteKdcdpRpylyjPCWEeQ+hC5ZcC/tiAdKAEc3djYB1y9yS8l8G9vgWLerbggGWdmBaxI5rVA/FuBOt+tQ3/YAfZSm5ZxfKLHSiGsSOX+Qgw9RQA5QXiQQVzOzPIUchD4qWv44cnl5qxSFr3u66d2HWk3vaEV9Ti5OZmYVmB8oX2g87fzVy64sB+eYSuYdqczMQvB5QA5LKbl+Vcv9bYkcgczY4aZe93NTC1yl6nXvSkV5Sg5kZmYj4HQ1NDDgynr7+11Zu0nOeD5XajEHH8j/ReCylYPL1tCAb08L8rRo8dxCTkzoqFXxsW02m66M4jU+7p2UzMySxPdvve7zcmTkBVc2NDTkyvKWH3c9dZ4HZiYmXNkYlYELFMW2rx/c5SLTZBUctfK278sK9Dk5O7ZavGYRNMbofsgGitYxWl5oWYyBDlTQRlrHyOEt6uYK0BoKRpM2OOjzlMZILA7kbksujK3U5297xvcFue2R26iZoU9bAsemcCS5T/b0+nZPN2C9e/Wq86Fca4OFWaVazl2WXH5jUN1VuEe6Jjkm4rOVsfvvW3ke0Tc2QgghhBBCiK5HLzZCCCGEEEKIrkcvNkIIIYQQQoiuRy82QgghhBBCiK6nY80DiqKYI9ojgWWFtF8RcRKJlkiEZyCCaoNIrA3Xi2jR0HyAKApwMwABdG9fnysjUaoZi8dIqRhAjEaiwgLaQ8IvM7NAondQUwZqDzg7lI6hmQV0H4B6wOAAvQNAux0zBKA2tbGPwDQBzsXrYc1mGdZdTlhKuUICy1ifE3y+78wUBIkkmCchr5lZG/KlBfdNub94n0FfD3RwzBrEIIdqNX8/AyBap3ukOMRyjQS5vSCi527zhRMTY64spi0eHPLzUDWD/ANzkTZ0cBMMBaanJ7FuahKJi2Faw/7NsT08p9I1KT4JGHTk0BeTk/4eWyDqNzMbeZH7Yz59/T4HWo1pV9bbC0YTMyzofgnMIgzE6Iv6wBQg+Ni0276eNPK31mYTjoXErFbBNAbmuiaJ/2m+MDNLfHkG5kEpmRRAXtEamKRsDsKGQv7YasX3byx/5xMTdJfH98/MzJQ/Cro3ZlzQgvtuTvtrJjDv12o+NhmYHlFOmfH6RGseGRwUsBbQmtUDc5UZP4/U62CYA+tYC0yCKLZofGFs5EA5VOkD8xU4l8wDYut3gHE/v8/zmFEUoG9shBBCCCGEEF2PXmyEEEIIIYQQXY9ebIQQQgghhBBdj15shBBCCCGEEF1Px5oHZJWKZW+yW3uegyg6IgAMJAAE4RqJrUjw1EShP5NmfgdV2nGcRdn+uKlpLwKNiYtpl+w2iLBQHAfCZhLosumBWV7S3KEIvn+pK1BYn/J9B9qJvqQokNqYg9AwS1l8iKYLJJoDAWBOu0WDS0YREXxSl2dVHzMSPuLu9tCP0V27UThJQu2oDH/euTBmI0Yc07kX0VNsU8iXdtMLtQsShkaGPJmGVGv+/EbTj9uyu343W/7cGA3YeZvGbbPh+4yU8VmFhc3VGpQnvpMa0L+tJhlakPEFdzrNd82mv59axc+9jeiu3/PaE/HImIb5d3DQG1AEMKKhOORTE/7ciFC2Uvf3TSkU2v4eU5gdGjPlxO1mOBVYBcwDcpjPKcvLr4Es6Ka5JaVugzZmGQj4d8MMhmawdtsLrRstX5aav5feXh5jJPTGnejBlKKV+xyg8URGSGbc5/xMUG7HexrfMSE7zf2Vmh/LCWQWLEO4i31PjzfYMOO2T01544IWtB3XHCgLFjNsAFOAGZg/wYyI6qY5LCbgp7W1p8Z9VAbKFXzGMMPJZf7aWBTlv4fRNzZCCCGEEEKIrkcvNkIIIYQQQoiuRy82QgghhBBCiK5HLzZCCCGEEEKIrqdzzQOyqmWv2408AVErCaibOQubaSd7okLiZBA21UGUWu3xO96a8a7WJOAi4wJqdwGi1JgIL0tIbAoCtVY5kaKB6C26Ez0YNpArAO0aP9i3CKoGI4SIDr1N4jwQQ9I9UmzqdYjtbmzYzKYLvp52yTI0RzCzgkSbJHKEvqxmMB2UEPXN1gN5yX87oTEGddN4iJgHUHzQxCEHcTsIfMnYIQUTBjOeh2i+ChEhvLseJFbUZADGTr3H71Q9OeV3rEchOwiODeYQM7NatVxuNGDH+yyl3enLmR6YmeW5j1kv7DhucM3ePn/c9BSI7SN9jv0G89DLL+50Zduf2ubKjj32mFJ1mJlZ8GOZWpnh9EDjEwTHETF5Xvg+J6OBCgi1SZBNZgYxc5AEDFRI3E45FAzWUDKiiaxj1B90P3Q+TAM4FmmHdzOe1+gZJYV0qdb6XBkZBcTMYLgc1ic0Cii3FsRMj+j8CqxPZOSBsYG1KTa+6fmqBc9wlZKGSwZjJGaKQutTgHESfeaaf72CnuH4oSlNfdupjyg2ZM5Q0iPo1Ra9+cNU2Xs20zc2QgghhBBCiL0AvdgIIYQQQgghuh692AghhBBCCCG6Hr3YCCGEEEIIIbqejjUPsJBaMkfAC+KvilfMRSSXKKJCcTHsiM2iKtpFl0V4uCMsiMcKEDnSVvIVaE87pqJPQDwGYuB6rxcpksCN2lgn0a6ZDQwsdmUTk2OuLMAuurSrOQnr8sgO3QYCywA5QOJOErViDCMiWxKT0y7FlH+0azfo2MmDIUoBuZHBBVqggKZdkyt1LxTc9UG5RpHAkkwyqH9jJhmkVCQBP5kh0M7ZaQJmDxFhM5o7QNvTCtw3XC+F7aJpzJuZBYjPxKQXJ8Nm59YAo4CBvgGoJCJ0LSkOHRzax5VNTc24skWLvNh5bMzPF2ZmVZj7aWdzmlsasJP3osEhVxbLtSVLlrqyl1582ZVVav2ubN3V17qy//mt/+HKli33fWZm1oI5MIH4VMDoguYw6B4LjYgRDTgSBBBG520vhAfttSUw/003fV6YmQWY5+s9/h5TmFsqYPTDgvdInkMfVWjubsH8CZN3b2z+BGoVf369x5+PzzIwQAswBOjt5d3l6Xwyi+ir+ziw8QCZB/CcmqX+Hil/K30QBxi31B4yHjAzS8DQgHK1gOWuWvXHUa61IuYBNZjXElo3mv5+6LmjDvNAbF6juZL8FZLIOujOhefMWN3T035Onr+G5mBwFa279JFCCCGEEEII0aHoxUYIIYQQQgjR9ejFRgghhBBCCNH16MVGCCGEEEII0fV0rHlAO88tf91rV6vlxUUkbssiAuY6CAhp19tKjYSGXqjVhh2gY2JyFBKDKqsCIjES66UgOe7r80JVMxbckei32fJith4QZ1Kf087OZmYzM14ISiI+2iX7pZdG4Iq+z0hkaGaWwk72AYTjCaiqS+9wm7B6mtIAhfCQQ1g3GSFEco3k6CTWJ/ErqSFjO0O/FUhMSfU0GjTm+Zokoi8gp6tV3xfUP+3c92OI7I6M2YLjrpzJAHoexHbJhrHX3+/ngskZbygwMOCNAnIQjldoN20za5PyHHaDZ6MAX/f4pBfzDixiET0ZBVAf1ave2AQNG0gMTiYMZmYw57zw4iuu7JJPXuHKjjvuWFf2V7ff6sqWvmsQq+6BnejbbT9OiFrNz+cUwthO9AkIvbHfQCDchmvmEEPcud14nJBpB42HJoiOcU6MzHV0LPVRWQF/DZ5FYuObmIS8LG1uA/XE+pxMCtokWqfnMBCOQ7jxXDOzAM9cPE+XMwqgvqj3xUwTyuaaP25iyseGzIiieY7PKACkC9VDa21MwM8GNWCEBOOJ+jeDZ9cYZfJ/t65X+kghhBBCCCGE6FD0YiOEEEIIIYToevRiI4QQQgghhOh69GIjhBBCCCGE6Hr0YiOEEEIIIYToejrXFa3dsvR1li3kurBbLhtwPrpngKtaCHDNDJyqwIHCzKwKbhNZzXc9GApZ3m66skrm3UrIJcjMrF3Ska2skwg5JLVb3OdN825IBLl0kKtZCk4rMSebWH+4umM2W/PrqUFwYnXANSm2Cfud+OPI3Aac0sw4ZuzGVapqdCFLIPfNzNq5z39yncvJIQlyKAW7vJhjXQpjrAqOWFni20jOMT3kLBZxycI+gnYm4DBTFNAecvwKMbcoX/k0OKD19fS4svHxcX9cvQ+u5+cgM7MEcnDfRYtcWa3mr7lt+1Ou7F1L3uXKdrzwK6y7CblGTnTP7njBlZFr3MEHHOjKXhzZgXWvOPBQV7bfuw5wZffc/X9c2b3/63+7MsrT/n7vGmdmVhRTroyWnbLOWeTOFDMoKuvcVaX1hfK8DW5ame8LM7MKrJcFuEoSNF+hKym6QpkVME/zGgrudDB507iJzWs4f0Jf1uC5paxzG3vgcR5QvqBDIfR5Am5jMQc+WjBzSHRa26gMHesiiyAdy+GB/IUcKug5IRJvehaiZlarMB7gvnNYH6IOmxAfckWDYYtjOVR8vGJ1Z3A/bl0Gx7gY+sZGCCGEEEII0fXoxUYIIYQQQgjR9ejFRgghhBBCCNH16MVGCCGEEEII0fV0rHlAK8/nCO9IFN3b2+vKIvo/FOfRNYkC3v/IKKBa9aJ+M7MKiAXbLRBTknAMrhfAUIBEfWaGgqtQlOsLElUPgDg4JgBsg5itWgVhXsX3W0qCeRCexe673S6pjgdxHeVKFUwKMhC3m5lNT0/7QhDcpRQbEp1D/hQFC2cpjvTXi7ImA20SHxbl/x7SbvvcqKQ0TsCoAgSkISIgbKMwn0wTKN4wvqF/SNi5CzifxO0gqKX8rWc1V4bmFWbWysvlL91PBseNTXpxeprwvLZ40WJXtmXzE76e1AvCDztstSt7/tmXXNnKlauw7rM++J9c2Usv73RllZqvuweMFL7xV3e4snVXfgHr/tadd7myfRYvc2U7nn3elT37wnOu7OT/cKIrm2n6ezFjU5VGw8eW7pHyl+b4GO2mH8ssevftqcHamKckLubxTfnbbPl1sKff3zetG2E3hOwprE9Es+nbg88oYEbQzCNraMOXV3t8TidgUkBzy+48ByVg1pPBHAZeBrSsWhH8gbH77qmTQRIYSNG8RgYHkFdJ5L6nprzxBhmOEGSaQOYBMzFDFjiWniup6bSO1Wvl+tGMnz/J1CfLyj0r8jMGVs0GXW8BfWMjhBBCCCGE6Hr0YiOEEEIIIYToevRiI4QQQgghhOh69GIjhBBCCCGE6Ho61jygMdO0dvvXIjASQ1YqXmTbbPJu9wmo0VMQmbVLGgqQCI8E769W7oqKwovmaIdjElW1E9wOnqsmEX7Ftye207VvTzkBnxmLNlPadRmEfQWJWuG2SQBtZlYD0XCee8FeHfqNNsdNKA4tFpMXUA+JSA2EwLSrdLvt64ndN0WCdrxPYy4b7kC6Igv9EujLet3HocjJKAByBcZDQblvZgmYRbShnSSyJSVmDgYQ8R2boQzCU0GBJIx5oxzAqnmXbTDooH5LyLgABMODA0ux7uee9QL3j5z3MVc2vP9Bruzk437Hlb348ouu7F1LvCjfzCxAv9lKEAhj/3pO+Mo3XFkeMei45rPe+KAHRLokqP3qzTe4spEdz7qywryA2cwsrfjd7XGH+CqsbdAeWgPzJicbrXl0TZqaKmC+UoF1KG5m4C+aQa4mMPBymKdJkJ3HlM0zvs+JffdZ4spofJLhR0rzkpklVd8f0G1WGMznNF2ReQDWbNaEdqIZDMzdBC05lZp/hjMzo9tpkyELwDkJa0kk3imsGwHum4x+yLSI1sVKdP2F/AVRfwbPvnSPOcQwRhsOBS8DwyWYzDjIDCtEDDroWXFeWYiNT7pe6SOFEEIIIYQQokPRi40QQgghhBCi69GLjRBCCCGEEKLr0YuNEEIIIYQQouvpWPOANMuiu8q/xsTEhCvjnZDNenpIpAbHkpAdRGt52wvEaffzXXX3+fNB1FXvA6E1CqZgt92IGC2tlNt9GMWUIIInIRsJQ814x13cBRrE27H95d25fKCRwJ13xwVxZuR+XA2BxcUkmCfzizY0Pp8mwV253ZXN2OQgTX1s6R7zvKw4OBIdOHZ8wougA4ynLPNtHBxc5I+L5HkF8jyH+NB4alMcSZQfMQ+gHKL86xsYdGW0W3kBonVso/E99iRktAJCaxhl9Zrv83/9f09h3R85/7+4stN//xyoB8Y8XO9dS4b9cSBKNeM8IEMBMqAgqJZaJNdqfd4IpA1mMHWYRk499VRXtv7Ga1zZCScdiXXvfMWPp1rd9+/k5JRv41sUVVcz2oke+gi6HNeXlhflx8wDaH6ogvA8kIkImJWQWD8jVb6ZJRntWu+PixsfzCVmtlOW6Ybvt2rVrzm4rpJ5QKQ9tLbS80SlUm59oXlyZtI/w5nxcwaBZhHwbJYGH9tqj2+3mVm9Ws5IaRIMZtBUCp4p42soGd7Q85qP98yMN86isdyA/DEr/2xHzxgcL8oBrBoNUOaPp7Ljy0zf2AghhBBCCCH2AvRiI4QQQgghhOh69GIjhBBCCCGE6Hr0YiOEEEIIIYToejrXPKCSzRGqFSDvJGETCRzNzMbGxlwZ7QZfo51wI7udz6cdlbyD6Ak0lyiqBlUVCcxmGl4sasa749L9UF9SX5BYlHYjNuMdd8vuHkubGZOANCbDnJj2/cE7XfsrhILE7SRWZnExmQKMTfj2hJJ/V8C6Iyq8hISK0MxGE4SPcD8kPkxidUNZf68XOTZmfL6Q8UYOIsXYLtdtEIwS3G8gbk98DjRA6G/G8cmqkGtkZoCGGNDGLCJsLnx5DiLLCgioCxDezkz6e1xx8G9g3af//hmurAVjp9XwolaaW2oZiMEj00UC82I5m4DykBmBGQvHR3e+4sp2jr3gyn6+9TFX9otf/qsrO/zoFVw3rG+Y0WAgUe+ludvHi4wQzFjgS3GgMTY56U0PaB0qIgFPYJwUIAjPKGYwPidA+D04yKLxZsv3x+TYuCurwJinPCfxdmw+r4HpDBocQRa0oS+zxOdPAWPWzCzANUnI3oK5l8oCtJvmeDOzCrQzw7nfz6k0j2RgANGY8TlgZjZTciap9/i1jdZQem4h0xgzszasty3Iv2YLnltKmlLE6iYDADQUgDiSOUhBa1PEFILK38w87I3QNzZCCCGEEEKIrkcvNkIIIYQQQoiuRy82QgghhBBCiK5HLzZCCCGEEEKIrqdjzQPmU3aH5HbKYrShoSF/LAgncddaEEiSsCm2mywJo0joFVJfRrvYT07Abr20TbCZJSQtJfEh9CW1Mc99/5BwLHY+9Tn1JZkZ0M66PSSuNN69vVLpc2UUW+qLPC+/Ez05H6QR8bc7NZJD84kJfMlYgmTVmH8oNgUzgth9wy7xrXY5UWAPCDF3xzRhAsYEjR3MtarPtTHYuX1qig06BgcHoR7fR9NNLxqmeJMBBI0HM7N27vOAhMSTYKZRTb15ytT4qCtbvoqF7MQEnL9kaKkry0EQm9IO7yXHzZvy4INmW7eaHXaY2cknu/LWyhUWTjppzim1KhjJGJs7DC72wvP//F/Pc2W1Pn+P7/vA+6AOngeoPIH5hvKPditvNkEEX+H7TsnwBnaT5/nTH0c7spPA3MwsJxMT6ItmyXWMePnlnVhOa1atBkY2MPXS2k9rViuyqzqaJoGBBD4LwTULeD6KrWNFyTWigHmfDJMCGJjE6g5wTTIzGOjrd2Wtlp9nczBIoD6LHUvzL8W2KPzzUQZ9HqubcrrssyaZUhCUF2YcCzq2kpLRlM9pusWYIQCtrem85+Gy49hM39gIIYTYm1m3zux3fsfswgt3/XfdOldeW/Meyz73+YVtpxBCiLeMXmyEEELsnTz4oNlNN80tu+kms29+05VXvvxlSx56aA82TgghxL83erERQgixd7J1K5c//DAWJ1uffBsbI4QQ4u1GLzZCCCH2Tg47jMtPPBGLw2Gr3sbGCCGEeLvRi40QQoi9k5NPNrvqqrll69aZ/fEfu/L8yv/mDASEEEJ0F0nYHauBPcDY2JgNDQ3Z+//j0Vap/tpBgZy3yPWor887X5mZFeAeND7uHWEs8VYOMdev+cQcH8zKOZHUK76eFrgHGbmVRGom94wAr7PkeMPX8zXFnLzKxowcQsglg67X3+9dUczMdu70DjdUd2+vd4aiPk8S30ExR5dWSRc86je6JjoCBl/Hq5+UKuvvXRQ5f/6p7N5CtFu+TRNTk66sVvUOKlVwRSP3PhyzZpaC8xu5D7XB1Yf6d6blHfhi8S7r6kc5QLnWAzkZm4PoftptmA0Kf4+tpj+3p+rdvV56gd2ihpcd7Mr669598sMf8u5gq35jtSurVny8Zma8y5CZWT3ihjif9mtj8cEHLXnySQurVlnllFN+fcCrrmgv77fU8uOPm3PuDV+ap815lY997GOubGTkeVf2yUsvdmWnn+Ed0PoHfP7sfGUE66axTA5oBOVvWaezGI3GNJzv86+n1zutUbtjblHkzjQ+6Z0QKxmNE18POWzS2mbG60at7uvp6/HjNuZA5a5XYye6Zu6drppNPyao2ygOVegfMAHbdT4s6xk4F9IcNt30/UtrLfWtmVkBa2ir4e+7D9zpyBGV5sk6xNDMrAlOnhTHAB2Ezw7m64nFO8A6xm5nfn2hNlJ7KH/MeI2hdYzWeVqXlyzxbpix1w26x/m50Wzk9je3/9hGR0fRifT1dI3dsxBCCPFv4uSTLbze5vl15XbyyZa/9MKeb5MQQoh/d/RTNCGEEEIIIUTXoxcbIYQQQgghRNejFxshhBBCCCFE19OxGps0y+aIcknYRAKsmPgwBzEaXTOYF2CRgIrqiYmLSZyMoqzc103C5B4S3EUE/CjsBwEgHZdVQLQOwryYIIzaTn1E55MJBJ0bizedT+JFOp9ik4HoMmYW0cr9NQOIFysVfz7lNNVTrXHd7bYXC1JZKMCAAuJAGY2GFmZmcE0S8Gdpub6cbngBakxsSsJdOrZVUhhaA7HzwIAX1puZTU+XE1BTPZRrvZC7McEnxSKH/OupepON3l4fh2rm2/Obv3ko1p2ZbycZoPz3a7/gyg5bdYQrW7rv/q7s8suvwLq3PLLFle3YscOVnfGHp7uyAAL8yy67zJU9u+NZrHt0dNSV0VRw5tlnuLK+fhjLVd+eRYvY3IPE5DTFk9CaoPybmPDjzswsL2hu8TndW/djh0xeyCAmZgZD4u9Gy98jGWJUq34Wo7Ecm89pzeuF+aYAFT6Joqme6JyalptTSciOxkE5GKXkHO8U12/KXzi36vOqBkL/qGkCrUUwn4O/DBoplDVzMeOY0fmUq/ScOT3lY0u5b2ZmSUnzIMg1euahXCHjDDPuD3o+yjN/P9QXdN+x5zXq3/lxaDZjhkkefWMjhBBCCCGE6Hr0YiOEEEIIIYToevRiI4QQQgghhOh6Ok5j89rv/PLW3N8Qpon/fV2W+rIkonNpw+/z6PeQ9Ptr2qmKfitYZPybTdr4CzfUAo0NbuIIm0UabOwUO7+0xgY27qLfdsY1NvA7WfhRbA73jXoP2mAutkkmbCKVpuV+80n6E9rILsv496LNhq+7bD30W2mqJ76RHfwOHn4bTxs2ltXY5NC3u64JeQ5bx1IO0NChfoyRJbSRrW89amwgpwuYB2pV/h18YwY0TCU1NqS9ojxtNrnuJsxhpLFJIQdoaigozwveyC41f3AAXR7pB+h37PTb77GxMax7YsJvzjg16X+3TufTHM9aJc4/0juRPKPR8PlHm1LShqqNyMakTdhoGjU2kC80j1CeNhucazltfAg5ncKsQWOEyirw+30z1tjQ/EAamwBzHa5Nkfmc1rwU5puAGht4boG6o/ukg8aG5rW3orGhNpqZpW3YDDaFtQg2i27DvA8hxNiYmTVpToX20Ok5zPH0XBh5dMC8on1WM5in6VmGrhfVi5TW2NDcQpuD+ompERnflIOBNDbQdsppusfYc0sCeT5//m29er3oWHn99UKZo/YgzzzzjB100EEL3QwhhBBCCCFEh/D000/bgQce+IbHdNyLTVEU9txzz9nAwICNj4/bQQcdZE8//bQNDg4udNPE6xgbG1NsOhjFp3NRbDoXxaazUXw6F8Wmc9kbYhNCsPHxcRseHo7+Uuc1Ou6naGmazr6NvfYV3ODgYNcGY29HselsFJ/ORbHpXBSbzkbx6VwUm86l22MzNDRU6jiZBwghhBBCCCG6Hr3YCCGEEEIIIbqejn6xqdfr9sUvfjG627hYOBSbzkbx6VwUm85FselsFJ/ORbHpXN5psek48wAhhBBCCCGE2F06+hsbIYQQQgghhCiDXmyEEEIIIYQQXY9ebIQQQgghhBBdj15shBBCCCGEEF1Px77YfO1rX7MVK1ZYT0+PHX/88faTn/xkoZv0jmT9+vV24okn2sDAgO233372wQ9+0P7lX/5lzjEhBLv22mtteHjYent77fd+7/fsZz/72QK1+J3L+vXrLUkSW7t27WyZYrNwPPvss/bRj37U9t13X+vr67N3v/vdtmnTptnPFZuFI89z+9M//VNbsWKF9fb22sqVK+3P/uzPrCiK2WMUnz3Dj3/8Yzv77LNteHjYkiSx7373u3M+LxOHRqNhl19+uS1dutT6+/vtnHPOsWeeeWYP3sXeyRvFptVq2bp16+zoo4+2/v5+Gx4etgsvvNCee+65OddQbN4+3mzsvJ6Pf/zjliSJ3XLLLXPK98b4dOSLzT333GNr1661z3/+87ZlyxZ7z3veY6effro99dRTC920dxwbN260Sy+91P75n//ZNmzYYHme22mnnWaTk5Ozx9x00012880322233WYPP/ywLV++3D7wgQ/Y+Pj4Arb8ncXDDz9sX//61+23f/u355QrNgvDzp077ZRTTrFqtWp///d/b0888YR95StfscWLF88eo9gsHDfeeKPdcccddtttt9nPf/5zu+mmm+xLX/qS3XrrrbPHKD57hsnJSTvmmGPstttuw8/LxGHt2rX2ne98x+6++267//77bWJiws466yxrt9t76jb2St4oNlNTU7Z582a75pprbPPmzXbvvffa1q1b7ZxzzplznGLz9vFmY+c1vvvd79qDDz5ow8PD7rO9Mj6hAznppJPCJz7xiTllq1evDldfffUCtUi8xsjISDCzsHHjxhBCCEVRhOXLl4cbbrhh9piZmZkwNDQU7rjjjoVq5juK8fHxsGrVqrBhw4Zw6qmnhiuuuCKEoNgsJOvWrQtr1qyJfq7YLCxnnnlmuPjii+eU/dEf/VH46Ec/GkJQfBYKMwvf+c53Zv+/TBxeeeWVUK1Ww9133z17zLPPPhvSNA3/8A//sMfavrczPzbEQw89FMwsbN++PYSg2OxJYvF55plnwgEHHBAef/zxcMghh4SvfvWrs5/trfHpuG9sms2mbdq0yU477bQ55aeddpo98MADC9Qq8Rqjo6NmZrZkyRIzM9u2bZvt2LFjTrzq9bqdeuqpitce4tJLL7UzzzzT3v/+988pV2wWju9973t2wgkn2Ic+9CHbb7/97Nhjj7VvfOMbs58rNgvLmjVr7Ic//KFt3brVzMx++tOf2v33329nnHGGmSk+nUKZOGzatMlardacY4aHh+2oo45SrPYwo6OjliTJ7DfTis3CUhSFXXDBBXbllVfakUce6T7fW+NTWegGzOell16ydrtty5Ytm1O+bNky27FjxwK1Spjt+q3zZz7zGVuzZo0dddRRZmazMaF4bd++fY+38Z3G3XffbZs3b7aHH37YfabYLBy/+MUv7Pbbb7fPfOYz9rnPfc4eeugh+9SnPmX1et0uvPBCxWaBWbdunY2Ojtrq1astyzJrt9t23XXX2XnnnWdmGjudQpk47Nixw2q1mu2zzz7uGD0z7DlmZmbs6quvtvPPP98GBwfNTLFZaG688UarVCr2qU99Cj/fW+PTcS82r5EkyZz/DyG4MrFnueyyy+zRRx+1+++/332meO15nn76abviiivs+9//vvX09ESPU2z2PEVR2AknnGDXX3+9mZkde+yx9rOf/cxuv/12u/DCC2ePU2wWhnvuucfuuusu+/a3v21HHnmkPfLII7Z27VobHh62iy66aPY4xacz+LfEQbHac7RaLTv33HOtKAr72te+9qbHKzZvP5s2bbI///M/t82bN+92X3d7fDrup2hLly61LMvc2+LIyIj7q43Yc1x++eX2ve99z+677z478MADZ8uXL19uZqZ4LQCbNm2ykZERO/74461SqVilUrGNGzfaX/zFX1ilUpntf8Vmz7P//vvbEUccMafs8MMPnzVA0bhZWK688kq7+uqr7dxzz7Wjjz7aLrjgAvv0pz9t69evNzPFp1MoE4fly5dbs9m0nTt3Ro8Rbx+tVss+/OEP27Zt22zDhg2z39aYKTYLyU9+8hMbGRmxgw8+ePb5YPv27fYnf/Induihh5rZ3hufjnuxqdVqdvzxx9uGDRvmlG/YsMF+93d/d4Fa9c4lhGCXXXaZ3XvvvfZP//RPtmLFijmfr1ixwpYvXz4nXs1m0zZu3Kh4vc38wR/8gT322GP2yCOPzP474YQT7CMf+Yg98sgjtnLlSsVmgTjllFOcLfrWrVvtkEMOMTONm4VmamrK0nTu8pdl2azds+LTGZSJw/HHH2/VanXOMc8//7w9/vjjitXbzGsvNU8++aT94Ac/sH333XfO54rNwnHBBRfYo48+Ouf5YHh42K688kr7x3/8RzPbi+OzQKYFb8jdd98dqtVq+OY3vxmeeOKJsHbt2tDf3x9++ctfLnTT3nF88pOfDENDQ+FHP/pReP7552f/TU1NzR5zww03hKGhoXDvvfeGxx57LJx33nlh//33D2NjYwvY8ncmr3dFC0GxWSgeeuihUKlUwnXXXReefPLJ8K1vfSv09fWFu+66a/YYxWbhuOiii8IBBxwQ/u7v/i5s27Yt3HvvvWHp0qXhqquumj1G8dkzjI+Phy1btoQtW7YEMws333xz2LJly6yzVpk4fOITnwgHHnhg+MEPfhA2b94c3ve+94Vjjjkm5Hm+ULe1V/BGsWm1WuGcc84JBx54YHjkkUfmPB80Go3Zayg2bx9vNnbmM98VLYS9Mz4d+WITQgh/+Zd/GQ455JBQq9XCcccdN2svLPYsZob//vqv/3r2mKIowhe/+MWwfPnyUK/Xw3vf+97w2GOPLVyj38HMf7FRbBaOv/3bvw1HHXVUqNfrYfXq1eHrX//6nM8Vm4VjbGwsXHHFFeHggw8OPT09YeXKleHzn//8nAcyxWfPcN999+Eac9FFF4UQysVheno6XHbZZWHJkiWht7c3nHXWWeGpp55agLvZu3ij2Gzbti36fHDffffNXkOxeft4s7EzH3qx2Rvjk4QQwp74ZkgIIYQQQggh3i46TmMjhBBCCCGEELuLXmyEEEIIIYQQXY9ebIQQQgghhBBdj15shBBCCCGEEF2PXmyEEEIIIYQQXY9ebIQQQgghhBBdj15shBBCCCGEEF2PXmyEEEIIIYQQXY9ebIQQQgghhBBdj15shBBCCCGEEF2PXmyEEEIIIYQQXY9ebIQQQgghhBBdz/8Hut9LdFDgeGAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpii_gtuV0hd.test import predict_hour, predict_simple, draw_keypoints_on_image, draw_skeleton_on_image\n",
    "\n",
    "print('Hourglass 모델의 예측:')\n",
    "image, keypoints = predict_hour('./test_image.jpg')\n",
    "draw_keypoints_on_image(image, keypoints)\n",
    "draw_skeleton_on_image(image, keypoints)\n",
    "\n",
    "print('Simplebaseline 모델의 예측:')\n",
    "image_, keypoints_ = predict_simple('./test_image.jpg')\n",
    "draw_keypoints_on_image(image_, keypoints_)\n",
    "draw_skeleton_on_image(image_, keypoints_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>결과 분석 및 회고<h2/>"
   ]
  },
  {
   "attachments": {
    "제목 없음.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAA64AAAHoCAIAAADG+iV9AAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAP+lSURBVHhe7P1ndB3ZeecLz4f3rnXf9d51v9w14zv3zozH8tiyZFuSZVuSFayWFWzFVrfUuclmDsj55JxzzjnngJxzTgQDQIIJOQNETgRI9vvUKfAQ3c2OBEG2WP/eXdy1az9Vhao6dX57n2c/+z+9jwkTJkyYMGHChAnTCykMhTFhwoQJEyZMmDC9oMJQGBMmTJgwYcKECdMLKgyFMWHChAkTJkyYML2gwlAYEyZMmDBhwoQJ0wsqDIUxYcKECRMmTJgwvaDCUBgTJkyYMGHChAnTCyoMhTFhwoQJEyZMmDC9oMJQGBMmTJgwYcKECdMLKgyFMWHChAkTJkyYML2gwlAYEyZMmDBhwoQJ0wsqDIUxYcKECRMmTJgwvaDCUBgTJkyYMGHChAnTCyoMhTFhwoQJEyZMmDC9oMJQGBMmTJgwYcKECdMLKgyFMWHChAkTJkyYML2gwlAYEyZMmDBhwoQJ0wsqDIUxYcKECRMmTJgwvaDCUBgTJkyYMGHChAnTCyoMhTFhwoQJEyZMmDC9oMJQGBMmTJgwYcKECdMLKgyFMWHChAkTJkyYML2gwlAYEyZMmDBhwoQJ0wsqDIUxYcKECRMmTJgwvaDCUBgTJkyYMGHChAnTCyoMhTFhwoQJEyZMmDC9oMJQGBOmg9Hu7u69e/fQzPLS0uKdO3fv3oXVra2t+bk5WEW3rq+tzc/Orq6sPEgIas7Nzm6sr8OmnZ2dOwsLC/Pzd7e3P85wZXkZNYRM0hCOuN9we3sb8o8M19c/1RBM9htC+Z7h3NyHDOE0UEOoBlsfawg7Txq+/0FDOCXUEP66jzOEa/IZDeHk9ww3NvYbwuX9OEO4KaghXO39hvfv34dV1HBtdfUDhpubHzWEVai2ubGBGmLChAkTpi+pMBTGhOlJBcw0NTl5rb8fsAlWAeDampsbamvnEqsT4+NVZWWtjY1AXbB6bWCgvLj4Ym8vWAHGdXd0wOrN69dh09LiIlhVl5fPTE8nDZsbGlCkQw0vdHejhj1dXWVFRajh6upqY11d0hBOBvJJw+vXru0ZJmD9sYZgkjRsqquDPwE1rCgpgfr7DaEQNYT9w+mhhnDcmoqKpCHsPGkIpAiHThrCKaGG8NftN4QdJg0729oAN5OG8IcnDStLSx8ZVlY21taihrdv3txvCJc3aQiXvaWxMWkIpIsawtXebwhoCxiNGg5cuZI0hK1jo6NJw/qaGtRwcmKivbW1//JlFJTh9ICbgbYRFseECRMmTF8eYSiMCdMX0fr6OqDV7MwM5Le3tprq60M+39X+flhdmJ8vikYDHg/KXgBbXqezJB5H+xoBEJ1WK1AdwOXO3btAVw6L5VJfH2wCko6FQj6Xa2xkJGkIu0L5EjUEVkYN66qrk4YAZ/FwOGk4MjQE+aRhX08PaggADXqsIZgkDQsjkTsLC6ihy2aD+vsNL/T0oIawfzg91BCO63e7k4aw86QhgClwZ9IQTgk1hL9uvyHKl6ghgDKAadKwp7Mzaei225OGcIXhcqGGVy5d2m8IlzdpCJcdLn7SEG4Kaoi2W5KG29vbQLGoYWd7+35DtNmAGkYCAdQQOBtWa6uq0BYO0HNdVdWtGzfg7sAqtAFgiQkTJkyYnn9hKIwJ0+cQYBaauTE4WBSLtbe0AO3t7OwAQgHSAVfBptWVldampvqaGmBiWJ2emmqsq+tqb99MdB8CVwEj9l++DLsC24sXLgBCDd2+DZuA9mCHUHludjZp2NnWhsIWagiHQA2BGoHDUMP1tbX21takIQD6fkNAQNQQKA30WEOU6VHDjtbWZFdrfXU11N9viAIlGML+Aa9RQzguNAaShsO3bycN4Wzh0ElDOCXUEP66/Yaww6QhQDPK0KghSqJ7hjU1SUO45nC5UMOR4WG44ElDANOkIVx2uPhJQ7gpqCHaTkgawk0EFEYN0Q5s1BC2Tk5MJA3hzqKG8OeUFxfDKcHthyPCdQOk7u3qgk2wK9gK6AxtJFjFhAkTJkzPszAUxoTpMwkYC/ALCBLlS6Alr9NZVV6eZLi+nh4UtqDm4p07kCADqwBGAMdQDYAJVgGPYBUwC8ALBHuDVdTjFsBxbW0NVj/JcGMDNYRM0hAqfNhwdfWR4fb2pxqCyX5DOJOPNUzgHRhCtaQh4ODaxxjC6mcxvL/PcGN9/fMa3v2Q4ebmxxru7u4/1ccaol7FjwwTPt+oISTUEDIAu2jbA0r6envLiouhgQSry0tLZUVFpYWF6POACRMmTJieZ2EojAnTZxLwTWNdXXEsNjo8DKsLc3Otzc1XLl0CeIJVgCFAJYCnRF1ML5zgMZiZngZuhjzwcSwUCvv9aJf5yvLy1OQkOqIREyZMmDA9b8JQGBOmT9L29vZaot93fW2tqrzc53Khg7GAfdfX13cTv6onKmLCtCd4VC5fvNjb3Y02k/ovXSorKroxOIi1lDBhwoTpORSGwpgwfayWl5Z6uro629vRn8ivDw5e6OlBPYAxYfoEAfWi/hhAw411dW67vbuzE0NhTJgwYXoOhaEwJkwfq9s3b0YCgcJIBA1SBkCMOuNiwvQZdXd7+9rAQH1NzfjYGKyurqwM3bq1lhhciAkTJkyYngdhKIwJ04cFvIuOuJqfm6ssLa2rqkJHR2HC9AUEj9PinTtoJ/HV/v54OHyhuxtrU2HChAnTcyIMhTFh+oC2NjcHr14FZAFYefDgwfDQ0Mz0NOYQjOnJdffu3c62No/D0VRfD/m9UkyYMGHC9EyFoTAmTB/Q6MhIYSRSEo+jThEYBGM6KN27d29sZKS5oQGdl2QnEQ8O3YQJEyZMmJ6VMBTGhOkDmpyYKIpGSwsL0YkVMGE6QEHLamV5eTfhLAFA3NHaigZcw4QJEyZMz0oYCmPChGh3d3c5MYsYZK7199+8fh117sSE6WlofX29qb7e73b3dHaic3ZgwoQJE6ZnIgyFMWFCfrm+fetWa1PT4p07sLqbmEoX3YQJ09PQxvp6R1tbLBQauHwZc8LBhAkTpmcoDIUxYXp/dXW1prLS73b3X768V4QJ01MWtLuuDQysJyZwATLGpqPDhAkTpmciDIUxYXp/c2Ojrbm5MBK5ef36XhEmTE9f6I8Pm5ubvV1dg9euYb9FYMKECdPhC0NhTC+0AD4eJPhjbnZ28OrVrcRMuZgwHaZGhobCfn95SQnqn4MJEyZMmA5TGApjenEFHDwyPAzpfmLcEtYnh+mZCFA4EghUlJYuLCzsFWHChAkTpsMShsKYXlzNz80Bf5QXF09PTu4VYcJ06Nra2rpy6dLQ7dtfuvFzOzs7K8vLSHi4xOR5m5ub8JlaWlxEY2Ksra7Ozc6urq7C3wXtzKWlJdi6ubGBGt4B8F9Y2E54SD/ecGUFNVz+eEO4dAvz848M19Y+1RCd3AQ1XLxz517izFHDlX2Gc3NzGw8NoRpUfqzh+toaHOKR4fJy0hCuyWMN0Wu1Z7i8vN9wfX19v+H+v3FxcXHPcH19vyFkPs4Qlo81BCuw3TNMuKonDeFYjwyTp5owhDNEDeHywrV6ZLi4uN8QLjLcaNQQ6mPRUTB9KYShMKYXV+NjY/FwOBYKjQ4P7xVhwvQsBLwF3AAZIAngMLTwORScJFAjgBHAK6zeuXOno7UVEnrOYyMj1RUVnW1tKAsOXLkCTc3+y5fBantrq6u9vaq8fPj2bdgEmNVYV1dfU4OGVUYN21tbk4aVpaWXL15EDbs7O6vKylBDoN6m+vqk4cTYWE1lZdLw2sAAaggEdnd7+7GGcPJJw9amJhTpUMOLFy4kDSvLym7fvIkatjQ21lVXo4aTExO1VVVJw+vXrkFN1BBuYm9XV9IQWPOxhtA2SBr29fQANSYN0bEKqCFUnp6aQg1hJ23Nzagh1IE/KmkImaQhQCrsP2kIy8caghXYooZwGo8MKyvRYOqoIZwDnAlqCDV7u7tRQ7i8cFuThrD/pOHs9HRDbW1bSwtqOD8/D9WmJifRQNrPgwDWgdQRfEc0n1gCyc/Pzs5CwwDy8EijJYjm5uB/pNo81JxHN0BNePbm5qA+koVajyo8FDQPQXCUxaUlyMDqErQkILe6CgldXUKakIm0AoVrKytrSEkij1RYWr6zd5ILi3dgN8g5o4I87AwV7HFtbX11bR3+XU0kKFuEHcFh1mCfe8dChZwIsoRtcN8Sx04IChO7XURO6mEhHAVKkkeFLHLcRCkoURM27v29kEFK0ZqJ00PXHpontj78E5Dd7hNaiCqxCf59uNs7YLiYPGiiKhQvwI2AZSILtwjuBiK4K5CD+5XYz1519Mn/LMJQGNOLq62tLXit9/X2Yi7CmJ4HwSseALGjrQ0FzedQ8EmBzwtCtIkJ8+C7pzASgcYkyl43rl3zuVzAlOg3EDCxy2Zrb2kBTARUBSuv0wmYixhOT4NVJBAYHx1NGpaXlOw3BDhDDYFZPQ4HaggkAs3XpOGtGzf8bnfSsKezEzUEYoNTfazh5Ph40rAkHgcQSBo2NzQkDd12+5WLF1HDomg05POhhkO3bgU8nqThhe5uqIkaAmYBQSYN4W4+1hC+opOGjbW1O3eBvfcMAamThkGvdyTRRAdDyJcWFaGGUAf+qKQhZJKGS3fuFMdiScORoaHHGoIV2KKGF3p6koZwenCspCGcA5wJagg166urUUO4vHCtkoalhYVJQ7gpYb+/OB4HTIHVq/39cMHhkd5IdFoD00A5XCjIPyvdgosZDLpcLofDbrNZnU67y+W0Ws1ardpg0DsdDp/XZ7fbzGaTw241mwwmkx4ydpsFlm63w263aDVqrVplNuiNBp1Op7FZzS4nWFgdDpvP5/F6PaDi4pKy8nJ/IBiJxiqraktKK+NFpSXllZU19ZCKSysKi8uKSsuLy6ogVVTV19a3Vte2lFY2lFc3VlTXl5ZVFhWX+vxBq83mcLpCwWgsBncDnt8oqLi4qKKivLysoqyssqqqrr6htQ7M65rrGtsam9orqxsKi8urahoam9pq6hrLK2vKyuETWV6GqLSoMF5UVFiOqKyivKyqqqqyErbCM1JYXBQvLyuBciiE0ngsGgkFYtFwLBqLhKOQopFoYTxeXFhUFC8sLS4uLSkOhwIBvzcY9EfCwULYc2EhVIZlCewLzCOhWCwMm4IBX2E8BocIh0NQDDXRVfgvFoGdw94jyIFi4XgUSehuA153OOArioVLCmNF8UgcUjQSCwdDiU1+ryvodTutZo1SLhUJZBKRSiHTqBRw02KREBwfDhGJhBsa6tEuhk8VhsKYXkTdTXyLoBm0PwkTpmeumzduAILANx5Kls+J4JMC54P2wm5ubAC0AVGh7AU42Fxf31Bbu9fVOj4O3NPb3Y2i/PVr16DytYEBpHN3exvgD2qOjozAJoCztubm1sbG/YbAo/sNr165ghpe7O2tr6lBDeGIwNZJw6mJiaZ9hjcGB1FDpHP37t3HGi7MzycNu9rbUUpDDfsvX35kWF0NRIgadrS2tjw0nJmaaqqvTxoCUicNd3Z2Ll24kDRcXVl5rCHanYwaAjQDGqKGsIoCJWoIeI1eczCEfHdHB2oIdeCPShpCJmm4troKrYikISwfa4h27qKGcBpJQzg9OFbSEJpkaAMDMayuvtTXhxrC5d1vCPtPGkKzAfJwndcSR7xy6RK0N2BXYAXq6eqCE4CGwYNn5wjU0tKSkZFx5syZc+fPnTlz6uzZ0ykp506fOfHWW68defetlHPnsjOzYMOJ4++lppw9ffLYsffeOX/u1Plzp1POn8nKTEtLPXfs6LuQYNupE+8deffNM6ePp6edO3fmJFTIz8vOyc5MT0uj0+lcLi8nJ5dIosjkGq5AymAL+GK5QmOAxBPJWDwRRyDmieR8sUqmMulNLp3RpdDZ5VqbRKETihUCoaQARzh5+vT5lDQikcpk8tgsHoPBYjAYfD5XJpNKJDKxSCaXa9Qas1xpkqmMerPT4QwaTC6RVK3Smix2t8HsUGnNKrVBKlEKBSI+j0ciEvJyc1lMpoDPFwmEUinsRyIUigQCgVDAk4ihRKJSAVXKaBQSviCHQaPweQIOHJrGpFPpLAaTw2LTaVRYYTNpBHw+viCPQsQzqBQeh8XncTksDlTgctgMOoVGJTIZFCqFQMDnMRl0DodNIRNZTDofqrIYXC4UsMlEMplIYtBpTCaVySAzaCRIFDIeV5CDy82iEAqYVBKbTqFTiRQKgUEjwyqViCPj84i4nPzs9NPHjvz+N7/8j5/95Le//MUff//bt1579fzpEywaWSTgcdhMCoUEjRn4YO7d+08UhsKYXjjBixje45cvXkS/ITBhek40Pz9fU1EB6LCY6MZ7HgQfFkDJ8pISOCugUlgFJIKTRH0AgG+AfuZmZ5MNSwBHwCC0JwbwdHlpCegZwAdK4OMGW9GaYAjABwno6uMMoY26Z7i+/gFD0McYbiUNE3qsIWQeGa6uot+UHzWE1e2E/+unGG5tfZwhVPh0w/X1/Yaoxy1qCJUf/Y0rK/sNYVPSEDL7DaFa0hCWn9sw4db8UcPl/YYbG59gCJugnYMaAh/39fSgDQO4++XFxV6nc/DqVdQQbb0csurr6k6cOH706LunT58C3oXMiRNHjx97949/ePn11149fuzo2dOnTxw/9t7Rd8+dPXX61LF3337j5IljkD939nR6Wgqg8JlTJyCflnIetr791mvH3nv77JnjJ44fASbOyU5PTzsP+83MzMjLyz139lx+Po7HlzDZAiqDy+aLJQqNTKXjCqV0Np/G4jG5AMQysUyn0dsBhVV6h0xlFkk1YqlKIlWQSJRTp0+fO5+KJ1DYbIBVKZvNo9MZbDabz+fz+AIuV8DjifkCGZsr4QrlSo3J7vBb7X6F2qjWWSx2j9Hq1pucRpNTqdDxeSIWg52Rln7s6NHc7Gw6FdgWzokDyM7nC4GEgYzFIpBQIhYD0+Lyc3OzMxh0qkKG9LdCMY/L57K5FBI5LTUlLeVcblZablYqIT+HRiawgYwZdLCC02IzAXdpgMKAtiwmFVC4ID8bwJrFpBEJBVQKicdlAYuzAIfpDDwOhyvIB0SmA+YywAQgG2FoXH42isJkfD6xIDcz7Xxa6ll8QS5aAkekkXCZqWff/MPvf/6TH/30xz/81S9++vKv/+PV3/36+JG3GVSSVCwEFCbg8vVaNfoofqowFMb0wgm+nKrKy8N+/43Bwb0iTJieA8Fbe2py8jnhYMAa9FsE2o0+l6soGgXKgVVYLszPo3iHCdMnCLgZHiG0lQIA3dnWVllWlux47unqmpyY+IykclCqr6s9dRJQ9wjg7JnTJ4GKT518Dyj23XfeOPLuW4DCQMiw6eyZE5kZKcC1CQ4+nZmRlpOdmZeTDRCZnZmZl5tbkJ+flnr+2LF3AYiB0s6dPZmaciY3JyM97RzsE/accv5sytlzBBwRaJXDE7O4IjZfIlVqFRqDSKbmCKQMjiDRNywVStVqvU1rdCp1NpnKJFcZVVqjWqMHSM3Nzc3JLaDS2WKJUqM1SqVKDpfPAXzlAnMKeXwRcLZQpOAJpDyRTKkxulxBrzdqsXqtdp/bG7E7A1abD/jYZLIrFBoOiwsnD6Sek5VBp5LZDASFBTyBWAz0K5VI0B5iPpfLptOpuIJcAj4P6NagM1stLpPJptcbpWJpTlY20il+8nhm6rn8nHQqsYBNp3CZNDaTLhFLNGqtRCRhs5h8Hkck5AkFHABc2A+DTuawaSRCPiSoKUBAns9hc5hMBoNBgwQHlUrESoVCJpVwOUwKCU8h4ugUIlAxPj/7zKnjcJvyczOBvAGFOQwqHDEnPfWNV3//21/+4g8v/+a1V34Hy1d+96v33n0TrGQSIZ/DIuTnahQyDIUxYXq8Fu/cARQO+XxozxYmTM+b4PU9/3Bo2uELCGZ5eXngyhWUWuDz0lxff6GnZyPhEoAJ0xcQ0p28vj49OYn2H/f19vrd7sa6OjS+x6GpLoHC7x19B2gVGPd0grEAfI+99w4UouB7/hxQ8nFAYSDgs2dOnTt7JjszA1eQT8QTCABm+QV4yBAI6elpJ068d/7cqeysVODglPOnszJTYPne0bePHzty9vTJtHPniHgSoDCgKiAv0KpMpUOTSKZhcsWAwokkhlWJQieW6yRy2GpQqA0KpZbLE5JIZByeRKVzJFKV3mBVqnSAv2wOj83h8wUAwTKxRKVQGaQKnUCsUKj0DqfP4w3b7X6HI+DxRCFjNDoMBptOZxKLZWQSOSszPT31bF52Oo2M5yR8FYB8eYizAofNZnPgXy4QMpVMIiAoTMjnsFkatd5kchhNsBMjm8U9efwENBhyMlLzstILcjJIBTkkXC6xII9OpcIZmEwWtUrN5/H4PK5QAAlF4Xw6jcRikAm4HFxeFpVI4LBYPA6Py+YKBHwhIpFcrgDUNpssKpWaDVUBhREaxpMJBURcHjQ2UlPPFuRl5Wdn4HKzmFQSi0bOSDn75h9fffOPr7zzxh8h/fH3v3351/9x9MhbeFwug0oEIM7NTJMK+RgKY8L0eN1PhBO+1Ne39pmHl2LCdGiC5/PWjRs1FRWoO+bha3d3F0gl7Pd3trVBHs5neWnpboJgMGF6cu3s7MADFvL5OhIPGJQcWt9wfX1dojMYEPZsaso5QGHA1nNnTwK/AkXBpvS0lDOnT5w4fiQj/XxmRipgMdBwRnoqcDCVSiWTSTgcHlAYh8OlpaWeOHEMIDg7Kw2AOIHCqSnnz8B+gKrPnTmTnppGJFI4XBGHL+EKZSKZSqIA5NUoNEapUg9wzBVKmVwhhcFlcERcoUIoga16mdIglmn4QhmDxaVSGSQyHVBYLFECCqvUeiBgLk/E44vFEoVMppHJdGqtVaW1CiUqiUxrNNstVpfJ5AIIdrlCRqNdIlGJxXIg3Nzc3HNnz8KfAyick50GVMqgkXlcJoNOIQFxkolUKoXNYQtFAqRXmJYoJOFZLKZcptJojEDhcrmaTKKdPnU6KyODjMfh83LwedmI225BLi4vh06lyWUKrVavVKoSvcvChL8Eh8UEsMaxWVQOE/GVIOHyAI3pVAqLwQQaFolEUqlMrdYZDRazyW4221VKLZMBKEyARCTgIMGZ4HH5wOVEaIY8RGEqEXf25PG3X//jkbdeh3T8yNtAw6+98ruUc6dIhIKC3Cw4t7ysNKVUjKEwJkwf0YMHyR92D+3liwnT5xJAZ0tjo9tub21sRLvQDlkPEg7BAY+npaFh6xn1TGP609ba2tq1gQF07OPS4uLA5ctotIqnLUBhQFtAXuDg8+cAW9859t4RAMQj774JCSHFtNSTx48BHyM9kSlnzp45efrUibTU8wQCoDCNTCbnFxRkZWVnZmaePYuYw97S084DUkPNnOzMrMx0KDx65N2sjKyCAjygMJ3BpbP4bL5YIFEA+8JSoTEAEEM+4SYhpNB5LK5MIFaLpFq52ixTmcUyLYcvZbB4DCaXSmPDHgAXTWaHRmuSAk9LlSKxHEpUaqNCaVCozWqdTarQg5VWb9EhyWq3+xxOv0yuodJYbA4HgPLkiWPvvvMW6vcMyF6QnwOsSaeRKWQ8iYQD9uXxOAqFXK1VK5VKkUjIoFOZDAqfz1PINVqNSaU2qFR6DpuPK8DTaXQum8Oi09gMmgCKOCw2k8nl8MRiqVgsE4mlQLcSiVTAF/C4XBaTRqeRREKuSMBn0ah0MpFGJlER52AKl8MRCATAzRq1Xq8zQzLoLWqVDsppVDKFQgIIhkSlkCCRKQQSIb8gJxPgm0kj5WWlv/fOm++++RosT773bsqZk6eOHYE8lLMYVABuQOGCnAy9WoGhMCZMHxZ8r98YHJwYH8c4GNPzLHhKayoqgBV2DjEgKxAwMs1EIkYY0ElfT8/01BQUolsxYTpYoT7EsLzc14f0ELe2osMNn6rqamtPnjwORAgICxD81puvQx6Y+N133nj3HUBhpCv31AmkwvlzQMzHIaG9wgQCjkqlUijkvLy89PT0RBiK0wCXsDUzIw3o+cTx4wUFwMn5R4++CyxNJtNoDDaJDNzIoTF5TI6QI5CweCKhVKnUGiUKNawyuWIGR8TkSgCCpQqjTGlSaW1ag1Ottys1FrFUzWILwJzNEQLUGow2oGGD0Qo0TKUyqTSmTKbW6C1KjUmptah0FoXGBBCs1ZnVaqPJ5LLZ3DKZisnkcHkcQEmg+VMnT5w7ey49LR2Pw9ETYgDOMulcDpOHuOpKdDqtTq/XaLQymZTJpLFZdKlEDJxqNNqNBptBbwUs5nD4gLwqlUYmk4OJQiZXKRQKmUImg3/UcESxRIYmoVDM4XCRcBMMqkwqlklkXDabQaXQKCQalcRiUtkcxFGYw2HJwVauBnOVSgsYzWazkP53MokM3EylsJkM4HIi0jeci8vNogG4U/Dnz5x4+/VXj779+smj75w/fSIj5ezp40ePH3kbUJgL8E0mAA0XZGfoVJivMCZMH9HUxERhJFJdXo4G2sSE6fnU5ubm7MzMIQ9NA/Ctrazs7epCJ0R4Jh3SmF40QWOvu6PD53I1NzQcgnN8TU31kXffhnT8+HvvvP3m0SPvpKacg3T0CBS+lXL+XEZaOuJCfPL4qZPHTp44eu7sybTUcznZmYBiCLjRaAUFBTk5OTg8Lj0t9b2j72ZnZpKJhFMnTpw6eZpGowMunzh5IiU1jccXcQUSOpPH5oogMZFhcyKuUCpRaORqvViOoDCNxadzhHyRCiBYobZAAg622IN2T9ThjhjMbp5ARmNweQKpXKHVG2wOh89m9zAYnKNHjh8/fpLOYAEZ6ww2hdqo1JrUerPeaNNoTTK5TqU2Armq1XqJRCbg8+HEKWQSEU8kkagsQG+JQqXSSaUKAV8oFopEQqGAx5NJZVqtTq3RKhSIhwMrgchyuVSnNZiMdovZaTY5dFqTVKICbNXpTGqNTqFUAROrVGqlUq2GMp1JqzUqVVq5Qg0J0FYkkjAR0QCp5VK5gCdgMuh0GhIpgs2mMVkAuPkUMkHI50slMrkcYFrB5/MoZDI0OWg0KlTlsNl8LuLGAU0RuPD4vGzgYAaVcP7M8Xfe/MOxd988f+p4ypmTqWdPnznx3qljR3C5mTwWnU4hkvF5BTmZGoUEQ2FMmD6soUSw+pJ4HB0LjwnTcy6Ag8Nx0n3w4EH/5ctAJGUPZ2TAhOlwNDc729nejk5EAuDyVFuAVVWVb7/xxhuvvfbekSNHjxw5d/ZMDnBuZsaZ0ycBf9NSz2emp6ecO3sa2Pb40RMnjqanncvJSs/LySERSXy+iM8TEgjEAhwe+C4nOwuMaBQ6nydIS03LSM8WCMQcDi8tLQOHJ8mVGgnCu1KBWMkXKdh8KeojIdfogYYFEiVsojA4DI5QItcrNTa1zqnS2g1mr9Mb9wZLfMESlzcmU5kYHBFfJFdpTBarx+eNedwhKoX9xhvvHjl6jEKl6w1mvcEqV+okco1KazJZHAajDThYqdSr1UaNxiCXq4QCEYfNAeqVSuVKpc5gsFksbqfTb7W4VEotcK1MohAJxXKZAugWaiDuDWIph8Xi8zhymVSj1hr0ZrPJDkCs11mUci3gvDzRiZvox9WpVFqZDHhYbzBYdTqzRmc2mOxmi8tidRoMZqFQxGaxxCKkV1giEvN4iG8Fh81gMqk0GrEgPwuPy2WzmEgFRFIel0siIo7LiShvSLxjxF+CQiISCvD4PHx+DpNKYtJI58+cOPLWa2dOHMtMPX/+9ElIQMOpZ09RSTgRnwMVEiicoZZjKIwJ00cEBHyxtxe+8g+5vw0Tps8rYNOZ6enuzk50XrdD0PTUVHV5OUDJs4pcgemF1ebGBuovAU/79WvXnt77uaqi8p033nz9j68dO/remVOnMzMzcThcbm5uelrK+XNnAIWzMtNhee7sqbNnTp47fzo7Ky0vOzM3K5tIQCbLAHDE40kFOAKXy83Py0tLSedyhVKZsgBHJBCpMrmazxfn5uFpDC6AqUJj5IsVAolKKFXzRHKOQCKUKlU6o1SpFcnUALgMJk8gkis0Zq3BZTD5dEa3xR70h0oheQPF3kCRweIVyrRihVZndDpd4XCoNBIqUalMJDKDSmOJJUqd3qzWGOHE5AqtRgu47LI7vLA0muw6vQXwVCpTAblKAHRlSo3WYDIDpDpNJqfLFfC4AwaDRaM1QjW5Qq1Wa7VavVQuk0gAWmV8JGAbJ+EgodEbTEaDxWREfCSAnpGwa2IZXArIazVGvc4MhK1SGwDKtVqzVm91wM59EY83ZLd74Og8Lk8mlipkcA4yLo9LpVGZTDqbRWfQSYDCBfm5DAYDiWgsEsJVhTYGgYCn0agikVgikcMS6RLG55FJBWQSjojLAxQm4/Pfe+fNo2+/kX7+XG5mRurZMylnTmenpaCOwmIxn8Ugk/G5+dnpGApjwvR4wQcDDbmPCdPzLKCBns5On8vVVFf3VH0V4ECriVAqwCJTk5PYDyaYnpUWFxcry8qKotHxsbG9ooNWRXnFm6+/8fYbb547ey4rMwsHDEsk5ufnZ2UgMJyZkZ6fl5ubnQWZrIyMrKyMnOyMXFhmZRPwFKlMLZEo8vIKCgpwLBYL/snOyuPzJQCzVDqbjcxyoeXyRHgChcUVKjVGFIV5IoUQwFesYPMlAolSrTfLgVCVOpFEyeGKZAodYK7R4jNZAzqjx/wIhYsgWR1BJYCmzmwwuwGFg8HiaLTc4wkjOGt2JFyHEY8IwFCtzgwZIGBAYafL73D5AIgRv2GNQaMxqtUG4F2tzgQorAf81ZiMRrvd5jbDqtEKPA1AbDCajUaTXKGQyeQKmYLHQQKiKeRyo8FksdjNJhvSMWyyAzFLpYgzQ2Ksmwn42GpxGo024GA4KzgBg8nucgd9/qjPH3E4PHKpUsgXquRKtVIJLEyj0bKyEG8TPo/FYdOAcYkEPJvNFovFQqEQriqVSgMUptNpCRSWCYUiGo2CK8ihUgg0KpFMLKCR8DkZqcDBp44dyUpLSYRyPpt29lx+ViYgMuxWIhGwmRQSLicvKxVDYUyYPqAHDx4s3rmDBUbF9CVS/+XLIZ+vranp7tNsvE2MjbW1tMxMT++tY8L0jDQ3N1cSj0cCgacX8b2iouLdd949eeJkVlYWsCyJRCKTyTgcDmA3G8g4v4BEJBEJREDkgvwCoOKc7ExIudk5BAKFL5CyWLzc3Dw8Hk+lUgGIIcvlS6RyDZsnFsvUcqWOzRFSaByeUKZQG5Rak1iu4YvlIpkKUiKemkZjsAEiy9UGmUILOwQUBsy12IMWW1Bv9hosPqcn6vUX+oNFkKx2v1pvUektBrPLZPU6nEG/P+bxhlzuAPCuze5BO4ABQ9H4EkqVHvJWmzvZNwxLm82d6CQ2QwIORjuSgZ4NeovZ7Ezwqw0KTQC7ZqtKpVKrtCqFhkFjcFhsnVZnt7kcdg/wrsXsBPBNDJiTKZVKndZgNFgsZgcgtc3qhpNxe4JwVoDLFovT4fDa7W6DwSwWSwU8gUal1mm0ANYMOj0nO5tAwAn4HA6bTiIWMBnI3BzA1rBbAGIulwcczGazBIiQmMMMBjLDM4OOTEdHJeMphILEILkj508j03yknz97/vSpjJTz+DzEd0Ii4sukQg6TSibk5Wena5XYsDlMmPZpa2urrbm5o7UVgHivCBOm51sLCwsDV64gAaeeWhgH+Fy0NDb6XK6ujg7MawjTsxU8gdf6+y/29j49F526urq0tLTMjExA4ZwcAFwC0DASJDc3D9CWCMCLjNmiEolEWM3Ly8vNyYH/C/Lz6XS2QCSj0phINSIRrHB4YGaaUKySKvQ8xM/BoNQYeUIpiycCLFZqzVqDHWhYKFVIlehQOa1CY9Ia7SqdWaO36AxWpJtZjjg/mG1+mzNsd0etjjDk7c5QMFgcBBS2euVqI9CwyeoxWtxGs8tm9wJ0ol2/SO+vExlIBzgLS53eAjtUKHWQAfZFE9T0B2Jud9BgtGmRsW4GFIhVar1arUcNzRYHlCC+E2abGmoYTHqNkUQg0yhUg97gcLgBba1Wl9Phg4xKqZZKpQqFQqPRGw1WQGGbFbY67Q6Pzx92uwMA+wq5RqPWJ3yIFULAWT4fONhkMMJSJpXxeTwelyMS8tksOpkETMzTaBDfDPgfMnK5EuFfRCI+ny8UClgsBoVMYLNoHDaDRsKTcHnp50+fPXns/OkTaedOp56F/Ins9DQSLh9BYTGCwmyAZmIBIT9bp5ZjKIwJ0yPNTE/HQqGg13tonpeYMD2hDiGQ2fb2NkBwNBi8fPHiZ/zOwITp6ekuPJGJ30CAhldXVtDCA1RzczMAbmZmZgbi/5AFKAxcm5uLTKgM5fn5+Wg/MWSys3Py83EAvvA/4DFfKJHIVADEie2IaHQmiyOSI8EfTAnPB4vWaBPL1ZBXac06owPgFUqAg5Vag0pnkqkMar01gcIWg9nhcPr1RodKZzPafFZn2OUtDETK/eEyuytitQe83pjXGzWaHFKFDqwsDr/V4TfbPKaEawQgLECwyx1Idg/DKoCvUqVXa4wJ7LRZrC6oabG5AsG4z4fsCihXqdIZjFab3aXRGuRA2QlWhmqAwgDEZrNdq9VZzDaz0cagMVkMpkFvtNncCRN3IBBzOr0qlQZxJpYB7erNCAe7AIWBoYGGPe6A3e5RyFUSsUyJOESoAYWlYqlEJNao1HBOkIwGo0GvTzhLSDhsJoWMFwr4wMFGo9lkMhsMRtg/oDYSmlgmE4lEAgGfyaRTKUQel8nnsanQbMnNAgIGDk45czL9/JmMlPOpZ8/kZmZQiXggZaGAIxbx6FQSLRFPTYfFFcaEab9mZ2bqa2pqKirQoO4HqJWVlYaGhkAg4Hgyud3uoqKi69ev7+0X01MWfOPeuHHjwoULvU+gvr6+27dvP9X+1LW1NXhon56PxMry8tX+fjScMCZMz4OAg/t6ezvb2zcOelrmlpaWxOwYZ1NTU4GAKRRKYgrl9LS0NIDjnJwcIGM8npDg5BwSmUYiUQg4PIPBFInlErmazRUwmGw6AwpYQrEMOFWjt6t1VqFUpTHY9GanTK2Xq/U6k9Ns81rtPp3JodAAAZuUWgSFoQ6gMECz1e4JBOIeb8wFyBsqCUYrwrHqWFFttLDGHyp1eaJOZ8ju8OsNVqlCq9SaTVaPzem3uwCIPVabG3WBQPuGPd4Q2kMMfKzVmRJTbFiAbtEOY6vd7fGEHHa/yeQ0GBDHYovV6fb4DQazXKY2GKx2hweAGEwsFofFbNfp9Farw2pxioUSkUBk0JvsNo/F4nR5AuFIkccbhApyuRyA1WS0Ohxel8vrcLgsFqvFYnc6vGCoUKiAYxWJcXhAw3KZQg6rMrlWrTEbTVazBZJRb9Bq1EIBF1CYz+MiU4NYbGazRa83ghVAcIK2EX8JHo9Lp1OpFBKPy+Jy2SS4GXnZ2ekpif7gU1lpKQDB2elp+LxcGoVIp5G4HAafxwJ0ZtEpTCoJoBtDYUyYHunBgwd3FhamJicPds6CK1eu0Ol09DUK79MnEboHeE0DFmND+J+qgFwrKys5HA6ZTIZvPvgu/MICc9gJj8drbGzc2/uBCggVgKC2svLAfXnhIsAnAh2Qdwjdz5gwfXZNTU1Fg0HkR7zbt/eKDkitra0AwUePHgUazs/Pp9FoOBwuJSUF7STOy8uDz3R+Pi47OxeHI7DZfAaDDR9zeMnz+WKhVMEXSZGAwTy+UCTWGszAvnqTS6U1CcQytc5itLpUOqBeI5TbHAGHI6Az2hMobIZCqVKv1lsBjiG53KFopCwSLY8UVsdK64vLm4pKGwtLGqKFtYFwmT9Q7PPG7A4fkKtYqpYpDTqjw+rwuX0htzeE+gqjfhEWqwtFXsBiWEUGycHRDRZgZbcniHYbQ8ZsBg52AA0DW9vsbp8vCNApEsr0OrPD5TOaAJGNZrMD6Faj0VotdmBihUyplKtMJivsHzjb548ACrs9AYPBpNMZTEabzepxA4h7AIUdwNEmkxlo2KA3S6VyoVAkkUgBahEaVqiUcgUkQGEg9GQy6vWAwiRiAYfNVqm0gMKwBzg60LNIhAyhS3QJCzgcNi0xCzSLyWCzmMg8zHhk9uaMlDPnTh3LyUgFFM7JSCcU5AP+UimIHwWPy4Q8ErqYQdVrVPc+Wz8FhsKYMH1BXb9+Hd6q2dnZJSUlg4ODY0+m4eHhzs5OpVIJr2mj0Yj9Wv2UBBfW6XSeOHECvgJtNlskEolGo7BEtT//qYLK4XDYbDbn5uaeOnUKvrv3jnFwAlotLSryu903D/rnAmDrhtraq/39hxO3GBOmz67l5eWqsrKyoqKJRLDhAxSgMIDvkSNHAIXRYXM5OTnwGoclfIqhpKAAn5dXABxMJtO4PDGXK4TmLoVCYXH4PKGEL5bx+GIgYZlCaXW47e4gQKpIomSxeQq1wWBxqPUmhdagQ6IohF2usM6QQGEDoLAJUFils2iMdoPF5ffHCwurYoXV0aLaeGlDaUVzaXkzoHAwUun1lwRDZYFAEaCwUm3gi5RiZJoLewKFw4Ckfn/E5w07HD6T2WHc8wm2WW0us8WhUushmcx2gFevLwwQDAkywMo6HWAuEmXCZnN73AGZVMFkcvQ6E9AtmKvBymjT600qFeCvxWS0qFUavc5gNtvAFngaOW4garU5tVo98LHd5rHbvC6gZKfLagUSNptMJoMB6dMViyUAslKpDIgZgBuZg0Oh0mm0eq0OIB2S2QgHMMMql8MiEgqAcSUS4G6VTIaETuPzBSw2m8fj8vl8LpfDQSZ4phDwBTQqhcVkkkkEConApJGz01NOHz+ak5GWkXL+zInjOVkZsCsyCQcozOUgvsVUZIZnYqJX+MuMwh/upfjMnRYfMvyE3o5PrnlQhge1n/367DU/pMM3fE60vr4+NTGxvLSEhq48EG1tbZHJZAKBMDMzs1d0QKqqqnr33XcrKir21jEdqODCvv3224Ct8FTAk4w+zGjm8yppCN/c8G3w5ptvNjc3Jw5yYILHrL21taGmBh7gvaKDEHwQ+np7vU5nRWkpnPxeKSZMz4fg+RwbHR0fHT3ANzYq+IQCCh8/fjwzMxOPB+rNy8rKysjIyMzKBBqGEoBgAoFModApVCafLxEJZWQS4kRBZ7K4fBGHK2Sx+RyuQCJTWB3AiCGt3ioQydlckVJj1BisAL6QAIXd3rDXEzGanbCqNVrVerNMpUvMCYeEg/D7Y9FYWShaHopWhOJVRaX1xWUNQMZef5HLG/P4Cx2ukNnm0RhsYplWKFGrtBYzoKcb6RL2wp4BhZ0+s8VlNDpMJiSkmsXqBBrWaI2QbHBwBJpRFEYo1uUO6g22RMw1oGaX3eYUIbMis9VKlVaj02iQ7luT0Wo0mtVqIHqj2WSBpckEzXyb3eZ2Ib2/QafTp9MbgVlhO/C0w46gsN3uNJstNhvgMNhqFAqlVCqVSCRQzWiwQII9q5QqAF+hQEAkEEQCvk6jVikUcqmUy2GTSXgWkyYWi4B5c3NzcAX5dBqNyWQJBFAdCTMMTMxg0PD4AgqFBNBMIRMhcVgMYkFe6tlTORmpZ0+eeOu1P6aeP1OQn00k5LOYVLRXmEzEUYk4pFf4S+cgAWe8srw8OzOzmXAP2trcHB0Zgc8D2mkBm4Zv30bmxE98Nubn5mD1zsIC5JFwmBMTI0NDa4nomNtbWx8wXFnZM0x8daGGqMNo0hANq7lnODKCuu0jhkND05OTjzWEcti6Z7i9DVZJQyjcb7gwP580hJNHDWHnHzBMzL0OhnAyU5OT6Of/zvw8rMJyz3BqCjnVhOHd7e3xsbGk4dra2sjw8CPDhYVHholA/Y8M795FDeErdr8h+righnun+tAQdSIEQ2ig7zccBcOJCdRw8c6dpCEITJaWluAkkb//ORCcW2lhYWdb2wEGU2tqajp27Njtg/79DpXRaExPT0cvNaYD1NzcHHwRwgt+b/3gBB89mUyWm5uLvhMOSvAxhI8VfJzQl8lBCT621wcHS4uKgLOxXmFMz7PgNfgZf+P+LAIUTktLO3fuHA6HA/DNzs4GAgYOTs9Ihw8vEj2CQmMwkKnWGEyuQABUp6TRGHg8gc5gsNk8Oh2YTcjliURiudnitDu8Or1FrtLLlDq1DnjXKk/MgQzsiwT61Rh1CTiGVY0BUFgLeZUOONNtd/jkSp1SY/IECgPhsnCsIhgp9fjjdlfQ4Q45PRGz3Wuxe63OAJCtSKaRq01mq9fpCgHUAuYis1cgw92cFovH4fAbTY6Ep4TXgBwX8Qb2+kL+QASWSCYYc/vCJiRMhNlgtJhMVp3OAH+YTCxkUMnpKecZdJoG6Qw2Wyw2wGKdTod081osyDg3vclqtifo2W21OnU6PRJGDfEntjscbkBhq9VhMJisVqvJZJIjkikUCpVKBYYWM2wCFNYokCDEGiqZ/Nabr+flZomEPA6bCf/zuRw6jczlMBRyCYvFOPLu26dPnaBRSHCJkbMTS3k8vkgkYrGZRCKeTqeykVASeAoyhI7DotNzs9IzU8+dPnbsyFtvZqSewxXkkIgFgMJ8HptBJ1NJeEBhg1b95UBheL8DVKGu8UDAACsVpaVoeG1g4ng4XByLoSHfr1+7FvR6G2pq0BEqXR0dAY+nr6cH8jt379ZUVIR8PiAeWIVvjv2GN69fRw1RTEQNYbnf8PatW0nDwkgEjbcFhrCprqrqsYZQnjSE+mCVNITC/Ya9XV1JQzh51BD9uTNpiEIkalhTWYk68F3o7oZVWKKG9dXVsHrrxg3UsCQeB0PgbFgFnI0EArUPDS/29iYN4Rwaa2uThnBNUMO52dn9hihSo4ZwF5KGYb8frjxqWF5SAtdnv2F1eTlqePnixaQh6EJPT2NdHbQrIA8QD/cXvRTPSlcuXXJarVXl5Qflgwt/DpvN1mq1e+sHrZGREXhZd3Z27q1jOiCVlpaeOXMGbYgeuKanp0+cOAFtpL3151s7Ozvwpk02XzFhet4Er9nJiYm+3l6Agb2iJ1Zraysgb3ZONoVKIRAJSLi0RPiIfFwBlUpnMjl0BofDFXL5YhZHwOOLAXlZbB6RRKHRmYDIHI5IodQJRTIoN5qsRrNdq7doDVY1gK/BZrC40FFxUqUuJ4+QnYOTyNQJFLap9WaxXC1TAQoj/btanSUlPTsfT3V5o6FouTdQaHMGzDYPbHJ6wi5vAoUdPqc3bLZ7UOfjBCKHnO4AcLDbEwDehZ1YrR6nMwBomogpAXCMxA/WGyyQ9/mRjmEA4mA47vEGzRY7QLDRaAYORmIDS6UKiZiIyz925B0iAadRAwoD0dqAWSGZzWaDwYDEcBCKFVJkNg2D3mKzAXlbYSsSrc1iAwgGRDbozWq11mg0GY1GqI+iMHwzmkwWo9ECBwKilYjESrmCz+Wlp6YQ8Hk8LpPJoDHpDCYwOJXI57EUCqlAwEtPT4UmCZ1GSaAw0rXM5/PFYjGbzSKTiWDCZtEAhWlUkpDP53G4uNycjPNnz508cf70ScBiQGEyCcdhM4QCLpNBhT1TSTi99svQKwwcDLTU1tx8Y3AQVgFTmurrS4uKxkZHYRUgD7AYYAv93rp98ybQLVRGG4gAbUWxWP/ly5CHdzoYAuGhDH3nzp39hsO3b6OGKI2hhrDcbzg6PJw0rCwrW0owNBjCppbGxkeG0WjSEMqThlAfrJKGULjfEDAxaQgnjxrCzvcbwqERw5EROLfy4mKUL+Gc4cmFJeQBhUsLC2H1an8/rMIXGOA1tAfRdwRgrt1shj8THWYOSJo0hHMAhE0aAkOjhlOTk/sNtxKMiBoCxSYNbSZT/6VLqCFgsdtuTxo6LBZAatSwu7Nzv2F1RYXLZgMAhVVo4UCL5drAwPqzm95ibGQEzg1uAXpHnlzQfjty5EhHonnzNATnyePxnkbn5YsseCfC61Uqle6tPwXRaDT4CtlbOSDBSwNehhPj4wfYkDvYwaOYMD0NbW1tNTc2whfWhUSf14Govb0dGSlLJNAYdAKJWIBDggcXFBQwmGyhSCoSKwRCuViqEkqUHL6ESmdzuSKBUEqlMWl0NpPFVygMRpMDUFgsUWi0BhXSy4v0ASu1JkBhk80DNKzUWoQSVUZWQWZWvggJrGbSGZEAwwKJUqLQAQoD16q1ZhyRRmfxgW79oWLAX5PVbTA7rXa/L1jo8ISgjtHqcnpDNpfPaHVCxuOPurwhR2IYnN3hAeRVwE7N8D3sUqmNAMF2hxuWBqMVUBhA2eX2e30hoOFAMOJwunV6o8FgBA4GTlWrNUqFQiWXiQR8QEyRUKDVqC2AtzabVg1UrDaZjGq1isNm0ygUWCrkiLeD2+31eHwA0wYDYLNFrzfpdWa1SqdUqhMlZqVSKZfL4Q0LRCxXKJHJNfhCPlcgFooUMrlaqVIq5Ao5MrWzgMel02gkIoGAz0/0CoOJDMAXqJdOp7JYTGQWZqEQvgThHxaTTiLiGXQqAC6ZjAMUFiRQuCA3Jzs9JSsNSXk5Gfl5WbA3Nosh5PMYdArSMUzBP+8ojKIeoHB7SwvQVWtTE7ydQTPT04Cz62trSJ3tbaCu6YdD/tdWV+HLABAQ/aEQyGxibAzt9wVDIELYiv78DTj4AcO1tY8awnK/IUppqCEk9BdD1HBudvaxhlD+yPDu3f2GULjfcGlxMWkIJagh7PwDhgmEhfOHZgBQI9q5e+vmTeBmWEIebicgNVwoqAyrcDW6OzqAXFG3B/gr2pqaLl24gHaZD926lTR8cP8+IGnSEA6BGqKXLmmIXivUEO0GThpOJgYugGFvV1dHa+sjw+ZmaK+jhiNDQ0lDONWeri7AaDSCL9SMhULRYHB6agpW0WtyyIKDwvkfYJTKxcVFPB4/ftBDOvYrEonY7fa9FUwHIWh5MhiMkpKSvfWnIJfLJRaLD9azBZ5baMhB+xn9CD+54F0EH+3p6Wl4j+0VYcL0/One/fvwjRP0enu7uz8j0HyqOjo6yGQynkAgkkkFeGSWDMiTyZSE+69KpTFr9XaN3qbUmPkiRU4eAfBXrtDy+BI2R8TlSfV6u05n5UNduRqSRK5Sao0ao1WuMcjVRq3RDkki10nlOrlcL5PrFEq9WmM2mJywQ55IgaIwJIBjk9VltCAx15AEHGyyaXRWq8Pv8ceMVjfsVmeyOTwBSEDDTm/Q4Qk6PUGgYQ8yDM6NBAZWaPQGK7BvYqicw5oIAGw02dCUoGGvxxvwev0ms1mlRlwfAIKBcSFpNRq9VqfXavU6HYLGarUVDu5wQCHkAWxVKgWPy2YzGSIhX6/XOxzuYDAcCIQAmPV6o14PSG3Qaow6LRC2CQqBvmG3wMBCkYgvEHC5PAaDxWEjLQkAXZVCAbs1A0cbTTqNVioWsxhMCpkM8EqnUWQyiUKpFEskXB6XBijMZj1CYYEAIJhIKEigMI1MwlPIBA6bCe2S/Nyc/JxMYkFufnYGoHBeXhYel8+g0/g8LsrNQMNGg/Y5RWEA3OHbt+FFjPJu/+XLhZEIMBbmr5YU4CzKi7s7O9BmgCVaDnwMq+h9he8woGcQ+mUGhbAJruGe4e7uxxlChU81TPYY7Rkm8PqzG0LJyvIyfG2j7QTIlBYWlhQWAj7CKpAxMPqBR4s8ZCFhL/v6nmrIs7GxseczxvDOvbtbO+sbd1fX7y7DcnNnfefe0wp5e7CCB7Kqqmo48UvOU9LAwEBDQwN8TPbWD0Krq6vwknTZbOhPSU+ugf5+wIuWpqYv+8cQ05+8piYmrl65gvoBHogAhYlEYl5+Po6Ah0SiUIgkMpFIoTM4ApFCpbUZzB69ya3S2XgiZU4+kcuXavVWhcogFKt4AoVGZwPAFQhlSpVeLFWK4F+dCZhVrjWIZGrIawxWoUQllelsNh8kQGGNxmyz+9Q6K+wQjaem0JiAtt1AtE6fRm9Vay0Gow3ZoUyjMzksdp/WaEsEJwbi9QMEQ4KM1emFJRJEIhC1OzxanQmdLwOSWmNEJli2I13FqIMEbDUYLHa70+l02+0OYFmNRqPT6VSIlMCsiWmXLSYDwqaAv0qFEvDXmUBhpFfYbNKolVKJSC4Tq1UKs9nk9ngBhT0eH3CwWg1QbQACRiZetthtNqfVakd8JzQapVIpFAmBaNlsNovF5nGAZVEUVsJxDXo9HFGr1kjFEj6Xx2IwyCQCjUqWSiWwWSyVcHhcOoPO5/MAqaVSqYAvABQGAkaCqTHoLCbUx5OIOCBdBo2GK8gj4vOpFAIuLzsX6RXOJuALoBz2jHIzi0k3mwwo+XyqDhuF52Zny4uLY6EQOsn40uLi0K1bi3fuYP0Tf6qCr9vbt27dvHEDcBnU3tICd//a1auHdseB4CfHx2emp9Gu98PU/Qf31+4u31mfmlsdnV8bXdyYWt9euv/gYLo3DlO793dWtxbn1ibGl27emr98fab36nTXwFTH1anOwZmem3OXRhevz6yOrWzdubt7kB2imEDw3HZ1dLQ0NkIzcq/oyXShp8frdLY2N2MdEJiecx3410R7ezsOh8vJzSEhMyxTSWQqUHF+Pp5MYXB4ErnKrDO6tQaXQm0RSjQ0llCuNiQmjXNI5AaBWK3W2SRynUQGOAlwrBDJVKrEIDm5xiCQKtSAoSa7QKwUipUWixuSRKrSaM12h0+FdDOr5Gpk4mWZyqBQG52eoNXhlSn1SpVJr7cCWPPFCmBlBJQNNkBqk82F9ge7/WGAYMg7PAFAYdRRGJBXowUCdiachoGakQnndHqLWmNAZ1c2mSxWJFivxWg0AQQDDQO9AggnmFgLdJsIgWYGkoVNEokUttttdrVSrVIqYatOp1GpFFqNCpJep7MikYq9FmRcnQ4S6iOBHMJqN5utiXATJq1WCygsEiETJvN4PKFAIBIKEf8SsUQuk8MmrVan1WiVcrlEJBbyBRwW8C2Nx+UiZ6XTKlRKgVDAZDHBBkoUCoUQxOcDAVMpJDaTCXBNJBSQiAWAuYiXMQEHHMygU/C43Py8rIL8HCiBfcKemQwgaiqLSYO/8LlDYcAgWK6urFSVlUWDQfTHdEwviNDO44319eqKCvgavnL5MlpyCLqzsFBWVFRZWnrII4Q27q6OL12/Nt3eN1bdO1rRN155ZbL25lznzMqNzZ0Dc9V42traWZ9dHQf8vTzZ0j5c1nyrsPlWvAnSzRikRkg3oki6GWu5Xdw71jA4c2FyeWht+08hPtf27sby5vzSxvTy5sza9uLuM+r8hk8KfHC2t7cP6iMzNzt78cIFdEgGJkzPv9ZWV6EdeCAtN0DhvLy8jMxMIjKBMjUvryAzM7sAR6Az2FyeRKmx6k0ejcGl0TsRGpZqNQa7zuRU6+1ShRFoWG9yAcjKVXqlxihKuBRL5Bql1qhIoLDGYDaYHQLE4VhmMNr0BiTOmgoA2eKUKXQ8oVyls5qsHqlCD6xstXsAhUVStVSuU6mNfKFMKFVJFDqgYcBlg8VlcXhMNjeCv/4Q0iXs8rs8IZsDCROBdv2iw+NsdjcQMGScLj8cFDZBBavVabM5AFKBgYGDkS5cEwC3HhATlkC6NpsNKgENwxIQViaTI6PmrA6FHKqozWazXq/Twl+v1yForFDATsxmhKpRAob9AddqNFoAaEhwFJSzgV+lEqlQgBAwOskcJOBgKFdDO0KHdDkroAzqICjMBg6Wy+VwSmAPnCyTyTgcjkDAR3clSAgImE6nsVhMNnsPhVlMIGEyiYSn08hMJg0ImIjPpxDxdCqZx+UIgJ4ZDAYN8ZF4vlAY3uCTExODV6+iNHzrxo1rAwNPaRA3pudZOzs7N69f72htXUhEwVtaXBwfG/uMT+oX1tzMTMjnC/v9QAB7RU9fK1uLgzPdnSNlncMl3SOlvWNlfeMVlyYqL09WDUzVjt7pW99GfMefZ+3c255bHbs+29s1UpkgYBSCY8DBH6LhhhvRhhuReiRFITXfLr402TaxdHtr58v6+/v27ubM6situb6rU839kw1XpxtvznZOLA0sb07fu7/nPvSlFryTn/bnDhOmA9HK8jJ8ZdRWVqLfGk+otra2/Pz8jIwMHA5xGM7Ozs3IyMbjCSwWhy+QqrQ2HaCw0WN1RowWnwIhYxcUylVmmdIkV5s1iCuwVq4yaA1QaBBJlJCUaoNMqWMLRIjvrsUhlqlgV2qNUaHS8wCQlcCJVrFUBXmwsjp8MoUesNtsdZmtbr5YKZJqZXIdmIhlWrXBChAMSWdy6M1OWFqdPpvLZ7A4oT7Qs9Fk1+pMcBizxQEJRWG90QocbEeiqtkBu6HE7nDbbHZgVoBMAFCj0Qh0i/baArMC87pcbpfLBRmUj+FvAHq229xQF8gZ6gO5qhPBIhQyBaAwWCUEvGzX601yuVIikQDFJhDZDEgN+wGQBQH7SoQiiUisVqg0iFuyWgPIrIWEhGlDIlSo1Cq5AlCYz0VGxSVOCQ5ogr1DZYBjNpuNjm/m8/lCIRJdmE6ns1gsQGHgYDIJx2LSyCQCgYCj0agMBp1MIsI6k0plMWgcDpPLYTHAgEZj0KkWy/OEwncWFqrLywsjkYnEMCMA4kPrEcT0vAlu/XpiaoN7u7s9nZ1lRUVDt2491edhdXW1t6sLjnWwAV8/QWvbSwNTbe1DxcDBPaNlF8YqLk5UXZ6suTJZMzCFpKvTQMM9G3eRAYjPp1a3F28vXOoerWy5XfgwFbXcLk4kyCDpIR8XNt0ELEYSYDGgcN2NSO31SPOt4qvTPXfWZx+8/yX7sK9tL9+c6+serUjcPmjGlCeaMVVXJquvzzbPrt7cuXeoTiDw6VheXBwbHV2Yn3/y34vXVlefqo87JkwHq7nZ2Vgo5HO50OicTyhA4YICXG5uLpAUgUACCM7Nzc/NzSOTyTQ6iytUAP5qjV63r8TlLTLbgyabX6VzKLU2KIckUepYPIlQqtaZnFqjXSRVon7DPKGEQKEJJXK9ySqRq7k8sVypkyq0fKFcrgAGNEtkaiEYm2xWp1+ltcgVepc7AHSLzKusMMCqSKxUqIwAymj4CKXWrNZbIWNxeC0uLxC2zmgzmQF/EY8IIGC3J2B3eIB6AYiNJpvbE0QCDBvtsBXKHU6PBWFW4FqAUI0OcibII/24ibniXF6Pz+v1AsUincBqtVart1jsTocHlomOXj0S7EGugPoyKdJRDDVNJhMy7M2ITK3Mhb9QLAa6tVptTqcLkHwPhWEbLEQiqViMxmVTwBXVqA0GODjipAGCvei0WinAbgKm4fDAwYDTUK5SqtgsFtwLoGF04mU4Co/Ho9MZbDYHnZKDQiawmHQykVhQkEehkBk0JM+gUthIgLaEizDidkGjUwGFaTar+TlCYXiUi+NxJLRtwj8YEybQ6spKZWmp1+m80N39tP2GofV19+GAv6etnft3r8/2tA4VAkj1jpZfGKtEOHiiOsHBtVen667N1A8iqW5y6fLu4ULVZ9H9B/cX1if7p1rbhopabxdCgkzbUHHrEMrBn0jDt4CGkQRADDQMqXOkZmzp1s69L41P6sbd1WvTnfD3dgwXP2zGVKK3rx9pxlTDvZtdHdy9f3h/ETy3Vy5dKkwML04Oaf1igg/ClYsXO9raDvMXEkyYnkTra2tN9fUNNTWz09N7RU+g1tZWHA6Pw+FIiIMEmUajARFnZWfn5uXl5uPwZDpXpFRq7DZnzOkusjqjRqtfa3LrTB6lzgEoLJCoaCweVyjTmZBgEVyBhM3li8UyCpWRm1fA4Qm0OpNIrGCy+CoNMtcGTyiXSFX6ROBhuRrpM7Y4/XqLy2L3+YNxm90DGK1UGuRyvUKhN5ndbk/E4Q5pDDaZygAobLC4zIDCbp/J7tYnenwdTp/T5fP6QoFgDPAX9mC1uYB9A8GoxxMym51mix1WPd6QxebUqHWIo4FSrgEURlDTZDbZHU6/1xMM+kKwF6BbwFyj0WgFPLY5nADQFhuQ+x7XypBO38SUyDKgXovFajIhUYQBd4FjDQaD0+n0eqGR4nXYXcCyKqVCJOTzeByhgC+VIqCsUmmEQpFcLjOZEA8NMDECiQOfmpHjJhAc6SpOdCsj/dMikZBKJeNwBXBfhEIhcDCcA48noNOZXC6fx+Ugw+woZB6Hw6QzcAU4BIXpVDyugEohAx/TaRQGg8ZmAxLTGDQGi8F8XlAYDQoGpzJw5cqFnh50NjhMmEDb29uX+/rqqqvRIGt/MppZHekYLk2A1B4HX/ogB1+fqb8x2wDp9lzT0uZTDMf2BXTv/u7k8q0L47UJCC5qHy5uHy4BLkRT6yMU/nQabrgZBxSuGQw13yq+OXd5+8swnO7e/Z2bc32A/ujt6xvf14yZ3rt912bqbs41Lq6PvH9Yvd2AwgDBbru9pbHxCd0lof1ZXVHhd7uvDQzsFWHC9Hzr3u4utNyWDmjafEBhPJ6Ax+MTKEyi0+kkEhE4GIcnEElUKoPLFSrEMr1a59SbfHqTV6N36EwuvdmN9gpLFTqeSCaSqbRGK/zPYHHpDCaLySbgSfn5BSwWR6nS8nhiJpOvS0yrweFLAYWNJrvR4gATo9VlcviMNo/LGwmECgFkNVqTTKYRS9Ravc3jCfv8cYvNp9JZgIONVrfF6TM5PGYgVIfHiHg+eFwuvwPhz3AoXOgPRF1uP5CxxxuAvBfBX5fJ6jDZnDan12iyadR6pVwhlki1OqMViXdmc7uDoVBROBQPBSM+n89mswEHw9LpdDmdbpcLQWGdDnH/BfxVKBSAqBqNRiAQQN5qtQEKA7wC0FqtVrfb7UcUACuwha1ymYxBo8H15PG4CoUcqgGBA9DKZNKEW7IFcbAwIW7KILSLGnUyhk1wGlKplE6nEgjQTsGxWCzUR0IuV6AozOcLALIpibk2xCKRgCcgEaElQ6VRKQV5uVDOZjGZTDqHw+LzuUwGg81kAQqbTc+BgwQ05oCA5xNhULa3tg42wBCmPwFtbW3Nz83dTzypM1NTaMTiA9f6+vrw7duT4+Of8SPxJNq5dzfhGlHUPQIcXNE3vsfB/fv6gwGCgaVuzzffmmucXLq4e/95+Vzs3tsZX7rRPVrZOlTUPlTcPlSCpH0o3DZU8lEUfgwNJ1AYpeG6G1Gg4Yabhddn+7Z3n/ff5WdXRzuHS+FvTzRjKi4+vH0fbcaM3Onc3DmkoYEPElMRtbe0XB8cRIdbfGEtLy8DT5c9nMYIE6YXTQkUxufn58OSSqUCchEICBkzmGweXyKRaWRKg1iuk6stWoNTrbNL5HqZyihXm0RSrVxlVGlNcrVOodHpjBaVWk+jM6kUKoVCJeAIiXk6mDKZksMWcrkiHTLnnFYgVqg1RoPJbjA7DBaH0eo0WJEu4UC4OBgustoQFAZWFklUBrPL64v5fDGr3W+0uBKeEmG7O2i0uw1WJ2KYiByc6AZ2+/zhSLQY8Bf1FYal3eGFcqgGSaM3aXRGjcak05o0ap0M/gC9xWqymc22YCheGC+LhOP+YAhQ2OVy2e12h8Nhs9mBaD0en9VqB4QFPAUMTeCvFZYcDgcI2Ov1Av5CCZhAxuf1+b0+wGfgY7PZolapaRQaXAc6jQ62Wq3GbEY8MfQ6o1ars1jMCTLWQ1UwBxQG9oV9gqAQDqdUKuEoJBIBEoNBF4vFcrkcaBiOLhQKGQwEhfl8HpVCZDHpMqlULJRQqTQQhUTCFxTQqFQ23EsWE2qJRCIOm81hcwCIzUbjM0Zh4Jv+y5ejwWBHaysGwZg+WTPT09UVFV3t7U/jUZkYG0PniEYnxnuqWtyY6Rmt6BopQUNGAEhdTnAwylLAwdeBg2cbb881Dc03D803jd5pX9t+Lma+ffDg/vjS9e7RCkDejuGSRMc2QCGCwh+k4SQKP6LhJAo/ouF9fcP1N2K118OwvDF78XmOQHz33tYA4hZS2J3gYPT2PbYZc2uuCZoxC2s34KLtGT9l3dvdhY/Gzs7OE3rVwx7mZmcBrNGY35gwfSkELcD5ubmpycknbAqCAIXz8vIyMzNhCSDFYDAAYclkMjKCSySXyNQypV6q0Cu1yIA5td7GF8nZHCEknkAmU+hUaoNCpZWpVDqDWa3WAUwTSQQyiUjA4fPz8mFvIrGUyeAI+GKN1iiUKqUKjcnsMJqdRovTZHMhRGtx2V3BSLw8FC0xWVxqrUmm0EoUWkBhtyfi9UY93qjbE/b4oh5/zOYMJDwxrEgyWGEnQNVmi8PjDYYjRV5fONElHHR7gmaLU486E5udWp1Bi3gJW/VgYraZjDabxWEymI1mazgaLyosCQYjvmDQ60NAFqA2Mf+sw+v1+/3BJAqjvcImkwl4FCAVMtFoNBKJOJ1OAFmoD1Yuh1OjVnO5HJFIwKDTszNzqGRGYvI5RBaL3e3yezwBqxXpA0bH4cF+AIWRLuLESDuUhoGDBQIBHIVKpTCZDLFYpFIhXhlwaNgKYM1kMpHAakIBhYxnMmhyqUwiktDpyOg4KplCIZGRbmAW/MeGSmDF4/HYbDbcDovZ8oxRGB0U5Xe7G+vqsFEamD5Zg1evQvu0OBa7cxBjhD+k0eFhp9UKO998+hMKjCwOdI6UPPpt/SMcDCwFIAUcPLzQMrzQCss767f2jJ+p5tbGesYq24eLO0dKO0fKUBR+LA1/Jqfhj9BwzfVww83C2wsDz21Y5bm18a6Rsq6R0occvHf7EA5O3L4kB6PNmPHF7u1dzOMLE6anLvheqKmsrCorW1l+0p9iWlpasrOzU1LOZ2ZmkUhkAoGQm5tLIpFYHC6TxWGyeVw+MipOrjbrjC692SmRKihUOoFA5nJFcoVWpdbL5EqxWKJSaSQSGR6PKyjII5MIJDyhIK8AiE0kErFYbLFIptUaxXKVQq23O7wub8jqROZPNtncJpvH6QEirQhFS40Wt0prgqTUmgCRXe6Q1xMJBApDoeJwpNQbiFsdPpXOjM7iASisM1gBnbVAxw5PMBT3B6IAxCWllfHCUpvDZ7a53K6A0+43m2wOlyceLXW7fQ6nqzBSGAqEkIBqZmsoHI1FCwOBkD8QBJZNOPt6A4EAADFkgIYtFiS2mkajARIFQjWbzQDE8EcBucZisXA4DBCMdAJbLC6ny2w0cdis/PwcFovOYjCBg2USldlsg+1wNJPJ4nEHAv6wzWZHRu8hg+g0RiMSgyIRw82q1+vgELB/dGwc0DCPxwXehRKVSi1LhGCDAwEWs1jI/HMA3HhcLomIl4hEQoGITqcBN8NxGTQ6XHkgaXSwHZx5Yg5nNoAymD97B4m52dnOtjYAkb11TJg+RrMzM7VVVd0dHU+DVuE1Cs/h5YsXn/aEAgB512e7ukZKHjtULsnBtxMcPLLQOnKnbWShZXYV6PBJuzo+Tvfvf6ZOxOXNuUsT9cDBHQkO3o/CHYDCX4yGH6IwJEBhhIYTfsPTK58yDHxlfWfr7iH1tib14MGDoYXL7UNFvQkORm8f0oyZrnvUnZ/gYPT2oXdwZeuQ3NyXl5fHRkcRb6Inc5fc2tpaXVnBZtbA9OUSfEGEfD5ITz7LTFtbW15efkpKanZ2LpFIzs3Ny8jIKCgoIJGJODwuvwBHJFGBhiVyvdbgtNp9SpUWTyBmZGRRKUyJRKlQaUViJMiXQCBis7m5udm5OVlkMpFKJhNweMAvqVTKRXqYJWq1Xq7WaYCmXf5gpMjpDerNdovd63CFvL5YNFYRiZWbbV50EmZkkjmHDza53JFQqKSwsDJWWAEoDPWBkhUag8kGXO5Q68xSuUYqU5stDkBh4OB4YVlZeTUs3b6wFyHj4kAgZne4w9HC/r6rxeFYYcA/0N5RXVqsgT/IbPEHQkDDwWA4GAihncGw9Pt9kAFqTIxdQ8IDA6GiTAlFAK9qtcZms4VCiE9FAoWRmg67U4M4RVCysjK5XA4YGvUWk8FmNO519+p1RrfL7/OGoD7QM+wh4VvhQlHYYjFrNCqJBLBWLBSKUI8IOKJMJgUSVqt1Wi3QuxEMgKGBdIGVeVx2TnZ6ZkYak87gsDg0KpXDYfE4XDaLjfhrc9hAw+h+UBQmk8lGw7ObbQ6+VJDhcYkf8tbW1g7BQRPTl13wBQ/vuGSwswMZHpEUPJC7u7uH8Bxu725cm27tGS39uKFywFIAUkPzzSgHjyKpdW7t8oP3n9aQsq2799qvzF8b/qQY3nDagzMdHQ/7g5MpScOPcxr+3EPoAIVRv+HOkZqVLWQK7o9qZ/d+08U5V8Xw7J3DHmO3c2/z2nQbNGOSHJy8fQgHz9TfnN3j4L3bl0gLa4cREgce4GsDA4XRaFd7+xPOmDgyNNTU0HC1v/8JHS0wYTpMQVOwraWlvbUVnb3/SQQojMPhMjIy8XgilUrPzs5JTQUszs7Jyc7MTAcBHDOYHKFYpdLajCaXWCLPzMo+ceJUdlY+IK5YquDyBUBmTCaLxWLn5eUCDVOpFBqNRiaRgMBkchkb8VJlS2VKrdFiNDtcrkAgXOjwBIxWF8BuIFDk9xcC70ai5TZnAJ1Nw2Tz2FwBqyNgc4bCsYqSstpwrMzhCZntXrXOIlcDCrshD5UlMo1EpjJbnJFoMaRQuDAaKwEmhkNECsviJZXhaLHLF6yuaZgZnqwNR6s8zuHm+saisMWoM5pMbo8/FImFw7FQIOR2uwFPXS6nx4N4ACPwmhBk1Gq1AGBfJAKKdTrdFovd4XADQHu9XqfT6XDAwmO3O+UyOYvJKsjHCYVinz8YCEQcdk9i9g2tCgknrHc6oL5Hp9Pb7YgDhs+LdBLbbS6j0Www6NVqVQJ/5cC+AL7ocRVKpVqlMRrMVoBtu9NmswOaJy43E65xZkbKubOn83PzKCQKlUoFOubD/4lQxCDIQFME8BoyTAYDj8Nr1ZpnhsJ3FhbgkR26devLAcGP/0bAvieejba3tm4MDk5PTu6tf6m0cXfl6lTzhbHyD/iY7vtt/fYcAlJohyJw8Nid9rE7bbOrF+/df1qOm8vrO2zblUxpb3n75NLqY/oCHzy4P3ZnoAuB4NKEe8AjFP5UGv4EFH5Eww87hvdo+Hq09nq4f6rrQ+HV7t17MDC0bCu5/S697Ti7Y2B4GbB4Y2t3Y/ve3d37u/cePEz3nxLDbewsXZqo7R0rTw6V+9DtS3Lw3u1bQJoxsyvAlE+9Axv+5EsXLngcjsa6OviA7JV+IV3s7XXZbHVVVQfb2sSE6alqd3cXjYf95M9ta2tLQUE+oDCRSCaTqenpmaD8/DyAYeDgVKS3OBs2sDh8kVQllmlYbH5mVs6ZM+ezs/JYbIFIpuQKhFQaDZmSgy/EEwh4Ao6B+KwyaFSaWCJWKBVsFptCporEMqPVYXV4XO6Ayxuyuf1Wp9/ti4bDpQF/cTBYEiuscniierNLb3EZrUC6PoPFY7b5Q9HyeFGlJxCD+maHT623IpM/W902VxCWSo1Rb7B6veF4vBQ4OBiKAwpHgIZjJUVlVSXl1eFIoTcQudI/uDF7Z6irs9ZhrFYJmzzW2sKow+6wO73xwtLCojJ/IORwOICAgWxdLofFgoSGAA42IcGDkQjEQKWwigZm87h9HncAUBjyYIXAsydgsdgkYrRDV2Y22yPRomAw5nR67cjUHmisNC2gMGB0wufYbLOCoS8YiNqsLp3OoNYgkdQSQ+bggMg00QgHI64RKq1GD3XcLj9QOBjKZNC6YAMKUyiEnOz006eOnzp5Mjc7l0ajCoR8lINR7whoigAHw5LJYFJI5IK8fL1O98xQ+HJfn9/trqmsPLQZDb6A7m6sXuq9OLPwgd6y+3c3By9fHBo7mIn+MX0BDQ4MRIPBtubmA/wNd21t7daNG2Ojo08+5OKTBSh8baalb7ziwy7CM8hQOdTHdD8HTy51DM21l3Z2xRtvl7VOlbZOHmwqb58K142d43f95HztG6QWiefapZtLH3KZWN6cuzhe0zVS0j1ajqLwfhpOovAeDX+eIXQfQmGEhm+gQ+gijTcLp5Y/4Dd1fXQVr+n7j8z6l87VvEZsUQWvuyuGnWVDSCr/cHJVDCOpHEmeyuFAzWgyBWtHo/VjscbxWMNeKmqe2H9NytomqzqnqroepfremYYL881XbvWMVCWm0vi4Zsy+27ew14yZWbn04Om7PgMKj4+NdbW3Qytx98niCl+/erW0sLC7owNDYUwvplpbWwF809LScThCXl4BZPB4ApVKKyjAZWVmZ2Vk4grySSQSArtcHpsv4vDFDCaHTKbTaWyeQCJX64QSOZPJFoulkIgkEpFESHirMkAAbUB4PB4PdigQSvRmux042B2wO/12d9Dli3oDhYFgSTBQEo1UFJXUun0xg9WjNTv1FrfJ5tOZAYgDwUgpJKcn7PCErO6AWo/0CpvtHli1ufxmm9vp8ofDRZBQX+Gi4opYYVkkXlpcVl1SWhUIRAKB6Pjw+NbU1Ob4UG/MrT7zVoWcd6W2OuCBM/EVFpeXlFZ6fUEAY5DH4/Z4oIFsMRqRaTDMZsSNF3jY6XT6/cg0HG4XoHAAks8bdLsBbZGgbA6HR68ziURilUrjcHoBygHN4bhutw/kdrsMBqBQPayaTDbU10ImVYAJAK7ZZNNoALU1wMFms9lqhR1aDQaTTIaEjNDANp3RboN9+Ox2J4CyXI44K/N4XDqNQsDnp6acO/bee1mZmUwGQyJBnIyR0BEJAQRDTWBiaJYQcIT83DyzyfwZ33UHj8JX+/uBZloaG59W4Ij72zeuXayprq6qqqyqrr0+vOeu97l6ixZGBlLfOVPUdGlvPaHthXF6yhmVo3hvHdOh62Jvr9fprCovP8B5uQEjQj5fZVnZ0x6+ub27fn22/eJ4Zf8Hf1tH+xQRkJpHQGokwcHjix1Tyx23Zzt8NZ2G2KCl6PaBJ2vxbW3kxklOB6DwT1Nrz/I6m/rm9n9M7t3fvTV/ATi4Z7SsZ7S8+yMovJ+GH3UMf16n4Y/QcM1gqGesYePu2t55vP/+rYk1ReD6W5RWONXXiS26yM1gzaivasRbNeIsH3aUDSUT/FGmwlvmh2l/3ly0L59MRbc+dFksSAlaiGRgh+bCYX/d5a7hykuTHx4ql3QRTnJwoiXTPn6nYzyBwoczCvDevXs7d+8+eVtudWVlcmLizsIC5iCB6UskaAHOz85OT05uPPFgkvb29vx8IOC0xJxzeejgOQaDSSSS8vLy8Dg8HZnjl81ksVnIHL48vlgmlWtkMq1MppPINAqNUSRRcjgCuUwJIAi2RBKRzmRQwYzJEIlFUplEwOcxmCy+UKIxWG2ugMPltzn9Tk/EFywOhEv9oVJ/oCQSLi8sqnb7YxaHD2jY6gja3VGLM+T2F0bilYFQqdsf9/hjQNKwE5XObHX63P6oPzFUzuMN+v1hry/k8SIh1YpLKqPxknC0qLCoDHjU4wkFAuGJa1eXr19evH5hpKlUcuwVS+bJ9rA3hMSaCMUKS4uKy7xen9PlcHlcXgR3PXY74C/iEwwo7HI57Xab3++NRMOBQNDp9LqcSK+wx+33eoNms12vM1qtDlgqFSqbzRUMxoDIo7GSUCgW8AeDATg9oFgb0C2gsMVsF4mkQqGYzeYIhSK0A1ir1Ws1yMg6ZMYNBIUtQOEqFeIvAQANrAz7R1yKzYDISMw12CQSihg0GpVMTrhl4ygksoDHV8rliVBriW55BgMdewcoDJRMIVNwOBzs+Zmh8Nbm5o3BQXjh7q0ftO6vTLDyTnz/Rz95+eWXf/ZvP/nda0fjdd2f970+OdD2L3/5LX2kaW89ofWJ669959uZDMPeOqZD1/zcXFtzMzw/Tzir1n6NDg9Dm/cQIkjcu79ze77n0kTl44fKISy15yKMovDEUidkbk1fmbmzPr+0Pb+0Nb/4MCXyc/vyyQSFe+XJ9DGVF1fu3ppYxan7Xs5r5Dv7Wy7NrW99gKWWNmb6xqu6R0uBg9H00Y7hx9Pwvo7hz0XDD2OrRepuREcXb+ydR0JLazvVXVN5ygvn+F03x5EflIDW7j94gKT7H0j3HibI7957sLl9b39aWd+BXS0/THdW7qJXDLk+S1uzd7Ym5jYepdmN4em1ocnN6xPjF8erL09UId35+4fKPXQR3mvGJHr0gYMnFjsnFttnV64cWjy1F0HA+hsb6x9I6+tPo0tlZ2d3Hfa9/0jrG8/xaMIDa7esbe7eu/dJe7t7d2dxafkDaXFpdfVRq/WZaG11taWxsbK0dGJsbK/oi6q9vSMvLx9QGI0unJ2dDcBEodAoVBqZTEnErxWKJTKhSMwXChhsDoPNFUuVSGAyvV2lMirUBqFYzuUKxCIJUBfiIkymMJksGp1JodGYbBaXy4HE5nCFErlabzXbPRY7EjLCGygMhsvCscpwvApF4XhhFdCtzeW32P0ef6E/XOYLl8VLaotKGyKxylC0zBeIA0MbrE6T3e3yhXwJ4gyF4y63z+X2+wORULgwFi8tKa2KRIsCwWg4UhgOFwIfhwKhsb7OxSvts5eaF3rrnYSz4iO/qjJLw2AUjheXVhaXlvt9XrfHCRzs8XjdLic6BYZarTUaTS5gYacDgDYajSRQ2ON07HUM+31hq8Vp0JsdDrfZZNVrjV5vCHVWRs4NzjAYeTi6zgnUCygMtmqVXqFQ8/h8QFXgX5vNCpuAd5EhdDZYIrHVgHeBhmFpNBgTjsgugGDAYihRq9VKpUrAF1BJFCqZAgQsFopgKZNI1UqVSCSiUgGPyejQOj5/z2WClZjAGfj+GThIwCGTAP70foDbnrn2xx999R9/9qpCYxBxKN/++le+9+9HRu98LOU8tv9jcqD9B3/1j4boh1D4xuvf+6cctmVv/dOE9awcuOCxWV1ZOdgLu3jnTk9nZ//ly+jch09Vk8tXr0xWAwrvD0O7z8f0AxwMaXKxY3nr9p7xUxCwr7ti2F05DCC4V/RQ9x7s3p7v6x5BOLh3rAJF4ce6SXw6DX8EhT+VhmsGQ92jdVu7H/7YXhtZKWoaB37dWz8sbe2uXJ1uvDyB3Du0SzjZnf8BF+HE7ZuA24egcMfC2uCDQxlXsLS4ODI8PDcz8yQDMOBjtbW5ubK8fAhRBb+A7t2/d3toqLOru7e3L5m6unsuX768dqBRkKGZff369Z7uzosXevsu9EC6eKGnt7vr6tWBrSdzxX562lhZaG6pm1h6otMbmlyzFN2KNYzP3Hn872PwhNy+dau7s7Wvt72vtwNNvd1tvT1dywf3M90X0PLSUnlxccDjGbr1pKEnE73C+enp6QUFBciQt+xsWCWTqcCySGhaFpcvEEtlyAxtAqGAxmCQKVQWm6NS64xGm05vARTmi6TI+Cw+F9AOiV3AZAsEYhaTR0QmP6NzAMKQMV5sqUKlM9kAGs02twfp0C0ORspiRTWFJXVB4OBYZbyoyuEJm+1es80HKAybissby6qaK6pbisvqY0VVvlCh1ekzOz0OX9AbjLp8YV8gAslmdzldvjjSuVtZXFJZVl4NfOz2IBPOoRHWYuHQaE/LypXWuUuNi32NMRGB9+4vStWsaNATLy2vqWkqKS33IRAMQB0EFLbbbYDCgKQmE0ApEuXB4XAEg8FwOALw7AHydgIK+wP+SDgUByB22D2on4PZaI0gHhqVQOSRaHEgEI2EY5FI1O/3u1zIYDuoHAxErRaXyWjVavVoJAqL1aLXG+AgPh/cUi+QNzpoD1BYr9cDi3s8fkgGI+JwrFAopFKpQCBkMVlkEolKofJ5fJFQJBQIZFKZKhHpAplog0LhcDjQOAEBEAMWAw1DIezzGaDwzPQ0AMfTCA27X1vTV//ww6+m8PaA1cLL+a//5S/rrqAT2D4YH7reUFtdUVl9feQD85WvL0411lVX1zfPLiIN3KmrHf/6te+Y4s2Q391c6W5vGplb2ZgeevuH38MLnavrK421Ve29/duJa/gQzHavXuqqrChv6by4ubNXtLow03/11ub21q2rFyqraoYnP/C3L89PNNVVV9XUT8xh8Uc/n+C7f2F+/kCYGHZyf18j7alqaXPq6nT91enax/uYJjl4sWMS4eDOqaXu9e2n6Jt+d+f+7J3NnXuP+dtXtuYvT9R1j5T1jlYACj8hDX9ix/BjaLj2eqThRnzygx7DqBJ9vYfd1bpzb/PWfPeVySq0Ox+9fQgHf2SkI9w+5N4l0vLmpwSGOxDBA3y1vz8eDne0tT1JBAn4CNy8fr2xru7awMCBfLIOVsDodQ3N9c09LZ2XWjr2UnP7xcqaxhs3P/ADwhNqZnYWXu+X2+qudjVd7Woc6GyETH9HQ1Nt5dj48zURelIzIwMF+NTGG3f21j+/1jZ3gYMVgevayA1j/Ca0Ofc27NP6+kZrc+21i5XD1+qHriZTXWdr6dWrV/cqPQttrK9Dw6itpeXJg6kBCgMEp6Wl5TwUrqCARALiZTKYPCYrAVpipFcY0IpGpQJOwQIAS6XU6PRmtdYEKIxM58Bl8wXIz/FCoVip0IpFChqNCQkwmI6EuWWrtHq92aYz2e0uv8sbcftigXBJPIHC4WhFPF4ZjVdaHX692Wkwu52eWEl5U1VtW1klLFvLq5riJTWBSLHDE7S6vO5AxAME7PI5vUGvP2SxOB1Ob2FROXAwJMQ/OBi1OzxeXzgYKgyFC4ui4fGeptX+5rmL9ctXWrsjVl3WsUIRrSTorqiqrqttjceLvT6P1+sNBiNAuoC/NpvV4bAn5l52IwPjXJ5IJB5DRsIhXcleb8DvC8JqYbw0GikCJnY6vHqd2W5zl5VVl1fUxgvLgIkRVg7HY7F4KBQGzIW/HFA4Fi32ekIA024XMDcyzxzgKTCu1WpPoLAPmDjhm6HWaDQqlVqr0TscgNo+g8EIHCyXy0UiMTQ9WCwW3BHAXGBfCRIlApmITqlUisV7W+FeQCGs8vl8Bp3BYXMoZIpWoz1sFIbj9XR2wl/W3dl572mOT9qevvbaj7+eLXWjq+V24Ve/8nf1CRS+1Fp69r3XXnvjrT/8/rdvHjnd2LsX6ujOxCA15+xLP/nJv/305+dyqFNr9+ZuXgAUthR3wNZSl+rll1+vvzx2d2HsvZ/88I3jqTRy/r//9Ccv/eyXKk8JGuH0wd21oFn68q9+9vOf/eyln/07Q2y8s4H8gn+jtTI9A69Ri068+8cffP/7R8/mDozs0fD8cD8h/fTvf//qqy//5nQWqe/WlzIqwjMRtKkaamvbmpu/dPMU3r23ObTQBSic/G39kY/pQ5BCu4Qnl7omFztmVy/v3Hs2XXRjiwO9o2UIBAMKJ2gYRWGEhj8bCu+n4c/rNIx2DF+Z7DgcX9tPFaDh1PK1/qnq/UPl9t8+dKhcohmDQDDcwanl7o27T7fZjwrO7UAiSECDsLujw223tzY1PYcofOvWrdrGjsHR5RvjqzfGVhJp9ebEenvvtbb2jp2dA/uh4Ep/f19L7d2Jq3cnryLLRNqdujbQ2dB3ofdw2syfV9NDV9Izj9dd/+KhxMrbJ9Wh6/NLW1Pzm46yIXlg8NLND090Pzw80tNRsbvcfX+t9/7qw7R+YXqosb21cWPzmXWZw03ZRJxY1p/cXR5QODc39/TpM0DDWVnZoLy8PBwOTyIzODyJUCQXieUCoZTL47NYDDqNSkNidpFpNAqHy1UoNVq9RSRVMFhsgDKgYMAyADWJWC4UytgcAYXCIJOAnml0OguJD2a0GK1Ojy8CKOz0RDz+wmC4PBStCIRLg4jTcLHJ6tGZHHqTCzZV1rRVVDcXltYCCkMqKquLFJa5ATp9QX847vSGTFaX0xPw+EJGk81idYYjRWhfbLyw1B+IOJ0+QOFQtCQaKyuOhicvNK8NtMxfbly52rnQ39No1YVZhCqXpbW6sq66AYn2EA4Fg6FIOO7zBm1WZPY4kNvtRbqBPb6AP1xUWAbgC2gbDkcjkVg0UlhcVFZSXAEJ6NZh9wAKA+lWlNeUllQWFpZFo8WAwqFQLB4vAhr2+4OAwna7OxSM+f0A3H5gbrvNbjAYgG6lUpnRhLhPOOwurVYPUCuTydBylUpjtTptVpdWg0zAgToQw1UG0kXZFwTcDOVoRzIUolthiQCyRAKtGSadwWVzqGSKTnvovcJbW1vNDQ1epxMZnvwEv+J9qrZnrr3xk7979Ryx8+LV7tba02+9/OqR7JlVBEyrYpbsrHSdxeWwaF76p79/O4O9jnxwdtXMjP/11/+QS2Zy6IRXX36l7frs3NDll77+PU/Vhbnhrp/987d+807+1Oq93fmR4z/97n/5719568R5vkDwmx//419984dtgwjCdpS5vv21r7z81nGpXHH+yO///C//Vu6pgvL+MvfXv/KVb37nu5k4KhWX/hf//b/li1BG35IQz//opV8r9DaLTvLzH/zTeaL4Of3t7fnTjcFBdPK5hfn5vaIn0KFFkEC1sDaMupnugdTDoXKjSMyBPdcIhIORhHQrHs4v7B9SIpZwW89o6YWxSkgf6hg+HBquvR5pGypf3vziHV0Hq5Wt2cGZRsQ14iEHf9hF+CEHw72bWOqYW+3fvXcY82gCth5IBAngiav9/eXFxQDWzxsKA+m2tXd2Xbx5e2rz1uT6rYm9NDS1OXBrrq6xbXrqYGYzWVtfb2tumLrafX/mxu70IJKmkOX9mesLNy60NdXdWfwwID4Pmhnuz8k/1zayNn3zYqy4ZnXr0WNw7zNMozM4siJ0DVy8sUfSa5u7sYYxsedasgS0s3uvu6tj/Gbjg42+++sP01rfg42Ldxe7u9srh4YP4zeQp622tvb09KwjR46dPnU2LTUjPT0DgDgnN59EZYukGoVKL5YoOVxBImwwgsIUMtIrTCKTAG9FYqlGa1Kq9XyxjCcQ8/gCDofLYDALCvC5uQVEIo1AoAEN0+hAYmypXKPWm4FffcF4MFIKsOvyxL2BEl+wxO2LOz1RmyuEdAlbXGa7LxQtr65rLyyti8QrK2taaurbSysbURT2BmPheIndHTBanD5/2OMNoigcDMWBgwGIERRGWDMQCMSihRWReEVRJDx9qW3jatvMxYbF/rblqxdGGys9lAwvM7816q0sKQWrwuLSeLw4HisBVHW7vO7EzHPAr7FYUTgci4QLi+JlsDUeLykuLi0pLi8uKgcIhmV5WTVkvB4AaGcwEAUOBjJGk98X9nlD4QicWNzr85vNNmgN2JA58HxOwGKrXafRyaRSYaIJodcbAYVhJ3KZEi0BigUe1ukMdpvbanFCBmBXo9Eolch0dFAB8BdWUQKGjDEhKARDtAIAMQjpJGayuBwOlUI16A99ig043ujICHDw9FMOCnt3dvDtn33jz/7i6795+dWX/uUf/vf/7/+ZQjeif+udhbnN7b13hIJ6/h9e+sPE2u721KVf/NPXjuUrEhx0/3JXy9Ty9tT13p9+4wdqu4+ee+zHP3+n8ypyzptTt478+Ft/+8PftA8iY/56K5xf/x//N9MKyLtOOPmrb/34lcujyNf22syN3/74Wz/67bnt99+/Wu7+6n/9L+/m8pfhsA/WTr780ku/Owdfj3dutvzke9+SeuqRY77/vluU9fff/UXPKDbv/2fS7MxMbWVlZ1vb8tIBfC0dWgQJVDu7m2OLfTdm62/PNyX7g/e7CKMsNZlgqZ3dZ/NILG1MXRyvujBWfmGsAqHhhx3D+2kYReHPR8OPUPjTabjhJjIF3YcGzz1D3bu/M7546dpMXdJFGDg46RqBDpV7eO+QZszKFrwlDgko4e365BEkAH9Xlpfh/XwgH6uD1ezsTG1D69Xb84C/N8dX9ycoae64ePny5b2qT6aR0dGupuqtsYGdqWt3Jz+cLrTW3bjxvDyQ+wUonE9IL+u8JKLl0iTW9d1HD97ozHptz/TG1sd+369u7OqjN0O1o3vrCW3v3Acaru+dSe5oZnauo7Vmfa4DIeDVC/sTwPGt/tqe7q57h+65hApagHOJCBJP7ube2tqelgYofPzkibPnz6cDFmdl52bn5FMYXIlcK1VoeQIphcogkyk04GAKhUymUql0KGFzBSKJXKnS6Q1WuUrHZPPxeFJmZnZqavqZM+dOnTyblZ3PYPIEQimHJ+IJpQqN0WBx2AAvw8VFpXXReBVwcCReFYpWAgrbnCFjokvY4vA5veFoYWV5VVO0sCoUKSutaAAaLqmoD0SKAIUTc2eUOgE9HZ5AMOL1hewOj9sTiMaKo7HSeGFZUXF5AIjZEwwBHMfLQpHS4mh07konoPDUhdrZvobJtsr5nion+QzrjZ9V6PjFIV8gGC0qLi0sLEUdHnw+xGPY6XRHIoWVFTXAuwDBKN0C9VaU15SVVkEhLCEBChcVlgX8IQBchJgLy8KhOPA0sh9vCAmC5vHDeXo8PqPRjHTgqpB57gzAtWqNgMeHhgKHg4SSMBmt6JQccrkKVoFmFQo5gK7ZbEUm5nD4TCZLwmVCCeVAusC7QL1qNRqNWA80jHYwA/gCSaMza/AS020gSvgNMxiMZzNsDrS9tfW0+962Zwbf+unf/90P/51IZZIJBS//4kc//PnvyjsH0a3TI4NRv0urVr72m3/72+//YWLj/kBj+Btf+Wt/4wBaAdXUtc6ff+u7P/nZv377Oz+KNextWp+8+dYP/ymNrkNX76/ceOVn//x2rm7+9qVX/uVv0nkOtBykwB37+2/+cGjtwUCZ67t/94/Rtj1PDFnuyR/+5M25B+/3FRu/+hf/76l8hlqj0WnUae/99s++8u14659Cq/oQdHd7e2Z6en3tYIYtH1oEiaTWtxdG73TfnmscXkBmJvuwi3Aizaxc3Lh7AH3eX0wTS4O9o6V945UJFH48DR/UELp9KPxhGq69Hh6Y7r53/yn+iPS5tLo1d3u+FVD4ox7eD+8dysEd82sDz8qz5U9Sly5dau26cnN8LZE+gMK3J9cvXptobG5bfeKRW7u794Dnhi613Zse/BAEQ4LC8f7OrvbWra3nzi9rdvRaVtobZ3PwOAp7YHRurzSh8dkNReB6vHH84zzsS1snFIHB+Y8MudvYvre09qh3GW7B4KWa+2sXkPQRFF6eagNQnps7DI+gj+oAI0i0tXVkZuadPp2SkpKZmgocjMy1XIAnMTlCkUzNE8m5fDGVxkS8h8lkWNDoLDZXyOGJpQqdQqWXSJVSmQo4OBXh6fd+//tXX3/9bdhVekYugUiTyTUKpU4sU8uUOo3BCpjr9gOeVpZVNpdWNMWKassqW4rLGr3BYkBhg8VtsLpt7qDbHw3HyopKauPxyli8orSyoaKmqaisxhcqBOL1h+OAwoEQMjAOODgRQy0ILBsvLI3FSwuLyouKEBRGeoX90VC4KBCMA8POXmpf62+ZuVg73l46UGib7SyO8LNwv/pOsYRU5LP5fD5k/uUogGxpLFYSDEZ8Xr/b7Ydd1VQ3Ig4P8ZJwGHGKKCutrqqsh2VxUUVlRW1VZV1ZWVUsWuj3B1wuOI9oBI64d/QIrELGZgcaDsDeEkHTdCKRiM/liRPTK1PIJHxBAYfNEYulNpsrHiv1+8J6nVkmUyiVCp1OC3yr1xsdAP3+iNVqA9IFDgaahqUiMfsG6iORGF1nBCwG2M3OzqbT6cDKIH5CQMYgIGNgbqvVeqgo/ODBg9XV1c94yCfU9sy11176eirfin7ux682/eTbf/3HVA7kb3TVnnjtd//+q1/99re//upf/vnXf/D6zPb7baWOv/nzb9df+UCbeOpa1y++9d1/+ObX/vpbP6zr3/vpbWPyxls/+C5B5EJXH9ybfe/VX7xymn/7QuvP/+5vJI4KtBwUEhV8+1vfvjj7oL/U9YNv/aDm0t5gC3X+6R//2+szu+/X2iX/88/+8w9++h+//vWvf/nLX/7i5//21omMnsFnhj4vspYWFy90d1/t7z/AAG2fqrXt+cmlvgRLtYzeaR1bBBRGIw8gRDW3emX97tyz+pH67r2tG7NdvaPlfWOV+2k46TT8+Wj4gx3DbUMln9AxnADiBA0n5mSuvxHtGq1Zu7u8d2bPWg/ef3Bnffj2fPNHmzEPOXivGbO5c6h+HYt37gzdugXtwyd6xz54AK1B+DhsHGhAhifX6upKY1PbpcHJ25MbH+LgRELguL656/btJ421Mjc/395Utzp8efdxKLwzNbg51t/ZVDM5eTDOGAeo+YmbKUd/8lff/IHS/ehrKKn+oWWJ91ph0/jdnQ/TMFw6oWvgo27BH9LK6lpHW/3CWMuDjYsf4uC9tHbhck/Fsxo8d6ARJLry80lZWbi8PDIsC3BAuyw6iyeQKEUyFZsvFojkPIGYyeQwWSwGgw1kLFfq5UqDWmuVK3UkMj0zK/fU6bN/fO0N4OCXf/+HY8fOFOCoBBKTy5NIZWqhWClT6hUaEySdyeHyxYKRssKS2pLyhsKSuorqttKqlmC03OYKGa1ui9Pv8IY9gZgXcaIoARQuLa2trmutrm8tLK0OhIu8gYgXcDNeGiss8wciAME+fxgS5BPeEcUIDcMmX9jp8CY6hqP+QLgwHJq+2LY20DJ3qXaspbDbJZ9tK2p3yFhv/KJYSin0WHweVzAI4FtSVlZZgnj6lkQisXAkXlFZW1PTXF5WA4gcCIRCwQgQcFlpTWlJFdBwZUVdRXltSUl5LBYPBAIO+OO8oVAoDtgKR3c5feFQPBwustpcRiRQmhsqWK12AZ9PJZMYdCqbyYAMhUwETlUo1Ha7JxYtiYQLbTanSqUGytXpNGq1BojXbnOHkXI7isIGZOoPLQhQGMAaBBCMukZQKJT09HQSiQTUCxCMdh6DpFKpTCaDEofDcagovLW52dXR0dfT8+QN908VisKZ4j1gBWW8/pN/+Om7O+9vsjLe/sZ3/t0dK+8fGKBmn/iHH78+ufGgu8L9tT//WmnPB16jU1c7fvbNHzG4wld+9f3XU2iLiV6A9QQKF/DtiSrvP9gcfevXP3wrSz1+peuX3/wruj6GloMsjLRv/cMPbqwkUPib36+8sDcQXpl36sc/fQNQuNEl+fpff9UUrb106WJnZ2dXV/f1W0ObH3lPYfo4weM7OzNz++bNleUD4CSAzsPnzs2dpfm165NLvWOImylwMILCM8t9d9Zvbu3AN9Oz4WDQ+t3lK5N1fWMVKAo/pOGHTsMJGkZR+LE03AWF4xXdY2gq79pLZZ2je6ljtKz1ERB/tG94r2MYaLjhZqzpVtGd9Q8Ee3m2unf/7vzarZGFtuH5pkQzBr13HRNLqHNLx+zKxbXtmcP08IZHFxpysVCoo7X1CSNI3BgcrK+pgb0d/sfhE3RnYaGuoaX/1tzHoPAqlDe09F67dm3P4ItqYnKyo7Fma6z/41B4Z/Jad1PN8PBjAps8W82OXstOe/ON46dT8ujjHwmMCALYBeQN141uowO9E9q8e89UCI3OTw+LcWdxqaOtbmmy7eNQGMoHLlQclJvK5xU04S719XW0tc3Nzu4VfVF1dfXSaBw8nk4kMgkEBo3OY3LEHIFMotAJpSoWVyySqiVynViqlso1QpFCIFYo1QZIYqmSSmWePHn2tdde/8MfXn39jTdPnknJyiNT6AIylYcnMdkcIZsjYLIFIqlGpjLA3sQytUZnlgPj6W1OV8jljoRjFYVl9bHial+o0OYK2D0hTyAaiBR5g1E3kGSkpKikurKmqaKmsbCkKhIrCYRiXn8klJjAwu+P+PyRBBAjHcPBEBJmGFAYmVbDG7LZ3B5PMByJBgLReDA0c6Vtc7Bjtrduuq2k1yWbbi28WeoyZrxboWEVuc1ejysWi1RV1VRV11dV1UEqKa8sq6yua2ypb+wAGi4trfT7kXBqxUWlZaVVJcUVZYmp7IqQWTyK4nEEhe12J5xSHOk/jnq9SJSJUDgei5fYHV611mg226HE5XKLBAIyEUchQMJTSUQWgy4RiVVKtclk8yJkHzSZLQmAFSuVCkBfiURms7ujsWKHww2IrFQqE1MzG/R6PRAwl8tlMplAw8DBsInNZgMHs1gsyMAmQGdQAoYlgMICgcDlcn3GgbAHg8LwgMKbOuz3Tz79YDSJCBJfSxc+BNbdhXf+4zv/+mrG7sb467/87svnxGi5ip7+999/ZWzt3kRf5T/+9Z/TTaVo+drizOa99yeudv34b//FXdFR7hN/5b9/ReVHnHq3p2+9/cNv/fT1rLkN5Hti7ELZ977+P4ja0p3F0SO/+NYv3iMto64fD1bO/vGn3//3Y+vvv3+52PH9b/xLRe/eq1ORe/Jf/+316Z33h1uD3/zbv7FV9aPlmD6vNjY2murrI4HArefSde8zavf+9sbdheXNscWNIUgrm+Mbd+d37z/j8ZOJmTUQAr44XvV4Gv7EIXQ9UH+6+sIUpCokTVb1TlaiqWeyIpEqO0bKHqLwJ9FwomM4MrH8FCMrfwHt3r+7tDkxtXwJ2jAPmzEICk8t9yysDcIdfPD+obZpAVuBAzwOR9MTRpC4f//5jCBx9+52a1tHz5Wh25MfdhSGdGti7drwnbrG9qmpJx2Fsrq21t7cMH+997EOErtTg8u3L7YjI+e+eKCGp6TpoSs5BSmhmsask2/z9eHH+iB2DizQTJfdFcNrm3vbq7umFYHrS4kx5Z+shOtI58hgw0cdhZG03re50NXZVjk69myCzcGju7W1BUD85O6Xvb19PJ6ERGICQTGZQq5AgSShUiTXCyQICkvkerXOrtJCsgETcwVSvkBKZ3BycwuOv3fy5d+98qtf/ea1P75+9lwalSGQKS0qrZPDVxBITCqVQ6dzIbG5EoFQyeGJWWw+gUA69t6JtLRsJpPP5UlVOovbH40UlodiJb5Q3BOI+cOFkHcHIk5v0B8uisTLAYILSyrjReXxwjIg4EAwBvjr9QFJByCDzjbncvtRH4mi4opItBg2OZw+2ApFoVC8KBSZvtK2fbN3qr16pD7WauKNNYRvlblC9HP1Jn6Jx+J1uyKRcFlZVWlpVVVVfX1DK7RF65vamlu7Gpu7autaysqrwxEkFkRFRVV1dV1FRU15RQ2gcHFJRWlpeVFRkdPp1OuN4VC8vLw6FisKhqLRWFFRcXlxaUUgGLHanA6nx+cPWa02DpORn52By80qyMnKy8og43E8DlsqlWo0WoBgq82mVKmAZbkcLgAu4t8gEFisDuB7t9un1WiRCZvhP5kMDRxBo9Hy8vIYDEaiogCWPB5PKBTCHgCRxWIxUj+hZ4PC01NTlWVllaWlBzLk/5O1PX311R/+9S/eTa+sb22qr1Vy8v7yf36Fri8EfEp/9z/+4cevl9e31xT7fvLPX/+Lb788urL9YGfu9Cv/+vf/8ktvvLyiOEQpyGy5Nj11o+/7f/kta1nP++8vH//V9/72u78amF6/Nz/yzkv/+H/82V8SObLamqqc47//n//rGxU9Q3BQEzf9//3zv6GKDY2NjToR8W/++qsMfQTKL8TM//y1fypL1AFJs977lx+9MrH14MH6+NHf/ejHv3uvsLKhp6ertDBeVl6/iXUKf2Ztb29XlZU5rdYrFy/uFX1Rra2u3hgcHBkePpwIEo/V/Qe7kJ5hT/B+zaze7huvuJhE4c9Aw8mO4e6xiuIOhz4g0Pp4aNL5+Do/HzIaHxdJXiSV9bjbPxsNAwrfnLt0/zmbs+3B+w+2dpaXN8fvrN9cWLt+Z/0GtGTWtqfv3nsGrgWArZPj4z2dndAsfJJnGL4Srg0MwFv68sWLzxUKg27euNHQ0nNjbAXA90MofHtqo+virfaOzgMJrdh/5cpAZyNQ74c4GBLw8Y3e5r4Lvc9qcNgn6M7UbYVKcHVuq63IUUDmjSw8/jlsuDCbLu7RRm5MzW/enlwTua92X/2snjxDwyO9ndU7S92PGTa3eXH8RkN7W/Pms4undlDq7b0IaEsiI+TK5kiAg3lClUCskSiMYpmWw5dJFUa9yas1eoCGhWINncEnkhhp6TnHjp1+7Y9vvfLKa++8cywtNZdEZgslWq3BDUki1dHoXCqVy6AL6HQ+gyEQCpVcrohMpr333tGXXnrp7XeO5heQc/MpJCpHotAh827Y3W5/GGgYUNgbjDu9YViGomXReEUiCkRZJFYCjBuNlgRDMeBgh9MLyesLAwH7/GG3JwDlpWVIUOFgKA4VgDtDEaRTNgp7CQanL7dtX++Z6aq9WR0pk5JuVLivF1ljrNR6I7fMY3TaTHa71e3y2O2IK0J1VWNTU2dLSw8s6+pay8pri4rL4oXFQL011bW1NQ0VlTVlFZVl5VVl5TXl5ZXxeFyr1YpFUkDh6qr6WKzE6w/Hi6BKXWlZVTga8wUCXr/f7nAqFHIqmZidcT47IyUz9WzKmROQIREK+HyuQiFTKOUqtUoilQLIshhMOpVGIZMhq9HqPR4/YLRSoWSz2Hg8nkwmA/UqFAoikZieno7OoIECMXAwLOl0OoFAgBJYRd0kYMnhcBwOx6Gi8Nbm5sT4+PjY2JP8fvcZtbs4dOK33/u//p+/+OGPX/rJT37y3e9891g68fYc8mpoLLT99Iff+dFPfvby7//wu39/6Ue/PT20gAy9ai/z//JH3/n2P//Lv/3bSy+/fqR3ePHO8OWf/8OPrMXtsLW3yv2tr/4vtrF4Y3b0/Bt/+PUf33z7jd//8Hv//Lff+MdcrmltG7mOM7f7Mo+9/O1//Ocf/uil7/zz987mM8YWkUEzl0qc//qdl6r69sbD6Ujnf/rLdycSncptpc5f//xH//qTn//q17/5zW9eoXK0z5eD3vMteHyv9vd3dXQ8eRCl8dHRoNcLjdnDiSDxnOvBg/tjiwMJFK5CU5KGk07DH6XhpJvEpZkakiDrb/7+f3317/7ya9/4q6/9/f/622999Zv/9Ld//+2vwSpa/tdf/4rEQuuZrPyom8RHabjhRmRgqmP3/nM65+3u/e2de+s79zbuPzg8R/OPCj4OAMFPOBgD8Hd1dXV2ZuZAnI4OVisry/WNLZevT32oYxjIGPgYoeQb1/eqPplmZufaG2pQd+GdfRwMcLwx2t/ZWP18zrJxd2v9xs1rq3fff3B35UJ39+zyx00X936obvRdWpvQNQApWDt6/zOEWkO1vrHZ3taYcBfu2xs8h6b1vnsrvX1dlQMDT+qg8oW1s7MzOz0NDcInd3Pv6bnA4YpIZGAnHo3GJ5I5bK5CLDPIVCaJHFBYLpbptSavzuxTaKxsrgyPp2dl4VNSsk+fSTt5KiU1PZtIYnE4Cr5AJZEbAKABo4UiFYslZLHEQGtcrozPl8sVOjabn5Od+/vf/fbHP/7xuZQMJkdCovHJdAGLJ6OzhQyOQKLUGiwuhydkdwddvkgkXh4vqozFy6OF5YDCAUBkX9gfiAL7en1Bh9ODBo4IBCP+AILCUF5cUgHJH4gAB/uBjSMxwNbq4pKGaPBOf+fG1a7l/pbRlrJSBe1mlfd2uauQm9Fo5JQ61Fr4i2ViqVQiEPB1OkMwGAUabmxor6luLCurjsWLQ6FoMBiORGJFRaUlJRWxeGE0Fgc+LiwqC4djoVDIaDQplZpwOF5RVhONlHh90XhhGXAw2AZCIV/A73A5pXJZYv5jOpmEy8/NSDt/Kj3lVGb6ucyMFAaDIhZDgwHp2RUm5oejUah5ObkEHB64ViKRikQiDptFo1KzMjJOnTyJK8BxoW0hEpFIpPz8fOBdWKXRaLCEPaBuEgUFBVQqFTZBNRDKxxaL5VB9hQ9TD3bWquI+Jh0aAAwWm2t1h0dmH77Zd9frysJcNtvsilzoaiksq1tcR9H8XmdDuZDLFss1zT39wLbbyzNBp+/yLSRoGrxciv32cHnbxspSdVlZb/9gR0M5j80yuaMzS4/eOFNDA06TGo5pcgQmFvZmj5sfvupzB4YfnsCllppApPThqNzdrsYKpUTIYLLhie+6OPjcdTU839o6oGgko8PDbru9JB4/tAgSz7PuP7g3tHCxb+wRCu/R8KOOYYSGk07DH6JhQGEiLwN4F0VhIOC/+4e/+a//7c/+4q/+xzf+8etQAuV/9fW/EJkoPZMVbcOfPoQOULhvovHuvS99bxOmJxFgel/fxZbOy7cnN5JBhSENTW9dvDbR0NR6UAHg7u7sdHe23+5rvT9zfS+ocCLdn70xfqWjs7V5Y+PL3WDe2Lon8w/mKi54q0bmPudEzQOgvur76wj+IssECj/YvLg02draXPMMIy6vra4219eXl5SMP3EEifaOTiqNRSTRaHRufgE1NS2fROEJRFogYA5fRmUI+CKNUufSGL1iuQFYOTeXlJqam55RkJ1LzMdRiRQWky0RCDV8oYonVEJ9Kl3A4chYTJFAoFQqTXK5USbTicXKvDzcyROnfvnv//G73/6OSudqjW6NyasxeqQKI4XGz8PTCWQWV6iwuQJufwRSOI5ERgtHiyPxkmC0yO0POVxIT7Db43O5vVabw2K1O5xuj9cPCcjY5fYHQzFITpcPyu1OZLq5S72Xrna0dsV9q9e6twa71wfbp7tq2j2aybbi4Wp/iTCnzSaMmyQCNg34k88HkmTL5XKz2RYIhMPhaCgMBBxxuTxms1WnNxpMZrvTnQBxN/wTLyyJxoq93kTVaAxgPBaNx+MlsWhpcVFVUVFForva5/H5HC6XTm9gczgAwhKJSCoR8rlMQkE2Li8zPy8zIyOFQiVyeRwej8vhAPGyWEwWvgCXdj61IC8/gbas/Pzc8+fOnD1z6p2333rrjTfwODzUZLFZBAKBQkEm/0MQm47EZQPkZTKZgMJJBwkAa4BjAGUcDqc7zImX4S22Ae+Pzc3P2BH9lPRxv/h98i+Bn+V3wo+t84ENn2FHmJ6FnkkEiedW9+7v3JjtAt7dj8IfoeGHHcMJGkZRGKXhnrGKola73s/X+fimsMheJM8gnfyz/+c///gX/6Jyc8xRsdbHU3s5pV3OjpFSJLbaJwaUSKBwtGu0emsX+8nkk/SnHUEC1eTkZHVdy6XBqYFbc5D6E8uB2/NNbX29Fy7sVToI3bp1q7Oxaul238rQpeWhi5AgszJ0sbe55tlOL3xQqu+dUQavf4HZyxfuLLY2Vc2ONC1NtS9Ntu2lqbb+C5W9vT2f5bvyKekAI0g0NDRlZefl5OKIJHpmZsG589kFBAaTI2VxpSQKC0dgsPlKidIqUVjYfAWVxs/JIZ49m5mZhSfTuDSmIA9HzcfTKHQ+MHEBgZpXQKEzhEymiEIGppMoFEal0iIWa2g0TlpqxskTZ17/45sp59PVWqsnWOaPVsNSqbFTaMJ8PCsPR6cyBRaHzxeKOTx+ly/o8yN+Dh5/GAjUZLNbHE6D0QzJaDRrtHpIZovN4XDZ7E6zxW6xOiEZjFatzqTW6FRqjd5g7GxpXxi83Bu2LV1p3bnZuzbQPH+h7npVcOlSw60qX5Egt80hKjZLxFymSqPV6g06oF3Ylc2hN5iVKjXswWK1qVQasVgiFkuRAYNGs8Xi0Or0UBkN4uZ0uoPBYElJaXFxSSwWj0TixUXltTVN8XipwQBna7LbnUhEYZVaKEKmvVAq5DqNWq9RCbgsKplAIeEJ+HwyhchgMJAwwBwulULF5+OyMjLTU9NysnMAanG4gtSUc+8dfefdd9585fe/e/WV3+fm5gLa4vH4rKwsAFzIk8lkYGLAX7R7mEgkAhNDWwSOCPns7OzU1NRz585JpdLDQ2Eg4P7Ll1ubmp68xfYMhBHs86q1tbXh27dHh4ef3EcQWmugvZUXW7v37w7OtH2oVxhNj6fhjzgN945XXJyuuTJff2252V+t+9eff+8//af/9F//23+hSnJ6Jyv7F+p7p6o6R8uS4dU+mYYBhduHyjZ2DiaG9J+k4NH9044ggWpra6ujs7u6vqWxpbuxpauhpQsy9U0ddQ3Nkwc01RyqlZWVjtbmjsbqC231va2Q6mDZ1VTT1lS/sPBs4uYeoHZ27zvLhqINX+S7eHf33qWLfRe6Gy9faLzQXQ+pr7u+p7O2tbl2avpZhnk5wAgSlVXVR44eP3b8dGZWfnpGLjAugcQikNkFBFp2Dj4nn8LkygRSAwAxmcajUDlQeD4lG0dk8oRKIODU9LycPEIejpyZi8vOJ5CpbKFQxWKJszLxaWm5kBGLdByOjERi5Ofj8/IgEQVChd0V9oUrfOFKh7dIprTkFTAyskjZeZRcHFUkU5lsTq3RrNIZlCqtQqmRyBRcoVip1RutdpVGBzwKVMpicfh8oUajM5ksRqNJqzPq9Ga1xiCTqxNJJZMrlSpNcSS6cKV3IG5fvNK8db1rqa/+zsX6sdbSlf7W29XBImF+s4XX6NU69Gq3J+APFkLyBeIef8RsdanUOgBii8UOYC1XAFrrAG0BaiEjlctlcoXJbDOZ7Xq90Waz+Xz+cDji9/s8Hm80EistKfd4fAqFUqVSGvTGRHA0RDKZDCyVCoVKLhPxuUwGFRKVQiISCCQSmc3mQAK0PXXi5HtH3ktPSwfSzcnJAfDNSE87ferk22+9+dvf/voPf3g1Kxsh4LS0NKDbjIwMqIPOmE0gEICJYROsAigDHBcUFECdY8eOHT169L333gNQPjwUvre7W19d7bRa4WHdK8KE6Yk1NjJSFI1WlZU9h5NjfXmVQOGWR77CEx9E4U+j4UTHcPmlmdrLs3X2Ivk//ss3/o//8//3xyO/+f5L/wQ0TBHndMOeZ2o6RpEu4Uc0/AiFP0zDjTdjrbdLNu7ueRxh+qgAW/+0I0gkNT8/Pzh47fo+wero6MjB/p7z4P796enpa9cGr1+/MYik67CE1fGJiSfqdH8+1Ng3K/UNTs1/QTeP5ZXV8YnJsbGxoaHhoaERSLdvD42PTzzDMccgeHQPKoJEeXnFK6++9sqrr588de78+YzMrILcfFJ6ZkFKWta5lIyMbByNJeaLtRSGEMpzcvGpqVmZWTgOXy6R6/Px9HPnswoKKGQqi0Jj05k8vkAhlxu4XOmbbx751x//9PTpDBZLyuPLGUwemcKg03kiidZo8VgdIacn7vDELY6gXG3JyaeeS8k5ez779PlMIpXBE0kFEjmHLyKRaHgChUCiMjkCtc5ssrlUWqNcoQYIptEYXC5fqVQbDKZEJ7EBUDhBw0aD0Yp0D5tswMdRv3+qq3W4wrd8tXV9sGuxr2mhr3aup3rlSstwbahCTmwysnoiprjbFgrFAqE9Dvb6o25PyJiYz9nucJnMVmBiSDqdEY4oEkmkcoVOb4KtaA+0Wg2oq9br9MhUcEq50WiwWMwajVoqlUglIoUcieIghySXSaSIR7JELBILeXwui8Oms9l0GpUMLAwIy2KxmUxmfn7+8feOvf3W26mpqbl5uSQSiUqjAePmZGedOH787bfeOnXqFJRnZmaePn36LNJFjwgqp6amQAa4GcgY+DglJQUg+Pz587CEamfOnDl58iSPxzs8FIb3VEdra3E8fvP6wYxswIQJNDI8HPR64blavPNE0xk8DxEknh/t3Nu+Ot0EKHxpojoJwcmUpOEECj+ehi9OVUPSeLhf+8Zf/W//2//nvZTXmwZj7nL117/51//5//6/yKLsrrHy7vHyT513A6XhBAoXb9x96vHIv7wCbP2TjyBxCLrQUmXxRBY3/pS9pCbnNoSugeaLH5iO7nNpc2XW7zK19P3JfpVXVlX/8bU3f/u7V994890jR08cfe8UpHeOnHjn6PEjx06dTckkUjhMriyngHLufMap0+ePnziblYvnCRUcgTwlLffkifN4HFUgUPD5cjYbGScnlxtZLOHPf/HL//Hnf/Hb3/6RwRArVGauQEqmsFgssUJt0Vk8erPbbPMbzG613ipXG6lMXk4u8fS5jLSsfDZfCCgskWnEEjWTKWAw+Dy+VK4EEHVa7B69yQH0qVIhE0/AUoP01JpMJotebwICNlscZovT5Q4EQ3GH0683WuKh4HRP+0R9bPl6x/rNC0sXW+cv1Kz0N61cbr5V4alVUWuUxItRUyHAud1psVqtNrvN5rBa7WazVaPRG40Wk9mGQHCiy1khB7qVw6ENRrPD5XO6fHBEOAexWPT/Z+88AKOo1rfvd7167QXpvQiCCAoiIipgLwgiioqKCgIivaf33ututmSzvffe+ya76b33nmx67/E7sxtCQPSiQMT75/FhnJmdbTObmd955z3vCQ4KCg0JCQjwCwzwDQsLCgsNjAgPgdA3LDjClhwM1kAOC4qICAkLDQoPA9v7hwQD+/n6ePr4evkH+Pr7+3l5e0Fh3cuXz507dxk0MtzdAwIDA4OCfX393N3dL1686ODgADZwdnE+f+EC4F0Avo6QHMBDZ8+eBRR8+fIlsA3gafAooGN7eNi+BmwQFRU1jVHh0dHWlpaGujrAHBOr7umeblmdHR152dng4n2L3d1qqqtpJJJUJLpXQQLIhsL630Nh4N/S8NSkYTDNqJWHxXssXr7gwf88ePT8t7oCdo5VA4xmhT73woqnZz7lEX7BVMJLrpZcNybzb2nYjsJgTe89FP5DAYoFp1kwnVj+SwL429PT02K1TsNASHehMIEX1m75OL/2rqsZfLs0ODyGFZXjJRV/IUt4Uh21OR9tXeMYQZ5Yvjs0PDzc2NBQW11966PxK1Wa/d/8sGv3F3u/2P/Fl99+umffzk/27N79+d4vv/3ux6PHT11yhPIlvE+cOn/06IkjR0/9fPzsRQdXdy+/iw4uPx3++cjhn11dPAMCIwCzAvv5hweHxHp6BX7z3cEdb3148ODx0FB4LBwbEhbjFxAeHBobDU9AJpCQGCJwDCI+IgYeBUMBgw3cvQKCw2ORGBIKQ8IRGXgCA5NAAUbHEyIiYWGh0bGxSCQKi0Ri4uJQgIBhMDg0+Bo08DCU6BsHqBiFRqHjcTgCmQx1pAPAyqGR69PMrWn67vLMnpLUljR1W6aup9Dcmq7L4cZrYB58/zPZbLgEbIuIQyBhmHhUAviHQsDjYEgkEmCxPQUZh4fqrCUkAErGEolEKo1OozGBSSQaPA4ZFgb1ToOiv+Eh0ZFhsTHhUZEhYIqAw8B/MdBIGWFRUWGxsRHw2Eh4dEQU2CwqPDY2Kg4eA4uJCreFiiMjwsNCAVEHBgeHREREghVBwWERkeCrISKjYsPCI0PDIoKCQ0JDw8DEx8/fy9vbx8cb6ooHlaUAqOzv4+vv7e3r5u7h5ubu5ubm4uICoNnT09MXbAD++fl7enkD3J8+FL6ne7qbda+CxFRNReE/ouHfKSiR06SCUwIWL1vw6GMPn7j8o6GQk92kSqmWptXJs5pUsSS/5auWzJw9A8kITq9XXDcmc+KNaPgeCv9fVmdzVXwsPClnoi77HVV2kioajmts/5/toKnLaA4hFdRZb+ksN9TTQkLHSAzpd9VdA9B402s0EoEA0PDEqr8qgMIHvj/85Vfff/f94QM/HP7m2x+/3n/gwPc/HT129txFN2fXAA+vYGdXr/MXHC9ccHZy8XX1CHRw9rp02fnkydPQ3fnTZxyhNFdPF1cvH99A/4BQX/9QH/8wPwDHPqGBQdExMfERkYjoGBQynoCIJ6ASCPE4MhpLRONJKCwREY9DYgjQOHMkZgKeQaBwcWQuhS5kcaVUBjT+XAKBAvDX08P74vlLHu5eUVExcDgiKio2MjI6LAyqfRYKdUULgVASwuLI6OhIGCwWhUImYBPi0WgmAVuZYhysKOyvLmgvMDWYJZ055u6i9FqTNJ0Zp4h2pboczqJH6JhIIjYeT8CRAeficThcPBqDxOKwVCrUMQ4ax44FlYbg80UsFpsF/kGD2DEZDBadzsJgsHFxCBQKhUYj0Sh4PDouAYMEUxQShkvAkEkUPIEIHkCikGQ8nk0ik9DxcTHRcYg48EQCeEssFoFAxgJmBmgPA98OonoUKj4eBdYDcCViceQ4BDomNg6sBlgcFR0LyDgc4DTg5ejIqJhoW+JyNNgABkdFRsYGBAb7+0O1MPz8/MA0IAAKKweHhkRGRYeGRZLJ1JssE34bUHh8fLy/vx8014aG7tLioPf0f1md7e1ZGRlFBQX3KkgA2VE46woK35iGrwSGATFflyaR36L98fiXjz7+yAWPo+YyQV6LNr1OnlINjUIH2DejXhGe4Dl73kwHv5OZjSpLpfh6Gr6KwhM0bCjj3UPhPxY4wba2tpaWlDTU199khOOGAq/T19vb1trac8uhtdulxiLzGy+s90cLJ5bviMa0QmIcjtL9P50eBQgYcLA+86/3KutqKImLjDKkl04s3026jRUktFr9L8fPHDl64pfjp4+fOHPy1NlTp89duOjo4uYLWNbbN9zLN9Td29/F3cfTVljNLzDS0cXr4iWns2fPnz179tKlSxcvXXZwdHVz9/bzDw4JjQwNj42IRiAxRHQCGY7ARkUjAgPDo6LhGBzRZkICnozBk7FEKsBcRDwebIkns6gMAZMjY/NVDI6cI1BxBEoChR0ZiwiJiA4Lj7p44eLRw4dPnzoV4O8fHR0dHh4JBUeDgwIDA8LDoZq7QJGRkVAablhIRERYXBwMkDAmHkVOQBWbtWNN1YO1Ra15hgazrDs/tasovUInyOaiuYHn0We+TiOHmPkJfCaDy+Vz2Ww6lUyhELD4BCweR2OweHwxny/hcUVikVwqUXA4PAYDQDATMC6JRKFQqDgcAYvF4vF4AngSPgGHxeCw8QkYBCIuBnwAGo1Op7OxeDKRQNUKZWa+lIfBg+YuPA4RD3YHkUQgkNBoDITBMBj42AgEPDY2Bg7gGoBvLCDsBBQaC4Mj4VBtCywS6rQHYBlgMQIO3gGFsGVKo6FB6sDHwBHB9rGx8KgoqJ1gH2sDKlQcAgWTQSshPDKKQqVNX1QYEEZuVpZJr/9HVpC4p7tV9hzf8tLSgXuJDbdPAIXzGyAUzrmCwjek4atpEpM0bAsM5zSq/GIcHf1OplVLizuN2U2qrEZlVqMKADGgYYDCKdXSwDhnLD8irVYGoXDFNSh8LQ1PoHBShegeCv+BAMLm5+ay6XSzyXQr1VTGxsaKCwvVCkVeTg54zYm1f6v62hsSYDC1OXdi+c4o9NL+ze98Vtn1P5sePTo2TpBW4CUVQyN/PTXCWqDd8fI6rzj+xPLdpP6+PsAYKRbLrVeQMJmSLlxwBBx89OfjR3/+5eSpM2cBCV9ycHH1AuwLONjHP9w3MMzbLwRAcFAozDcw0tMnyN3T19ERGtzh8uXLDg5OgIMDAkPDwqOjY6CCCnAk5Dg0Do7EREXD/AOC/QKCQsMio6Kh8mSA2PAkGkBhJIYQBUNFw+OxRBaFLqQxJUKJXqpIEkkNLK4cR2JFw5FRsfDoWJiTk9PxX46dOXMqIMAvGso3iAKoBzg4MNDfPqQw4GMUChUTEw14GAaLBhxMIuLxOAwBi8xP0ow1Vw9U5rXkGJpSNb3FmR35yaVqdg4/Huv0U/SR3WnEkAwJUSngiYUyPo/LoEMoTCACRiXQmAw2h8/lithsPjCXI6TTWBQKjUwGWwAChrKKAQfjcDjwZkQinkqlEIlEQMBoFCDa8FhYJAaDBpuhMXg2k1eWklVrMGsJFDwMFhMTg0ShAW4n4PAoNBpwcGxsNDTgXFREaGgI+FJQkQmo1AQU/wZ7AOByAhYH/sUhEbEAnBEIdHw8eHMcAQc+KkByAOXgQ4N14ADExMSCfQKaB3YUDgkJBTQcGh4WEhZGplKmD4UHBwZUMhkZjy/Iy5tYdU/3dMuqq6nhMpmgidr2j61wBLizs7+lpafG2l3V2lvbPdg2MvY3R6Yno8JTUfi/0PC1aRL6fHZ2gzK3WS1JJpAlsTRlHFgDaBiwL6DhtDq5HYsBB0+g8G9o+AoKQzR8D4X/qwC25mRmgmuRQau9pQoSo6OpFgsRizXp9X87CjeWZ0ZFRaYWT8fQbqk6IRpHa+n76wH1u1xJOS2h5MLa5lvK/ehvryMnIDXJd+NFHLTiQCNwYGDgVu6K2GU2Jzs6uJw4cfrQocPApyAUPn/27IXLl5xc3bz9AsPDIuNCI+Eh4XD7YHLB4fCQCFhoeIy3j/9lB6fzFy46OLr4+QfbKijEI5AYNIYQA0OHRsRGwZAIdAI8DhUcEubu4eXq6u7nHwgoLQGDwxMo8VhCHAoLODgmLoFE5VIZIjJNIBBrxTIjmycn03gkGpdMZzM5fBqdGRER6ebm5unpERERZkshgAPUCwoKDAkJioyMAByJRqMBj6JQSAQCjscnMOgUNotGo5HIpPhcs3assbK3JLMhTduWndRflm3NNORLyJkcZPTxfaHfv5+CC8iVEnVCjkggZnOYNBqBQsbTqBQmk85kAS4m08H/WVBGBJVKp5ChAZ3ZbA6DwSCRSBhMAh4PCJhKo0Hb8/kCDodHIhGxODQCGQ2DhyOQsSgUhK0qmby7orI/P69IJmah4Sh4DMTQE0WR49FoJBIZB5WYCAsJDQ2OiAiPjQXEGxcLi4uOgVKkoaAzAWAvHhWPjowBDQRYfAIGi8cRSEQSAHOb8OBdUWgEAgk4OyoqCvC0bai50LCwCOh/EAqHkijk6UPhoaEhS2KiRCAov+WbF/d0T5Oqra4Gf4vgT621pWVi1V9Sd1dXYX5+RVnZdFaQGB8fa+ttLGvJym8w5dSps+tU+Q26kmZLXUd+10DL3wgiNhQ2ZNXKAQr/NRrOt2pNRdwLHke3v7dl8xsbXtv+8mf7P4wm+KRWS1NrJmg4tUZqR+H/SsN3LQqDI9g31NXZb+3sbwJtmKHRv+3WBPi1NNTXZ6SlgRPsLVaQKCooUEqludnZd+4XCC4H3d1dNk+qq7//+gTWokTOxnVrY9mmieU7ofEBMT0BQeAM/s+GgyE1tfVHUIt06X89XNpUmRfiH5KS/3/ipm5ycqqrm+f585dOnDh1+vQZZ2dXFxc3BwcXR0dXT08/gLyxiAQYEhuLwMFReBgSFxGDjopFx0Ad3aLc3L0vO7h4evlFAaTF4KEiu0gMEpUQFYMIComMiIEj47EIRHxoaISHh4+7m3dwcDgMhsDEJyTYAqXxWFI8FjAxjUIXkGl8IoXL5EhZXBmFzseRWCQah84W8IQyDk+ExRKjomIB0tmjvwkJGECKYWFQbzOAxXYeBSwIGJREIjCZNAGfLQRXSg6dQsFmGlWjdWVdhan1qdqeovSe4vSaRGkGF53GjPE/8H7A1zuMSM9cES5JyJAKeRwenc4g0elkLospFvC4XAbgTyqVwufzuVwueAsymSwQCGUymVgsZrFYYBEwsQCsEvJFIqFEIgVmsZgEIhaFjkWhYxKwSEwCCoeLT08yjjTVjlYXN6cZFUQ0IwFJIQK8JQIaBiickIDGYjGAhqOjIwHHxsXBANbHx2PAnoKqWoCNiAB5SWCKjkdHxUTDEHA0Jh6BRgEaBkQOhaXBvwQMBmJqVCwsFuwowNPh4RH26DIA4XBb6QoimTR9KAzOsBMVJO6aFLR7+h9QZ2dnXk4OoNi+f1oFiZGx4frOssxajaVSmFIlSq+R2ur4AvqU59Yry1os7X21gLQmtp5e2VE4u24ChafS8HUcDPkKCk/ScFadMrNWcdr50MOPPPTo4488+tgjDz74wH8e+s+K55bEs0Iz6qHBNQANT0aFIdtR+FoanuxCZyzj320oPP7reGd/S3V7QXFzamGjqbDRCNow1e05rT3VQyN/T89LcI4FJ3QwnVj+SwL4e6crSIBPWFBQYLYkJ6ek2Z2Smma2pKSlpV/3V9zRWBaPiDPn3oYOc2DP9PX19/b1g6ndvX19g8Njv473ePzy2fY9h619/7MsPDI6ztHVYEXlQ+D73kigydPXP3DNzuntB2umtoWKk8UvLnseTtdPLN99uo0VJFJS0z28/Jyc3J2d3dxcPXx9A7x9AlzdvN3coNzf8KiYmLj4OBSU0QtQOCYOExGNioYBGkOEhEb6+QUHBoRGxyAwCUQCkZqAhXJVUWgsAOIYGDIWjkKgMAClQ4Ij/P2CQ0Oj4HEA1WyJtQQykUyn0DlkOg8QMJnOx5NZWCKDSOXSWEIWV0Jl8kk0NoXBY3LEdJaAQKLHo3HR0XAAdmh0PCBIDAaiYTgcBhCRwYCitny+AOApQFIuh8XjMkVCLkBhMjnBopIMVBa05piaM439ZdntOaZCGcVMi7ZQwpw/3ey991VVpEM2B5ksIKpETJGIyeLSWWwGoGKpkC/gsWk0MovFkErFUqmEwaAD2hYIeHK5VCDgczhsgMjgTeVymRRcUEUAiQVCgRBsho5HxMSGIpDROMD8CWgKAVeenT5qrR1tKG3PS05hkVR0Io0IOBaXADA5HoXFoAh4LIGAh1AWAeAeDRAcwl+A9gQo1QOwPpgHa+Lj4+FxcRA947AoTDw0vjSZDPYpVAgOmxAfD4WXQfMABoNFRkZFR8fExsbZClBEASoGx2BaUfie7ulu1jRXkACMW9tRklIlM1cIUqskED7WKgBo5tYp8+pV+Q2qgkZVqdXU0Vf3t9DwJArn1qtuhoYnA8P2LnS5TWqGErH02YUvvbLWL8Zhx/uvbdr64hnnn5Y+u+jDT3cYCjkQDdtQeCoNT6LwNTR8FYXFdw8Kj46NNHRW5NQZkqvEKVXi9BpwBKX2ZkxBo6auI7dv6N6AL7+rzs4OhUprSMpKSs1PSsmDnJpnSs6RKfUVFeX2bWqLkoNDI3Kr/nrt2+tUVlaemmxJT03JSEtJT4OmqSmW9PSM/p52i5pPYAq6B/+eZuc0qLd/BMYqkSf/7mh8VqsV2htpyRnpKXanp1lSU5IbGq6OHtfRVJkQh0rJnThAd6FuYwWJ5NQMVw8/RycPd3cfT09fb+8AN3ffy44eTi6e3n6BQWHhYVGwqFg0DI6Jio4LjYgJj4IDzA0KDnVz9/L29ouMjI2PxxMA15EZOAIFKj2GBYhHQsfjALAhERh4HAoqxxsWDYejMVgSFguIjkpnAlwVsvkSOkfI4EgYbDGBysKTmYCAuQKZSKbli1V0jpjJlXAEMrCGzhLiCVQ0CotCYXA4ApVKA/gLpnQ6QygQSyRymUylVGl1Or1KpQQozGJShQIOl8ugUXAGCb+jML0uWdaRn9xXmlFvFmUy43Op1HwKM+irT/2/2CL0P5lOicgSxOsEeIWUwxfy+OBFRQKJEOAu4F0Wj8eRSMTglYVCAXhpkZAPHgW8CywWixQKuVIpA3zM5jDZbAabDY05F4eIjYgMioVFxGMQKDSaSSY1FGYPN1YM1hZ35KdVaCQGFthfaCIOCwFsQjyJkEAhEykUMgBiDCBjLIZCIdnGriNDeRG4BEDJdhQG1AtoGKykUKlkKgUY8DIWh8OCl8Jh0fHxCAQC0DCYARgcFR0bA6EwLDwiBgBxSGgYaTpRGLQv71WQuKe7VtNcQaKttzG1Wp40wcFyQJOTHAxYqrBJC1zUpKlstfQO3tLQIX9NAIULf4PCN03D8nyr1tn/1Jz5s+IoAVX9yd///MWerz/MrFM4+BxftGwBTRGXY9VMovANaXgShe00bEfhu2S0ubHxsbqOUtCMAYcvpRpwMBTOz65T2A6fMq9BWdCormnL6J9eGgYn2H9KBYnCwkKtMa2kprusrre0tgdyXU9ZfV9yZklSktle1ShLTViz7iWSKsv+lFtUb1+fUafJTlQXpBgKUvR25yfrDEpxUdH/iSGfRKY6FLe0f/AGmTPj47+mp6emJopKcpTFVwzm083ilOSJw/GP0G2sIGFOTrvk6HnmnOPFSy6OTu5Ozl4XHdzPnHcBU1dPb5+AwIDg8JCQ6PDw2JCQ8IDA4NDwyMjIKA8P9/Pnzrk4u4SHRSChO/mEBIC5OBJAYQC7eAIFhcbGwVGAXOPjE+LgyJhouG14NiIOT6YxOTyhlC+Wc4VyQLoAfIVSDY0joLOFPKFCItcrNUlyVaJQCtargSUKnQCQMZNHZwDI5XI5fD5fJBbLxGIpmAIIlklVCrlWqzMZjCaNVsPlsplMqkjIEfDZTDrJIBHUpxtqzOLe0ozWHGOJgl5Kk7ewSjtYNamhQtbFs2zPw0nxngU8dBaXmyHWayRasUyhUKnkcoVYLBcKRLZ4s0ShUII1EigHQgxomMmiM5g0Pp8rk4F1IiaTTqORGQwKi8VgMGhYLBqJgiGQMUgUHHCpiM1oLcsdqivqr8zvKshoTdfr6QksLJIKWBiHJRHxLDqZyYDIngZ1vMMRCFgKhTiJwngcFvzfjsJ2GgbzTCYTPIFEIeOIeBwBtEaI4B8AazQaHY/BACyOgcGiY2CxMGR0DKBhOJgJj4wGAD02Nl0ofK+CxD3dCf0TK0jYQDM5qYKfUiWejAcD0JzKwcXNutJmfWmzrqEzd3RsuluPI1NQ+C/QcEGL9uDxL9dtWJ1YxKvoNX97+LNd+97Lb9EyVYiVzy+Hkf1zrZqUKSj8X2n4rkLhlp56ezMGfAXb4ZNDHFyvmnL4NMVN2oaO3OHRv9597c8KIOw/ooLE0NCg0ZSYlltR3tAPULgMQLDNFQ39+WUtGl1ic3MT2KylphAdH59d9ruBzD+lisrKNKN6oDZvpKFwuH7Co41FtbmWRKPhVgab+KeopWMgil4EgHhieYo6OrqSjMrupqTxvsyxnnS7wXxvsznJqLC2/GO6I/ffvgoSSebUcxdcjh0/d+rMxfMXnS47epy/BKGwg7OXm6ePp6+ft19gYCCg4Uh//wBPLy+oVq2/n5PDpQtnTrq6OIaGhCDjUGgEJgEamyIBBXAXQ8AlEJGI+Lg4JKCyhAQcGoWBweJgcAQajSUSqWyuUCRVCaUqnkghkmlUOrNCm8gTK/lihViuU6gT1ToLmEoUBpFUwxerxDIt2J7LlwhFAH8hSyUKmVQhFIq5HIGtsINAAnhZbzQYTGo1hMIcFkUq4ogFHA6DkqqRNWclduQaeouSa5JE5QJJJ7t+kNM3yO0bEg6UxRvFPmcNcLcatqKVW97Kq68QlLLwXAKFqlLrlWq9QqlVKDRyuUoqlQPmlYiEYiFfKODyoagzk8NhisQCgYBHp1MZDCqTRWOzmRw2i0mHKjpgcVB2BBFLSFRI+moKRusL+8tzugrTWjO0WjKSS0DTqUQymcSgkpgAhZl0QLcMBt1Gw1BpNjKZSCKR8XgCDuAymUggEvB4PJlCIZFJNDqNzWFTKBQswGQ8Dk8kkKG+e3QAxAlYqAwcQOFYODwWqsGGsY9HjUBiomJgDCbzJvPKbgMK36sgcU93QnU1NRwGQ8Tj/YMqSLT01EEgWClKr5YBcJzk4PwG9VUOturLWwzAFS2mnsHbdpv4JgWhcJPBlrUMofBVGq6/isJXabjuGhoGaFjYqvvuyN6t2zcllwlLOoz7D+3Z/eX7hW16tha96vnl0QTfnGYIhf+Ihq9NkzCV8813Bwr3D/fm1pts8WBJeo398E1wsP3wFTVpS6BmjK7Mqm/rrRz/dZoyUAG25mRlUYlE4y1WkBgbS7VYSDhcosFwJ1C4tqZGozcXVraV1feW1nZP2g7EJkt2VlbmxKa3ScMjIykWc3WOZaypeKShyO7hhiKw2Feda9Grqmumo0LF3668is4QUkFSzvXdi/MLCnPTlWM9GWO9AIUzr057MwqzVdnZORPb3fUCP9ch0NIaHLyVuyJ2JSUlnzvvePzEuYuXnJ1cPN29AhycvS85enl6B3l5+7p6eLh7+gQGRQAU9vH1dfOAVrg4OwEOvnzulIeHc3h4GJvKUPNFMiabiIlHIhFoVDwGlYCIQ8Lj4tAYKHkCi8UjEEhAwyhUPJlM5/ElErlGLNcIpCqp0qA1pKp1yWK5FmCxXG1U68xKbaJQCl5SyeXLOFwJ2J4vkLA5Ar5ABPBXAFgUQDAX0CeTQganATKDzpRIZVB6hFanVCi4PDaHQ5WJuRKhQC3iVWWa+ytyuwvMdYnCQhmpnps6wO4eEHb3i7oBCnezrZaI8ExEXA+3cYjXP8TpHeL3ikNE33xzMA6J0epNaq1JrTGpVDqJRCbg83lcFp/HFPBZYhEHzLDZDIGAy2ZDoWAO+D+byWKxOGw2l8NgsaiAhul0mkGhqspMGaotGq0v6KvI6i5JrzKJZPFRQgqWw2VyOSyuLaJMpVMAQTMZ4BlUIqBfHIZEAmRLtFUsBqRLASiMxeEoAHnpNAYLYDOTSCRA3epsKEyhUcFKIploqyyBxxEIyHhUHDIOg8UlYIlIKIc7IToGxmKzpw+FwW802WyWiUQV5XdvstGfUndbvYTP4opVTW13+1DSo4PdRpWYSueWVEERl/8lgSsrm04Xcrm3iMJdnZ35ubllJSV3uoLE+PhYqTXTUilMr5YCkAL4CBATsBQAKXtM0R4PLrMCCDbabLB2F43/Oq2Bq0kUBoT3BzQ8ScDX0DBA4RbdLxcOrH/5eX0eq7Qz8ZtDez796oPsBpVb8Nl5C2dTJLF5tqjwdTQ8icK/pWGAwmDl3YDC9Z3l4LMlQ/nBEAeD75tzhYPB4ZvgYCt0+AAK17SnDo5MUy9hwAGNDQ3ZGRmV5eW3WEECigrL5XciKgxeMD09PSk1/zoOtru8vi+zoEZvTOrq6px4wu1QY2OTxaDpqsgebSweqi+c9HB94UhjUXGaMS019U5A/10ovW2oucySq7t3cHDInGRoqjBAIeHujKke78tqqTaZE3U9vX9PN9C/UWZzyvnzjidPnffw9PMPDAsIjvL0CfXyCwsOifLzD3J19/Dy8Q8Ni4E6yfn7e3q5u7q6Xrhw/syJX5wunfXycobFRqXrDNbcgkKdnh6PQsbFIpCIOAQiKjYmFh4LhYghmCMgkeiYGBgCgSKTaXyBVCJXS5VasVwN2FdjSAEoDJhYqtQrtUlqvRnM8IQKDl/K4giZTD6AYDaHT2ew2ByeQCgSisVcHo9KpZFIJDKFTKNRAYkKxSKFUqXVaFRKOQBmHIUSFgNHx2NzzMbB2tK+ipx6i7xATMjioJuYWQPi3uHCwcHU3gFeTx+3IxdOLMWIhvgDA7zuAUHXkGiwGJH7wavvf/DxTiQ6XqXWq9QGmUwpEgH+5nLYTEDDQgEbmMOmc7lsHo8LpUTQGEwWm8Fk0hlMCGYpRAC0NCqZzWSUZKT3VpUM1xSM1OUPVGb3l2XkSSgCWLCYRhCJ+GKA91BGB43OoHJYNBaYoZEB9BKJWCqVZE8VJpGINBodfN8rhdtoYAoFgCeqSxCAiWQSwGUcAY/GYKCCEyQKFo9FQOH6BCKJisUR0PEQCgNInz4UBu/U1tbW1NjY2zs9Y1qOF2SY6RSyUKZu7vwvt87BefDPZkSNj3SFe53fsW37p9/9ophS+708Te3r45eYf4P7UH+j+NjID9/e8cHHn+LZ8j/5PcGeudUW9h1VV1dXQV4euHjfYuWHmqoq0JSWCoV3uoJE/3BPbr0hpUpkC6DaUoSncHBRM8RSAKRs8WBjZaupstVY35E+cseqdI2MjvcOXE9OI2ODRVdQ+KZoeAoKAxdYtYFw52UrF+F4EWXdifsP7Vn70nMAjpevWvzmO5u1ucycRtVvUXgqDU+mSdhp+C5B4aHRgcJGi7lSmDbJwVfC+VebMTYOLr/Skunsr5948p0XOI+Nj43dItWBJ4NT9G3MFQafp6+vr7e3Z2Cgv66uVqs35RQ3AOq9joOBy+p6Smq6tIbksrJbGswMesf+/h7wNcD79vUD+C5MNdoiwVc52G4Axy0lGYl6TWvb35CRP/0Cvw6+oY6hLO/qArsH7KTB0tLyNItysC0VCgNfi8JgzXBHWkayorx8Ooa8vnUNDw3V19VVVVbe+k83OTn9wkXn06cv+vqHhEbEhl0pIRwTA43iGxQcGhoWGR4BCw4ODwgI8PbxdnFxOXfu3Pmzp92dLwMUTkDH1WTljFZXl2hUCeFB8OjwWFhMZExUYGhgeGQYGo2yFzdAIJDR0bFxcUgbCkvEUqVIphLJAAoblFozQGGFOlGuToSyhNUmoVTNFSq4AhmDzacxOACFWRwek8URCMRSmVwik7M5HFviLAlAIYfDYrOhSr8isVSlVul0Wq5I5hwQs3P/z/u+OUTFoNqLc7oKUyr13HwRPosd18hPGS0cGusbH8rsG+L1ddDrkoKjKxO0Q/zBfmn3UHb/sHHYSqo9+P43s+bM2vnxTjgsTqlUy2RyKA0CiMsVCvjAfB6UB8Hj8fl8wOtsCpVBpjIoDCZAYiKREB+PwOMSoEE3SPiKnNSx5rKhmtyhquyh6tyB0vQ0FloAD5LQiSLBFRRm0BgMCodJZYKXoeBJRCyFgqfTAeiTKBRAuUQahQplCuMJYIZKhkoIx6OBUDjQziARgQENE0gQE2NxeAKRQqbQwTwqHhmPiadQ6WC3YxJw8DgEXyCYPhSeXo0YRKRP3n1r2/Yd27dtO+8eVtv6ByQxbpYTQ5DEjj8TSWkt0r2x4bmfLgUw+bLC8qv318ws2KubthBV+RPLd4HGh5v3f/jGu3uP0ujMtOyCPxUvqspQBYeFV7T/749FDFCYQiCI+fw7XUGis785q1aZXi0FIAVxcJ3yKgfbYoplttQIOwdXtSYCN3WnjI7fqT5Yg0NjPH2tIrmhf/Bqm8eGwsbcOsV1KHyTNJzXqJaYCW++86pr4JmSDuMPx/bd/+/7//PQg0uWL4wl+WXWK9JrZKnV0puk4T9A4bauoakf+06ra6A1rUaeWi3JgDgYymwBO+T3OBhqxrQYrd1Fo+P/p4fytlqtSUnm5JTU1LR0jUYjkasLylvKbxQVBi6r79XoU4oKCyee/JdkbWmxWMxpKVC9CHNSklYpby5OH2u6JiRs9zDg4/rCDJO6sLBo4sn/62pp7xHIEy3mxIw0S1qqRamQVxRofxsStnu8L6uyUJOWmjw09A/4DXd3denUanAOr7nlChKpqRlOTp5nzzl4+wYFh0WHRcZFxsTHxuGQKDwSlRALQ0ZFx4WERgcEhgAU9vHxcXVxu+zo4OTs4OXh4u3tSsCgmvLzRmurSnUKZIBnRJBPeHhoaGiwp6erv78PDBYDh0Mjq0VHR4MpCoWiUOgcrpDHF3P4Yr5YLpZrpUqDSmsBlsgNIqlOKNXyxUqocIRQzuKJGGwekyOgMdgAiO1lIiRSOZXOANxHpdKYTCaLBUVhmQymUCCWq9RaoxmGY+4+cGHta7tWrt387pvb4X5eFQZhhZqex8fkCVDtubnjvb+Olo8MSXsB/pYgjagj+3KieUPCkUFFz6h1ZKx5vJ5R+cXrO5965skF8+e/98674WGhUqlYLpOKhCKhQCQWScViCSBjDocrEIhEYhmAezqDSyTRyXQWjcEikohoDCoBj8HjsUIaswk0FerLhqpzekszBsqzewosiaRoHsxXTMdy2DQWm8FiUMhEPIWEZ9JJdBqRTMaRSTgqhQggmEol0ekUGviqFCoZ0L8NcsEMJh7KRUGhEDg8DrAziUK2AzEAZxKZRqWx6Aw2mUKNT4hHY9AUKljDIBDJmASsWCqZPhS2xwbAj3XoFrp03KTaavP2vbV5+4f7MARSqK/T9wePGzP/oK9eP8zly/Xv7y/t/BPRlEw+atP69fLc69PzmyoKuBxeaX37xPIf6vfiN38qrPNfg0D9dalvvrbRM141sfwb/cEraPFe6zZs1JT871eG6uzoyM3OLikuvtMVJFp6ajImckwnUoTtIDXJUnYOrmiBOLi6LammLam6Nbm9t2FkFIrgAg9f8cT8yNjV+UmDlbb1k77hxtDn6RhwgmV955GE4pUVVXWNjUErJ1C4fgKFp9LwBAr/IQ2DNZk1ciQtmCiMymtSx+B99333yaETXyHpQdkNquwGZVqVNK1aevM0nPgbFO7qG5Ga69H8stbO6etT2N5Xk14rzqqzd5WDOHjy8BVPNGMMwBAH2w5fVaupviN9eHSa7i+3tLQUFxbW19beYgWJXntd4e7bEIMfGx3NyMySqYxJqXmJKXkmS7ZUoU3PKb9hVBjwcV5Zs0af1NR0tYbXnxU4mWVnZZqU4uI0Y3G6McOoMijFbWVZ12VHTBogcnaiJjvnH5MRe4tqqK83aEUlucryfHVhlkIl59aUGMYHsq6DYLvH+7Pqy/QWs7Gv787eK7stAudwmUjEoFBuvYJEenqWt3ewg4O7m6evX2BYSDgsCoZBxpPQaCICmQCDo20oHBUYFBYQGOjn5+/j5evl4+MF/vP28PHxwKMRDXk5Y001DemJ2FDvYG+XSECOwcGers6+vh4RkaFhYSFBQYFhYdBYGAm2m/UA1ADaMlhcNk8MeFcoVSs1ZpkykStQcvgKnkgtlGp4IoVAohLJNGKZmiuU0phcnkAiV+qUaqNAJAMvQibReFwhIFHbwG9MHocnk8g1hiSGWHf0YuDL279csXbbomUvPPP0nA1rVqN9HPJ4qAIBuiFTM9jX1lnb3iVr62d3totadHEkn29fl3sED/G7h8SDowUQJlYmlm55/qUnnnpi9uzZc2fPfvP1rb4+XlBnPYVCDNUXBq0qFZQ6DNVxU8gVWqlcyxPIaAweg8lnMFhUKK6Lw5Fxcqq4jJvfq24astQPFRV1l6Z3FqY2p6klcB9yiKOIgmLRCCQKjkjCEqBiagSAwjQqkUohADSmAualkMFLsdkswNxsDhdgPzSone0BLA6HjkejUEg8EU9jMKh0OjQCB6BkiHrBhnwWG0opwRMImIQEElhLZ9LoLLCBXKGYPhQGhJGdmWnQam+95t9/VY6OumDOEgzPbFsazM3MqL7Cpv1dVrVMQKExEtMmeu/lWcSHPn1l8frXw+Kp2sT0YRsKTFVzdbGAw6RQGSk5JfbH6iuK/E/88OySVd5R6KT0vN6hq0/pbW9IS09r6YZwv7QwM7+kvK25RsxjCqSajn7o+pSbaqTTmdklV9G8pbFKLRdRqVTwE7d2Xe3v0tveqJLwaDQaON6gkZeeV2K/vtWW53MYVNAoLK27mh3b2lgp4jKYHH5J9TV03tnWyEZ7Pffc8m/P+cq0Sa2212+sLuax6HQWd+rG1vpKhUQAPoZEaezqh3CwtjTH+5e9i5YtdwuPF6qM3f2DXdYanTaxdSLhZLysIMucnDs09utIX0dacnpdU3OmWcMVSBtboRyY3o5mhYgL/sQtmVdjPKODvYkaKViZmJo7MHL9rv4LAo2rooKC0uLiaRsa4xbV3F2VUSPNrrt6b/0qBzdf5eBKGwfXdZhr2sxEhSmGlZUgLL/txokr4tilB30sbx9Xf3hGeyk6w5wL9aoZGRsCKJxTrwAf73dp+A+70IE1+c3anAYVgP7USklSMS+7XlnUZtBk0ROLuBm1MoDCN0vDAIUrBJMoPDQ8ll7YHkkv/tYj6Sc/S255x8DQaE//iN29/SMDQ7ZmwBRf136Y6tGx6w0aA1c9DqEVlHNgm9Z1FKdWS2zNmBt1lbOHhKccvuq2xNr25IHh25n5+nsCHzEvJ4dFoyUZjbdYQQIabU4muy2jzbW3t2l0xpziRsC+ZXW9YGqyZOlMaSU1XWV1Pb9B4T5LRlFycsqt5Dp3dfeYdOrmorTRxiLg/pq8ZJ28ItsyeqOo8EhjUU9VbopRXfN/o+fc6NhYRkZqcY4CCgP3ZgAXZCpz0xUQ+PZcz8HQmp6Mgixl7p2pJXLbBS4B+Tk56SkpoCE3seqvKjMzJzg42sPD393Tz9Mn0D8oIioWDUfiYmHoyKi4yOi4iCh4aHh0SGhkcHBYSHA4cFBoWGBISIB/gK+vDw6FrM8BKFxnzTbjQzzDvV3gsTFhYRH+Pt4B/t6h4UEhIUHBwUGRkVEoFCoeCIPF4QG00Wl0DoPFZ7BFAIVlShPgYCZHwhWqBBKtRKEXyXRXqkmYxTINhy8RSVVgUalJ4vKlJDKDzRbIpGq5TC0USvl8EcBUnTZJoU9xCUl445Ofnlu/Y/GStXPnLnt6xrzHHnvyg1dfksb5NGYohnvbW60NWYbUbGF2vjgvXZ6i4TNwgRfQF47mIhT93I5+UZc1u6Gpvv7kL788+dRTM2fNmjt7zuyZs155ZZOXl4dIJFIpNQq5VqnUyOVKiIOVWqXaoFAZJHKtUKIWiRQcFpdKoeDJeA6VU8OpGOJ1D7C7+7md/Yb67sIsa7YxT0ZFuRyj+njlMQzZzGQ5VUih4ql0EofNZLNoFDKeQoJCv4DvAQFzuXy+QCgQSfjAQjGLy6XQaQCTcbYOc+j4eBKZDHXQY7AIZBqRTKPQmDQGF8qxZgtpDA6RQgWgTaLSqXQWg8kFlKxUqacPhaezgkSenj5v5rxQvHRi+Yr62+qDHU/t/GTX53v3fPjJXprECFYyEB5b1s5/ev7yjz//1j0M038tn5Vn6o99+9kb27Zve/P1He/tIomhIUBTVLxdr26YOWPOuzs/9YnE1HVcjSOWWlg/HvpenQ/VAMLDPI6fOeN49pf33nr95Vff9AqNk/HJ336+67XNm3Z/+0tiHkTDfZ1NXmcPfvzxzv37v373rbcvekQ0dUGXgf6OxiCHE2D9Z7s/Wrf+xTffep/AV4NjVZ6h+embvZ/u+ezTnR9+8/P5jHKoG1xXS5XT6cOffPLJ7k8+OXU5uLH76uepLEw998MHM2c+s/HN9045eBfVtNQXJh/74etPdu3Z/fEH+w+ezCqDAjADbRWOZ45+8OHOb/Z/+fZb73qEosCHSJJQd29d89Qzs9755ItfnAObO7vzDJxPd3+fkm9PfxzDh3seORHQOfxrf23BpeNnnVydvv38g/d2faXPqBrvbfBxhD7/F5/t/vjTryhSi+0pwxRE6O6dH+/97NPPvjiQlHsbGkX/uAoSzd2VAIUnOXgiNQJKEb7aVQ6AVFVrEoDgug5LbbuFoUuKF2ZTlVUTVlyxbZ4yZX7SYOXE+knfaGO6uhorKj/kC6HwR2d1jrDM9CIoaXIShfMawG/5pmjYTsC5jWqAv/Z5MJPXqM5v1hS3GwpbdRIL4bL38Q9376DJYLlN6vSaKzT8hyhsp2GAwmBl/zCEwlkl7eci0t89qXnrF/V+18QwSmE8vwzBLUXaDGYgc646XlCWILpK/wRpBUlWSZRCBjM0ZRVDXc1Q2ayu5uhqeIZann7C4sR6SZLdDZKkOpo2HS2Vs5MUmTXK/IaJw/cbDoYOX7Xt8NWAxkx7ct/QdOShAli5CytIFBUVaY2ptrLBvRAKN/TlFNfLVYZce7owRMMTLq/vLa7u1BpTystvKaRXXl6RZlQN1hUAzB2xlYkoTDWmm9QDtQVgcSoHA4NHa3ItliTTwJ2/V3kruu5A/OXj0t7eaTIoOhvsddMyx/uzWmuTkgzizkbLjbrNZfZaLZZEZVPTdFew+WsCu+V2VZAAKBwSEuPtHezlHeDh5efjFxweCYuIigsOjQoMjggOiwkJB44Oi4gOj4iJioJHRsaGRkaFRwHejfQPCMKiUTXZOaMNtc0ZJnyAS7S3CxIOi4iODQ+LiAgPDo8IDg8PjYmOQkLFh+PBFIXGQChMotNoHAqVTWcJZUqjRK6nMYUAhUVSrVimkyoMkJVGhSZJqTWL5VqeSCaWqZUaCIU5PAmDwRWJ5EqFTiHXSCVKiVihUhu1xgw4jvfx/jMrN76/ZNm6+XOXzJ61cOas+Y889tScpx4jRPmP9bZ1tVnTzUa1XAURLSSJUa+lJqB+/uJdn0MHuF4wzOnQnz/7nsfjZGdnf/zhzicefxLA8JzZc2fNmrVx4wYHh8sikUSnMykUUFRYLJHJFRqZUiuRa2QqvUJjBEwsFkoZdAaBQjIzkwZ5XUPcnn52Zz+7u0/Q3pGSXZUk5MM8I4//nBOn6+d1AERu4FToOAqegCUS8bkcBkBcAh5Lo9J4fKFEIheLZSKxVCSRCSVSAMSAjkn2VGIiwVZGDTA0k8HmUplsIoVBojKpDC6dzWdyRAy2mMLgEqkMEo1BZXIBH9PpYIczAcRPHwpPZwWJjrqCL956aeXG7Ri6eOoYQvz4wDdeeyscRdEoReeO7Nvxyf6C2s6aPP3xr9549pW348h8c1ruyJSo8EiP9fKPH7+weXtQNAqPgX2wdcOmbZ8WNXd1WGvDL/783PK1gXBccmZBz5S3yJLGvPzKiwxLJZgPctgPmk4f7z2IRKN//PydOfPmv/3+x5c9A6OCPZ9bvPCkBxxs09lSE+rpGIvEaXXaEPeza9ZuoGuywXojN27Dupecg5AcGnb7hpUvb9+dVd0y2Nd2+dDnH+35nidRcanoD3e8csoXCTbWUSJe3LA5AstgkLDhoci6Kd0EuzusIkLQC2tX/ugQrE1Kb2mq8jj1wzu7vqPzpEImcde2Lac9YsCn720q9PbygaEJOq3K78KPa19+U1fQ3NFQEXzumxXPrfaLI6tNaWO/julZsGVLX1El23tRjAae+2Hbe7+0Df7aW5Hx6Wsb5y5fdc7FPYHEqKxrZiN8X3/zLRiOplGITx7Y/c7nP1V3j3dXWT54Y8MRhyCJiB/q62vJg/bSLeofV0Gipac6qxYaWvlaDr6mq5w9NQJAMHAd5JTO/gZw+QO/zbHx8esNBS9/44mg5m987Urwedq7hpzjsg54JeHEFWV1E/fEryRIKAEKQ76Cwjeg4SsobKfhXPClWnT5zVrAxAVWXYEVCgzz9ZgL7ke3bt/08CMPPfb4IwRBVL5VO4nCwP81MJxYfhWFS+u6Y5kl+90S3zmh+d7LDHBWaKoDCGs3W1sDiBYALk0JptDMbxsJZHnlNZZVXTEEx3ZKnjRAZ2CirBIvKY/lJAdRlTilKrNaWdj0eynCUHo3lNnSbgau70jpG5qORhrggLutggSAEqMpKS2nYjIzGIoE13Rp9Mnm1HyAwjZDiAxc0dCfWVCrM5j6bqFf9cjIaLI5CdDtaOME9QL8bSvLSlRLm4szAPgOX6krbH9oqL4g26wtuLXU5JsXQLTGxsauKyNaDw8PNzQ0TPYjHxgYqKurA1P7IlBbW5t1SoATzLdeOcuBw9Tc3NzR8edS1wqLirJS5GNd6RMx4J6M0a701CRpRaEtXXhqYLgHQuGaEl1ysnnk7u45fSeUnp7l7x/u4eHv6eXn7uHj5e0fGBQeEBTuGxDiHxQeGBIdEBwJUDgiGh4di7TlS8DDomJi4ChYLCooOCIeiarMyhlrrGtONxB8L8d6OKDjYmEIFDIOhYDFREQER0WGI5EIwMEoNBqFQmFxRDKZSaOBSxmHRGKyuRKlxgxQmMEWsnlSiUIPyFiuMoGpTGlSqJMUmkQ7CgulSrnaANqWHJ6YzeFLpABkNTKZCkpbgLrfmdBk8bfH3F/YvHPhspfmzl02Z/aC2bMWzJgx69GHH35nx5vpKea+ro4ci0ElkwF4Vao0cplUIZMBro2LQ29Yt/bl5xbu2bZp/ZL5D9533/Ztb2RmZijlyg0vvfzE4zPmzFkwZ868mTNnrn1h7eXLDgq5SqPWAULlC0RiiVxk6wKo0BgUWsDuRoVczeMJaHRWITN/iNM9KO8Z0Pb087sGuN1NilQTPSr8zDds5+hBXu8Qr3eQ0w1YuZJfrJRIxGI+h00nEjBEIuAbulAolkoV0OtLpIC5xRLo7eh0Bp5AwBGg+mp4IhFPJFEYLDqbR2PxqEwenQUNU8LgCJlcCZMrpbEEJBqTTAcbCKAOiHQ2iUyXy6cxKgzeaRorSIwaJaTN61YuWfH8kbMuqQVQ9HGkr/nwnje/Oe1tD5nmJvJWP7c2QZgK5tFeBzZ/erD6N2PQF1lEa5fO90Xz7IvJIvSzS5eFUw1gPoMb//rGbSmV19/6zJbBXn1tEysZgjzfc/sWrlgrMBaA+VwNYdZTT+w66NQN7fC+s3u3vvP54RZwhhkb6R+YCEg0laW9tnmtl+3tYi/8+NauI/ZoUrzP4Xc+PwTeqTKF+/KGl1D8FNvqXxGeRze8ua/C2s8IPP7iax8VtkKff3x4eOTag9pXa3n7rS0R/DQwX5su2brhxVgmFA4HIgSefHHb7twm8AFG+q4AfWOu4uV1L8cyoD2TSA3cvPV1U9XEIdMyYatWblWnTqBw8MWf3v7oJITClVm7N67YvOtQbSe0d8f7ar/66M0fL0fYz6C5OuqLq9bTtSUNacz1zy9HiidGkBoeuQ0ZEv+4ChLtffV2ppzg4Bt1lbODVC0EwckAhZu704ZG7xRO9Q+Oikx1uozmgaGrF7zrUfhGgeGrNGyLDefUQ8FgAj9KaMTmN2sKW3S5DSqGAnHK4eCrb2589PFHnnjysR3vv+YScFqfx86qVwIUvnkaTgIoXCnps6EwUFffsDK10RGWdTkms7ntv0dArzYAxsaHRsbAN510T/9IVy/k7j5oChoGrZ2Q22zTpraBhtZ+4Ma2gfqW3vSyPHmW3Fioyq27kiJ8PQdfc/iAAQr3D99Uz4FbFyBX6BveGr+CJ9+uChJ1dbUag7mwog2Q7pQsiN607DJw8c4pqssvs+aXNueWNOaWNOUUN+pN6bdYxbapudmsV/dU5YxcQWE79WYmagpSjWDlaGMxoGQAwcCAjNtKMxP16o6O6chgATIYDPv27fPx8bEvikSiPXv2wOFQTASISCTu3LmTRqPZFwH1Xrhw4fDhw9W2lEJAyYcOHTpz5ox9MT8//8svv3R2dm5pub5O8O9pYGDQnKRvrDBMLRYBeLeqWJ+SKBnpTB/vz4KAGKooDAWMR7vS0pMVZf+Q8hFAt7GCREpKuoenv6OTu7OLp6ubl4enr69fsH9AaGBIeEhYTFBIdFBoVGQMAhYXD0dggKNjEeHRsDhUAhpNCI+AoRGo8qzssabG5swkgs+lOI9LOCQsHoNNQGPRiLioqPCYmCgkCgoHo5AAhuMJRAqNxmEyeACF6XSeWKJRqpNkCiOXr+AKFDKVQalJUmktCg2AYKiyhEprlii0fLFcJFMqtUaV1sQXSthcrkQmhwo7SOUyuUok1cAS6IfO+ry8fd/iFZvmzl8xe/bCWbMXzHxm3mOPPf76ls0yibiwoEAjFWrkYimgZyiUC56rANiq0hjJdM4HH38285nZs59+9OEH7nvw3//vPw/c/9PBH2uqa+LRmGXLVj311Kz58xfNmzd/9qxZL7ywzsXZVSaRKxUqoVAkkSmkSo1MpQUoLFfrZSqdVK7k8PlMOquUXTjItZWkqBgcEHWD+WKGguh/wv3bXZnRuiHR0ICga0DePcTr7uBZkyR6oYjLoFPIZByVSqBAReIYHC5fIBAJRJDBe3G4PDKFCtVPw+NJZDKJQgHNCiKVQWfzWTwxkytmQQQsYnCFbL6MzZdTWUISjQUMRYWZPCqNg8NTAFtPHwpPu8YyTLJLv3y/dOHcbbsO5la1D7QWv7566aZt7zs6XL502fHnQ988/cSCUIISbIry+O6V3T+Wd11/CVGQI5aveMVSODHi0Vh36QevbTzsEAPm09jo1ze8aS67HlCy5fDXXt/MTq4C8x7n97+59+CA7VVbK8yvr90YmiCHFn4dgbvs3/bZgepe6LEuay2bhPF2d/vlp+/mLl/uaUPhmPOHPv7ipL3Jj/H+adunBwA3Gwh+Cxcu/OLgicuXLjk5Xt7z/tbla9/NLGkt0DNefH7NgV8cNclXyrpNuRx2VyW+tWNLGCcZzGspkcsXzP3ix2OXHRzAbtj30WvLN+xQ5UHI3dlcScUh3F2cfj745fKlz8Op0PYGkv8rr201lE1cKgAKr37uDU2qPZo7GnLp8Ls7TwEU7qnI+HTTi+6xbNv6X9uLtVtfXLHp3c8vQW/jcOzQ14vmLIulasY6y/Z/+Pr2j/aRuMqOvjsbfP2zsqOwRCC40xUkegfbAWXmNagASE2y1G+6yk1wcH0HhMJNXRnDo3eqATkyOv7bIgwAhYuboW5zUHbE79PwZGAYOLdBDWh4/4+7X96y3tH3BJoZcsrx4KtvbHj8ycfmzJv5xbcfhyBdJRZCbpM6G3BwtQzyFBr+46ThpAohhMLXVpCoqO8BBN/VezUd6E6rtbc8r0Fe0HhdM2aiq9zUw1dzpRnT0Jk2NHLNx/4/IkDlKSmplvTCqRwMDBYLK9sUGpNCDS7hSTKlTqtP0puSNbokjVbfdmtFzTIyMibqptkI2G5AwA2FaUaVFEw7KrLbyjJbSzJaSzPaSjPyLNqM9LTrz/t3TGFhYffdd9+6devsi+DcCBY/+eQT0HoZHh7++uuvweKxY8fsPXdzcnJmzZp1//33m0xQVl5ycjJ49KmnnrIvcrlcsLh06dKioputfVFdU5ucJB9sS7mmblpvZl9LapJBVFtq6GtN7bWm9DRbupvMvVZLfZk20ajp/Sd0mLOru6tLq1IJuVxwMp9Y9VdlsaQ4u3hcuOh42cHVxdXL3QNC4eCQyIhIWEQkPDQ8JioagUAmoNA4JCohLi4+JgYRFR2HRGITEsgxMBQGiS7LyBxrarJmpxF8HNFeF0iIaDw03BkBGnoYFhsLj42BwWJi4GgkJgGDJRCpUF4Eg0ujc3k8mUJpUqrMcrlJKNIIxRqFOhFwsEafotanqPSpWmM6oGGAwiLAr0qNWm8Cfz5ShYrN4wEUVqu1arVOrjbGYpk/nvF+7f39S1dvmTP/2dlzFs6cNW/GjNmPPvLEq6++CigyPy/v0vlz8OgIjVorkaoADcsVaoVCo1TqoMHkVPo4FPH1be//6//9v3//677HH37ooQfuf+LRRzw83Ovr6728fGbMmPXMjFkL5y9auGDRnDnzXli7zvGSA7iAajUatUar0hqlSq1UDo3/LINGBhEz2HS5hFevKRqU94w2joxWjwwJ+zuZjUkJeHzA2fATh8sx2UO8vkFdz3DV4JC2t4vVksRTc7l0MoVIIRNpNBKBgMNgEkCzgclic/k8Lp/L4/MZTCYOT8AkYAkkMpXBJNMZOBKFQGHQWAImV8LgiOlsAc0WHgbzLJ6EROfhKUywAZHKJFJYRBIzHkMQCKexgsSYrWNyZ0fHreSx/VkNdLfgorwWzHjaNYbWVJP78pIlL2zYuv/rr7744ot9X3z+xZdHlMngPDIS5/bNK7t+KO24/pTIRfk9v+7DvMorZ+cx677tWw+c8gWzKUzk1g1vJhZfX0HiGhS+8N32b36xY4a1OvXjl7ZE4exlHAbi3L/d8fmPDUO/jvY0u544+NaOt7765tvdH7379MKlXvF8sIWCGL527csBcDyfgf9o68bDl0PBSkGE08K5c97bve/LffvAV9i7d885l+Bqaz94FVSY19tvvLJ1+/vhSEbPlG58QJ0Vxh3bXw1hQZ0IOSi/hbPnvLfrs6++gnbC3j2fnnMNKG8dHu6odDrx4/a33v16//5PPty2aNFqpC0qrCP4vvLaa7qSibCWDYXf1KbZTzTjEU4/T0Hhl0OwEtv6X2tTeJufX7h+67tfffU1+KCf7/3swOGT6lTolJ0oZX+z+92Nr2w5fMa1ZEq3v79dXZ2dBbm5ZaWlI3e4gsTI6FBla3p+g3KCg692lbuaY3oloAihcH27pbWncGx8WlsOdhTOs2Vx3CQN5zao8ps0Z5wOPfzIQ08/8+Szq5f+6/5/zZ4784djX8SRAxILuSXthgKrNrtBmVErz6iRT9LwZGB4Mmn4tzQMUDilSvrbYmqAIUZ/08/1zqmjvwHsk4LG33SVs0IcbO8qZ+Ng6AjWAxTusFi7c+5cTeipArvC2txckJ9fW1MzegsJEuB1AFI0NzWBv4iJVX9JAIUTE5OSM0ugnOApKGx3VkFNcnqhSmeRyeVlpSXV1VWVFeXgEnuTF6TfE+DFknTTdcUiABn31eSn6JVJGhmwUS1LNmpSTFqLQaXXKBubpm/UIcCva9asOXLkiH0Ri8WuWrXKzc0NzIPdHhAQANA2NjbW/mhtbe3u3bu3b99eXFwMFktLS994441du3bZFy0Wy9q1a7/77rvGxputtlFeXpFukQ93pF2Dwra+ccW5ar1GmJIkSdSLLInyFLMSQLNBKwE/p4kn/xPU2dkpE4uZVOqtJ2GazRZHJ1eAwg6ObpMoHBIaFRkFD4+IDQuLjoWhbByMRSAxMBgqJiYuGqAwIiEeQ4yBI7FoTGVWzmhTc1NWGtrtPNL1BAUWRoCG/iWhUfFwOBwWB4+OgcXGIBIw0KBpJCg7gsNgcukMHl+ogLIglGaZzCgWQ1nCCnWSWmeBxp8DKKxL0RnTNPpkkUwjUWjkar1Ka9IaLAA62Ry+VKrSGSxGcyaRrTp6KfiNnQeXr3199oKVs2YvmjlzLoDXRx55bPXqNUwms7q6+vSJEzs/+phOZ6oBtso0crlGqdKpVHqVUqdUAJ41GAwpYeGwNWvW/uu++x68/1+P/ucBMDN37hwikVheXv7lF/see/iRWTNmLpi3YMH8BXNnz3lx/TpHx0sKuTzRZNFok8BrQqnDIolIJOXzuUoptyw7sbegcDCzZbx7fCRtsJfZWkQ2K8lYMRVNiQwuxFqGOH1Dmt7xrrHRilEru1JFZbIYeCqVSKcz2GwWHRqRGXAxnUqHOs9x+HwOX8BgcvAEMhZPItEA/nIoDLYNc1lkOo9M51MYfDKdjSfTiVQ2gGNgPIWFJzMIULowAGImFk8FB5HHF00fCg8PDWWmp+vU6ltvsf0pjfdZv3xl2Z4THsUV+W+uee6kW0RdXW1JSXFJSWllbb1tbIF+uNs3mz89VGWL0U6VGBey/Nkt6aUTvDveW/7Blo0/uyDA/M2h8Lfbvv7ZVozh1+aq5I9efDUiQQEt/DoAc/tmxxcHW8Z+zdeRVq18/lIApqKmPs0kf+31De5IDtiCHe+/dt1Ln+75dOfHH3/305m0IigyLYY7r1m3kaLKrKmqACfE0rKyRmvb8EREbyQjSX324J61L7/FNZbYV9k1FYUlmKDVK9ZSFJba2hqwE0pLy5pbINDX0GLWPvt8IIpeVVeXrmdtWf9qLCkJrNfivV/ZutVUORGS1HMQq5/dor9SmS7s8uF3pqBwYLzQvr4pR7rlpVVngklVNbXgOgdUU9fYd2Uch5rS3NiAy2tXPecexbKvuRXZEba4oOBOR3Nvo6zdZYVN9u5WU7vKQTHFSQ62hYRT7DTcPThxX2LaZENhU17D9SgMzf8eDddDgWFlGtU30uHjPW89/sSj991338zZM045HFSlUwusuuI2Q2GrPrdRlVEjs/mPaPjGKHwlQeLvUt9QR2GTAaDwtRxsT42Y6Ok4mdliO3yW9r6y6WnGAJbKy8kBHJBoMNxiBYnC/Hy5RJKTlQVec2LtXxI4R+kT04urO29ULKK3vKHfYM7My7udsAX1mTOpB+vyAf5OpeHh+kJrSUZZZpJaKjAYTZVVVZVV1eUVlTW1tXe6b8BUtba2ajSa3NyJe3eAYpVKJWBc+yKgE5lMBgjYvjg6OpqSkmK8Ug9kaGgIzKempoIZsNjT06PVarOysm7+83d0dplNqs6GpGtQ2EbDPdaUikJtskksl0nyC4rAnikpBbRTOS3ZjLdNA/39BXl5mWlprTedNPJ7SkwyOzi6Aru5e7u5+3h6+dujwgCFIyJhYeExMDgaIBSYxsQio6Ph0dGwqGg4DIaOQyZExsAoBEJjcclIi7UuKzn07MHIMwfoMcGkeBQBT45HY6G8CHQ8Go1NwACqpFMobCZLyONLOVwRg8W3o7BKkwymIolGJNVIlQalNlGtS7anSWiNqSpdEk+kEEpUMqVeqQEbJwpFUjabJ1foDeZsjsx83jNu+64jq156Z+6iNc/MWjxr5rxnZsx57NEnli5bhkAgQGvZw81t08aXAwNDNPpEhcYgVegVSj3gYKVSp1JoVQq1Gryu0axRG728fF9c/9ID9//7wfvvf/iBf4Oz+oaXXtTptIkm0xtbtjz8wAMznnxq1jMz586ZM2vOzPUvrXV3c9UodSZDilJpkEiVPKGIy+NqZPyyTENfdVZ/WfpAQ91Ae0+tKDsbr9BRmSqJSKVQiDn0dJqml9M2yO0eLRge7/+11pApJyI5FCSHSeLxBYCnRWIJXyzhCEUMNpfN5nN4UBlmNlcEVS+mMCl0Dp3Np7G4FAYAYg6NBSCYR6JxAAoDOCbRuICDqUw+YGISjQ1Wgm1IVHYCngJHxHO50zjExuDAgHJ6KkiMj9aW5hdVTrSVu+uLP16/5MDlsJautq93vPTNKS/7+ikaQPv+tHbHl0XW688p6SrakjmzYxlq+2KaJP75VSthbKgYgoUe99pLb5h+i8Iy2Jatr7AmUPibN786OonCH6zfHI6xJ0gMwFz37/jiUMvor3qyx+J1r0uyoPhoXaFx/YbVHigu+A4uJ7/68Ivv0WhkRDQ8MR3KNgbKVeNWP/cCWpRhX5zU0JVAZkUyZ9Wzq33QEx/Yrs5yw/ZtrwSzILQt0dFffH41jGuv53BVaJ8zz63emtcCvU5FimDDmg0xNhQ2s4LXvbxJmjPxNdNUlOVzlxIlULJyZ33+12+9tGP3uXaAwuXpu1/eEIAW2Dcbbi/auW3jdxci7YuTGh4etl9ax4cb9r+9ec8P3rd+Fbpd3eamUwPDXRUt5qImzW9ThKdwcHKDjaWau3OnrTDtpEZGbVHhBmVBo+b3AsM3oOE6JUDewha9IoUcBHP+/NuPFy6ZN3f+rDfeeuXEpR/osrj0SklesyarXmFH4QkavomkYfPdgcKjY8O17TkFjao/ThG+wsEp4Aj2Dk5T73uArblZWTQSyajT3VIFidHR21VBorurS6MzZhfV/3aY5fL6vrwyK7gMW5uvP4Xeinr7+hINOmtx+mS3uUmPNRX31eRZDOq6+ukb/2+qrtuZf2rf3spzJwWelJmRXpqnhiLBU1EY2JYcXJilyP0n11cGuwVcYoBu8d4CUFKSxcHRzdHJw9sn0Ms7AEz9A0LtKBxty4WIQ2AQyAQwEx4RGxEeA2gYoHBsLBIeh4mMhvHZrPaqmpHm1vIUveP3O30O7ubBQqioOAKeiMES0Rg8FsprZdCoLMDB0DgYQqVUpuELpCy2QCBSKjVmjT5VoU4UiJV8kUIk08hUBoXGJFeZwENaY7JCY2TzxDyBTCbXqjRGpUovEkuFYolKZ2aIjBd9EW/tObJqwzsLlr04e97yWbMXAhR+4rGn589fHBgYVFdX5+fj++zyFSeOn1JpDGp9klJrVGuMaq1RqdYplBqVClilVCnVGp1GY1AqtCHB4Zs2vvKfB/4DaPg//77/X/fd9+knO8tKS9lM5splyx558D8znpox85lZM8F/s2ZsfGmdi6OTXKrUGywSuZYPvoOUX5ym7S5P6S9OasvSDXQ2NZTmqXBIKRGjEvO1WpNSbQBtQr1YWczPaqKXVdBz2qoa22rLjOx4ESFaQMdKJSK5XCORa8RytVAq4wpELI6QxRVxQPuBLwEzNCYPmMER2miYR2cLWDwxnS0EKExhAgjm01ngISGgYQqDR2FwyTQ24GAihWFHYQ6HP30oDNqy4AyrkEgq73gFiTEFKeKrbw9GIRLYLIbzqYPPrnweI4DALiHo3KrnX/SLjgeNby6LgiUwKuqgm4AifMCc+c86+kWJlcbBKSMw91hL97+3ccMbHyNwFCoetffdV9/c+U1xM9RQTqTEbFyz2VB0/c21TEnMxk3rGWYom9bl9L4tew9OoHCl5e3nXgxB2VMIBqIcv3ht13fWsV8L9OQVK579xSVMIRNd/vnL/zw91w0JUPhXhP/Z9S9uOHjkl1MnT545f5HEVQ6B5kR7xbc739j6/j4Sky+TSfHxcSK5rm/kVwkTE43AyOTycI/T6ze8Qdde0yG6s1z/2pYXA+hQhtlYd83Rz9/Z8vYeLJWjkEvxGBybpwHrdfSY1SvWOAYhZGLuL/t3z5q9CEaFcLkihb1m5YrvT7vTBPLOgVFredqO9Uve3fsTmUJ1v/TL4pmPvLv3bOsAQOG0D9et9UFM9C8EzBDrcWL1C68Ew7ByhYIB2sgkUmPveGGyJiIiisWTsInwN19c5xBMmdj8FnS7iql1dnTkZGWVFBWBM+nEqjuptt7KUltvuWtAakqKsD0k3NCZ1j0w3SFhIAiFm6CoMEDhgkYIhW9Iw5MoPEnDuQ2qvCYNoOHiNoMmkx6G9vj6x90LFs99/IlHN7zywjeH9sBIfinlouzfp+EbJg2bK0U3TJCYfnX0N5Q066emCNtH07jR4bNYu/NHRqcpHwxwQFNjY252dnVl5S1WkAB/BVqlsiA3968h11WNj2dkZJqSs22V1HqulE4DKNxT3tCflFZgSU657X9uAObyk/UjDbaR5Gz1IgAHg+lYc0l1jiXFYr7L66bdUdU3NFpMsgF7urAtNcKOxeN9Wb3W5OQkVUPj9KWL3M0ym5OdnNwdndx9fIN8/YLB1IbCEQB8Y2KRsTAUOh4PUDgiEhYaFh0REQvWwODouLiEOAQWHocyiNS9GS3D6Z1lMsvZzz+48Pl2ETyYjojCYzE4EgVDpODJDCqdTSbTCQQqiyUUi1VSqRpCYY5QKFGrtBadMQ2gME8o54nkYqgqmUGuNspVRqUmUWuwyJU6JpPP5QjlMpVKrVOqNDKlSqE30ERqwMFv7/151ctvL1r50rxFz82au3jOnIVPPTlzzuwFjo4uoB2IQqMWLlj45uvbwdOTklK1epNWpzdoNHqNWq/V2EBYqdUo1VqVUqORK9RaaLUhJDhi40svP3D/Aw/++9///n/3PfTvf1++eLG+vj4oIGDenLlPPfn07Nnz58yaN2/W7DkzZ7yw9rnzF84JRWK9zmhUqYtT9O3Fyd2FpiazsNYi7+tqyzFJONgIuZCjU+s12kS1Wg/19lModTJ5MltC8w3mx8O72ptLU9QqQoSQECPhUhVyhUiqEYhVQqGUxwdcy6WzeBzb+HxsrpDFgcpBMMAUMp/JFbH5UiZXDMCXyuSBRYDLDI7IljcsoDK4BDIDR6BhiZQEAhmJwnC504jC4J062tutzc19d/5GdkGS5Mev97z+xhtbt762fcc7rmGolh7ohNtWW+Bw4odtb2579933dn6y65SDX2EVlAVbU5j65ftvrHz+xdOesQO2IbiuaDxZydjz4dubXnl162uvffTZfpYayqAFShdg39+xM7ns+hsx+Tr8+x+8J0qHUgiC3X7edfi8HYVbatK+2PZ2HEUPLfw6GO9/bNeBE80jv452N7qf/XHz5k3vffTpgf373/vok1ASVA6ZHOW6dv2m4+ccLp47/fmnH2zc8hZLnQnWJyupX+75cNv2t957/4Ndu/bAsGzw8nxs6Mcfvr/jrR1vvfOeR2i8te+aXlDdNSm7P34nVjDxyTM03O/AK+x469333tu5a18onAG+cL+10unUj5s2vfrxrs++OXDo4w/ewvPTwcZDXY2Xjny98tmVe49ermzu+vXXYXKs58Z1azdv2X709IWff/zyp59d2wZ/7a/J+fa9tyMIMvtbADVXZl84duDNbdvffvudjz7eecHZq6FvvCxFfnD/3u073n5rx44Dh89m2Eoa36K6u7qKCwpKS0puvYIEIPZp6DZnFyCkxs58e1QYoPBvQArY0tCZ2t5XMTo2faOpTWoiQQLKiwUoPBEY/mMangwMQwYPNWoKrbqiFr0xnw0n+x868dWS5Qv/9a9/rVn3LEuFzLdq/yhN4jc0DFAYrPzbo8JAI2ND9R15V4p+XNdVzjy1GdPYmdYz+M8DC4C/4BQNztW3UtRsUg0N9SqtsbCyrdw2xIbdYL6oqlOjt5SVTeQG3EY1W62JWkVXRfZoY/GIrVIEMJQ9XFeQYVIXF1+TPPZ/TQMDQxazob5UN94/McSGzVBIuLIAGmB5eHj60kVuu4aGhmprairKym59oESzJdnJeQKF7Q4IDAsMCrcFhmF2FI5DYOzJEtHRcfC4eAQiAYUkIFF4IoZazi8aFHUPCro7eI1xx11P7XydGeoiQMZKcWwpTU6hcPAUBpHCiIXFRUXF0GgssVgJzOaKmGyhSKZR65J1xnSAwhy+hCuUyVR6pTZRqU2yj6ah0VmkMi2dxuGy+XLAjyqNUqtTGhLpIvVlf8R7X51eu+WTJc+9snjp2nkLnp09d/FTT82a+czcY8dOVlXVcDhQtvr8eQv9fQKTzWlGvUkpk7GoJBw8Ki48kEnASDlMYJmArVZKdFq1WqVWAUK1cXKAf/Cmlzc/8dgT9/+/f913332znpkJg8Grq6uP/3L8qSdnzJw5b8G8JQvnLpk/d8Ezs2Y+t+a5M6eOy4Xc6uzkznxze46xIVlWJCHUZZk6rPVGIUnCIgPk12hMao1BqVTJFTKpXC6Ti1UyLiHGN/jCwRyLvqu5LpWHUeHCBASYmMsQSxQ8vozLFrLYHCqLQWWyOHwBh8dnsblMNofO4gA+ZvGEbL6IK5TyxQo2X2aLBANEFjJtlAxQGKJhJg9PomPxFIDCOCIZhcbweNOYIDGtGh+tKMpiUPBxSKRYaeiY0se8pb6cy6Qg4hBECjMlq7BvyPb9x0ZyU/RodLzKnPWb/TGWn5GEjUfFY0nJ2cWTmNxWWyqTKq1d1wcYOhrLZTJZTStUzCU3I0lpShmxveJAb6taLCmqsF8ax4qzzUqd2Tb83K8tdaUsKh6Dp6Zn5memWgqqmsb6Gn7a8+FpD4T9tFSart26auFxT3vNndGCzCQiNh6BRHME0vJq6AU7mmpEXAY8Ds4Wyq2d13+k4d5WlVxaWHP1Rm1RloWMT4AjUGy+pKRqgkcbqwrpJCyezMwuLEtJ0pZeGYiupjibkIBmy7Td/RCT9XVZpVxafAIxu7iqqjTXlJg2OPrraF+7Ti7NK71mxKam6mIOgxQHg5NorLTsQvBdRge6ko2qeBQSR6LnldVNbHd3aNoqSExqcLi7qavAlmNqqoayhJNqIZYCttg4OK29r3zaYorXCaBwCYTCE6NI/CkahmZszgXbNEFAXNJhTC4TounBP5/99rVtLxOF0QCFM6HOc79Pw1dQ2E7DNhSW3Q0oDNQ31F7dllJm1dmaMVc5GLRk6idSI6ApOHxjY7c55PmP08DAQFKSJTE511YurQFyUUNeaZM5vUhvSOzpuf0HdGh4ONmclGfRtpdltpZClSJaSzI6yjKrspMS9dqOzomavv9nVVJSkmwSdzYkdjYkddjc1WhurzOlmuXl5beh0PvfqNtYQcJssTi7uDu7QAkSgIP9/EPsHBwSGhURGRsdi0CiEgD+RkXHARqOjUHGxWEQCCwaSUChCUqCspvTNsjq7md3DgkH8mKT3fbtJbhfLCQb6pnl9ezqdEY6m8rHk6iBQSF+fv4EPFkkUoolKgiFOSKxTKPRp2gNaTKlicUVcQUypSZRY0iezBWGykdINQwGl8cBKKxUqg0aUwpbpnP0R77/5dkXtu5dsfbNpcs3LFq0et685U/PmPfEE8/88P2hwsIitVrz6uYtTzz62Ldffksj0Hh0FgmD8XFxcDp19MCnH3y09eVj+/cGOl7AhgfBg7yj/D2YRKxGLtGrlWq5XKvRK1W6mBjE119/t2L5yocffvRf//r382ueVygUOTk5n+zc/eijT86bu2ThghULFiyfN3/J7Dnz1z638tKJn3J1/O58Y4NFUSRnZHLj26qL6ioKVWyiUiK0obBRpdbJAAfLhAqZWCGXqlUylZARH+BIifZraairzjQlksLl+DABCSHmsTgsLo3KotLoFDqVxqRzeFxgBotJZzCoNAaNyeYKJQKJQiRTSRQagUTJ4UsBB0NF05hcYFvMGFokUVlEEp1IphNJVHR8wrSi8N9SQWKqfu9W31+4Bzj1vuHv3UP8vW3A/JQn3Pi53U1529asOukaZ4/ulqarXl29wimcaFv667r2rafoJr7CzeovPOXu0LRVkJiqYdCG6Kts6MysgVKE7RAMpQhbu3O7BupGxv6evxQgOwrbB1S7joYnURia/w0NFzZrS9oMxcCthiKbwUxJu7GsK7GqLxnAMU8Xb8rnZNcpAQpfoeH/UlDibkNhoK6Bppq2lAoIhY3VbZPNGEtdh+0Idqa09ZZMc4Y3+Gu9qypITKqyslKu1OiMyTqj5YrBRV1fUDDRBeK2C7yjViVPMUFlIuxONWmNGuiaPbHF/2F1dHQmGrXJScpUizrVogJOs6iTjPKU5KS+f07dtBvqdlaQsFjc3D1c3Ty8vP0BDQcEhgEODguPiYyCusfF2FA4DhEfE4sEKAwAEYHAIFFYFAqXEE/MoWUOcboHZD390u5BfncPux35y2Whd8yAoHOI3zfE6elmt6Yx06g0FiwOFR0Nw2HJfL5cItcKxHLbWMpQVFitS5EpDDy+TChRqbRmLVQ1IkWpNat0ycDQqMs8oUAoVij14CGB0uIVgfto/5l1r32+Yu2OJc++vGjJ2vnzn501c+Hjjz2z59PPs7KyzWbzW2+98+hDj7756mvIyFhUeLT72XOXjx3Z+962fe9ve/35pS8umb1pxYIPNq0/992+X77cteuNlw9/9WlskJdKwjNolEqFSqM16QwWDk/s7u7z/Jr1/3nwofv/37/ee+ftjPQ0lUq9/oUXn3j86XnzFi9csHzRwmcXLXp2wdwFL6xc5vDL92liYpVJkMWLz5UQulsbirNTNQK2Wi5VgRdVq1UqlUIuk8tEKrlYqZQpFTKNXCSlY+J9zqWoRd0tDZmCBBMpVEmMFFJQPDaVzmRS6Qw6EJMO0JjFYTJYDBqDTqEymGy+UKqUKDQShVqqhIrNcQQS0LqgQSnCUP85JlcITGVwKTQOFTIbPAuTgJvWBIm/q4LEP1Gjg+3OP+9f/+Irh4+dOH/29Kc7393+0f7EnHv77Qb6J1aQmKqx8ZH+4Y6ugfr2voq23rKOvsrugYaBka6x8WuyXKZZNhQ25k/pNndDGp5E4UkaTi4V6HOYV8zS57J0OUxdNhNMjflsUwEnqYiXWSPPqlWA6X+l4cnAsOUuQ+HxX8d7Bq1NXXk1bWZbVNiGwhAHpzR1ZXb2V01/T0eAsHdbBQm7BgYGqquqKisroLINlZDBfHV11Z2rTgC9Y3VNBfRGVRVXXFlV3XXL983/BwSOb3OztbyicnLPAJeXV1pvuerC367bWEEiOSXZw9PDxc3NwxMaag6gcGhYdHgkDKBwNFQpAglQGIHEACYOC4uJiopDILEoNC4+Hstn8RvEVUPS3pGKwaGcvn5+1yCvh+UQnRGthsaP4HcP8LuGOL117CoBS0xjsEgkOpHA4PLkMoVOJFXzhAADNUqNWam2SGQ6oVAhlmrsKAzgWKE1K3UWYIlCxxdLxTKl2pgi1mUExtI//f7i+jc/W7lu+7JnX168bO38RStnzloAOPiddz4wJ5kzMzM+27P7oQceXL96redlJwYGG+Liev7H7498vuur97fv2b75vU1rt7/03Op5T6+dN+O9DWv2vvny5mfnvrpq/q7trwS4XVTLRTqdXqU2qjSJelMymLlw3mH+vAX/vv/+B+//14/ff1dRXoaAx82fO/epx59YvGDxkoUrFi9etXjxyvnzFq1esezcT18bGHFZvLjKFHlvZ1tGkk4jk2oUEpVSLFdC+AulJMukSqUUSpRQSJUysV7KFaCCBEg/a31lVXaSiRCSRIlUkKNFbLxIwucLxVwOn8kCEExnMGlMFoPBZNIZLB5fLJZDNeauoLCSI5CyeVJAw7ayEmw2XwxQGDAxlc6hM7h0BoTCKDQGAPX0ofD0VZD4n1BVUUawl9P+Lz//7Isvz1xyUyRm/oNzuO6k/okVJG6o0TFoHLS75JY6hMJWkx2FC2z4+19pOL9Rnd+ocfQ5sf3dLW99sPWdj17/cPeOD3Ztf/vD14Fta954/5Ntu/e9x9Nh8pu1EArbaPi3aRKTNDyZNHy3obBdQyM9XQMNbb2l1u4Ca3d+a29xZ391/1Dr6PjfcBABtt5tFSTu6Z6mU+DnaisgcRsqSKSkpLh7uDs5Obu5ewIa9gsIDgmNDA2LCgmLjIyMgcch0fEJSBQmNhYRER4bERUXi0jAYAhCDq8oNbU3pWG0eGS8/9ehrP4hfm8brYHpBCtBZwwJBgbUPUM5/UPyPiujVk4XcdiA4rg0Ko8vUMgVOrEM6hYmlumUmiSFOkki0QoEcqFEI1MnqvWpAIWVWgsUFdanyNUmsVyh1OqlhtRQDO/zQy4bd3y54sXtS1fbOfjZmXMXPvL405s3v65SqUtKin/47punH3t049r1Z48eR4VHkWBxoS4ursePHt332bGvP9/7zrZ97+94fd2qZbOeeHHp/JeWzH19zZLNK+a+vHTWxqUzP35tfUyQl16j1ektaqhwm16nTxKLFQcPHZk9e95/HnhgxhOPenu4NdTXnTtz+unHH5s9Y8ai+YsWLVy2dMlzS5Y8t2D+kudXLLtwaJ8SF9JWVdDR2mTUypVKhUYFGF+oUIgUCrlGodTIZUoFtF6hkMnkUrVcbOCRRXGemWpOV2tzCjfBiAsyMWByOlIlZqtUKqFUwebxmRwGk81ks7lsDofD5QpEUpFELZSqxXK1VKUB23AFCq5QyeKJqEwunc0HKExjcQlQZWIWjc4iU2l4AiE2NpZOp08fCg8NDaWlpKhksqqKf8xAjn+zxodrq8qLyyp7B+9h8O+qrraWy2SK+fxbROGOjo6s9PSigoLb3qX9n6iRsaFSCIWhYdUm8Xfq/G9puKBJU9Ck/fLAJw/+58FHHnv4yacevx+qunPfgw8+8PgTjz72+CNgHuj59SvZamShVfebwPAf0TBA4bRqWf9dhsJ2jY4NAiYeGukeHu2b5pFQrhOUIJGbW1NdPTr6128pgEtCaXGxXq0uzM+/h8L39H9TAIVdXV0vX77sCgWGvXx8/f0DgoEDA4PDwiJhNhRGoTCwWGR4ZGxoZAwKnSAXiMszM3orioebWsb7fh0tGR4S9w7xegsJGXjviEp8wRCvf1DfO9Y5Olo53syrkGGJPDKOS2dyOSKxVKNQQQUiJAq9TGXU6JPVWotUqhMI5QKJSqIyAQJW28ZbVukgGlaojUqNXqY3R+P5+465v/zut6tffn/pmlcXrVi3YPHK2XMXPfr40xs3vcrjCysrK48d+3nGk4+vWLzE18WTgsbCA0NIsfAoD3fXE0dPHfj6zMEfv/lk5973djy/aM7iGY+98tyyNfOfWT3nqc0rFqyb//Qry2ZtXTl337tb0dGhBq1RqzertODcYDAlptCZgg8/+vSxR5/49/+7b8GcmWQSobi4aP+XX9orDc+bO2/RwqVLl6xcsmQVwOI1yxe5nz3W3lxfX1Ou08rVGpVWI9drpFq1XKVWadRKjQLsArkKYmGpRC6Ry+UGmVDHgKtxQc2VRTUFmer4wERKpJGJUDGxSglfKlcIJDKeSMATC/lCMV8gEgiBJTyhlCuUCaVKoVTOE8l4QhVfrGHzAAHzGBwBiyck0Zh4MpVMoxPJFBwBj8agwiNC6XTK9KEweKeuzs7WlpZb7Ol/T/c0VT3d3eDKXVFWNnBrOeg1VVVkPB4g9T800eL2amRssLQ50T7C8H+lYWjG1sEOmCaBhSLcIuI9g+EugHqBfSMvg8WoBK8jp/fPnjfz2PnvUivEuQ1qgMJ/TMN2FIZo+O5G4f89gVM0OFff+0O4p3+WhgYHq6uqykpKurputXNkcnKyi4vLpUsXwdTd3cPT08vL28fHxz/APzAkJDwGhkChsMg4TEwMIjw6FgE8dyEAAP/0SURBVI1GaqWimpys7tKCwYbq4cHuiuzscnpmK6u+kl2soojpCQml9Nwhbs+QqBcKGA/+2p5XpyOgJAlxAjJexOEo5FoIc7VJgIYVahNAYY0+Ra40CsVQDzCpyqDUmhXqRJnSqNAkAqu1RrXBgqRKfzgX+OqHP6x59eMVL7yx5NkNC5esnjNv6RNPPPP88+vIFGpNTc3lS5fnzJ41d/bsH7/9gUNmSugcTEQUJiwc7ud98acDl478cPmXn7/dveudzRuXzXxi1dwZW9aseGHhrPWLZm9aMX/t/BmbVszZunLeGyvnHvnsbQYeozMCHDeq1Hq1xqTVW/z9w5cvf+6hhx66///dt/Gl9QaDPikp6dVNrzz4r/tnzZgxf+7chQsWLV60bOmy5+bMnnv6xHFrU5PFpNcqJSqZSMxnS4UcpZSvkot0KqleKdUoFUqlUi4XSSRCmUypVihMMpaBFJYto3W1WlPFFGWcezIjVktDSOl4uQgan1oCrFSJZAooHiyUCoRiDp/H5gsABLP5IjZfLJBogFlcsW3gZQGTC6EwiUYHxhKICXgsMh4RGh5IZ04jCt/TPd3Nqq2uppFIUpHoXlMNyBYVTixogEYY/gMangwMQ/O22HBJq768M7G2P4Uuj9vwygv+0Q5lnabyrsSKHnNyqfDzbz7+9Mv3zcX8/GaNHYV/Q8M3ThpOvofC93RP9/SHgoZ0USoFHA4A4olVf1XJyRZXV5dLFy84Ozu5ubq4ubl6enj6+vj6+wUEBYVGRcHh8HhYDAoOQ1JI5GSNsrkgp6M4r7OscKSvp6YkjxHlz4+O09EkYhqPyWTwuCwzV9vMreljt3fyrG15jf3dHWUWjY4Al+FRUipezuOrVXq11iRXGeQqk1o3gcIiqBiCUqbSydVGmVIvlYMZk0qXpDOl0ISaIw4RWz/5+fktu1es37501abFS1+Yt+DZp56evWLFc3B4HDSUhq/vovkLnnr88Z0ffoyGx8u5EimDhQ2PCHd1Qfh7nT3wldupo86nftn73jubVj+7fNaTLy1f+PLKxRuWL3j52YUvLp2zfumcNfNnvLRoxlur5u7esMj99E8KmUqh0itUOmiIZrVRLFKdO+cAsPvRRx79zwMPfLH388LCQjqNtmzRokcefGD2zBlzZ8+aP3fevLmLlix7FhWfkJKSevnixQRENAUVHR3iExvsjQzzxcDC2OQElZSvVivlKqVMJpJLxHK5WqbSaFWiJD7WiA+vL8puqihUIj318d5mOlxJRsjZRKVCqtDo5VqjWKnhi2RCgUzAF7G4LCaXyxXK2DwoPCySqQUSFYMtslUXhvrMURhsCgOiYQyekEDAx+Pio2IjODzWtEaFu7u729vabjF6d0/3dCcETqNFBQWV5eXTORDrXSuAwmV2FG7UXEfDU+cnaXgShQsaIXoGNHzW+fCGzS8Y89hlHaa8RjVweVdSJMbr2dVLaTJ4cZvhGhS+IQ1PSZNIqRLfQ+E/1vj4eHNTU15ODuCAW6wg0dXZ2djQ0NnRMbHq79b42K8j3WP99cN9FUM9xYPdhYPdRYM9Ex64gQsn3GtzT2F/7298w5VT3NdbALkPcu+N3HOtu+3utzsfuMvuvisz/fmdE87r7IPc0T/V+faZ9mvdNtUDwLnArVfcMun+HCvkXDBtvtZNNjf2ZzcOZENTmxuuuN7uATDNqhu46tr+rNqBrJqBTMj9tulAZrXNVQMZVdA0p2aouGGkvn2sdxD8aCYO1d8q8LuVi8UsGu3Wh/ECKOzu5nz50gVnJwc3V2cPDzdvLy9/Pz//gIDAwODwsKjISFgcDCVgcfItKV3FhT2FOc25GX3tra2NNSJMZLzPZSo6hkrGkyhEOpPB5XJEfF4iT5/LytDF8Cn+ATkW7UBPR5FBZiAj1ESkjIyV8zgahVqhNMiUJqUOqpgmU+gBCgulCqlSq1AbFCqDTGVU6swGc6ZAnewUjNm+98Ta1/asXL9j2erNi5evm79w5VNPz1u0aHlgQFBjY2NUVOTKFcuffOTh9WueDw0M4dK4WqFcSWei/X0CL55B+LldPrTf+9yxcz8deGfzxrVL5r2wZN4rzy17admCV1Yt3bhi4fpl8zauWrzkmcdXPP3wRy8s+nzTkkOfvE5LwECDM6v1KrVWAT6TLkkm1Vy44Aje9KEHH3nyscfPnz1TX1fn7+Pz9OOPPfbQf2bPfGbe7NlPP/HU5ldfS0vPiI2BP7/6+W8/3+34y0HnYwccDn3x8+fvHdjz3rlfDsbHRSrkEqUGoLZcrVSBF5dAWcMik5STRIVnCIndrU15Wq4iyiGZHGmmI1Q0pELEUmn0Sq1ZqtaLZHKhUMbniZgcDosH/q/kiwAEq4VSJUcopbOFDI6IzZcAFLZVVWMTqfQEAhFHIuPIRDQGLZaKx8BZ5iZ0G1B4eGgoIy0NNNqqK//ZxQvv6a4SuGDnZmcX5uVNw9At/3cEJUgAFAZcC1D4CvtOEvDU+d/ScF6Dqqzd+P3RLza+ui6jSlrZbYbSiJu1VX0WHDdy4ZJ5CEoghMJ1Eyg8lYYn0yQyrgSG7TScWiUGK++h8B8IICz4Q2BQKIl6/S1WkCjIy5OJRNmZmX9vrvD4yPhAw3BHel+zrLue2VFHaKvBtFWjWquQwC3VyJYa29TuGqT1qhHAzbUIq81gpsnm5jpE01TXxzVOcQNwg20KzcDr6yHXNcDrGuG1wA2wGpurJxxbdcWVDbEVNpc32h1TZnOpzSWN0SVN0cVXXNQUBVxoc0EzcGS+zXkTjsgFtk4454qzmyOyrMDhWS1hmS3hmdawDGtYus1p1tBUm1OsIck2W6whZmtwks2J1mCTNdhoDTK0QNa3BALrWgK1LQGalgC1zaoWf2Blq5+i1U/e6iezWdrqJ2n1BRa3+opafYTAbZAFbd7A/DZvXpsvvy1Q2B4p60RpulnJA8mlI9AYTH+nBvr7iwoKwO/21rtQJyeb3d2cHC5DKAyY2Mfbw8/XOzDALygwIDgwKDg4NDoqRsDmlGdmdpeW9eRnNaaZOuqqutqsOhYO63uRjoxikkkMCo0BVb9lcDgcvkDA50P37InwGP9zh5De5yoLs/s6WvLUAhMZribESoloOZupkCjkSqNKa1FpzXKlQSxTQpUi5CqlBkqcUGiSNKZ0lSk7GMXZedBx/ba9K9dvW7F689LlL81f9NzTUErCYkcHl7raOgKB8Pzzq5964rFFc+YcP3JUyOZxKEydQKIgk5Debj6nDse4n3c6/JXj4f3f7/rgjfVrVi+Y9doLK19ds+LlZxe//OzCl5bN37Rq8Za1zy6f/dSaOU/s3rDi81ef/Xrb8zGeDnqtQa0zqrUaFfhQao3RaGaxhXv2fPnkYzMe+vcD82bNiIPFgNb4wR++f/iBBwEcPzNjxqOPPPz9999XVFQeOnhkzqx5L6x+7sM3Nh//7F3nbz74/u0Xt6ycs2HVop++/ZxCSFBrVSpowDutWCLjC/lSqdColqXJ2AZCeEWmqaOxxkgI0yO909hIIwMuZ2FUMqlKZVJqjXKNWiiWCPgSDk9kKy2sFEi0AomGI5AwONA4zGy+lCOw1RhmQTWGiVQGlDFMZwITSGSFUjV9UeHBgQHVvQoS93S7VVtTw6LR/gcqSNxVmkThoibtdTRsx9+p88DX0TBA4bPOP82ZPwtFCy5u0Rc2a4FzGlQX3H9+/IlHcdyIiajwH9Lw1TSJmnso/N9lR2E6mWy6ZRROTU7+eytIjI/82lc5ZFX01BLay6NaSnybij0bS7waS7wn3VB6jevLJu0FXAdcPuFauysg10y6EtizetJVHlNdZXNltUfFhN3Lq93Lqt1KJ+xaUu1aPGGXomqXQuAalwKb82ucgfNqnHPtrnXKqXXKrnPKrnXKqnUEzqx1zACuc0yvcwBOszm13iGl7jJwcv3l5IbLlnrgS2bghktJ9ZATGy6abDYC118w1F/Q11/Q1Z/XAjec1zSc0zSeUzeeUzWeVTaeVTSekTeekTWeljWcljaeFjeeEjedEjWdEjadFDSd5Dee5DWe4Dad4DSdYEM+zmo6zmz+hdH8C735F1rzMWBK0zFy88/ApOafic1HCc1H8c1Hcc1HsE1HsGDafBjTfDi++af45kOopoOI5mPxrS60jhhZjzRzpPZvOw+Dn+voCKSbxJo/kMWS5Ori4HD5nLPTJcDEfr6e/n5eQQG+IQH+Qf7+EeGhfBazPDO9p6y4My+zOkndVJzT19WRLGNjfS+RY0PoJByFTKSRSEwqjUNn8tg8oUDCFwjZXDaDTsTBQsMdfqFF+VkBPTfVZsvoJlK0Gh8txCGENAqgYa02UaM3qzQmsRSqjCuSAeo0QUPN6VM15jwMTfXt6YCXP/hx1UtvrVi1cdmK9YsWr5k5c/GMZ+adPHm2rKycx+Nt3PjS008/MWvG0++/9Q4FT5TwhBwSTUKhSXAJMDeHiwf2Bp0/fPHA7sO7d3z4yguvrVnx0rIFb21a99rzK7asXrr52fnAb65d/vraFS8smvn+hmf3v7nuwxcX7nl5kc+Jbw1qpVprkEOlHiS2jAa1Sq2Pjo7buG7T048/8fAD/3ph9UqZRJKRnr7jjW3/vu/+Z558ZtWy59CoeGOiadPLW+bPX7Js2arVy1d8/c4W+KWDMee+PfzBq5uXztq0apGn0zm5XCxXqjhcAYPJ4PHYSpk00aBL1UoSKVGJtNj2ppryVL0i1s1CiUjnIjS0GDmTqJIq1FqjSqcTyeRQPolUJZBKOUIBRyzliBRMrpDG5jC4Aq5QyuaLbUNsCOhsPoXBoTI54FEGW0CmMJQq7fSh8L0KEvd0J3SvgsSdEJQr3DKBwn+Bhkta9URe1LwFs1e/8Owlj2NxpIBorPfR09/MmTfzueeXK1LIRa36CQi+joZ/pwvdPRS+Gf1vVJAYahlp0fRUY9pKA5qLvZpKfJtLfZtK/aa6sewaNwCXT7i+3BdyBeS6qa681lXetVNcU33V1dVeVTZX1kCugOxZXuNZNmGP0hqPkisurnEvsrmwFnIBZLf8Wre8Cbvm1rnm2JwN2SXL5sw6l4x6YGfg9HrnNJtT652AUxogJ0N2tNhsbnBMAm50AE5sdDA1OhgbLwMbGi/rGy7pgBsvaRsvamxWN15QNV5QNl5QNJ4HljeelzaekzadkzSdEzedFTWdFTadFTSd4Ted4TWd4TadBuY0nWY3n2I1n2JCPsloPklvPklrPkFtPkFpPkFuPk4Cth4nWn8hWH/BQz6Gsx7DAjf/nND8c3zzUXTzYZT1EML6I7zldEJHKL9PlT3W+c++R2cxJwEIvnjhjJPjRQiF/TwD/bwCfb0CfLxDA/y5VFJFZmpPaWF3XkZtoqo609LX1V6cnoj3v5wQ4kNKwOJxRBQqPjwsDAmLZVEoIp5AJJQKBWIAqRwOm8uik5HRcPfzcjK6q9XaVl2SJSSaiFFKXLQIC5MxqTql2mBMVuvMAIUFYrlcqVdDhSPStOY8rjL1rCdyx96TUIrw6i3Ll7+4eMmambOXPvX03AMHDhYWFatU6jdff2PGk0/MeebpF1avDguJVEqVQhaPR6JR4HEMeAzS2+XCd3suf7/rxN4d37y98b0Xn319zbJt61Z+tOWl159f9ta65R+/vOrTV5/ftfn57c8vfn3Vgu/e3nDo3Rc/XDdv76ZFTj/uUorYao1OIpVJxUKVHBofTqYAyKpwdXB9/tnnnnrssYf+ff87O7ZnZ2UxGPQdr2w79sFRjBOmvqyeiCPNn7No4aLlS5etXrZ4xWsvrgk8870O6cvyPefy9YfbVy/49rP32QyyVCojkSl0JlUmkxq0Bp1aJaBh9NRoXYJPvo7X3dZsosXJYp1TGOHJzCglKVrBpWnVGqXGIFNpZCqdQqMTSAUEOoYuYHLEMgZXSGdzmVwBNNYGAF9bzzlgFhQ8lnIFUiaEwkwbCt/Uue725ArfqyBxT7dd9ypI3AlBucItiYVNUOIvRMONNhq+6S50BU2arGrZ8fMH/vPQg48/+diqNcuWr1z8n/88+Nhjj7gFnQXUm9s4UUFiKgoDX0vDV5KGq2Wp1ZJ7KDxt+rsqSADy7i4crKN2lAU3F3s3lfg1l/oDN5Vd48Zr3QBcPmmAwn71FROum3Tlta7yqZ3imuqrroZQ2Luq2ruyZsI2FPYqr/Eqg+xZWgu5xOZiCIU9imo8CmsnbENh9/xa9zzIbnl1brl1bjl1btmQIRrOqnPNBK53taEw5PR6lzTIAIWdUxucU2y2obCTxWZzg1NSI0BhyImNjjYUdjBAvqy32YbClzSQL6oaLyohAxS+IG+8IAMo3HRe0nReDKHwOVHTORsKn+U3nbWhMGROM0Dh06zm0zYUPsVoPmVD4ZPU5pM2FD5BskImWo8TbAY0jLP+ggVuPpbQfAwDaNj6M9p6FGU9grAegjX/EN16FtOVoBoqqZ84rtOl21hBAqAwgODz506BqYe7s5+vZ4CPp4+Hm5+3JwOPLU+19JTk9xRkNphVVSmG3o7WupI8epRPnLdTAhwRGYEICYoN8As/duzE0cMHwwJ9OAyaSCgWCiQCnojHEfC5QiGXS0PH4gNdU6Scvq6OptK8NE5CIilaR4iWEeJUXJZBZ9IYkqVKvUypU+uSdMY0fWKOMrEgCMXZdchlw44vV63btnzlpiVLX5w9d/kTT8/9+JM9OTm5JlPiRx99POOJJ+c+88zyRQtPHjuu15vFQrmYI5QwOQnhEUh/b0yAp+ex7y5+89GFr98/8snrn7++/pNXX9i7bdM377/+8aY1e7asPbrzdcfvdp794t19W1/4/LU1Z/e8cW73Kz9sX3Xk3fUXv/mAR8Pq9SaZXC0TS9RyiUIhlshEKqXCpFIfOfDDnBkzH/3Pww/9+99HDh0sKC5IkVvKMIUDxt7xwV8NZN2rq1+dPXv+kiUrAAovXbT449c30nzPpSQEikMdnL79aN+7ryKjQ1VyBQ/sIKlQpdUbDMkSoSDSz5GH8kthRGjivZoqCytzU4WRriZMUA4LnkgNl9LilFKhQm1UaE0yrY4rFmIp6Oj4ACIbS+fxqEy+bbxlPosHmHiiiASDI+QIZDyRgsUV05k8CoWpVuvGbq7ZfxtQ+J7u6W7WvQoSU2XvNmdHYTsN2wPDN0nDefWqYqvOkMNy9T/9/ifbX3z5+Zc2rd297z3fiMtJRbz8Jk12nRJ4goBvlCaRWTsRGLbTcFq1JLNWcQ+F/4c1PjzentJXGdcGQbCvDYIDIF/Lwdeh8F/iYN+pKDyVg20hYYiDq27MwV5lUzjYFhL+Aw52vyEHQyh8yxxsQ+HfcvAl9QQKQxwMbOdgGwr/loMnUHiSg+1R4Ws5+CTgYPJNcTBklPUownokznoYZv0xyvpTTLs/YyCt9NdpvK9wGytIWCxmJ8dLF86dcna84OnmDAjY28MVzOCQcYVmY3dJXk9+RnOqvsqs7rY2tDXUCOPDgy8cgwVHIOC4c2edDh34+eyJS0cP//zhe29/uXdXaJAfh80Wi+RCvhRgHo8nEotkEj6fg4ljRwfkGZX93Z21uampTLSZEqMlRMqJSLVIqNElKtQmmcqo1ifpDckGcw5ZlHrUKebVj35Ytn774uUvLl76wtz5q554av6Odz7SG0zZ2dmf79078+mn5zzzDPBnu3bxONzEpBQ6lSvlyRQ8ETYqKtLDGebpEH75RMDpg+EXj/ifOnD8s3cOfvTG4U92nP1658nP3rm47/3wU98SPE4mOB8LPPK59w+7Yk99HXN6b+CRD30Ofnjpmw8pqKhEU6JSqVXJlGqlQqmSSqUipVRSk5vJRMVuWffCE48++cC/718wa5aYJgB7ciRvbLR0ZKxtvF/edfmLCzNnzlywYMGSRUsXLVqxctlip4N70/DhaYRQUYTr2f0f+rtcUMvlSrVaqlIoNCaN1iLk8eHh/lxsZKYwQQlzTOQg2621pXJjJcFQzzLkMglKUpSCT1fpE1WmZIlWEwQLO3rxkEvg2XBUUCQyBk9h0Fh8JlfA4ohobA6dx2XyoS50LK6UxZNA+RIsHo3K1mqMN3kH7PZEhe9VkLinu1Z/bwWJ0bGR3qHOzn4rcO9Qx8jfPebcBAo3aoqaIRSeSsOTKGwn4Knz19FwaauhpFWvzWKwlEi2GmXK5xRZdYVWbU49xMH/lYanpkmkVd1D4f8icCr/51aQGBscbzX2lke0Fns3Q8FgGwT/BQ6+gsI3ycG1N+TgKyHh6znYFhK+joMnUfh6Dq6FOPgGIeHf52A7CidPQeGb4OCrKHwdB8sbz8smOBgKCds4GELh/8bBpyY5GAoJ34iDIRSe4OBjgIPtKAw4GGk9Gmc9ArcegVkPx1oPRVm/D291wvUl5k8c4zuv21lBwmJxdrx8+cIZN6dLHq7OPp7uHq5O8IjgLJ26ozi3syCtJd1QlaTsqKvobm8xsAlRl464nzoGi0BFRWC+/OLr1zZt/mbvNx6OHkd/+On97du//+YrWEykSCgWCaU8noDHA3MKuVStEAiFGLgAHlCUahjo6ahKNaQwkUnUaA0hWkbFKcVCQJwKTaJSl6jXJaqM6UEo8e4fXZ/d+PasJc/PW7Rq5uwljz8555VX35RIFWVlZQcOHJg185m5swAHP/3s0mVhwWFpqelyhZpMYCmEarVQTkWhYrzdYR4OpHBfUpg3KdQTF+gScv6w77FvvI98FXbmR6TjMaLnaUmkqxbuBSyLcJaGOami3NRwZ2WcEy/0fNDJ/ahQH4NeL1co1XKVSq1WqGQSsVArkzbnppXqRW4nDi+YM+/fD/xrwVPPqMKlY83gbALIb3S4fHBIOkJ1wC9duHjW7JkLFyxetHD5oiVLjn3xcRoxOp8Zl8eKgzkcdj3zk5jPVWn1Co1GozNr9BaFQikV8jVibpqEZsD5S6Pdm0vLxlrGB2V9fez2RnammYrVilj6pFR9SpZYpz7hfGrrzld+vvz9efeTJy8fj8MgWTwhTyjlCCQ4GglBQJFYDCYUDOZTaHQyjUKmkIl4okquGp+2BInhoaH0lBSVXH4zucLN7f3l9T09/SMjo9PYqLynf6A6OjqyMzMBAfT19k6s+kcJUK+1u7bMmlnYmFTQYCxsNJVYLdXtOW29tYBHJzaadtkSJJIgFAYQbKNhaOZPJg2XtOjBc9PKRYoUsiGXBVaWtBoKmjQ5Vzj45mkYQuGauxSFx8fHhkcHwWcbGO4B+218OuNgUwQQ9nZ1m8vPzZUIBFkZGTcZKblFjQ+Pt+h7y0JaSnyuBoP/GwdDKPw7HHwVhf+Yg/9LasTvc7AtNeKGHDyZGnFjDoZQ+EYcfCUkfB0HT6Iw4OArWcJXUyNuyMFXUyNuxMGCKRzMvcrBUGrEVA6emhrxRxx8JSRs52DEBAdDjrUejrYeimz+LqT1Erp/umh4YGCgpKgI/BW0t7VNrPqrSjabnS9fcrhwzt3Z0d3VxdPNJTLYP0khbC/Kbs9Pb0o31iQpWioK+7raU6RMjOe5gAu/BDg7EVBkb7egD999/9WXXvxg247zP590P+twaN9XO99+8+TPP5KJWJFAzGZxuRy+WCSXStVyiVzFY/HjIwWIgOr89P6ujvIUbTIDnkSFKYhwCRmtFvK0BrNCm6TRm9lS0ynP+K0f/bh0zabXX/nwy/e+37h6yyub32Aw2eXl5SdPnJw5cxZA4XlzZq1YuvToT4fFQqnFkioUyJgUvlZqVAkUQgqNEBkR5+UqwSEMLKKKjJYmxAjgwZxIb06YhyDKS4XwNyUEp+CC04mhGaSwLEpkLj22gIUoEiBLxPG5HDgl8HxcoKtGpVKoVBqlSq0BKAxaHwKjUtaSn9aaqVOTkR9te/2BB+9fOXdJGapw2Dg61jkCTiNDqX1DomGeO2PZwqVPzXhq3tz58+cuWjh3zk873zJjQvM5qDJRAjvosuORrxgUnN5k0hiMeoNZb0oxJpoTE81GnT5VLkhjI9MwmHZd7XjP+HD+4CC3t5fTls9VmzQyoyXLlJrLkYvOeJz+6tjesx7HTjkfPnBsn7OPQzwRw+DxOCIRioB2D3SLiIugshh4LCbEz8PT+YKX80VPh/N4VNz0dZv7UxUkFCkNnvE5WFGFPLmhpKa7s3d4eORW+4Te0/+kaqurWTSagMNpveUKEuAvdnqu/ZMaGOmtbM3NqFElV4pSq0Tp1dLMWllWnSynXlHUZGjoLBwc6ZnYdHoFkK4coHCTDYVtUeE/S8PFrQZLMd/R+/iO91576ZUXXtn60v6DnxL4UQCFwaNTaXiSgK+n4Sld6NLAnrn7UBhAcGd/S017Yak1raQ5pdSaWtWa3dxd3jfcBR6c2Gi6BH66oEHIpFITDYZbROG05GRwop62ChIdqf1l4S3F3lch2MbB16Hw73Ow/405+DoU/kscPDVFeCoHX5sacfMcPBESBhxsQ+Gb4uCpIeEbpghf4eA/nSI8ycHAdg6+EhK+noPtHeYmUyNuxMFQSHiSg2OsP0VZf4q0Hgy3fhfU6owZyL7VMO3NCPxcR2269d+tJSnJ4cKFS2fPujk5uzg5Bfl5q3jMlsLMroK0pjR9lUneXJg51Neda1IQfc9HOp30c7yAiojhkLnezr6Hvvn2k7e3vbZ21e43Xzv59RfH9+3ZuXXDp+++FhrgKeRzuWwuoGEeTywUyYRCiUIiVgmYXGSQAhveVFHc39VeqBdZ6PBEaixYIyWjNAoF4GCdKQ1Nk312xHPD1t3Hvzgv95elRSVz/NlinrS4rOTy5cvz586b9cwz8+fOXTB/3r7Pv2DSWTqdyWi0CIUKHlOmlZrkPLmMxedhcQnB/lw0TMckmXk0M5tooqAMhFgTPsZCjEkmRaUQwtIIIRmkkGxqZDEXAQi1XEqsUlKrldQyCU6B8sJHeCplcrVWp1Yq1WqVQgmhcKJa3laY0ZKuyZdQPE8dATi+bN6S1PCkYfXoaOvI+Oj4sGlwSDwScMj3madnPPnkk089+dQzTz2zYuG8ox9vk4Q4ppEj85hwjt+FU19+gIKFGY0AfY0anVFjSEpKSbekZRtMllS1vFDJrBck9nM7RyqGRpqGByQ9A+zOKnGe2WgyJGXqzOkYBv6S7znHgPPnvU66BV/yCHY6efnY8Ys/uwe4x+GQGBImJCYoKMovDhMVGuh+8PNPPtqy4ZOtGz/ctO7i0R9v8k7a7YkKZ6SmguN6M3WFSbLKUHKhOLGeKK2MZZYguaWSpPr8is727qGh4XtMfE9XVV9by2OzJULhLUYC2tvb01NTQTtt2ipIDI70lbVkWirFlkohQD0AfAABc+qVuQ0qW1RVBeizoTNvaORviHZfh8LX0PBNdKEraNLmN2ouuh194snH7rvvPjB9+JGH7r//Xy9tWkuTwoqsOoDCN6bhKZ5Kw6CRkFWr7B/+exoGN9TQyEBtR3FuvSGtWpYCNWMkGTXgQ8ryGzQVrWntfXVj43+9jMNfU4vVWlRQUFdbe4sVJMpKS406HXipaUDh3vKhCljbdRxsQ+E/4uDfhoT/mIMhFL55Dr6+q9zUFOHfcDAUEp7KwTffVW4qB99MivCf7Sp3oxThqxx8K13lJjkYPYWD7SFhOwdHQxz8U4T1p3DrjyHW7wLb/clD1c0Th/yfIHOiyeHC+cvnzrs5Onu4ODOICfV5aT3FmS3p2kqdsDYjcbCns6Ygkx7iFOd6MsDhjPvFi3QcVcyWI6PQwe4e+95786ONK498vNXpwM6LX73/1Zsvvv3SyrNHf+CyKGKREKJhNo/LE3G5IolYqlUqVRwqO8pbQ4K31lV0tzbkK9k5dFIWma7DYhU0gkGtNiZlBiOYb+z5ee/OI1nwtCHR0LBicLRmfLh3OCIscsGChTOefGrh3DlzZ89+8403EzA4gyFJozHpDWapRCPkqKQ8jYQj5VOYAiJJgMOIiRg5BZfEo6XwqSYKOpEUl0pBZFDhqeToZHxIKj44nRiSQ4ss5aMqZfgKBaVGzazXcmqVtCRCEDnSXSoUa3QmKD1CrZErlCKhIFmj6CjMaE5RFokI1FDPNza/OnvmgtBDQf26nvHBX8eHfh02jeXBs956cTu4Cjz68CNPP/X05k2vOF26QIOFCUOd9XHeSfEBCRcPHvzo1WAvR51Gq9MalSqNQmtITMlKySwwJVksCkmpjt+uzB5gdw+XDI60Dg/IegY53TWyUpMpUWNIESjUftGBP5378cfT331z7KuAGG+WhOXq7/rWrtff27vt4KnvvYK8MSRsTEK0M8Blx2NffvjWpsXztiyes2H24wd3f3CTiZG3KVe4q+smc4VJ8kp1WiOYqW3uM+e1Cox1AI7j2CVwdglPX5td2tHSMTAwdI+J7+nX3p6eirKyqoqKW8xBr7ZVkBBNVwWJsfGxqrZ8c6XoCgfLAfxNcjBgSjt3ljTrmruLxsanO33ZhsKJRU2a4mbdb2kYQuHf0PDU+ZJWA0cdv+zZRc89v9wj6GwcyT8G63PgyN7HHn9033c70yvEBc1aiIZ/mzR8HQpP0DBAYQlYc/eg8PDoYEVLTkqVFBy+1CoAwVAzJrtOkVuvzKtX5jcoS62m9t7q8ZsbwehuE/hTAufqgTvff3S4Y7SW0gFx8JS8iN9w8HUofDd0lSu6MQdf7SpnQ+Hb2VXudzgYCglPcjDwn+oqdyMO/gtd5a7j4MOAg6MmOPhQmPVQqPWHQOv3AZ1o4Vjfne0mNDg4CC4EJUVFXZ2dE6v+qixJiS4Ol5wuXnS57AgLD8lP0vUWFnQlZ9So5WUmeW9bU0tNuSjOL87pZ4S/q+OpY45nzvIoXBlPxacJkMFBhz5+0/fQHoLrMaL7z4gL37t989Hul5/7+qMd+HiYTCLk87gcDrCAwxYIBVK1Sq9TKMTEeGqAg44c19lS11FX1SwtaGdW1dPyknFcHZ9lTMpwCcFu+fA7v+ORQ4LeIWHvcM4AQMyBov7jXxx/8omn5s+ePX/2rKWLFzs7uSYmJev0SWpNot5gEQoUIq5KxFZwqBwmniQkk/U8ZpKEaxKw0qXcDDHTwkiwUJEZdGB4CoTCobaocFguPbqIhyyX4srl5FoNu9EgaNZz0ylhpDBHCZ8PXlmt1qnVWplUJRQI0/XqrqKMBrMij4PWJET8+OXXz8xevPWFNw00PdiZI23DqSjz4Q8OPvbQo48+8simjS+fO3tOIBAUFeQ31lTlaUQmZIAo8ELEia+O7d3h7XBSJZNpNUaZQi3XABTOScvMTzJo9FxqvpTaIDX0KlvH2sZHKoaHBH197A4zS8eXCWRqHYMv9IsKOeNy9ucLvxy/dDIEFsKUcANjgj75+v13P3tzz3ef/HL2GAyNiERHHnc44uZ96czRg9vXPffGysWvLZv7y/490xcV/lMiyysVyQ0TCxA3jNdb+9IK28SJ9RRFFZJbGsssZqirUwramtr6+wdHb7IQxj3d0++ptrqaQaHIxeLpqSDR1teYWi1PqhCkVdk5WA5wMBcaqs3GwbYkXcDBwBUtid2D0x1QsaNwoQ2Fr9Lw7ycN2wl4cr683eQedHbu/FmRGK+ydlN1j7mq25xSKvz6h92LlsyTJOFL2432wPBN0nB6zV2EwuPj4zXtRZZKsblCeOXwAQ6GDp99J4D9VtikBjuwa6Bp4jn39FuN/dqi6y3xt07tJ/cXOPgKCt8kB99VXeWucPA1qRG/5WAbCk9y8A1ShO0c/Ke6yk3h4D/dVW6Sg6d0lQO+ysGRVzn4YLD1YJD1Oz/rkdB+Q/bEob8z6urqUisUfDb71ke0TbEkebo6uzhc9nJxlrNonRkl/Zr6HkGdVV7YUVfd0VKvIcPglw4ivC/BQvxOHTnkfO4MA0viEDlSpgDu5ery7ceSUEcj3E8W6sL3v4R3Onb6kzc+e319hK+rRMgWCng8Hp/L5XPYfAFPolIYtFqTSiygR/miHQ4nsclD3QOjFaND4v4hbl8jrdjMEmkNyee8YVs//A7nShsSDQ7Ke0atw6M1I0Pyfq8DnnNnz104Z87cmTPef/c9FouXmJSq1iZqdWaTMZnD5DFIXAlbyqMyWTgcB5egYhBNfJqORbQIaFkSZgaPmM6Mz2Sh0mnwZGKkBReagg/OIIVm06LzWPAiYXyphFCtZNTpuU16bh4zmhXlJOVxjMZUjcaoVmklIoWIL8wyabuLMuoTZdlMhAkfefHo0YVL1sxdsIKAJYGdmZ6U/tHrHz320GMrVqwAECyTSktLS83mpLjIYIOE29fRWqzkCoMduahwAR2fAAuTCfl6XaJSpVNqTUnJ2Zlp6Ro+XZYQlUqPy2dTOgtqx7vHhwxg5/RWUvMiAn3CkWF8mVisUrPEMhKbjaGQ4dgEv4iggOhAZ7/Lh059d/TsYWdvx/OOZy84nz967uej54/AMTFB/t6ffbD9/S3r3t743Lkj300fCo/9mbrCAIXlU1B4qqztA1kl7TJLA01ZjeaXxTCLSfJK0/9n7y7Aokq/x4FvmiDdit2u3d25dncXIgrSSHd3SXcP090zdHd3M3T3/t87g4i5uqhf9/ffs+e5z2UWkGHuzHzuyznnZnCqGrq7+4YAikc/77/4/yYATSb+l1zefGLwGvqZfyiZSAyPDBU0JMWWIqAFxQo818HEdxwMAFrUyChuZBY3Muras7/zwjCvQCK/nsKj8OdoGChwDIKlbTH35C+tWrcsoRBV2h4LXXi5jgpA7BJgLDVT3AdmU9zKBvf3HQq/0fB7LXQpUO3Bj1Ir3NJdn1xBjCtFjlsPJmVVk8Yv54PfGDiNqWpJ+W71LeApUF9Xl5mWVl5aOpFjGHyfttbWmurqifcefTq6SvtLHZuhVrnPdTBE4Qk6+L3SiPEO/qJWubES4b9xcPVbDv78EmFuq9y7S8KfdPC7rXIfKI34SKvcJ0qE33Xw37TKjTrY9rWDLUcpfNO08bJBi4HvYOM3HEsCgEHEYqPCwiY+QSI5Id5IT1tLU9XGzDCXzOincHqi2nvgnQMFAwOdvak4uLv6PQf1xx62ptbmJvdvXlOTf+huZeZuYR3h/spC6aH5/dMMZwOWiynBWhdrqYUyVbV6cPbS9uU6CndQUSEYNAqFwkLTJKJRCDgGj6eRabF0Gh0b4u2ofBumY9Kb1THS/Vd/Vk8forMH1lqITifT4p++tNt44JKrmh9UHUHvHm4ZGEzuAxzUvKIhKSEpIyE+T3amzkttBjOWxoil0KD5azHsuPDAYE/7VyEe/lF+fv7ODu7mRqFOFgCd0W5WRD/nBJhfWrRfGswrLcItOdgx3tcmzts8wccs2d8iPdg2K8whD+ZejPItwwdV0MOr6VGFcDeEoyYmKozNTqbRWRQSFYPEY5Co7HhGR25SJROVEu4a42tjqaq0fOWmdZt3M2NiwC8zICBw0tQpP//8s6aGZmFhYWxsjLm52eFDB/atW+Gn/6K1sqirsaYknl5dWtjaxCnMzWRSSQw6m8aIYbASk5IyWSRcsJ0hzFKL6W6agQrsbGlsy6tujMyvCktCu7lp6sgZWOt4hXiFI+EUVgI9NoXAjI1A4XTNjB++uHf25rETV45oGWs6eTrefHRz/c71f2xZdeHOJRs3G3VdlYuXD505uX3/thXPH974rrXCnz9B4hMUHovWjoHs0jZSYl0EpcIbVQJM7Ictpac2lNZ0dvw3euL/m/g3TpBo721KqSQmlKFTKvDAecDBQIQAUjxK8iDFc3Aph1XCYVY0x3f3t4x+8XcJQOFSiMKjq8JvKDyuTALa/5CGwf8FFL7x4Oym7WvTKwhFLSxw17JryODGQIT9nPkzXQNNwY2jFObmuxR+O8GvKLUSB/7vj7AqPDQ8mF+fHPu2g8FdAHdwvIPBwwceuJJGRktXOeDl6Bd/ywCE/RdNkBjuH6lFtP/zkRGvKfyOg9+h8D9y8FiJMHDwx0uEv7hVjkfhTzj4i0ojvmmr3AdLIz67VW60NGKcg2+ZNd4y5Vwz4tww7sLEjh4B3yC+4gSJ1MREE0M9rZeqbvYWFfjUfmRHD6y9L6UbCLU3r4Ni98pO+Y6LiY6Hi4OFucmVC2cfXj1n9VLRQvmZm56m3v2zVg9Pk2xfkh30sbbaOBtNgo2Gv/q9hwfWvrh+HBboTcBgsRgSDktGIrAoBA5HoJGgYgZ2HBkbaqdLMvHqQ3UM1QwONgz0YDp6YW11pCoiNf6RutWKnaeeXNVqDW8cYA0PtwwPZgyVeOWf3HlcVExEWlz88P59keGRTFY8icomQ4XC8Sw6M8zH19fBzc/R1dfOxt3MwF5b7ZWhhrexWqC5VqStAc3PMRXmkw7zSA51Sgy0T/C1jvcyT/A2SfY1ywi2zolwyIe5FSN9yvD+FYzQGlZ0GcYL7ayJDg9ksxOpUOccGVAYh8YUJLE6c+LK6dGJYc6xftaeBlqrVm85f/l2SWlZT0+PrZ39tGn8U6ZMlX/yxMzM9MD+fdLSUvzTpxzdsAphqlZIDB/sbhsZ6K8ryIzDwuqqyrMz0ygUGoXOjo1PjY9N8HO2tpC/FqQlR3XSq8yM72hpyEJGpPl5453N9NSuaxs9NbDSfKTy4KnqU9+gYAKVhaUyI9BYUzvr20+ubz24dseR9Ua2Op6BHg+ePdi0d+Mfm1fclr+paaxx8tqJQ+e2Hju7cd2G2ffuXvjM5YOvQOEvmiDxORQei87uwYKKDoDgSGqlD7oUmNgLWTI6eqLzv9ET/8fj3zhBoqolP6EMw52K8KZE+PWS6uiCInBwCYdV2sQua2KXN7Fbuif6J78vivcpDJLHX0jAYxr+UAsd2AL1KqjdXrRsLi0tvLo3qbiFXdLKrupJtHmlIyYh7A+3460Kv9HwJ8sk0gGFKyAK975N4aHhkdqm7u7e79qd1trdkFxBGD2N4S7n8xzMPQcYffiKGiAHQ6cxjYzq1tSBoe9RfQ4O3X/RBImukv4SGw50KY3PXRL+H7bKffBqGh908BeVCP9jB3+tVrn3Hfx5JcJ/2yp3x/pdB4O8acy5pNei6zn0zRaGweH6tSZIpCYlmhjoammpudubl0dn9sM7gUoHa/uBUHuwbSzbcEc9VVcbq1cuTpbmRpfPnrx5+oiV6iOTJ7esFO5oXz9sfu/PUJ3HMGNFlLUa3k6DaKsZpiOn+OcmuVM7gt3siGgsHktGownwaDTY4oh0AoXFZsUWpiRmkqPzQ8i9qPahmoHBuv5eTEcfrL2GVoMix99TNl+x7fimLUf91AObYPXDjcPNKY0mtwzmzpotLi4mIyHx6N49MplGocWQqCwShcmgx9Ip9KiAYEIEihwVHfnKJcTBys/C0NtQzUtf0VPvmY/+c6KHZTbSLzvaKyXUKTnEITnILtnfOtXPIiPAIjfMtgDmXAh/BShcjvOrZoY2xiJrqEFkbyNcZCCTGUOmUshEAhaFJeKwpamxHTmxZdTI+GCHOH9rZ22V5cvXa+uZlJaWVVfXqKtrTZnKP30an+ysWZLi4tOnTRUUFJASE719ZB/ZXjfFz7oyjlSbnZSNDnRUuhXq5ZgQS4d65qhMYGF4WKD2w8v61//0V70TG+zc3dZckRHP9jGP8TH2NJa7dmX3HYXzlx+fWb9vza4j215oPPcM8AqBR4Ygoh093J68eLjv5Nath1erGSgERfp7BXq8NNI4dfnYU9WH+pY6Vx5c2n16w4aDC+evFjt//fj3o/AXTZD4IgqPBXhTLKnuZGdw4Iwqf+7oCbfoIkzM6OiJvv9GT/xfjH/dBInhkeHChuTEMvSHW+UaoBJh7primINjyptYjR2537MHC6JwU1wBt175Axp+77ob72i4pIXtFmQqISX6RPlmSikWSBHcQWJi0IFjO2fNkcbHBRRzV4X/RsPjyiQgCle9RWFOay849XUIL2hq+67Tl2va85IqUNyxd9yH760SYd5pzOhyPi8rmmO7+hpHv/gbx79mgsTIX/WY9gKD+s9eEp54q9wHS4Q/4uB/eFW5L3Iwl8J/4+C3S4S/qFVO8Yta5T6jRPiLWuVuvy4R5lKY62ATiMJX9Tk3DbuICaOHwQ8cKYkJxvq6LzXVXW1My2Cp/dFdfeSuoeaBgYzenshWtjPMyUzP2d7R1dHO0dJQ9fE9lTtXnDWf2MhfN79/Xu3sjpcXdpjePGxx588g7Uc4G3WkmZLL04tPD6++d3C1l4UuEYnEYkhwOBoWhUQiCVgClUimp8YlNuXntuQmNzASetJbR7r+GswY7Id3d8FbCpjFkfjYGwrGq7Yen790/Za1+5UvqRelFpFQxEWy84WEBCTFJWWlpZWfK4HvA1RNorDJFCYdUJjKQkUiY/FMNg6PDfJBeTpH2JsFW2hFWGkEmSgFGiky/G0LcMEFmKAsuHdauEtKiENKkF16sG1OqE0RzKkY6V6E9ChGepVhfWppIRx2dAUlkBVsS0FF0hl0MoVCIuBxaDQZjy1Pi+vIiikhR8QH2jN9LPWfy61cvTU8Ep6Tk1dcXHL7zv3JU/gEBIT4pk2fNmWqoICguJj4bBkplUsnqY56NDstqr1unJ91ZqiD8a0/lW+fxiPD6XQGlcbEYeCm6nLPTu20unMSnEuUpcZ0tXBSEb4xgMJ+lp5WqtduH9txcuMfu5du2r/2zNWjl+6cfKr1UN9Wx8nP1cbd7qHynVO3Dmw6tOLEtYNGdrq27pYWDsbHLxze9+cudX2V55ryO/7csGSb7LwNkqevH/nM9/2vUyv8+RMk/hmFx2JgcLiyHho9gRo3egJGr0orbPlv9MT/sfjXTZDoG+zJq4tNLscASAH2AUiNOvh1qxzkYGCp1w4GlqpoZjd2ZA6P/POlvk8HeEYUVLS3tL8xJaBwGaAw1+WfqeExCoMsaGTE5yP2HNzKL8B39NTeR8+v3ZW/tGn72t8n/X7t7um0chz48ncp/EkNp1bixyjc3TfESGs09sm+oMW+bRSfXtjS0TPY2jkwlm0guwbaucnb6egeHJ+dPYNdIHtBDnF3hsD3HJ+9/WM5DLZ9/cP9A8MDg3919vZkVSfEFqM+1Co3ehrDXc6HKPz64Ytp6f77krAfKr71BIl+zmCZc1OhYf3nORii8DsOfk3hz3XwW0vCX9Iq9/GraYw6mLsk/NrBEIXTa7Uzm/Ry63UyPuLg8UvCYw6Oe9vB75cIAwd/sEQYX/8C36JCblYhfvxqGmMOnshV5cYc/Har3P2Pt8q94+AbRpwbBpyLL9usg4Y7vslr7FecIJGalGhqqK+jpeFibVKIYvYj2weIvcOc4aG8wZbwaoytq7OFvouDg4utZaCTNczV3tdUO9hY1fPFbbv7p/Wu7LOUO2+jcFXl9A6L2ycQJi+8VW9rX9r99Mjqa1vn22k+JSAQKAwJBkfDo8EbDh6HJzNp1OLU5Lb8TE5qTF1qXH9b52BVfw+6oyOCU4UpSI5LD8Owr8oZrtx8dN7i9SKSc0XFZvv7BsKio0REhUSEhMRFxWfPnKmhqgYoDL4ZkcICFKbRYykUBiwkkoEkESOjIrxcopytox3MMO5mWBcDhK0m2d04A+5dQoWV0ZAZiECis0mUiXKY4XOYmRLRXi3e2ygn0rEI+aoI6Q5MnAtzjvezQFi9CDB6TowOpdMBhWkkIhFQmAIonBrXnskuIoYlBtqjHPRunj+/Y/ex+IRkBoOdnZ176vTZSZOnCQkKCwoICM6YISIkLCEhsWC2jNHdSzS7l2QzZayeAtP+ZYqfmc7VAwqXDpEQ4XEsFo1EcLc2fHRql9q53c5PzlO8rDpbGsvT2DH+lgkB1sxAO283k+tyl1fsWrpi++JbcpdVXj4+eXX/iTsHzjw6euHhuatPrxy+vvvQza3rDi9etXfRkRu7D13cdfXR+QOn9i5eu+D0tVPnbp5evH7e/HUzV+9ecEf+/ODg96LwF8UEKTw+Gpp70wtbwXcLJ1e8ii5yCC8IIZbHZUFtduAd8b82u/+CF9WVlZGhoUQs9ltPkOjub8+tZfOKX99xMIDmWInwOAfHVjbHNnRkDA5/q2JogEWniAIT3+zM4tZh7jNiCCqQiOM5+B0N59e/pWHe/vsaLmpiBqMcN21f8/vvv/HPmD5t+tTJUybtO7IdxfTJb6RD68SvKTxew2MUBjlew2mVePB/e7kXHEnOa35qlbxXjrLnMeXyyxjwY9uF5lsHv0nbkHzwNHeMKHAAyd1xgxW5R79JT0SxN6oEpA+qFGx9MaUB+DJwzsxL8OITRq4Ip7zJKFolglmFZNZEUItcEPRAGj65nJBV826rHNfBo6UR4x6+GGhF/3uVCw+Dx++7FPlMJFriuwF/xwZHTNzBIMuNaypMaitN66A0qa0wHE/hcQ5+t0T4ixz8wRLh8Q6GKFyllVqmkVilCfjLc/CnS4QhB0+kVa5OCVf9HFUN7As5+AOlER9plXvj4G/VKjdaGjHmYEPODX3OFR3OQ9O+1PzRQ+GrBhAwhUCAR0aWT3iCRFpykrmxod5LLSdL42wcsgWd2wHjDJUPD1b3ZXihA41UXlnquznYutiYoj2dCvHRccEepFeWWDsdrLMhI9I3nU3OiqeHuVia3T0Toatg++ic9Ytb/ta6KlcOW2vI45BIFIYMR+EQSLBLJOGIyXRyZXJsTRytmIpsrigE5ksPh8fbBqX4wJPIzLjE1FAk/dJDvZWbAIU3SM9aMn2GuKqaJovFWvXHSsEZApLiEjMlJZ88fgxUDShMoDBIXArjMHg/F2diaDjC19vXztTHXDfYVAvhqId11qd5mhdggiqZ6Np4UjEVgbEztrp9VuvULtWT2zTObtO/vNP2/mGs5bM8mEMJ2iU9xCxM767dw+O2D/90Ub2DiwxmMBhkClQrTMBgqARcWUpMazoTfMPEADtvQ9WN67Y+eKyUnJzi7eWTkJC0e8++KVOmCQsJCQsKCgnMEBMRlpISX7FglsPTW1j9p+HyV3yv/hkufxln8Uzh1OYXN08xcdFJLGqop6PilSOPD683vnHUV+N+SVpcW11lQrhrrK9pQqANJcTd+ZX92cc3Vh5avfnw6kfPbjxSurnzz437rm3feXnzzJXSczfN2XJh7f7bG5bvl12yd/aWi8vXHluw48TaC/dOHTyzd9O+dev3rJFeJCG7XHLboVWq2o+Ghr5XgQR4jW5tbW1saOj+jFW3r0jhsWhp788ubSMn1UVSK8F7oX1YgT+ujJHWWFbb2d49ODD031LxvzJ4b/8TF0BXZ2dJUVFVRcVn1gz94+jub8urY6dV4TO5DuatKQJIjXdw6TgHV7XGlXPikgpSUgtqs0vas0ravm7mlrXHZzc9t0059JQmb5UMLFjf3Dc8Mkph8CO9Q2Guhl8L+PXCMLTPFeEYhaHbG2hwqqeuheLtxxfuPrlkbKeGjfXPa4A+h0fhT2t4lMLc5FKYzKNwdWM3+CHvmyQcUqDdMoyHM6qYaY3geT2WpMQ6fHwteAHhJdjHxtaMT0xMNZrFTTaUKFY1+CbjE0arBPwdS/CKAUAcSa0OwOfaRJL9KcTkcmJ27Ydb5aCHjzP68FU2x0GnMe2Z36G4BRz/tTU1aSkppcXFEzmGwfdpbWkBT4TmCRfffzBGhkZqwluh6ogvdfAnryoHBFxr3lhj3lBr1lBn1tBo3lRrVv8hB48vjRhz8FiJ8DsOfrs04jNa5Rp1c1hyCNXdT7C3Axq1M2q/wMFjS8IfKxH+QKscR5lYrgC3PvDs1TH1mueIRiX0t2+VG+/gv2uVg5aExxxswLmux7mmy7mg2RlOHj0avmoACpNwOFh4+Oe05n86UpOTLEyM9LVf2poZJqPD8qLCYmz8yhkZ7XX1GAdDH73HPha67vYWLramWA+Hcmx4KRUFOFiTl84pL2ypqypMT8xJjKkqK2ZEBuBtdMLM1DPi6PXVFclMEjrUF4tEojFkFJaAxBIxaBIFiUwlIAupmBx0WDGb1N/dkc8iBmvJ+Wk8wPjYs5iMmIS0YBj57B2tlRuPLliySWb2Uj4BiUuXr2VkZBw8cJB/Op8UNEFC4tGD+zgCoDADJKAwnRmHhiM8zQ2YkUE4b7dgG4NgG31/Q+Uwc1War3UhKbIhiQ6yOgafFu4eqffMQ/7KK/mLDveOWd3YY35lq8nFTSEaV7NCLcqxrqm+uiFqFz2fngzTvoVxNmDg0XQGk0pj0ahUEgFPIxFKktlNKbQ8uHe8n42VutKc+SvsnF4RiSQTY1Mmk7VmzdppU6cKQ8vCM4QhCgvJSIlvXr7AX/MJWkfO784p1xO7A26dCFK/fuvQSu2n15MYBDo6Uvfx5Vt7lqmd2WZz7yTZz6GjuTGXjqa5GQAKxwTakiIDnP0CbuurHXhwcMPRpRv3rly/f+2Gg2vPyZ06fPvw7DXzF2xeeOzBvgsvDq8+On/pwdn776/bfXXVsh2yf17Zd+XBue1HNmzY+8e8lbNmLZVcuW3RM/WHn1lU9hUo3N/fn5SQQMThyr7SBIl/HG1dA/kV7YzUBhi90gdd4hBe4I0qISXVFVZ2tHYO9P/XZvevCvC2nZacnJme3tX175gg0TvQVdAQl15NGHMwsBSAFHdN8U2rXEUz11ItcbVt8SUN8QHEeMeIXPfoYvfoIrfX+bH9D+bHPvkVvBg8Be4ax/PWWe8YxdNSGweG+sqboVphwLvP1fC4Fjpwvwo5zIqO+Lr+lIrO+IxKAsjKroTa/hTebDXewLUCDuMzNQydOQAKv64V7hsYTsptNvXN1nHPGF/X8YkAJ0pjOTQ00j8wDJ7pA9zs6x/u7h2CiiV6h3g77V1vCi1Agn+iqa2/pX2otL6GlkuIKSJmVBHBTw7u5vutcsDBZa8dXNUSDyhc357xHa48BwgLngWhgYEsOn2CbXPZmZloBAI8rSZ4evnB6GscKHNr+giF33XwZ5YIV5jUZusWYOUJUQ+iw+9Fwh+iElWTKwyra0zrPu7g90dG/LNWufEjI9JbDQqCLjsLThUwO6LZrJtd+xJQeLyDx5cIf5GDP9wq16pKLpAL2zpzxdmlu8ufRnGUMD9Yq5zJGwff0Odc1+Vc0+Fc0Ggx9Rlue1P3/7Wir7e3uLAwNzsbvCmM3vRPA1CYtypsbqTHhgXFBzm4P7uBcLRorCoh+Jj76z32N9N6ZWvibG+C8rDLDvOsjKP0tHI6asur01iJ0d5wO71QU/W8eEZnS1NZPLU8La61vrowllpXmp+WFINDoXEYCo5AIVBoWCQaG+AdE+aTGOaVFh3YUlXaXFWKtdN59eySn/5jCsyfGZPAikvzD8efvqU+SmHZJXwzxPftO5SdnX392jW+6dMhCktKXL18CY7EEKnQqjCRwmAw49FRUYEW2qlwX5avE8xKK9JGG/xUCDvdbGxYa05iU1ZcdQy2AOnHcDXG2esmw3wL6Ci2v22g6hU/+eMBz0+hjR5mh1jWEb2SvbT9np11e3LKR+Mmys2MTafQmCwag02l0kkkAo2EL05kNSSQciLdmd5Wyg8fLVq6Fosjenp6mZqakYjEeXPnTZk0WUhAQERghogAv4SI0EwpsUObVyEsNBgO2lhjRbSWHN30hb/q9RsHVjkaqiQxcJ4Wurf2rX6wf5neld2eKncqslObKoqoHmYUZx22tyk92JGEjHQPiVByNL9ldHv98WUCstMklkjsO71HSUfhgfKDUzfPnbl95ubz8zdenNh+bs3CXTO3XVy+6cRiyUV8C1fP2nlo0+4/N2/e98eSdfNmL5eRWSFxS+Ha96Pwt5sg8Y8DvO0Vc9vsEMwqP2wpAAFgAS6uNqesrbn9vza7f0dUVVREhIQgoqKaOJzRm/5RgHd98GQADhj9+JvF4FB/MScpo4bwjoO5a4pvWuV4DgaWqmlNqGiKT8xPT87jpBe2phW2fN3MLG5lZTQ+s0k+pEBTsE6OolVWNfQCCpcBCjfQgPA+X8MQhV9rOLOKmFKKBZlegQf7INPKccklGJAQfKtJCYXI1DJuR937Gn6vaPgdCvOiltOdkNPU3vVtV/HHR/dAU3YdKatm1MGjS8JQifCbVjng4PKmUQdz8/utCudkZkaGhsayWF9lggT4Pt+Cwu1ZPcVWo5fVeNvB71D4cx1cblxTa94YfCdsnez6tbLrNs7ZtH72ht2L9lictirSK6kxrh2l8OsS4VEHQ0vC4xz8la4q125Q4HPB9qefftLdr9SsmwVR+MMO5lH4fQdzKfy+gz/SKtemSs57HLJMbN7B+RtL5SOaoFXhz7+q3AcdPK404qu0yr1xsB7kYJCXNDmPTAbKakYPiK8a0N8Hv0aNEHdVGFBY28RAlxziw/Szs3pwykn5Vm1JXjIuLNhAPshE1dPawNnBLMrDISXMq4KJ5WTFA1aynLSJ1sok+5eRek8R1tp1hTnD/T2Dvd3ZZCTKXJXo74xHRWEQGBKWRqEyqTRauI9HsKU+zsWU5GZeHEfv6WhLQgQGqN3xeHEjykGXTcTQ2Un0mBSfUOyZ2xqrNh2bt2i91MyF/DPE1q3bnJKcovLihQAfn5SE+Cxpia2bN3n6+FGZsRCFyXQ6IxYTDUc4GVEcddnupgQbjWgzVbjlS9Irywo2sbs4ozmLXUqLykD4ZRBgJekJdeUldVXlNSX5UXYG5pd2+T87R7F5kRdmByiMMnmsd3G/p4kGCRZIQYSywA/OYFFpLDKZSsDjqURcYTy9JgaXFeGGcbE4c/z8gUMnU1NTlRRfODk5R4aHSYqLTZ08RXjGKIWlxEVmSYneOLKD4WaSFmiTEmSbHmiXGWDj9OSc/IntUe5W0b5O8mcP3Ny+WPHkOoPr+0i+Dt1tLenYMIylCtlJh+ppRonwxGOird2cn1vo6Hjonrp/VGqR2Po9a288uKzxUlHXSFPVUE3TSFld79HVR8d2nV4/e53Eom0zF6yXFpSZKiIzQ3aR1PKNC5asnT1nuYz0YnGxJYK3la8OD38vCg8MDKSlpNDJ5Mry8tGbPh7fh8Jj0T8wXFHXBd5W0ezqADw0esIlqhDJqk4vbGls6e3p++YrOv/FP46a6mokDIZDoyc6QaK5OSk+PjsjY6D/m08kqGrNzqwhQpD6eKscz8HVLQnVrQk1rfHtfUWjX/wNApwT2ocXWAbmZpe28arnoVrh121zPA3zKPxhDb9TNFxPK2lhOfkaHju97+ipvX+e2X/8LJR/ntl3+MTu0xcPU1JCMTH++w5vN3VQ54HyXQq/p2GoQKLmXQrzglfc/H2iq781p5aWWU1828Hvt8rF8hwMPXwtcZzvVSsMzgaLCgpqq6snOEGitLg4hskszM//FhTm0DqhhjmTr9YqV25SU2/RZHnGBgB0y9ytd7feO/7HCYkZklIC0q6X3KqNa6qMqngOrjSsqDaqrDaseL9EuFq/uMagBGzHO7hSN69aN79Wr6AGmHicg99vlQNZq51Rp53RYVDge8H+159/0T/wAqIwtzoCILhOK6nhZXI90PCbJeFRB9erxzZogIx5v0S4QZXeqEqvH1sSHnUwATi4QQnPeYFv51J4pfiCows2l8qHc5RQPAc3PIvmPI9u4lVHjGuV4zwNa+RSmANl0D9qlRvv4M9rleM5mLckfE2bc1mDc0WzJyFr9ID4ISM1NcnSzMRQV9/EUB/l50H3sbN+eM78weni9ISiREaYkUKwsZKflZ6rg4Wfi21sRGAlA1NODE/1tmTYaaX4Wyb7WyOMlbyeXaN621flpBenxMLN1UI07webqnm5OUWEwsh4GpMVR0CjXAw03NUfhxsp0f0c2xtqK3NTo02VfJVv+mg/wQe5s2hUMj2Gxkr0CESdvKG6cuPRuQvXSs9cOENAbOGCJWwW28rCXHgGn6SosKy0xMrlS51d3OjseDyZTiBSKVQWhUCkB7pSbDVywhxjnDQYji+prsaxQS71yayeivyuqoLmslxOWUFrfU1DRWkyjYgN9ctOTSzOyfTTfxGgfJ1sq5Hmb1MY6RSlcy/ASL0kP6eV09BYWw2YCyxMptCJBBIOi6UQMPmxlBo2Jj3czdNEb8OG3YqK6ulpaYcPHwkJDnF1dhaawc8/dRqXwiD5Z0qKz5ERe3b+MMPJIMHLNNHPKj3Ige1m9PLSnpfXjoVb6xs+vnJx40L5A6s1z291ULxSkZteV5SNt9dFm78gOmgTvW2IyDA4ItzUxkjP1tAT5q1lpv7n+aOKGvJGZtoamopGplpq+s809eWtbDVvPTi5ee8K2RVis5aLSs4XFJ0lIDVfXGK+6Kyl4nNXSs75Q2rOCqnZq8Sevrw9/HmrFV+BwuBVtbOzs6219XNWLL4zhccCOKC6sTslvxkXVxNCLHeDFTlGFERQK5Pzmuuaerp6wf//fu+7/8XnRFdXV3lpKTi/mshKGIiKsrJAX180HP6tJ0iAaO6qBJDKraMAOwJQAmi+3yo3zsEJta2JHb3fZBGFF739Q/kV7Q0tb/oFeRMkIAdzETxew+MpDPJ9DYMsb49T1Lw3adLvkyb/PmXq5LH89ddfZwjyR1M9Q7DOYP/uk0vZNWTwhR9YGH7tYF6O1gp/iMLfM/oGu4oa47N4pzHce8p18LutcmOnMeCxA9nSPdFrX33nAE+lrs7OvomNZPlgQFfWiG4r0H8HwSD/oYN5FK6z4FifgdZiLU5Z5usWJ6gm2Z6zE5gquH3+9pyXuTXGtZWGlQ2m9Y2m9fXGNY2mdY0mtdVQgcSog+uNKjjGVQ1G5RzjygbD8rHSiHqD4maj8kaDYo5BcZNhSQ20Nvx+aUR6zcuMJr3cZv3cJt3sXpPygEuOv/7yK0RhnUxA4VrNpGad9FbdzCbttFad9Bbt1DpNQOHYWq6DOZoJbdopzVqJLVoJbS8TGzXYtVwHN6ox2zXjWjXYLeqsdrBVo9ePlUYo4ZqVSR1qlFYVUo8Gs+RJ+EqJhVwKh3GUkMDBTYqIjheYdkVkG9hRQjY/ixpzcItCeLNCWKtCePuz8DaFUM4YhV87uPHxN2iVG+/gay8hCl9U64R9/XJhcNyCs7j8nBzAjNGb/mmkpiZbWZiaGpmYmZjAvN2Yfo728pcNrx9KpaJrCrMizV+EGCgEWuh6ONq4O9nBvNwSYYF56IDcCI9kX7s4L/Now2euj8853Dvpq3IvzEgt3PiF29MLrk8v+hmqutg7+vuHEog0ApkW4OJoKX/L9fn1EP2nxSnstoYaur9tkOYtH9W74dbaNAyMQqUTSFQqM8HVD3H0suLy9YfnLlgtLbMAUFhKUgaNQvv5eEuKCokLC8yUFF25YpmruwdEYRINT6SSyQwKmRaHCMsMtS9BuDEtFeJcXuaifMviSJ3VZUNd7UN93X1d7Z31FTVp7LToAISdoYfGkwhbg9qSvJri/NhgV5KtDt3ZOMXPMs7DsDw1vqe5riaR2FCUnZaSTqEyQBKJgMIYEg6VyybVstHxIe7qT58uW7HZ/ZUPFoPdsnkrg07X1dGdOul3QT4+EQFBUUEhUcEZs6QlF82RUb98NErnUbTeY4y5KsXZwF/rwbNjm15ePmx297zisW13ty5WObbe5MZ+gr9TZwsnLtIj0lABbaFMdNQhhXgQ8KjgiEAHN1ufMJ9IbKSHv5uZtalfsJdP4Ct1bVV5xUe3Hpy/L39Bz+jZtZt/bt65ZPkG2eXrZecsFVvwh8y8VbNklovPXy+zZMvMP3bO3Xpw5c6DK7UMnn6/VeEviv8VhcdHQ3NvRlErcdzoiWBieWzm69ET/13N7v9WfLcJEiAA6YobYwGFASXHHDy+VQ5Y6rWDE2ta4+vb0/u5HWPfKN4/lIcgCsfzKAzll5RJgCziMKOI7noWinrmz/XB9nXqmD03tFGOyY1mZEYqvbzvE2UDTJnDvRbdp8skPlgg8f1jZGS4ujUHUJh3T3kP3/utcuMdDE5jOvvqR7/+G8fIDz9Bor95sMq/pdDgbxz8kdKINw5+h8L1XAr/+vOvr654tdt0cSyaqoyqdy7YKTxdhP6MXg8EDL6nbhFdgRJxN5T4BFf4MqfOqLIaWhsurjEordAtoMvjI28Hkx6j8jXTeA6u0y+q1StIUqTBbgVi7oXnqyXU6xfU6ORUvbUeDFG4QTe7TCOR9jAKdcuvVC0u6LLT77/8BijcpJNZp5UMBFytER/7GBZ11Y1+P7RCldX0MrmOS+FGjXiOZnzOMyzmxivcDY88BQyvNAI4uEmdVaiAJN5wQ162S34QBBDcpELlObhJmdjwApd23w99ySrjgX/eo6A1kosgCj8J4ygigYObFRF5933wF8ww501y73u1PIviKIQ3Pg2teOQfc8265IFX8X1P4nnDtBv2AMGcN0vCYyXCE7uqHLQkPK5V7saYg0G+5FzVgih8QaXNLfyvwa/859avOUEiNdXa0sLE2MTYyCjc04Ud5Or87LrOxb2EAJemmnKMi0GQ9oMgMy1/N1dP91eeDrZ+VoZUX4cyfERepDfGUt323km9szstrh12eXTJ5eFFD4VLTg9P2j047WWo8crVMyAwMhyGMLeyUr97w/L+OffnV6kBTp2tTZlkRIS+XKDGbf+XclhfFwaVgiNS8QQylRHv7As/fPHZig2HuBSeLygkLigg7O7mjsdhF86bLSLAN1NKbOniBfZ2DjRmLFQuTKJRyICqjDgCpgDtm/xKB/PyWlK4S2dNSX9Hy0BXx0Bnay+ntqU4qzqRXIjxT/G3oznqo8xU4MaKCWHuHQ3VrbWVqbAAsp1+7CujCiZioKu9OS+uGOWeTYxi0xkUGpMCFUhQsFgMEYvKZZHq2GiSr9u5k+fXrN9JodCMjU22bNmWk5Pz6NGjX3/6SZBvBpfCAuJCM2RlJNcunW/x6EKUzr0o/Uc4e22cm7HJgzMP960BCNY9s1//1B6VA2vVjq7zULtVU5xTkZUUafoszOAxwkIJ72FKQYbj8djA8CCfEF8EDgFDRwVHhoREhMHRMJ9Qn+daKueunzt14dCfZ3eev3b4wJFNu/asOHJ8w/kr+46c3LLr8Lrlm+fNWiW+aOvM5btnbj6x6NzdfQ/kzjo7moBXztHH/pPxFSgM/qX/7QSJfxzjR094IUt4oyd4V3j+b/TE/zx4b/8gJiiA7zZBghsj9e0F+fVU7priuFa58WuKXAcDSHGXFYu/Q7Hp+BilcONrCkMI/gIN59VRgYZLW2NKWti8LG5mQ9ecAzst7LwGGuBvahkuowpaXuU5+NMaTq8i/AgUBtHaXcO9m9B1+D7WKlfNrfDmncY0gNOYoe/xY4Pj/8efINFTPVDuCk0U/nIHf3hJeJTC5hCFf/n5F9dL7k2WLTXGta3WbSdXnZoxRYDwhNBs0ZSunqq45/nmOZvWy67bOHvD3c23mM8odUYVNQZlRVpZJsf0ds3fvmXOxs1zNhod1anQyQEOrtTNfXXB/tDifZtnr98ku+7G+kvMJ5gGvfwaaEn4LQcXqMao7n6yYeaq9TNX3lp3QXWX3OTfJhscUG7SyeBop5WoMPUPKO6et3nzrDVbZdeq73qU+5zQpJXYoBHXqBkfdcXpxNJ9G2f+sXnmylNL98Eu27doxgIHY685X1h+cNPMFRtllu+es95kn3ypAoKjQmp4gQcU9j+pu3fO+nVSi48u2GK46/4KsXnHF24tfRLarIjiPIcjzhmfW7xzm8zyLTLLzizajjln1KIQ0fosknnF6uqyPfb7HiqsO/6H2Jz7Kw9WPPBslQ9628HjW+U+VCL8sVa5259sleMtCQMHX9XkXFbnnH/RYuo53PeVS9G+4gSJrIwsO1t7QyMjPV29IDdndqinm/IdrbN7AkzUWuqrmWGufmo3AoxVQ7y8/P2Cvdxd7Y11QuxMi0nw3GifKMPnNnePG13eZ371qN3NU3Y3Tzo9OOMmd9bzxW0/WzMf/xAv3zA1Le3t2zZf3LHO5t7JECPF6oKs2uJchJVmkMZ9f7U7YWaqdGQklc5CY0lYAplCj7P3gh08L//H+oM8CguLSE6ePFVTQzM9LW3TurUCfFNnzZRYNH+OuYkZhc4m0dgEIp1MYhBJDDaZUkCMiLV7gdC6UpxI7ets7W+u7yjJactLbstJbExlVLHRJfigzFDnBE9Ttqs+00mX4aidhQ7oaqpvq61KjvBO9rdtrSoe6OA0JGKrqCEZuBAWmQgoTKaxSGQyFosmYtB5LHJdLC7AzmbD+p1H/zyTn59/5syZXTt35+XlnT135peffhLmEwAUFhbilxAWlJWW3LthVbDhs1gPvRgvo9hge5ynpeqVIze3LXtxdJv9zbOv7l4wOLX75ckt9FD3zpYGip+Nn9q1MP37cDsNfJgXkDcWg45EREcg4SgcOgIeFRIVFRENi4KHewT5KOlqXHtw9fq98wf/3Lbr0IYN25bs2LHk1o1Dqmq3FFVvnr68b/nm+QvXz/xjz9xNJxbtubry7ouTegbyQX6ugBCjj/0n4ytQ+MeZIPGPA8C3oKLj/dETBZXt/42e+F/Fv26CBC96B9rLmxMLG2gfKxEeczCwVN/ARCfGf2kMDveXN8WPOZgn4L9fGx533Q1AYS6CWcXNvGQXNY3ug08AAi5opOfW07JrKVC+R+F3NAwoDG75ESg8MNRT3pycV08Bv4G3W+Vixlrlxj188a09Zd/nNAYQ9sefINFd0l9q21hk9DEK/5OrylUYAwo38laFva759Nr3tVq1FugWbpi9QUZAJlk1qdqw8vnuZ4JTBTfIrr++4dq2eVumT5p2df3lHI20JpOakBv+s4Vm7V6w4/muJydX/Km693nZy6xGw5Lg616LxResllkhv/3+xdVnxPnErq09X6ye2KCXO+bgWu1MQGHzo9pCUwX/kFxyceUJAOI5QrN++ukno4MqLTqZjS9TLI5oyMyQ3DNvs9L2u/vmb5XkF9PZ+5SjmdDyMpn9IGSL7GqRaULnlh88u/yANL+48rZbnVrx8fcDd81ZJzxV4M9FO678cXiekIzINEH7w8qNLwgdalTqdac1kovEpwmdX7pnz5y1i0VmT/510qlF28uehHYqY2lXbbbKLF8oNPPB6mP3Vx2V5RfbN3t11m3XbiU49pzBXAHJJcKzQG6RWnxr+d7S++6t8oE8Cn+oRPjrtcrxSiOAg69ocK6oNZ5TbNawG+7+yhU4X3GCRG52nqODi5GxqYGBsZ+LCyvc11Pzsda5vc7Kd+vKC9NIUT4q1311n4V6uAcHhQf4+Xk6O4S42KVjwmP8HYJePnaVv+T29LLns+tuDy853D7tLncxSO1OhJlGpI+HT0CImobBqnUb+KdNOrFukcOjM8nY8I6WJoq/i6/KHV/VO74a91Ee1jFUEonMQGOIWAKFwoi39Yjce/rRinUH5s5fLSU9T0RU8tdff79963ZxUfHhA/v5pk6SBRSeN9vEwIhCYxEoLAyOQsBTiSQ6lUzLpeMz/S0IJnJpcK8SJrItK6Y5hVafQGpOZ3FSGZUsVAHGPz3YIdHbLP6Vcby7UayLHtNJO48Y0dnU0FxeVJsR39Pe2pQdW8+G17KiM/GhLDKJSosh09hEQGEMhozBFMRQqmIJBipqCxasUlRUycnJXrFixZnTp1NTU/fs2fn7Lz8LQ6vCAsKCfFJiIrLSEhcP7iS6GmWG22eEOwAKh1poPj+55/6uVfpn9/s+vQXXUnC6dcbp6Y3qopySNHbAy9t+qpcjDOVwvnZkHAqNwSBQKAyRiCGR0XhsJAIWHBURGBYYEOrrFuStZqrz8MV91ZfyCkp3H8pfP3569/59Kx/dO/Hs6cUnzy4eObNt2cY5mw78ceTy9lN3dx+5ufH+i1MGBvKBvs7D/z9PkPjH0d07VFLTGZPJ4Y2ecAwvcB83euK/q9l9z/jXTZAYi9ae6rKm2GIOs6yJ9Y6DeSXCIOvaUjp6q79P09X44NYKxxcB7XGY72uYR+FPaLiQwyTFB7n6m7j4Gbn4GzuDrZ+xa4AJ2HcJMInLg+c30nO4s+R4FB6rkRiv4VEKc5NLYQpvrvD/PFq7q4sameCX87FWuTenMR3pfYPto1/2jQMcwD/+BImO/L5i8/oi4w86GKLwOw5+TeGPOrjcqLrCqBpQ2Ia7Kmx71q7CsDJNPU37iPa0SdMurL1QbVQFuxcpLSC9Z+Euohw2SyONKo//c8XRaZOmBt/wbTGtfbH3mch04bCbAXUGJUlKjNhn5Fr9wkLNlGPLDi0WWwC/E1Srl1eqmXpv0w0JfnH0naAm/fwa7UzgYN6ScJ4yc93MlQtE5qBu+eUoUUMuO6+SWgoobHxQpVM/N+UpeqXkkkMLd8Y/hjW9TE5/it45Z8MqySVZCtg27RTVHff4Jk3X3PUwUx6RJgdzPaETet6iWZ2luu3WtN+nqmy7mfYoJEcuwvekrqyAxDqpJQVPInrU6YqbLk369TeLfXJ5j4JYN5xvrTwC/rkTC7dVyoc3P4crbTwvPk3w1WHFhqfh9fKhZrvuSEwTctwv16UII10wnsUvyj9pqu7WS3FXLOKumNc+8uZA1RFvHPx2acSYg0F+sFXOcszB0JLwJ1rleKURkIPVIQqffd70zHT4G1xzDhy9ICZ+3GZn59rbOZuZWllZOvi5e9LDA/30FXUuH7CRu1CYGleYxPLTuOuhdt/ZxMDXyy8kOCzMPwgTGpiACoPb6TnIX3GVvxymI482U43Wfxaq8ThC+wnMRBnt6YAMDzMytdq6bd+vkyf/8tNP+5bODDBWamuoyWJTX72446ZwxU3xSqDJCxoygslg4vHcgWtkOpWVaO0evvvkg2Vr982Zv0pSco6QiPhvv/1+4s/jubk5169cmTFtyiwZ8QWzZ+poaJIpDAKFCUfg0GgigQg0TEumkQuwQcUon4xg+4xAq5ZkfFsWqz6JwkmhAxCXUaPykT5pwfZJPhaJnqZJHsYJbvpUWzWyvVYRHT3c0znc39dWnl9OCK6nR9UxojLwoUwyhUqLI1GZEIWxeAoWW5LAyCSjb1y4smDhKm9vPwQCLiYqqvBUITY2dt2aVVN//x1QWExghojgDClJiYVzpJWun4oNtEuLcMiJdmV4mds9uaZ0bKfu6b1Ot8/CtZTi7e2xenoxUaHNDVVED1PP5+cCNK7B7XVoqCgShR6NxkZjMEQqnUhj4KmkCFSkq6+bg5uNq4+jva+zprXB7We3VV8+dXO3cnW1eKH84Ob1YxrKN+/ePnb89OY9f67e9efac7cOP3xxRV79+jW5o3KK59VV73q623w/Cg8MDGSkpjJpNGCX0Zs+Hj84hcdiYHC4uqE7taAFH18bTCh3iih0ioSu8Jyc11zL6QFiHv28/+KbRW11NQoOJ2AwE5wg0dzUFB8Tk5mW1v/tJ0jwYnhkqLWnqrIlgUthVkVzzGsKQ8ktEU5r760eHvl+w8LGYghaFY6DKNzIHK9hiMIf0fD4q9CVtcXqmD6bPW/mnHkz5y6YxctZc6Rl50iv3rAiiuRe0sLmVQl/pobTq7kU/gFWhUEMDQ82dORD3XKNjLIPlwhD68F1bckdveBF7PudxjQ3N5cUFdXV1k5wgkRZSUk8m11UUPD1KZzdAxz8msJfwcHlhlU8CtuetQciXCe77uyac3sX7ZUSkF43ay3mMbrNqvn5boXffvk14Lpvu2Vzo0lNuwUn5GbALz//rLznWb1hucYBZb7JfC7n7Or0i9tNa1qMK5qNyulPMDOmzLi85lyZVnqpVmqNTrb3BXtxPjHjI5qNerm1oxROa9LLib7pLcEn9nDT9Tb9vDb93A79PPMjmuAnMTzwoseoMPiS/c8//2x5RKNOI75ClVmnEau8/Z4UvzjsqnOjRtwGmRVLxOYVPMN1vExs0YzlqLMaVRnlirg/JBasEF9Q8BTepcFuU2O0qtHurz015bfJxKv2FQrwPXPWLhOdWygX2qNGARl3y1V2hsSxBZtrn0YWPwpYL7l4+8wVWXde1cqH1soHM69YLxCSub58b9uzcPIFk5n8oofmrK544NX9PLzt6VhpxLdvleOVRnAdzLmi2njmWdMTw6GWr//HLnDEfpWDNj0j097Wyc7GycnRw9fDlxwaEGKqbnDtkOnNo3GYyNqSvDAjRcenl+Uun9HT1gsOCIsOjWKgEBkEOMxax+D6UaOrh7xV7iLMVdDmytGGijCD56HmGr6ONpZmFvsPHpk8jf+nn8F/P+1aPi+VSWysKAowVLS+d9L+4WlnxesIbwc2k0GmsbB4Mp5EI9HYFFaymXPIruP3lq7ZM3veCgnJ2cLCYpMnT9m2ZXNCQry6qorwDL6ZkqLzZkk/ffQYTyATqYDCWCQKjydQcHgKBU/MoWO6cuIq8CEJHnqlpKCOvITGNGZjMrUmBltMCM2De6UG2Sb5WiR7m6d4GsU6aaKNHsMN5TJCnWsSaWUJtAKMXzHcrYoQVE0NSSOEMchUCoVNotCIZBIOT6ITiKXJsXA/3y0bdq5duzUuLl5XV3fypEnmZuZEImHx/Pn8U6YJ8fGLQ5PUZkhJSa1dNt9O+V5apEtKuF0h6hXJUcf85imD80ec7l6MUlHKcI5qCMmpx+R11HMy6KgA9WveiheCDeQIwa/oVDqezEbgSCgikURlUBhsApPu4v/qsar8I+UHKgYvtK0NtKwNrzy6fuPBZRs7Yw8POyNDNaVnN030nso9PLV9z4KdR5edvrnr9pPTajqPTK3UdY0UtLQfKyvddXex+n5tc+AA7e7q6mhv7/+BJ0j84wDPvrqmnoyiVlJiXTi5wg1WZB9WEEquyCv/TmtC/99Gd3d3ZXl5dVXVxCdIBPj4oKKjv8MEibEAGu7oa6hry6xsiR9zMIAUUFRjR3ZXX/3/xMEgIAo3c1eFAYXf1zB35xMLw2VtMZbOWtt2bdi2e8OOPRt37Nm0c99mQGHAgmOn91GSQ4uamO9Q+NMazuBR+MdYFQbRP9RV355Tyq2OKG9ij5UI8ybfcR2c0tZTMTz8Wde1/9ECnA2CZ8G3OCdsz+wpNKjjUvhdB39paQTPwe9QeK7I3E1zNoG8vuF65N2IaqPKepOa82vO8U/mj1NiNpnWVeoXN5nWMp4SgWsvrjlXoZsPvxs2V3j2IrH5iruekB8ja/UK2owrYbcDf/351zUyf8htu3dv0/UHm2/+uewQwLHm3mcNujl1EIXTal6mNevl2J8wlOAXt/lTr1k3q04rpUM/P5g7QQKisEG+/XFd8FMdW7L3yebr9zdcfLTxyrbZ68SmCwectyxVIs8XnrV77oYmjZhGdRY0NUKV0aYRk/YoTJJP9Nii7W3qzAZlcv0LYpcG02Sv3LTfp/qe0E6447VKYuHJRTuqFGAtLzDtyrhiueA1kguPzt/UoBCZeddDhl8U2Pfh6mMPVx8FeXPFfqEp/OcX72hVCCVdMJojICG/5mjb0+CmJwHftVXudWkEcDDnskrjGYUmOYMhzkTLGN4JXoFETlbWxAsk0jMynBxd3Zw9XF28vNy9UYG+YZbahjcO6ZzfiXC3am2swzgbWj44tW/timOHj9rbOMJDouLx2GIGnu5ta/XwrNKJTVoXd1vcO+Esf95H+WaA5kOTZ7cvnj21cdPWGYJCMmKzDqw/sG/lDq1nz+qqSwn+9hZ3DlvcPWJ177iPnhINi2TFJBEpLCyRgifTAYVJjARjh8Cdf95dumb37LnLAYVFRMSnTZ26dMkiMolkZ2MtKSosLSE8d6bU9YuXUCgcicpCYYgYHBlPoOKwFCKexCYi8qiIXIQfzVEzMdC2syCpNSceULiKiSrCBuZGvUoJtI73Mkn0NEp6pcuwVQpXvx6sfoPioBmmJx+seZfuoJwTZlOK9iwnBSQTwuhkCplCJ5FIRBIRRySzKPQMJkNbVXvOnGXHj58pLCz888QxcNj7+PgEBPhJiYkKTZ8hzD9DYga/qICAlLTUno2rwszVMqOck4KtcyKdoo2f2d49a3/noseDWwkWge2h9X3w7qHSkeGOYdarV54KZ3xVr0U5GVPxSCozjkSLw1OYODKNSKFTGLFwEvmpnsaW43u2Ht+x78yB5zrKZi72D5UVzl4/q6AsZ2VnYm1toKJ4W/HJxVvX9m/ZOXvtrplbDi06eHL9c+VbDo6GTs4m1la6OlrPPdxsv98lNr4o/nUUHh/Nbf05ZW20lHr36CJ/XClvUOt/8YNHdWVlVFgYCY/v/fYTJN6Okb7Bjvbe2uauIk5nXlNnfnNXcUdvbf9gx3dulRsfoxRuhCaF8TT8FoX/TsMFjYz4PASW7YdljSYxPsjJx/CPNUvuy1/OqCRChcLc6zO/o+ExCr+j4Ywq6OJ8Pw6FQfQNdjZ1FlW1JAIHV0KrwrFVLbEAwbWtSQ3tGe291eB3OPqp3yuGh4e/c5HPl0Z7Vg80VNh4og6GKMx18BiFbc7YghMttYPq5Kdk/BNcqlpKnXFNHfhMveJjy4+J8YklKsc2mtQACjeaVLMVyLJCs07+cbxAK71CJ9/lnN22uZtlBKQ3zV7vc8ml2ags7IbP77/8Boh8YNGu3fO3gdw+d9OOuZu9L9jV62TXQrXCXArr5pgd1QIUfnXGvIk7Oq1dLzfgot1vXAp36edaHtGA1qqlV+xfsG333M175m3aKrvm2OJdlDt+6U8QswVlDi7YxtGIaVBj1qnQGlRprRps1h0fYOVzy/a3qzPrlaGraXSo0a0PPBOYzOd+VI15w2Wp6JyrKw4CCjcrodpeYAoeBayWXHhs/qZGhcj0O+7i04Qk+YT3z1m9R3YVyF2yK7dKLzPZcaNNIZR43nCugKTC2j+bnwQ0QQPUPrtVDloSnlir3GhpBORgzmXlhtNPm57of/VV4fa2NjIeHx0RMfG2ufSMdEdHZycHV3sHV2cn13Afz1BrfaObR9RPbfbRV2yqq2aEuhvdOb5qrrS0hPS1y9ddbRziMejKGGpqpE+g/lOd6/uen9igeHy97qVdHkpXvdTvn96zQVhY8Kfffp8nvcD+sW2qY3KKV2J5Xnl2AsPu6Wnjm7uNb+23VbiC8HKNYycw2MmAs3gSjUBhAAoTafGGdv7bj95evGrXrDnLJCRlhYXF+KZNmyklGRYSHBIUNHeWjKSY0JyZkicOHY6MjCZSmFgcGZrXRqQTcFQKCeg6Au5lh3MxjvE2Tw11ac9L7MhP4aTQqxiIIox/bpRbsr8l200vxkWL7aRKtVKI1r4drHotUP267b2jbnIn493VC+H2JWi3IqJfLD6cSiKSyWQiEUsgYnFEEpvOJkQjTxy7KCWzSOulTkJi/NLFi/+YvwLc7ubiIjB9uiAfv6igoBg/n6iggJSUxLn9O0mupqlBtmxPY9YrI1f5K9a3zwarK0RqvCz3T+vH9vaxu0c6hocKBnPdyT4vrgYayGPDAqgMBpUdT2MmkKgxGDwVGpHBiPWNhJ9+dHfp3o3zty7ffHSXsb1FEDzKwNr8geIjBQ0FC3tjV3drbU25C6e2HT24Ytf+hSu3Sy1cL7J625xL1w+bWWi4u1u5OFuYm2j7ejl/PwqDF+iW5ub6urrPaW/6V1N4LGIyOa5Rhf9dte6bxsjIyCA3Jvh3sW7ufOKaiV2e4B/HyF8jg8O9A0PdIMHO/xDBvBitFYYczMu3NAz4+7caLm5mlbfHlrfHgSxri63oiC9pYStp3d+wdTU1JaykJYZH4U9oeIzCICEK/0irwrwA2O3sawTnMPXtmXVtqfXtaY0dOa3d5T0Drd9/OR8c/+DoTUlMLC4qmuAEieampoqyMk5j4+hNXy86cnuLTd+h8D9y8OslYR6FG8wbrM/Y/Przr97XvHvsehrNGuqMa6sMK6qhLD+96hTfZL543qqwXlGzSS3zKVF4uvCNjVfLtHNaTKpr9YsY8ljdQ2qzBGW2zNmY/oKFvhcGCHtj3UW6HBJzNxh9Jwh3L4T+GJ6nwq7hlkbUvEyt0UoFFLY7oS/GJ2p57GUL94Ia3Qb5wZfsf/n5F+6qcJ7dn9rg+5gfVmXeD8Xc9ARJvO0d9zCsSoWW+RQJKLxrzrpmjZhGNWa9Cq1FndWhGZN0PxBQ+NjC7YDCDYDCSrgeDabxnsfTfp8aftYo4Y7HH+Lz/1y4tVohqkUJ3aGMK5ELXiUx/+j8jQ0KEZl3X0lMFzo4dy3poinpogn+vCHhvBHzsnnuHdc2hRDCOX1A4adrjjY98W96u0R41MHfsFWOWxrBdTDn0ouG0/JNCobDHV+50fkrUjg1LdXUwsLAxMLEwsbCxtbD1cnXSs/o9nHVU1vsnl0vy05NpyJVrx6QFeX//ddJc2Tnnj952tvWJh2PTInyRzvqeWrdtZU/a/HghJviFYS5oqv6/eWzJcCRAELjvHpHeNMAcWioYuSvgb9i/CPNbu03urXb5P6ffpY6NDw+JjaNyojnVuLSAYjJ9FhAYV1Lny0HbyxeuXPWnCUSEjOFhERn8PEJ8E2zt7Fh0GjLFi8QE+afLSOxe8uWoIBgEoUBKIwHXiTSiAQ6hUTH41Ahrxz8TdRy8SFVTGxbblJHQWpTGqOaiSjE+GeFO8d5mVDs1Kg2SjRbRZAEU3mM0RO0sRzc6BHFVrEG61JN8izFuufg/Ji4KCIBTyQSCAQsHo/G4glMZlygT9CaNdvnzFsRGhYWGRFxbe9ljCm8u74zJpq5Z+m2aZMmTZs8efqkSUL802fLSClduxQX4JYS4hLnb0/1svHWUQq10E9FRhVRY7qyW4cqBodbB4eaBwcofQ2BxSF6ilGuRkQshsRgE+ksCj2GQGahcVToJIHOdvQNPnH/7vo/92w9ue+hmkIIPBxBQLv6eVgA4DqZWDuaur2ytrXSvnp+95GDK05f2Lz35KqV22WXb5q98+BqJfW79k6GDo4mVhZ6Ab7u34/C/f39iXFxeAymtOTvB87/36BwZnGrF7LkB6QweLcDJyRNHA54txtNzuscv/93CT77nVt4ybv99bf+aL7+nIb29rZ/vJQFzq/A2396SkpX54/lpH91AOTxKFzCAQh+reEvaaED20IOswB8AjfBh3V9KYEI+znzZ/pH25W1xQH+jqfwpzWcUc2l8I9RK/xODA739Q129A22g+wf6vz+i8G8AE/qzPT0kIAAaILEBK6OAZ6JWRkZqOjo1KSkCZ5evh9dhX2lVg3FRm9R+B0Hv6bwZzkYZKVRFY/CAKBul9yaLZoqDSoqX19gmWNaJ7/j8c8//+x71bPdnNNm1tBh3uh1yRWgRO+wZoNhWbl2DseorNequdmo/NnORwDNmHvhCc9IYOfq2nNthqW9FnW95jXdJuV12lm8EuFqroNBNulmR1xzF50ucnv9xTa9nG7DglbdTN39z8E3Nzr4oscwP/ACdA08u2MvewwLeowKegzzunTS6tXYHI3YSmXqGqmlC0Rks+XhPdpJXVrxOU9gaY/CShSQS8XmLReblysX3qPJ6tFgcF7gL684KDCZL+72q1L5iB2zVi0Qnpn30L9HndqtSqRdtRGbJnhs/qY6+fCih76rxOdvm7m87KFvjzqhWxXTrYxsVQhtfBLQ9jQEf05vLrdAokkOovA/bZV74+AvapXjLglDDuZcUmw887RF1WK4+yv//W30Ehu5uRO/xEZScpKWvr6qjp62kYm+qZmZpZm1nqrRnVPKp7YaXD+USISVZ6c8v3yUb9LPP/30y+RJUyTFJY/u22ejowVzsUE4GsKt1BEWylEmiigLVZLjS+Mnl2XFBH765adFc5cw7KkD7OGBZO7CZ8lIgVes3cPTxnf2u+vI46PCGMw47sInG0ek4whUHJGKJdAJlDgtE4+N+64uWrkDUFgcUFhQVHDGjEm//Kyprpabk7Nh7Sphgemy0hIbV6/y8vAi05gYLAm69gWRSiAzSGQmgURGhof4WhtmUREt2ckd+elteSlNacxKFjwf7ZsWbM92MyBaKRHN5el2SmwndYadOt1GnWWvnu5rWBBuxSH7VJG8SnCvMjC+VFw0gYCHaiOIJAIeg8cTaMx4E1MbmVmL1m/czmAx4f4wlil1gDE40v1Xb0pXmIr/ivlLZGSkVy1fsX/3zhtXLsO8PSuT42qzU+sLs2sLckqz0mqLC9rqaroamgab+oebBkeGRgbK+vrh3ZzQMqSjJTrMA4fF4MkMLJA9VC5MR+PpRCoLQ6JZefqdffzo8NWzLww1vMIDYTiEi7ezpbOZi6+tk4elsflLe1sjT3fL5/KXzp/eeuvu0WuPTmw7smbBGpkNe5Y/UrqipvtYSf2BnoFKgL/796sVhiZIEAjB/v7/ByZIfGakFwIKF/+AFG5vb4+IiLC2tnF0dHZ0dPrn6fD2Pu9D3s5np52tvYODQ+M/XYKqqqgIDw7+KhMkBgcHh77HUOF/QUAFEk0JvAkSH9PwWKXEBylc3MQqb4/nrgpDa8PAvmBr76knKi7sFW4FbuQh+IMahnbe1nBmNfGHpfAPEuAAzsnKgoWHx7HZn9OP8bEAFE5OTAQv1N9igkRPRX+5Y2OR4Ycc/JrCX+TgcsNKHoWtTlsDdLpcdG624FRwEQylfmmjSS1AsMh0kd0LdlKeYAu1MomPkVvnbpIWkMY+iKrUzXc7bxd83TtXLSH9BfvG+kvifGKkh7CylxmHl+wDX+Vw0iRXhZWnwo6+6aeySy7+KbZBN4u3JFyjldKok5GtSF4ptVRWUAZ107tYheF51mKp+ALwkxgeUOrSz06Ui14mvuAPicWRl52KlMg5z7Bep4119jwpVSK1aMY92XR58q+/q2y7lfcUkfIo9MG6c1o77jUoUx6tPzflt8nKW69lPw4pfBJhf/gFNFht4dayp5HtyoQn6878+vMvxrvvlcgFU69YX1i6G/xzxxdsrpILblKIfL7+NPiempsvZt1xLXngRb9kprv1Mu6sXqdCKP6c7uwZYk9WHwYObpLz/RqtcuNLhD/o4DetctCSMORgJc7F5xCFDZ2HeyfU3fHBAEfsVzlo4xMSVXV0FXV0VHR11fT0NfT0NJ7L6d89p3Rqi+qpzdRgj64GTpiR8+aF637/fcovv/72229TZkznX7106Y1Tx00U7gcbqZBdDGivTFle1rGBDq7GGvdv3VBWUQ72C2ktaB5uGBnuHBpqHhigDTQEljjIXbGSPw33dmLSWBR6PInGxpPoKBwFhSVGI3EwBA5LYKkZuK7fc2XRH9shCovPFBQUEZzBP+mXn+7evlWQn3/4wD5hQT5ZafGVy5ba29pTqEwMhoRBE7BECoFCJ1JiAKyJWFywhwsLHdGUm95VlN2Wm9KYwiijw3Lgnsn+VmxXPZqdCsX6Od5cPlrvQbTO4yiNB2FKV3D699l2yimv9HIi7YqxbikYfxIORSASySQamUgl4bFkEjEKjr987f40fpFLV65lZmZSPUldYS0D7N7hjqGh8pHWhKZg70B3d/fwsHASkZCSlMipqeluaelqaeloamqtreuqb+isrMylUTJQqGZa6QCrd6hxcLCwvy+qsyIsgxTqh0GGIxBwHJ6KIdDxFDqOREUTaHgKC44nOfkH3VVWua/83CM0wB8ebuNh90Dpzrlbxx+/uK5j9FxXX9HKXNvrlZWe9pPrl/ffvPvn1Udntx7ZtHj9/DM3jshr3b5w99C+Pzfcl7vs6+f2mYtxX4HC/ycnSHw6flgK19fXOzm7+gREECixWCILSgJIJpR4FhYPtkwsjokbn9hxiWHgoWTi0UzuDoOAZhLQDAIK7EBbPHI0CSARY0knwEEyoIwe3SfCmbhImpaadlFx4egP94Xx750g8SMHROHmBMBcHn9faxgqkwD5loa5O+9ouJDDjMuBoxm+UDJ90ExfJM3Lycdww9bVouJCKIb3GIVBjtfw2MLwOxoGFAa3/GgFEj9a/PgTJPobByu9mooMankO/tLSiPcdXMGlcCM0V/gNhcccXKlfUmNYlqeV8WDrHcFpQptmbzix4s8NsuvE+ETV9ilW6xWWvMy8t+nmcsmlR5ce3L9wt7SA1NW15/PVEjj6hf6XXeaLzJknPPvIkv0nlh1aI/PHhpmrY54gm/RyeA4GWf8ytUE7Te+AksAU/lVSS/9csnfTrDVbZNcITxPUP/C8VSetXjPe+OALcT6RFRKLTizdd3TRriVi804u2Qso3KYVT77puVZqieg0oUMLtu2bt0lWQEpzx90OdSb5ustGmWUSfCKH5m/+c9G2eYIyC4VnRZ4zan6BbVfGYy6aLxWZLcMvdmzB5p2yKzdLL50nKH1y4ZbyxwEditG0SxYbJRdL84nsn736zMItW2WWrhSdE3VCvft5GGE8haHqiDEHe4xrlRtXGvH3rXLjS4T/plWOuySsxLmkyLnwvPHs0zZH/5GBr7/u8LUoHBOfoKil/VhT/amm2jMtzRc6uvJyD9Rvn1M8s9n06umCaPZI918tpAacNmrfqr2//Po733RBESGxmTKzt2zY8uLBfYSncyELV5MRU5+X3lxeWJSblZaSnJ2dnZ+b31HVPlw/Mtw7PFjR348dKPPJ0rt1Wv/pJWR4MIMRT6LEkqgsAomBwpJROCICQ0CiiTgCS0XPae3ui4tWbJOdvUhcXEZAQESAn3/q778dPXQwLTX1zu0bYsIzZkmJLV04z1BPn0phorFEJAoHvgOezCBS2DgCjYAnhQb605CRnJy0zsLMttzkukRaCSUiM8ol0deM7arLcNSg2CiGad6wvLbf8PQus1O7rY9vfXVpT6jcGZjqzcRXesVY1wRcEJ6AJ5CoFAqdAkCMRwMKv9Q1W7V+15RpgkamxlnpmWiz6H5YV19s13D38EgXyL/qKusS4+NJJAIMFkHH4epy86szMktiEnJJ1Cw8qZQdX05jUhzskIZ6eZ64DjhnqHpgsKi/Dd6QBWeRMXAEMjoagcTgKFgAeioThScjcCQchYEkkAMjYUY29kb2jnb+/qqWBtcUr+09u/2PbYtW71x85tp+Va0HluYaLnbGloaqd64fuXLj6Nk7Z9bs37R+9/onKrduKp7ed27Dio2yR07s8PRy+szD5itQGPxL/4cnSHwwflwK19UFBIWm51Z0DvzV0j3SCrJrNNs6oWztGGltH2l7ne1tULaBbB1p52ZHy2h2Nr/OppFODnfbNNLRONLJza4GbtaPZmfdSFfdSDfI2tHsAbdUjzhYuxQUFoz+cF8Y/+oJEj9s8Cj8hr/jF4bHU/gjGi5vjzO2UVm5ZsnKtUtXr18OEuzIzpWeNOn3i9ePp5VheRMkPq3hMQqD5FKY+h+Fv098uwkSQ13DtWEtRfoQgr/UwRCF33MwRGHDqnqz+uBbQTsX7Iy8G95oWselMORgbhbXG1UmKLGU9z7fOmfTSuk/di/YYXxMJ1M1vsGwrFI3L+ia15/LjqyduWqj7Lp7m24wnqDrdPPqdHNLNVN8Ljme/uPoupkrN85afeaPY6/OWpapJ9Rrp7+mcDLIhpdp2YokzT1PNsxatWHmH8o7HoRddry86rjnWbOml8lNWokFikSrI2oHF2xfK71s66zVt9eeib7sAF1dWZ1Vr0qPuGB1eum+1ZKLd85eo7f7UerDoGZVasMLAvyi5dU/Dq2XWrJOavHFZftCTuvVPINzFFGNiojaZzC/4xoH565fLb7g8rI9wSe0NLZcVt10oUousFkhokE+BHvO8MbyfZukFm+QXHRs3nqb3XcK77q2Pw1OuGJ+YfFW2923mx77ch57v3HwB0uEP3ZVubccPL5EeFyr3LUPtMrxSiOgPP8MULgjDDN6THy9+IoTJFixcU/UNO68ePFIVVlBU0PV0Ojhk8d3zh5Uu7gbrW3fQ2wfaR0ZzBnqJ/SFGgadPnHm9u3b2i91XN1eRcNRcTGxRXk5NRWl5UX5uempCTHsyNBQE2Pju/cenDhxJtQsbIDRDy0MV4/0o3vC1D2Pb1l+48y+4AB/GjWWSGIDChMpTAyeiiNR8WQ62MeT2C90HVbvPL9g6WbZ2QsBhQUhCs/gmzJ53aqVbDZLV+elpJjwTEmxhXNnqSgpUSgMDI4ER+GBpCE+khl4AgWDxUeFh8eRcC35Ga25yW25SbXx5EJCSFqYQ7y3EdNZm2KnjLN4CtO94//8kuPNE/bnDnldPIx8dJGoep9uqpwTbJ2H84ohwggkMolCp1JpJCKORibA4aijxy/MnLNUeuacyKiIqPAIl6cO/cje/sRuaOW7fHAwY8Bez/bIoQN79+zYs3OLyQuldAQmE4lNg6EyQCKwBVR2GY1FcXSEG+onewdXoFN6azv6yjsqydnxeBIei46CwxFoHI4IfiFQXQQcQ4hCYrAkKppIiUZhA0Ij3YMitGwdbqo+PSd3bt/5XTuObd20f83OYxseKFwyM1O3NNIw01aSu3vqybObchoKW/7cvX7v2muPT+46s3rj4eVrty/ZsW+92yu70Qf+7+IrUPiL4j8Kf9Oor68LCAxJyihq6hypbxnkZQPI5gGQjc2DjU2DDZwBkI0gG19nw5vk1IPsB9lUN9BUB7ZQNtf2N9W8zurRbK7qb64E2QdtK/p42QKyHMrWioHm0j57K6fCon9I4a8V/7sJEj9icCkcD+xbwmF9QMMfKhoe0zBIQGEDS6Uly+cvXbFg2R8LQS79Y8H6zavuPblEjA8q5DB5F5wbE/D4/Q9q+D8Kf04M//ATJEBwCO2FejXvO/g1hT/uYKPxDq7iOZiXVYZVmZqZhCe4bK3MasMKroN5FC4GWcXVcIFmOlkOjbwfQZfHlWnnNBiWAgfX6OVX6uTGPyNj70cQHkRlqrAb9AuqdbKrtTMb9HLrdLMTnxFwd0NAJj8j1GtngOStB/McXKOZBJKjnVakTCfdCyLcCSh8Qa1Rj2M+CMt6jm/QiK9Tj23SSqpWY8Y9CMPe8CDf9s5VQDdpxNSrMWpVaI1qjEZVevLDEPw1Z8Ytj4rnmGZVWt0LPEeZ2KRMzHwYSLpqT7him3Hfr+kFtkkJU/8cXv88ukkRWf8MFnfDiXDRPP2Oe9OzqPTbbsk3HevlQ4CDm56GtiqEZ99xpVw0IZ43SLpm0yDn1ywfyJHzq3zwKuaSSdYN28Z3W+XGlwh/qFXu7RLhca1y40qEr40vjfhAq9zokvDF55xzT0H2sJJHD4ivF1+xbY4RE/NYVfWWEqCwqoKWppqpyWOlZ4d2rVc9f7jwVWw/tneoemC4dXC4ari1pCU5ITkhMSEJRGJSbFwCDI6xtHF8rqh89+atq2fOnti3b82KFSKiEj/9Mvmnn37Zv+oIzYLaV9bTV95LtSAdWLNTWmjK4b1b/f0CyNRYHJFBpDIIZAaGQMO+xh+OxHqh47Bqx/l5SzfPkl0kBq0KCwMKz5g2df7s2VgM2tXFaZa0hIyE6NxZ0vdu3yISKVg8BTgYJBq6TgcVyBiBwsJhsBQmraUopzEjtjUrriYWn4cNSA62Zb/So9irYy0UMObyePOnVGtlorFimML1wFvH4XLnydpyeQGW5XiPVHwQg0okkWgUMoNCoZLIRBKFam3ntmLVFmExma3bd4BfwtNnT2/uv94R0TyYPjzSOjyYMFzqmb9z+ZZffvlJYMbUZfPneBgZpyOw6UhsKhybiSLkkuhFrKSK2CSapxfCwiolLCqfSO9qaO6oaSyIS4ihUHA4fAQ4vcDgSRQGnszAkRhwNDEchoKj8SgCGY0nI7BEn3C4uoW1orGehqX2Y7XH8mpPn718fvn+hWv3zpuYapoZqukqP1R7es3WzsjOw+nYtVPLdized3HT6n3zlm6Zu2nv2r2Ht7u9sh994P8uvgKFwQt0E4dTW1PT+RntTf9R+JtGUxMnKDgsJaukpXuE0z6aTSDbRjitw01tw82tI81gpwXKZpDNQ83NYDvc3DSaLSA5Q7xsbRxubQRbbjYMtda/zrrRbKsdaqsZ5CZ3p3qwtWoAOLi5rLe1vL+ptHciFAbH1UB//8DAwAT/LsZbXZ7gH5f/zwSgcAV3VRhQeEzDn99CV8RhxOVEo+g+3PQGiaR7Y9l+ycWY0paYggZ63mv7QvnewvCYhqEdLoWz/qPw3wU4/qurqhLj44sKCiY4QQK8UJeWlDQ2NEzwOfXBaEvuKjGrLTb+Ugd/eEkYSoOKCoPyaqMqjml9jVHlOw6GUq+oSq+ozrCUY1zZZFzFMSqv0S8CDq6CMrdGN7/RoLjZsLTZsARysDbkYG5m1OpkcfTyWvQLWvTzOXo5dS/H1oO5FOY6uEYzsVYzkfMypVU3o1Ung6OVXK+Z0KKdytFKBA6uU4+pU2c3asS2aCW2vUxq00rgObgOkFeFCrJBldqizmzXYLWpMzkqlLoXhDolXL0SrvEFrlWF2KFK7lQltSrjGxVRXAfD659FAwdznkd3KCE7X6DaFOEchcjW57C251EN8qEN8sEgG+WDWxVCO5+Fdz4La38aMjZCmCPn2yEf2AJVCXs2fNjBIL/mVeXGtcq9djBUKPyEc1t1oLh89ID4etHe3k4hEOBRURVlZaM3/dOgx7Dl1FTvKqvLqWkqaGoqG+sraGts2bb22t4dFb5Z/ajegcK+kcERoL3esu7IILi6hva9Ow+OHDy2ccOO5Ss2zp67XFR8tqCQpBCfsMBUQFa+XyZN+3US/5TJgvzTxA5sPJxIS4ynJexcs1tAQHDt6j+eK8hHRCHwJBaWSMOTaTgiDY2jRKPwcDQBaBhQWFnPcfXO8/OXbpEBFBbjUpiPX3D6NAkRYT8fn6ioiAVzZaXFRWZJiZ89dRKDweEIEIUhL+LIGC6L4ShcNCw6kUFtK82vT2HWJ1IqWagclE9SoDXd5SXBRgVrroC3eka2VSLbKpOslVAGD6JULiM1r7EdlUtRDgV4jwQijEljkYkMMolOJtOoTLZfcNSVG3Jz5q+cxid479797OzsLds2SwtL+Sl5tzEBFIbqMFVGV7VniosJCE2XkhQ9sXcPPSwyh0jLwFHScdQUFDGTxCxgp5XFp8eHRRHdvbOwlNLY5O7m1taa+tz4ZDadiSWSI6IRSDSWTIWqhMEZAhILbsFEIrAIcNcI1GgMwSc0ytzVw97Xzw8W5hHk+yrAzz8q3NTR6qnqU0sbY3trA83nd9UVrvt4OvoG+V15eH3prqVbz6w7dHXb2j3LthxYf/T0PicXa/DKN/rYfzK+AoX7+/vjY2OxKFRJcfHoTR+P/yj81QO8O/ZyA1CvsqLcxy8gJimnuqmvor4LyrquSl7WguysgrKrsqazshrKKpBVUFZzt1WVUFaDrOgAWVPRWVPewctasC3rqClt5247akraa0o6akEWg2yvLRrNhpKupvLe5vKvQOHmpqbEuLjUpKTPOcX6Lz4zxlEYCPi1ht8pGv5EC109raSFXd2dWN+fys0UsG0YSAWoDUI4JhWiytvjCjmMMQ3z+PsJDf9H4b8NwFbeBAkmjTbBCRLg+yCiolISE78FhXur+yucGooNasZT+J872BByMDfftMq95+DCN6lbUKmbP+bgKh2QObzkLgZncRNyMJQvQabXQDk6Pe09BycBB9dqJkCpATKem3F1GgDBPAfH1Kmx69RY9VAy61QBgkHS61R4FKbUKZPrlEn1IF8QIQe/wAMH1yth65Uw9YqYBkV0A4RgZP1zxGsHQxRuUIgE2Qi2TyManoY1PAUIBhkCUfhJUMOTwMYngZwnARw5/0ZofjBw8CevpvFlrXJfcFW5N61yvNIIQOELzxpPPWrRtfsWl5rr7+srLy0tzM9vb5voN6fHsORUVR+80Hyi+lJBU0PRQEfZ2HDvsUObli5mW2D7sUODuX3DrYODScOZPukHtp0SEpEVnCE+ZdL0X3+ZNmWKsLDQTDGxuSLicwUFpWbwi07nF5o0TWDadCG+qUKTfucXEZGKCIsMDQ3nFxBes3aDjo5OUFAIBkfFEhjAdngyFVAYhaVEIbDRCDzYJ1DYGsYua3ddXLBsq8yshTwKz5jOJzB9GsC1taUFk8lYtnShpJiQjITwkYP74NEIHJGKxBIQGDwaDyhMhlSNwsIgCtM7ygvrUxjlTGQJJTwL5pbgZ0Z10iTaKpPtlGmO6gwXLbqLFs1Zg+6iHu+lmxthVUl0ryJ7ZeD8Yyl4BpUFHEwi0ak0JobAUFI33rTtKHC/gICovZ09g8GYNVPmp59+Wi67zEXPqaG8wVzFeKHELFHB6WLigrIykqoPHmWRmbnU2EwiK4can0FgZlFi8mLTi+My0rGUeBimgJFYlZbX09rZXAUonMqgsVA4UjgMAYuGo7F4FJYIfj9wNDESjgNbNJ4K7l0EHB0QDg+EoSNxFASJiiJS4HgymsIIRqLsX7m5ebq4u1irPb8rd+u0vY2xl6+ngobSxmM7dl/e80jrzv5zO/ee2H7q4kF9Y83P/GPaV6AweHUGZ2z/TZD4n0RfXx84TAkEAhmczdFoMBjM2NTCNwSBJicgCbEgUWOJj0WDxMWARGFjUBg2SDQmBg22aDYGDW3RKG4iWVAioMSAhINkYuEsbDQLA2PyEh3FQEcyMLyMYGDCoS0qlEZDJ1XltbRW9rdWTJTClRXQBAl4ZOQEJ0iAZwI4W5v46vL/jeBRuGSUwpCGx/gL5cc0zN3haRiaINEWm19Py6kmZ3OzsJERjnfdfWBLAMwO7OfWUt4pkwA5fp9HYZ6G/6Pw3wY4bsFLa3RERHxMzAQnSAAEA1LHsdnf4rkw0j9SG9hUrAcE/M9b5cY5mEfhdxz8msLc9eDXDgYIHu/gvHEOzobyQw7mjk57z8FaH3NwHDd5Dh5dEgYOrgMI/riDoXztYO6S8KiD6xXR9R92cBSXwgDBH3Vwo1wAhGAo/T7jqnJf1Co35uDPaJW79LpV7rWDORcUGk8+7AiEf8frkf+TYMSy5VRU7z1Tk1fVegYorPtSzdT0wt07M2fNVD//vDW0fihveLhlpBvdbvvAQUZiCf8MaQEBSQFBUQEhMf4ZEqLCsyTE54mKzRHgF58+TXDy1BnT+UWEhST4pglMmjRNQmJmaFh4YGCI7KwFjx7KRwO54ok4AjQjDEOg4EjQADVAYTiKgMaQoXJhKlvX4tWG3RfnL9ksM3OhqCj4t4Rm8PEJTp82+ddf1ZSV09PTNm1cKyYyQ0pMcM+OrWGhYXiAQhwRaBhQEo0nRaNwMCQmKgqWFhfTVVncmMYsoUZko7wTA61ortp4mxc4q+dUR1Wm28s4b8NEX9MEH5MkP9OsMPsasn9zTGgpKSiRgGDR6BQqjUCmkmksMi3GxNJtx96zi5Zvlp65cMH8pUQCydHBgX8632+//gY0fPLon1kZmedPn57y20/iwjMkJIRXL13ia+VQEpOaR0/MoSXm0pNzqfHZtPjcmPTC2IwCRgJwcFlidk1WUW97V3NVXX5iBp3ChCGxYVHwsPCIsAgYDIHFEqjgDCEsCgWtdhNoKDwpAoEJh2MxFDaeHo8mMdFEBobMwtDi4CRmBAoXEBrq7u6oqfzk7pUTJoYar7zcDCzNjl2/dOHJTRVjlf1n9p68+ue1+2dVNJ9+5l/SvgKFATIy09PZDEZ1ZeXoTR+P/yj8daO5udnUzNzbLyQoLDokHP7K009Lx8jhVah/JNU3jOgbSvR7nf4hxIAQaOsfRIAyECTePwDKAJB+r9MXF+ADZaA3LtALy01coCd2ND0wga8wge7cdMMEuqKDoMQEuaCDXTAeluHuVkG5SRVQpURFf/PEKFxbU4NBIIg43ARbJYCkY1ms9JSU/yZIgBhrmyuFloTf0/BYmcRHrkLHS1yMn6G1sqrOI3W9x2p6j7WM5C/dPME/Y/rZy0fBLSEop4JGxmdqOKuG9B+F/zbAU6CstLShvn5CEyRGRsrLyhLj4kqKir7RaWEzo6PEqKbY8O8dDFH48x08rlWO6+DiTzkYWhIe72DekvA7Dn5zNY1xrXLjHcyl8D9xMPXjDsZ+wMHPP+jgcIjC7zv4yccc/I+vKgco/P7IiC9plYMcDFKh8awc56ZyT0L66KHwtQOcyA0PDU38uGXFx8mrqt95+uKJisbzlxpKOtrqxqZyahpzli2fLTHX7IZRGam0v6kv1DBs68oDohJzJSTmCovNEhCWEhKT5hOQmCEoAxwsJiorNEN8+nThaXyiYuKyEuKz+KcJ/D5pqpS0bFhYRIB/0Mrla81NLckkMgqNRWGAWalYIgVLpKKx0KowGkvC4akEiMIsQ2uPjbsvzF+8EVBYTFRKYIbgDL7pgMJTf/v9ysVLWVmZp08dFxKYLiEisHHtam8vbxIVwJqExAINE4GGo1FYGAIdGRmZm5rUU13KSWcBCqdGOtPddZFmCsFat4K0bkQbPyTaKcV46sV56cd56MS90k7xNS6EO1cRfXMJIWxoLBsDTyHhyUDCCX4h0X+eujl3/joZ2cVCItJ7dh/Mzsq5fevW77/+Nm3y1N9+/nnPrh1paanXLl3imzJJXGSGjITImf2HYmD4svisAnZaITu9gJ2eT0/KARRmpxXFZZTFZ1QkZlck59ZkQhRuqaorTMxgEOmRUchwGDw0PCI4NDwsEoHAEEMjESER0Wg8GWg4GkOAoYkoAo3ESCDQ4lAEOgJHw5DZBEYiksiG42jgBMAvwM/E8OWT+1eszPU8vT0tHBwfKKmoGOqpGmruOXHw+qMbcop3tQ1Uvh+FwdHZ09PT2dn5OdT4j8JfN+rqao1NLZIzS3KL6/NLGthxGXZOntGEpPhsTmx6XUxqLcjYtNq41Nq4lNr4lNoEsE2uiU+qSQCZWBMfXw0yIb46Ma46MbZqNNlViazKJF4yK5MYlcl0XlYk0yqTqRXJlPJkckUKSFJ5MrE8hZvppCpyaOorq9DMuBJA4RZA4bIJUbinu7u6qgqAeIKE/W+CxPjgrgon8ij8vobB/gc1DFUJczVc1hJDigvcd3i7mLiwlIy4tIw4tJ0pISIm9NtvvwqLCgqJCKhoPyriMPMbaHmvKcwT8Pj9MQ3/R+HvGQMDA709PWA7+vHXjt7q/nLb+mL96i8sjRjv4NESYeDg1xR+t0T4Qw5+vzSC6+DR0ojREuFPOni0NOI9B/Mo/L6DuRQec/DrEmHukvCYg6GrK/NKhN9yMERhnoNHS4RfO5hXF/G2g+WBg4P+xsFf/6pyn90qN7ok/LTx5IMWQ4dvUR0Bore3tzA/PysjY4KDNUGwE+Ll1TRvP1V6rKLy/KX6C11ddSNTNSPj7YeO/Dxpiii/mJaCVnF+ydnjVwSE50jKLJKUWSAmOVdQZBZUIiwiwy8gJSQ6S1xstqiIrICQjKj4vFmzloqJzuKbJjhlMt9MmblhYZH+foFbN+9wcXYj4IlR0TAYAoUhAAdTsHgqCk1BAgrjSHg8t16CTDe29dq858ICiMILREUlRynMN33675O2b96SmZHx7Jm8AP80cWGBFUsW2tvaUegsgEUkjgQczKUwDoZEw6NhpTkZfbVlzRkx5fSojCgnqrMWzOChz4uLLk/+fPXseKDGZZjRfbjxQ4TRPaTRXZyFPOvVy9Qo5yRiJJ1OI9KYOBIIajSaJKesu3z1DmnpBcLCktP5BB88eJybk7t18+bff/mVf+q0Sb/8snPb1szMjPu37/BNmSwmIrBARkb7iWJZXFZ5Ym5ZYl55Ql5JXHYhMyWXlpDLSi2KTSuLSyuLzyyNz6xOz+9t72ypqC1NzIgj0mER0WGRUWGRkWGRsNBIeBQCExgWyaMwuFNhcFQ0hgwQTGUlEmgxKCItCktCkhhkdjKGHIvA0ZE4CtCwk5O94tO7TvYW/kHBDq+89M2tLR2d1PV1zty4pqStpqmvbO1gMjT0vSj8RfEfhb9u1NXW2tg5VjX2tnSNtPb8lVdc5+ETQokvLKwbya8azK8czCsfyK8YKABZDmVRGZSFpf2FJf1FJQNFxf2Fhb1Fhb3FhX3FBb0l+SD7SvJ6S3J7S7lZkt0DsjSrpyyztzwTbLmZ3lOW1l0OMrWnLKW7PBnK6tT+VEKFn2N0ZnxpWw1E4aaJUfhrRU1VVXREBIVA+P4TJIZHhvoGezr7Wtt7m7r62waG/vcXXh6lMJe/4zQMtu8UDXM1/E4LXSOjqjPB0FJp+vSpW3asvXTj+Pmrx0BeunFi94EtMwT4t+/ZePbyEVd/E4jC9e9SeEzDvH2ehgGFwS19/1H4kzE8PAz8+uP3fY4MjNRFNhfrVZcafb6D36Ewz8FjS8LvOnh8ifDfOphbGvHawW8ozC2N+JCDX1OYVxrxQQePlgiPWxIGDn6vNGLUwYQ6pfElwmMORn7YwdCSMNfBUGkEz8HQkvCHHAxR+DOuKje+RPgDIyMm3io3WhpxVo5z/kkXijx6HHztaG9rI+FwsPDwiU+QYMXHP9XQuvNM6YGyksJLNRV9fXUjY21zi3M378wQhy6hvHPH3uTk1OOnLguKzpeZ/Ye07DJJmSUiInMlxedKSc4VEJICIBYVmyMqPldMYsGs2X/Iyi4XFpaZPk2If5rw7FmLw8Oj/Pz8D+w/5O3lg0ZjI6IioqLhaDwJQyDjcFQ0hgrNFcYS8DgKBk9C4oim9t47DlxZuHjjzFEKC0AFEoDCkyYvXbgwMSHB1NREcAa0Krxo3mwTIyMqgw2wCBJLpGLxFCQaD4OjsGhUbXF+f01pcwa7ggnLQbgz3LQRxnJRendDX173Vjr56ukxr+enA9UvR+reghneh5k+RTvrMqK8WdB1jmOIVDaRQKZRGcbmjlv2npq7cJWoiOS0KXwzBIRcXF1xWJy0pOTUSZNmTJsOKLx5/brsrKynjx9PnzJJTFRg26o1MDe/+oySytTC6vTi6tSi8oTcIlZKHi0uj5FYwEwsZiUUs5PBLVWp2T3tnc1lVRWJ6en0mOiIqIDg4JDwiIhoRDgMGRaF8AsO8wc6RmHC4ciQKAQMTcJRYoCD8VQWikgNR2FgAOzMBDwtHoVnRqPB+QDZy9dH5cVjJ0ebsCi4d3CYm5evs/srHWOT51qaZo5WZraGHr5On/my+RUoDF6m/5sg8b+Kurpaa1uHstrOhrZhTsdIVn6Vu1cQKTY/r3oou7w3B2Rpb3ZJT05pTy7Ikp684p58kEU9eYXdeYU9+YU9efndeXld+XldBSBzOgtzuqDM7irM6iwCmdlZmNFZmN5RlN5ZnAayA8rUzuKUjuLk9uLkjpKkjpLE9uKEdqDhJFypryOMS+GhlsqJUhh6+/83T5Do6m+vai3Ir0/MrY3Jq2MXNMSVNaU2dpb+b6+sNlor/Jq/4xeGSxr/RsMgqzsT7z6+OG+hLJzskZiPZGdGgkwsQDl6G8jISlo6ayUWINPKcAXc69IBCv+thrkUpv2AFB75a2QQHICDveBkZmCob3jkn49umGCA4/9fMUGCF505PaVmtSUG/9jB40sjxhw8VhrxxsHcJeHxDh4rER7v4M8rEf6Agz+zNOKTDn6rVQ44+EMlwgqAwmMOBvnBEuExB49vlfuggz/eKnf3H11V7qOtcm8czLkg33jifrOS0WBN/ehB8LXjK06QYMUnyGto3X2u+OCFkoKmOo/COhaW9xWVZy/547dJ/CtXrAcUlpNXFZFYOmvuullz18jM+kNcfKGk+PxZMgtFRWWFhGeKiM4WE58rIblwpsxSKan5QoKSfNOEhAQk589dFhkB8/MLOHHyVFBQMBqNiYyCLIxE4dBYEgbwF0vB4EhINBaDIaDR+GgU1srJb++x29xVYfDNuRTmny7IN41/ypTZMjIEPN7Dw0NcTFhSTHDOLEkNNWUqjYHGUzAEKoFEh+qGMYSoKAQJh2+uKO6pKmxKZ1WyEPkY3xgvY4zFc5zlc4aTKtb8SaT2dYT+HbarWk64RQ7MKT7aixodQsEiKFQahRZPJrPJFAoSg796Q2HugrWSMnNn8Av++vNvc+bMZTKZZmamv//2K//0qTO4FcxrVqzIzc558fz51Mm/SkuI3Dl3MYeWUJ9RVp1RVptZVp9eUpmYW8RMLqCw8qmsAiqriMYupscU0WIqktJ72js4xaWV8ckFsQnIqEhPXx//4JCQiMjA0HC/oBAvv0DvwOAg8BtDoKOQOBSeiiWzMEQGikCDYUkRKFwUloAkMtCkmGgsIwxOiMZSgiMitLWVnRxsI+GoEBg8MDTilbeflb2Tpb290ytnawczv0CP70fh/yZI/A+jtaXFzsG5tmWwre+vzoG/CssaPX3D6MllJc0jhXVQFtWNFNeMFFVD2xKQ1SMlVSOlICtHSiugLCsfLisbLi0dKisdKi8ZLi8equBl0VBF4VBlAZQV+YOVeYNVuUNVuYNVOdzMHqzKGqjOHKhM73tDYXypr1N0ZnxJW+1QS1U/p6zXbgIU/vdOkACKau6qy62LS67AJ5ahk8rRKZXYtCpcRjUht45e0ZzW0dc0+qnfPcba5kqb2O9rGLLvWxrmfjiuUqKqI0FJ497WneuyKoh1fSnlbbEgwU4k3m3ZyoX+MLv6/pSiJmi6MKBwfh1Xw58sGs6qIQMK/2gFEr0D3Y2dlaVNGQUNiQUNCSVNqbVtBR29TUP/CxADtv4rJkjwYqh7uC6kqUSv6oMU/oatcu+WCH8HB3Mp/PclwsDB75UI8xz8gRLhDzr4Y6URYw4ef1W5MQdP5Kpy2h9olYOWhN9tlYNKI84+BtkZhho9Ar5BfMUJEsz4eDl1jVsKCvcVleTV1FX09DSNAYUtnmnrrd66Z8o00VmzFiUmJuvoWYhIrJCdt3HO/A2ys1dLSi+VkFggO2uJhMQ8IeFZIiKy4mJzJSXmi4nICglKz+AX4+cTEROduXDBiqjIaEDh06fPhoaGYrFYQOGw8CgEAovBktBYMhpHAniFI1EoFAaDIqBxRFfv8MOnHy1YsnHmzEVQrbAAoPA0iMJTp0qIiAQFBsCjYbKzpCXFoSES9+9cJxAIeCINR2TgiXQciYbCEiMjo1kUantNeWd5TkMao4qNLiIGJwba4G1VcVaKbHdtprMmyfp5vLtWKcK+iRVQFxOZSUfRSDgCAYcjkcmUGBo1hkSlW9i7bN5+TFJqnoCgyPRpfL/89PPePXuys7KuXbvy008/8fNNEwAc/u23lUuW5OXkqKupAAovnTfHydiiPrO0Nr2sJrOiFuykFpbFZhRQY/OJlDwCMY9AyieSC0iUAiK5LD6xp629Ib+gnB1bkpCIiY565ePlFRDgHRDg5efv7Q+2AX5AxrDoaBwRA+4jiYGnsDAkRjSGHIkmwbDkCBQhFI6H4xkwDC0EhovGUaNQWHMLQwc7q7AoeDgCFQ5D+gWFu3r4OLt7uLxyt7azBN/1+1H4vwkS3zMGBwdbmpvramtBNjVxMjPTjU3MkjKKsgurc4pqGLFptg7ukVg2O6OKkVLGSC5lJpcxk8qYiaUgWYmlbJAJpTG8jC9lx5XExJXGxJayY0rYMcWxMSWx7JJYVnEcqwRKZkkcoxhkPB3KOFpxPLU4gQptoaQUxZMKM1k1JckdJYkdZYDChFI/Z26BBEThAU55r531P6fwv3eCBKezJr2KHleKSizHpFTg06oIGdUkYL7sWnJOLZAfpawpqbNvQnfqHwegcGVzIkRhDvvTGn6zMDyOwmUtMeEYFw09ueQiNNjnzZQoaWFTk0JVtB/i2P4lLTFQlXA9lGMLw5/QcPaPR+HW7ob8+qS0KnJyBQ6cxiRXYtKq8OBHLWyMa+go6h/63mU24Lj9V0yQGIvO3J5yq9oSA0jD4x0MUfjzHfypVrmx0ojxDh63JPzGwVwKf8DBPAq/UyL87ui0vy8RHnPwWyXCYw5+u0T4TavcO6URYw7+p61yHy0R/mCr3HgHjy8R/uJWOeBgzvknjcfvNSsbf7sl4a8bzLg4OVW1G0+e3Hv+XF5NTVVP96WJsbaZmaqh8a4jJ/kFpcUl5jJZMdZ27mLSq2cv2Dpv4eY5c9fNlP1DQnKhrOxSaemFwsKzgIZFRWeLi88VmCHFN12Mf7qYiJC0uLjsgvnLYVEIX1//I0eOBQYG4vF4WDQ8PDIahcLh8BQ0jozEkhBoXFQ0AoFAY9BQVUJAGPrU5ecLlm6aNWuxmJj0GIVnTANbPjsbazqdvnjxAkkxQUlRgdMnjiIRcAqNiSdBl+rAECgoHD4aFp3MZnXWlrcVZ9Yk0yrZmHJaZHqkM9npJcr8GclOheKgTrVXTfE1LMO6NzCDKplhqWQYBY8kELF4MoVMZbFYCREw3IWrD+YsWCksLAUcPGXy5N9//UVJUTElOXnd2jW///bzDL5pgvx8gMLLFy/Kzcl+qaUxbdJvR3bsjEGTODkVtemlNRll1amFFXGZhdS4XBw5G4XJQiKz0SiQWSgU2C9mMrtbWuuyskvpjJL4BBwC5unr7enn98rbOyAkNDQSFhQWERIZHYnEIAlkDImKIlCwZAZIOI4Cx9MQBHo4EqIwisyOxtJCovEwLDUaQ7R3sLa1MvcPDgtDoKJRuKCw6FfeAW6ePu6ePnYODoFBgd+PwgAZ/02Q+G5RVlbm4upqbW1r7+Do6ORsbm75RP6ZibmtubWjhbWjgbHFM0UVLV0TI3N7Q1M7Q1NbbtoZmti+lca2RmNp9DoNbUAaG9pCaWBjbAC2vB0bY30bE17qjU9bXU0TJytvQOHSlK6ylNcUTuBRuJ9T0TcRCv9LJ0h09LWmV9NjS5GJ5Vieg9OriZmQg0cJyDUitbIltXewffRrvmNAFG5JBOrlFgqPapi7z9MwJOMPaHisTILDzK0mJ+Qi8moo4EMehQsbGTk15IR8ZHYViVca8b6GefYdE/DYfvYPViDR3FWbUQ2dxiSUoVMqcdzTGPDw8c5kSPn19Nq23IHvruF/ywQJXowMjHBwrRCFDavecvBHryo35uCPlgh/yMFjFH7bwaMUfsfBEIX/vlUOovBnO/itVrnxpRFf1Co3zsFfVCL8loPHlwh/qFXu7RLhca1y40qE3zj47Va5Kx9tlYNKI0494Fx80oWnjz723yzAidxXmSDBiI2VU1W9+UT+vuJzBXU1VV0dLUODlyYmWqbmJy/fEJacIyoxB40huHsGzZy/Zc7infOX7Ji7YNPsuWslpZfKyq6YOWupiOhsQeGZwiKyUOecAMCrNNiRlpwnJjpr3txl0TCkr4/fjh273N3dgX/hCBQsGolG4/EEKpZAQWKJPApHw1FoNIFAoYdHEy/dVlu4bKus7BIxUekZowUS0wWmT58+ebKykmJSUtKWzRtEhfiBhrduWufj7cVgxhBIdKhMAk9EYTEoOCw7Ob6rpqy1IL0ynlzBxlSyEDkoL4aHIdZKGWupBJJsr5YaaFFB9KtnhZXTQ+kRntEh3mQCksFg0uhs8N209axWrt0lKiE7nU9g2tSpAMIC/HwhwcFoFFpIUGD6lMkC0yGaT/v9t2WLFmRmpOvpaAtOm6olr1iZWtCYXV4DVQkXlsdlFlHiMhH41Mjo9MiIjKiIDFhUGiwyNSoiNSKsgELqbm6uSkkpIBKLYmPx8GhvP1/eejAMiUZiCTAkBobEAd2iiFQEnhyJwgIT4ygMNJGGJjMxFDYAcTSOiiTSwxF4YOJoHC0aQ3Z2drAyN/XyCw6KQkYh8MHhCO+AUC+/IE/fIGc3z7CIKHDgjD72n4yvQGFwdPb29nZ3dX1OY/J/FJ5gpKWl2dg7YwgsekwaIyYNgaHqGtlFomOQxCQUMTk4gmxi4e7mhw1FJARHx4VExwXD4oKj4kIi40JBRoCMDQsHGRcaFhsWFhsOMiQmPJgdERwDZSAbygB2hD87wo8dyUtfVqQ3SDa09WJGebJgID2YCK84LyuYjrpRYWJLWWpXeWoPoLCvMywzcYzCE1oV7u3pqaupqa+rm/gECX9v7+8zQWJoeKiwMZXr4NH1YMjB1aQxBwMaAizyWtAaOvK/fwUqj8KloxSGNMzj7wc1DD58X8MlTazy1lhwyxiFoXvUyChtjeF1y40mV8O8/Xc0DH4VYxrOhpbJfxQKt/U0pVfRYksR0MNXyVvOhxwMHj7wo/LuRUE9raE9f2j4Ww1h+KYxODjY19cHtqMff7MYaBqo8Wks1a0sMxpz8MeXhP/OwZ8sEf6kg99aEh7v4PGlEeMdPK404i0Hf0mJ8JiDP9oqF/2BkRFf3Co33sHjSoTfOHh8q5zlmIPfbpX7SInwZ7TKjZZGnLrf5uA93Nk1+sB/mwDvBfm5uRmpqc1NEy0to8fEPlZRuSUv/0Dx+TMNNTU9XQ19PXUDfS1Ts9tPFWUX/iEkNjswKDwoDLFw5f45y/Ys/GPvvCU75izYPHP2Gtk5q2XnrBSXXCAsJissKisiIismNkdScr609EIpqfnCQtKAwvBoFKDw6lVrLSwssTg8AomBIzAoFB7AF4WFZj4gMXgYAg0H+MPg8SQKHEW59Vh78YodEIWhVWHeBAnInVN++/382TO5ubmnTp0Q4JsqLSE8e6bkE7nHZDKVQmVicUQcHnxbBBYFK8tJ66oqacpNLY8hlLPRVTGoAnxgrJ8lxUmbZK+Os3pBd9HOhblU00MbYqJyMd4Gjy/JXT3l42yJR8JJRLKZld2WnYdlZBcLCIpMmzZ1GpDwbz+tWLY0IT7ezNT0119+FoCAPF2YH/xMk5Yump+cnGRkoLds7lyMf0RTbmVteklVSkFZXGYhOTYdho3zD4718UkK8EsLDUoND04OCwKZFOyfR8R2NTeVxsVmo9H5LBYegQgICvYPDg2NjEbjub8ZaEIcGU2gYclMOJYUGo2EYfAYMg1NAskAFEaR2EgiMwJF8A+PBlskkYXE0z08XllbmvkHR4ZG4yLhhLAoTGB4tG9guI9/qIdPQDQS8/0o/EXxH4UnGOnp6cHh0TVN/R39f3UN/lVc0WLvFpZT3l/aMFLeOJKc3fTKD4djV6aVDCcXDKaAzB9MzhtMyR1MzRlMzR5MzRpMyxpIzwQ5mJExkJE+kJE2kJnSn5ncn5U8kJXUn5XQB2V8X3YslDkx3GT25jC4Se/JoXbnUrrzKN3FtEFWRKmJnm1+YnNpSld5Wk8SsczHGZYBKFzHLZCYGIW/VtRUVyOioqgkEjhhG73pm0VLd0NyBSGhDJ1cgUuthBzMK40YWw8GkAJ25EKTUd4U290/oQXvfxBjFC5rYr+vYS6Fx2mY++EohcdpmIfgt65CNy7faPj1wjC08xENcytGfggKDwz159cnxfAcXAGtB4+exoCHj/szg3sBob+BXsJht/V81xexoaGhfkDYbzYE7VtEV35PpW1tqV7l3zj43ZERH7yq3PgS4Q86+BuVCAMHf7pEeMzBX1oizHXwuyMj3ikR5jn4navKTbxVbryDDb6kVe7tEuFzco0n7rVoWQyUVIw+5N8s2tvaiDhcVFhY2YQnSNDYMQ9fKN988uT+MwV5VRUVHW1AYTU9HQ1Dw2eaL1du2jlDdLazqxcCQ1m5+cSc5fsXrDw0f9neuYt3yM7fJDtv3ex5a6RkFouKzxERlxUVkxUXmyMuPgeAWFR0tqCg1IL5yxFwtK+v/8IFSzTUNbFYPBqDR6HxSCQ2PAIeFoVAYACIiXAkFonBoXAEPJGCxjEeKhovWb1HVnapuMRMQUEhfr7pUDUCH9/kX3/duX1bYWHhw4f3p0+dJC0hIikqtGHdGi8vbxYrhkggEgkEDBJOJaCbygvaywsas5PLmOgyenQlC1lICI4PtKW66rFe6bPc9ZL9zYsxXtWM8MaYqMwo56s7V66UmnH18A5N+YcO1jbnr1wTnzVXSFRCgF9g+rRpv0BX0vjp0oWzaWmp58+e++2XX4RnzBCczicyg19g2uQlC+fFxcVYmZneOHW2gJXWmF1elVJYHp9dRE/IQuDj/ULoru50F+c4r1cpQX7JIQFJIQHJoYFJQX65BIjChSxWOhKZRWdg4Yig0LDg8CgYEoslQuvlKBwZjadhiAxufTBE4XAEOhpLgGEIkWgCHE8DFEaRWFEYUjAMFY2jYsixGFKMv7+/g511ZDQWjmNEo6ihUeiAMBigsG9gqHdACPglfz8Kg3+psbGxurKyo/3v/+D7H4UnGBnp6QHBkSXVbY3tI00dIzmFdTbOQamF7bkV/flVg7GpNa7eKAStKD6vNzarIzazIy6zMy6jMza9Iy4NyvjUjviUjoTk9sTkjsSkjqTEdigTOpLi2pNjucluS2a1prDaUphtKYy2VHprKr0tldaaSmlJJbemkVrTiC2phOZ0fEsOqYMSkmuiZ1OQ1FKa2lWW1p1IKvNxiQYUbgUUrh5omliBBDiu+np7+/r6PvNQ/lj09PTUVFVN8I/LnxmlTZnxow7Gjzl4dE3xtYMBH3nuBB5t7ir+6/tenYlL4SQehd9o+NNFw+M0PEZhKN++Ct0HNQzuNU/DPAqDfR6FuQiGNAwoDG78ESjc2FGZUIaF6iJeP3zAwWOnMZCD62ngLnN/D4ya1rTvViYxMjJSVVERHxNTkJc3EQ2D78NpbCwuLKyvq/umBRKjMfxXa2x7uVlNmV7lJx3Mo/BrB79bIvyOg98uEf7HDn6rNOJjJcKf7eDRJeF/1ir3wZER3+2qcuNLhMccPK5V7kNXleM5uOH43SYF3b7kzNGH+1sGoAWVRELCYBOfIAEo/EAJUFj+7jOFxy+UFDU11HV1tAz0NQz0lHX1dxw+MUNsjom5PYUet2nvpTkrDi5cfWT+in1zl+ycs3ALoPCs2SslpReJic8RFZslIjpTWFhaWFhGWHimqOgsIWGpBQtWIBAQhWfNnP3w0WMUGovFkVBoAiBwaFhUQEg4YB+09okmoLFENAGqFcYT2QoaVsvXH5g9Z4WEpKyAoDA/P58AP58gH/+U335fvfKPpKREbW2tGfxTJcWEZSTEJMVEH9y7SyFTYmLi6HQmhUjOBO/ftWWtxdn16XEFxPACYkgpLaoAFxTHpXCyv0V2hENulFMR2rOCGtLAisxHvnp0ZPNC/l9WSfBvXSi7e+P6pStWCEmI8c3g45vON2XqFGEBgSUz53u6uDNZzEULF/7+668iQkDo/CIC/ILTpyxeOI9Oo/p5eLqb2TRklNalFVcm5hWzUnNxtOTgSIabJ8nOnmJvG+PukuTvw3VwcGpYcEpIYD6F2N3cDCicikKnUBloOCooNCJolMJUDAEajobGUzFEOgpPh2MpwMFhcFQkChuOwPiHwsLgeCSRhSIyUSQmxGIiE0uJw1PiAoND3NycUDgKhhwfjaL7BEW6+wR6+QX7BYd5BgTDsfjvR+H+/v44NhuDQJQUFY3e9PH4j8ITjHconF1YO0bhvMqBmDEK5/fGZHcCCr/WcEcsl8JxXArHJ7ePaphHYZDx7UmxbW9pGFAYJETh1lRaWyq1NZXckgooTGpNJTan4puzSZ2jFE5uKU3v5lK4dIzCzdUDnMoJUbiJwwFv/8kJCZ0dHaM3/djRO9idU8sGlgKQ4v5tfdTBwFI8Do53MFeirNrWtMHhb1XBPDwy0j/47iH6DoVfa/j9ouG3NTxWJjFew+OuQvdRDXPLJD6pYR6FR//A2ts/3NP3zc9Y3o/B4YGChuS4UiTvNGbs4eM5GPyEY8v5Yw9fR2/D6Bd/4wBszUxLC/b3/woTJNLS4JGR4Dn1PSgMDrbe4WZSW7lJdZleRbnBZzp43JLwWw7+YKvcOAe/uZrGeAdDFP77EuG/aZX7GiMjPnpVuXEOhq6mwXXw57TKffWryr1xsPqnW+UgB5+41/RQs4ceN/pIf+MAxqgsLy8uKmr/jOW2Twc9JvaRsupthWf3FRUfvlBSUFdTAdI01Nc00FUzMDhx+aaI9CJ1TcOYhLRdx27N+ePgotWHFqzYO3fx9jkLNsrOWS09c6mk1AIJiTni0KqwjIioNAAxcDD4UFhYcv6CZTwKS0vJXLt2HY5A4fAkJAqHRGBhcHRYFDwahYUWhtEkDI6MIZLIVCaZGq+kY//H5mNz5q2SlJw9RmEh/hnTJ09etGAeCoV0cnIUExMWExaUkRAXEZixesVyBwdHGisRg2cwaHGNldW99ZXtxZmVceTEcLe0qFe5mIAspA+gMM3NIC3IphjtWYDyyEd7lpKC6hjhZQR/vdunFs6YNI//92Vi06T4fhcVFhQWEQH/7pQpk6TFpVTPvyCYopuLOTFk9tI5S3759WdRESFRIQFRgRlC/FMXLZhLwOMSmex4FKkuvbgqOb8sLquAHJcehYnx9CPZOWAtLAlWlixXpyR/35SwkNTIiLTIsLSIsEImvbulpSQ2Lp1ASgAURuKCw6ODwqKiEBhoUjKBDMcSEVgSnswkUGOxZBYMg49AYqLQuHA4xjcoKhyOR5OhAgkMORZNjkGR2IDCBGpCSFj4Kw9nFB5QODEaxfDwC3N65QNROCTc3S8wGv0dKdzX10chEkMCAvJyckZv+nj8R+EJBkThEIjCDa8pbO0SlFLUnlvZn1c1wE6tdvVBIejFcfm9bEBh7sIwT8Ox6R/ScNKHNBzTDig8qmHGexqG1oahheEsciclNG+UwhndZek9oxRO+joU/loTJIaGhnp7e/v7+r71239bDye9ipJcgR392/q4VjloTZFbUQAsxYNUeVNMeRO7ti1xYPhbNc8BUzJSG3LK2oaH39zx1wUSPIuPWxj+pIbB/gc1DOF+XJnEZ2qY52BeZteQ8xvo4IcFP1tlfVcoqTw1/3sXjYDo7GtNrSInlmNeO5j4Zjmf+/BB93TcaQw4l2jqLBwe+R5qB8cteGlFREUlxMZOdIJEUlJoYOC3niAxPoa6hpuJrRUW1WU6FeX64x38ZSXC37BV7otGRkCtcmNLwmMOfrtE+O9b5b7LVeU+2io35uCPX1Xu461yjWceNZ683/REu5vMAq+tow/zvycYsXFP1LTuKr64/+LFwxcvnqiqKmpqqupqq+u9VDc0uCWvOHPh2odyKokpWUfOPZRdtmfBij1zl2yfPR9ysMzMZZJSC4GDJSRmi4nPEhOX4eZMcfFZ4hKzhIQk5s1bgoCjfHx8JSWlLl66FBUNx+KICCQWicQh0Xg4GgfH4AGF0RgyFk/BkilURgyVkahi6Lxq+8m5C9ZISs0WFBLl5+fnrQrzT5k6U0rSy9MjIiJcdpaMkMAMKTFRMSEBKTHhY3+e1Dax1zZydHIJiGPGNxbnd5fl5BMjaZ7mbF+rxFDnpHCX2ABbmrthUoBlMdanCOdXgPUtIQZW00IrScFOKvfmCUyXmPLLCkm+uUKTxPinigmJTZ0ybfLUSbcOXK/yLh1gD4/0/NWcwNG4oDp58iQhYUFxUWHwTwvxT1swTxaLQTdX15fGZlYk5pYn5BQzU7LR1PiACIqDK9bCCmNuQbSxZrm5JgUGpEZEpMFg6dEgo0piY3ra2qpS03MZMbEUJhKFD41EhsNQ4BeCwBKjsQQ4jgjHEAgUFpWdTGLEI/AUGIYAbo9AYAPDkTAMFUOJQRFZWEoClhIPVUeQY3GU+NCwSGcXuygUDkNJRhNigyKQXgGhfsHhfiERr/yCkTjK+Le/T8RXoPDAwEB2ZmYsi1VTVTV608fjG1F4EPwQg9/vafm/p3ANRGHOawonF7XnQBQeBBR28UUhGMVxBRCFoYXhMQ1ndIxpmFcm8a6GE8ZrGFoYfq3h1jcahsokeGvDLVmUTnLo61VhQOGMnkRyma8rl8L1X4HCdTU1OBSKTCBMcIIEp7GRTaenJiV96wkSTV3VKZWE0b+tc3ut3qwpQmSE1hR5kAIOrmiOrWiOKeckNHfWDQz+1T8IreC+mwPv3QIS3Pix28d9CJ79jS29Rt5ZCtbJ6Jjqlo5+nn7G1QrHvK9hQL23NQx9CPGXK+M3Gh5/3Y3Xa8N/q2He/jsaBr+i7Bp6TVMLLYWj6Zp+WTsmglrZ2jnQ1jXQ2gFt27sGR7N7sLNnsGtcAuv39g/1gOyDtqO/Ae52YHB4cGhkCFj1dX7afs1d1WMlwlCrHLfTcczB0MPHveNjDx/Y1ralDX6vGom21tbK8nJwJE+kWAj8CirKy5MTEkqLi78bhUEMdQ+3stqq7GrKdCvKdSteU/gLHPx5rXLjl4RHHQxR+EMOfk3hd0dGvKYwtzRizMGjFP6iq2n8k6vKvV0i/MFWufGlEWMOfqdEeMzBlmMOfrtV7rWDv7RV7px848kHjefkmpWNe1iJ4KVk9AH+9gEO16GhocHBwYkft8zYhKcauncVVe4qKd1XfP5ISempmpqSlpaqjpaavo68htbSdTsuXnuYmp5z/sYzqXmb5yzYPHvu2pmyf0jLLJWUmC8uNkdUBFoMFhaRFIFSSkRUSpS7z88vLCs7Pzoa4enpJSEhcf3GdRgcjsYSEEgMEokFIEZADsZDRbFYMpZAwVNogMI0VrKGiduaHafnLlwrJT1XWER8Bgh+PgG+GfxTp4sKCZkYG5HJ5KVLFwvM4IM8KiIoISo8d96CTTsP7z544eTZuw8eKvq4uuTQsCmRngxPU4aHCdPLnOVtxfa2ZrwyTfA1L8L4FROCinD+xXi/ClJgJTkkwkJjlbTolJ9+msn36xLRyVLTfhaaPHXyr1NkZGYijKIGKEODmf0jbUODCcMsC9p8mfl8/NMkxEXFhIWEZ/DNnz0TjUT0tXRVJuaVxmUDEBdQE9JgOLZXMMnOBW9lS7JzpLu6x/r6JoeFpUfDMxDITCQyA4UqT0rube+ozynIj0tlkllRcExwBDwaRcCR6AgcCYbBo6EBalDbHJEWR6THY0hMJIGKwJMikFi/EKhAAk1io4kxXAeDjEMRYkAGhUbaO1hFwNF4ehqOmhSNpYbAMCGRqKBwpFdgFJbEBK/9o4/9J+MrUBgcnf/DCRKcmjIcIszG2tLa1hFFYDS2fI8/pv+AFM6u6s+rHoxJq4EozCyOK+xj53SO13DceA0DCr/WMK9o+I2G4z6tYV7RMKThLGoXOYxL4RQuhTN7EinlgMKZgMINQ801A40To/C/boJEY0d5SiXunRrTd/62Xsbhrgc3x1a1xFY2xxGSY1ExebSUBlpKPfWrJjOtERNT88gscf8T6qWXMWZ+2YWV0FNjaKS/6g2FP6XhsYXh965CBy0Mj9cwtGI6TsMfoDB3VfiDGs6vpySX0t3h2Vd14vc9oR55TjfyyQ6nVIAMI1dE0Sqj6VVQ0qAtklWNZr9JXFwteDEhxI8mJblu7NcIdpjpjeyMRhY3wU5cVlNCzptMzW9JK4AyNb81s7g1ryY3uQKaH/zm4Rv98d44GHr4RpfzYyqawOlFYt//YhzeRAJg4vtMkHgnRvpHOjO7ar3rKowry3XLQVbof9DB40sjxjv4K5UIf8DBPAoDB395ifA4B48rjRjvYO6S8JsS4TEHg/xgifCYg7+0VW5cifAbB3+8RPhLW+XOPW08+ajx1P9j7yzg2sq2tz33zrQ4xD24U9epu7u7u1K0tNRbahR3d9eQYIEQ3N3d3d2h3z4JbZnadIaW9n5/3t87e/bZJM1JcnLynJW1177cdEq5XceqPyt/7H2dLH3HChIRsYnX1R6ellc9La94Tv7mZcVbN1RVFdTVbz9QV3l0V/HRg/krN27cdjgrO//cFTUEfiaeOBOPl8JiRdFoYSSSDIfhBfhR/PwIAL78fFDLxwuHSj7wwqZN48ZhyV5ePmZm5gQC/qb8DT8ajUoDEEwDNAyVkgAtNdAvgF1KIjSAHhLKjGbGpD54bb5g1T4RyflEkhgShYXqqfHy8QMU5uHj4+G9du1aSkrKn4sX8vFyo5FwFAKBRaOJBJKwiDRZSI4kPEtyxqJVq9bdOnvC8fm9aFutJGe9aGvNcPOXkRavoi1fJdq9KaTZlQQ751Ot83zNSmiWZQG2IfoPds8VE/j9NwwPx8qZouvniq9fNH/Xtl13bt+rS6odqR4d6Rgeqh8cZAyUWhWsnrOag2s6DoNCIeFIAV4ZMeHQoKDelu6S2JzCqPTiqLQ8emyyu3+0jSvTxDrc2DLa0i7ewSXZzSvd1y+TFpAdGJwdFJQTFFydntnX2dVQUJKflMEIiXDx8LVzdmehcAQ1OMw3KCSAEe4fEubhG+BBCabRI4H9QyIAJTt7+ZnZuDh50gAKUwIjKcFRwNSQGEpglG9AhL2zu7b2aw9fanBkuj8j0Tc40pMa6uUX4ukX4uhOCwmPnzwU/kf6vihckBx65sC2uXPmzJs/fz7038JTV1TSi2vG/vzD9NNRuBigcOdoU9doFhuFiyEUzn2Hwr5RxbFF/dG53Z/QMCtpmD2Fjk3D75OGP0yhAzTcPkbDf00ahmgYoPBY0nBrFrMn1D33+WPt/JS2MRQOK7c19YVQuH64pXawsWpCKDwRDQ59OP5rqiapgkRDZ1lKZcBHU+UAS0GkOO63dcDBFS2x1W1xla3xzoxoE590+8Ay+4DPuvSTEWAw+KXxD5uOQWUWlOJzGvFrrzI2XA8DTBybCeWZjEAonFTaDO3J52j436y7Acx+juzO39PwuFXo8uoZWdXhzvS8M08TN8szgV855NCiawDpUqOqfSKqvNkoHA46lR5hFe6hH+xCL3cO/mDwMtqxXxyoU2ZDK7GmjtmKWmLhW2zmUwRszmqNvYqMPAuNPQsN3Ius/IqicpPSq8FlTPBYyQjW2wd29aMU4fJ3Ef1K8A62JvROVgGQiVSQ6Okf7uj+JUpP9Nf0t4a01hjWVDyrKH9QVn6/tOJhaeUjtj/i4Pco/AkHf5Qi/Pcc/D4kPJ6Dx6dGjOdgNgp/vxThDxz85RTh9xz8xdSI9xz8pVXlxqcIf5aD/8mqcoCDDylCmcH7bzbuud6483LjvmtNJ5Vb72r2BDCHG3/CSpkTqSDxUSA5PDbhqtrDkzdVTssrX1RUunFb5dYdNSX1u7fvqyk/uK369NGyjTsWLtucm190S+UpL1ISgRKFw0kwARy/AJqPD8XLA+eBlr/g4+bm5+EW4OYC5mf7j985sRgiQGETYxNhYaGHjx74BwT6Umg+vn4+vjQKxd/bhwp1aIF+/nRWwYSgYEYEMybliZbV4rUHRaUWEsniaDQOCgrz8EIczMvHycGxf/++kuLiLZs3cnFOR6MQSDigYSQeixMWFMZjyTAYAYMXJQtLSYtL71qzxkDtZoSdfqqnWZKzfoztmzibN4k2WgW+VuXBjjk+ZqmuBvl+lmXBjgkOOkZqly4e3X/jyiUrIz2Gv08MkxEbHZ2RltHX2DPSMjrSNzJUOTAQPFBilb9y9sppHNOxaEDCAjBe7pWLFmenZ/Q2dwIOLohMK4xIzQqITHSjxdh7xti4xdq5JTh5pXpSMygB2QH0nODQvNCwgrCwfEZYbVY2QOH6gpKcxHR6MNPFw4eNwrRgJkBhSlAIVDctKBTgjZWDm6dfsF9QeEBopDctyNbZ09rB04vGoIZEuVHorpRgr4AwCj3KB1p2Ltze2UPz9QtXTx96VDqNkegXEksJiqQGRdHo0V60sIi4tG/8JeE7oPDIT6og0dVUeuPoBgm5eVcU1O0cXZwdbNRuXZw9Q/bo1TuN3T82WeLXQeHsolotE+fk4s7sqsGc6qHo9BoTOxpA4Ziivuj87ujcrg80/LdT6N7T8Gen0LGShlMACr9LGs5idod65AEUzoOiwr3lEAqX2Zr5ZiaXsVG4qbpfV/vfo/DI8Eh/f9/QQD84m40NfbOyS9tDEus6uqHo1/Bg3+RUkGjsKk+rCvoMBzeM52A2SMVVtcYDZ5bH51WUVTX0VjX0fF/Xt/SCF0FJL3XP7cjnttkJ2c1tnRAPDY9AKFw2hsJsGh5DYeB/TcPsp/ntNAyh8BgNh4IbtHS2p+S3P7POPno/xjeiip0I0dkz1NY12NY5APYcuLVzoLl9oLGtr4ll0AHPsa75g6sb//IKlNd1l9V2sV1a21Vc3VlU9cH5FR155WyDfltuTTp479gpwu/D+ey37z0Hg9fnHQeDty+upi2xZ2AysACcyqsqKhJiY0oK88AbODb6zQKHAbgoyi7tGBoe6WhtLC2arAoSn9Nw70hvSV9rcGu9RU2VZkXV8/LKx6UVD4rL7xWV3y0c852Cijv5LOdVqAHnVqjlAFcC386uvJ3FciZk1YwqyOlVqmlVKsCpVcrAKVXKydWQk6qVgBOrlROrlRKqFeOBayDH1SjGQlaIAa5WiK5WiKoBvhXJckSNfHiNPLNGPqzmJoPl0FrgGyG1N+i1N4JrrwfVXg+svRZQe82/9hqt7iq17qpf3VVK3RVg37orPnWXvesue9Vd9qy75FF3yb0e+KJr/UUXyBec6y841V9wrD/vUH/evv68Xd0527pzNvXAZ60gnwG2rD9jUX/avP60Wf1p0/pTJvWnjBtOAhs1nDRsOKHfcEKv4bhuw3GdhuPaDce0Go5pNhx93XD0VcPRFw1Hnjcc1mg4/LTh8BPIhx43HnrUePBh44H7jQfuNR1Qb9x/t3G/WuO+2037VBv3qTTuVW7co9S4R6Fh962G3fINu2807LreuPt6074bTQfkoZDwSZXmi+ptj/W7qYzB0sq3k/57AlvsChJUH++qivKxoW8WOG+ADzs4mbA3AQpfUXt06pbaecU711TvKt69o6yupnrvtoq6ksI9ZYDCm/YdmbVgDUDh+4+1eOHiMASZhwcFkJeLi5+Ti4+Lk5ebi5eLm4eHmw+gMFT0jAcYzscL55jGQ8CTvb18AQrLyEi/1nwNUNjHF1piA0CwLwuFIRqm+FNowdBEsSB6UEg4Myr5ua7N0vVHoLWXyRIAhQUAC/Py8PPw8kMoPH31mlWFRYVHjx3+z39/g8H44DAYAgbHYjCCZBIJR8SiCGgMmSAogSNJEIniS2bOvnl0v5Pm/QhbrSRX4wwvyzQ30+Ig54aEwIpoamEkrSYjrq0st6Eoq64oqyA7PSkhMTE+Pj05MTaMbmduqKaiTDOnDsYNj9SNjtSMDIYOBz2lCRGEObl5wCMiEXxwPp5rZy+11Dd3N7UXRqblh6fkhCak+TET3AMTXKiJrtREN2qSh3+qT1AGLSQnOCwvNLyQGV4UHgHa2pxcCIXzSzIT0oKDme6+VBcvCi2IERACLaLhBSVSB/tQA+2c3MxtHZ08fCmBYQGhUR6UAEtbFyc3PxpUVDjK2TvQwZPmTg3xC4n2DYn2CYp0cvPWfPXczsk5gJngHwYcT6XH0ugxgaFxtJDY2KSsyUPhn1VBIp5mOXfOIhOPkI7usWhfX3ebj532smXLHQKT2CM/SL8CCtePQ+Gk4o6s6sGcmqGo9BpjO6pvVElMcX8UQOG8DzQ8bgrdh6Thb5pCF/25KXSM1szw7hCPPA2AwqmtJZnsqDALhVPKoAQJFgrraekV/VsUbmlqiouOiYtNbAIENPT2fUroWOd9f7z7h/sHoDWJfCKqjj+MNfYsBCfBwcnKIW/prmZx8IeYIhsQAUt9AlIQB1e3xTd2JQ2N/KgVmHv6hm1ope6hFQATx4beo3BTFIDydwT8mTSJT6bQQWTMxl82DZc2R1V2xFd3JVZ1JpS1xoynYTYKA3+GhllJw+NpOLeOAfojrGlztU29lMgq8OFi7+pkqrY9F6BwZk3IGAe/f/tY0P/h7Wv+8PYBFO4daBm7/48UOJVnpqU52TsEBjFrGzrauobABwK4hdV+2h9vcFHhH1N95H6MvHaKb0QlMzLZ28M7OXGSKkh8ScM9IwN1/T0ZXS305ib3+jqrqlrjihq98hrd8hqdslqdUuAayCW12iU12sW1kIvqtItqtQtrtQprtQvqtAtqtQrqtPLrtPJYzq17w3ZO3ZvsemBN4CyWMyG/zgBueJ0OXP86Dbj5TUajZnr9q7T6V6kNwC9TWE5ueJnU8AI4seFFQsPz+IYX8Y3P4xqfx0LWiGnUiG7UiGp8FslyROPT8KanzKanYZCfMJqehEJ+HNL0mN70OLjpEXBQ06PA5ocBTQ/9WaY1PaQ2PfBjmdL0wLfxvk8T2/e8WfZqUvdsUvdg2b3prlvTXdfmOy4sOzffcWpWc2xWc2hWs29Ws2u+bdt826ZZ1bpZ1apZ1bJZxaJZxbxZ2bRZ2QSyknGLklGLkmGLokGLgn6Lgl6Lgm6Lgk7zLW2WtZrl3zTLazbffN1882XzzRfNN54339Bovv6s5dbzttuabU8M2w3su9wD+hIzBqvqRn9qTeuBfqiCRHlJcV/PP06ALKvtfmyZZeFbXFEPlamJjk+8qvbojMLd8wp3rqveVbqjpnxHWeWuktIdhZt3FJUePzx8/uq8JRtT07JevTERQEnAkGReHiQXJ4BgyICDebh5eaCQLVTmgR9Kk0AK8KPgAmiAy4IkEV8fipmp6fx5c3X1dWn+/j4Q/tK8vCEa9oEixDSKX4B/YEhQaHhQKJMeGsGMSnxj5LBy03Fo7WVBSQwaD+MXEODlFQAozAOwm3POnNnJyckvnj4noLGcnNP5+fjgAjAUQGYiCQFHw1iPzgfD8AqgUVgh8C/MnTHn3IG9bvqvi6OC2gvT2kuye2pKR7uaR7vbBztaBrvahnvaR7tbB5urogJ9H9y5d3Dv3i0rl+xaPn/L4plkLGrH4h05JhnDZYMjNUPFdnmXt10AHMzHD8dhcQgEr6ggyd7Erq+jv6OhtSAiNSckPjMoJs0vPNkndMy+kFMojHR/RnZwWD6EwhEFYeH5DGZdbh4LhYsz4lMDghjulAA3Co0WzAxiRNHo4a4+NDdvqru3n6Obp62zm5MHhRLIpNEj3H39rexdXDyoVHqUHz3GkxrmRgn1pIVR6NG+wdG+QVFuXn6vXz4ztbDwDQr3Z8QCIKbS4zwoob4B4f6hcQCFJy9Bor+/P4xOd3V0nNwKEkMWT2/tPHX7ownVo4MtapeOXXtkNrb9Y8RC4ZKficK1EAo3do9mFddpm75H4WEWCtN8oyEUjszvHk/Df0mTGFdQ4otJw58vKPEhaTgrAkJhKCqc1lqc1VOa1ZfILGehcGl74wiEwlX92jrm+YV/f4H0WVVXltvZub4w8HEPLghOqA+IrfkWB8bV0hNqX9rlrLsWtl0x4vKrJJ/wivaO7oH+vh/99d/Z35xXH8laPu1djulHJSP+wsEJAIXr21MHhn5UdvvQ8GhTW9/AX+upQSjclgyhMLQ/X6NhNv6yNz+i4Yr2uMr2uKwKekqhf2pxAEBGAMQlzVEQO/7DKXQsFA5/X0xtcHgUXM+w+5Oo0fqOoozqYPZ79z4e/CUOrm6F3rva9uTJyRUGx21hXq67h5+BfSS4AncIgvJAvtFOweUv7LJ3qUSsvRq2Vy1aRSvMytYjMTbm56JwfUd/aSP0jkO/w7YPDTYNDNT3AzgeqGW772uu6R2ofWfQZ2+yOzU97z34sbsHarpBy/ZQbXdrcXtfVRfoDFZ3fcGdn7jjg6uA2z9x26ceAm3leLf+1S0fuwK4+XNuGufGMZcDN4xz/V9dB7mM7dq/uuaDS6vfe7i8ujm7PD+usLeyfqStY6Tv31cs+e4CUJta0Jpe+A+cVdJOi64+ej9mp3KEsl5aUHy9Hz3yspL6yWvKp64pXVFWU7l7R0VNWfH2rVu3Fa7dVr718NEZedWFK7aEhccYmthiCLIIlJAAP5aVFwEDBuwrwA+MhAmg4DA0Ao5hGYtE4Ph5ESLCEhRfP3Mzs6VLlhibGFEBCvvSfHxo7p4+7l6+vtQAakBwUDAjJDQ8JCwqhBkZwoiMiIo3sXJft/2UhNxSspA0Fk2EC8ABCvPz8PBwg8fjP7Bhb3FaYVVShf41nXlic6b98ccfv//Bxc0NR2EAEmPQRCmpGfPmLVq7dsPJU2fvP9TQ1TX29vDKy8psqq2ur66CStDlZlcU5NYV5VVmJBbH0ksiaZWxgfEeFlcPbJMSFufn4p72229Evj+2L54xR0oE8Peh9YezY7MLkwsu7j6Pg6YDCsBgKBwOB4fzbtmwISk8ub9jqL2hNS8iJTskIZMOaDiW7YzgmPSgmPTAmMyg6Myg8OygsFw6Iz8UAHFYbgijloXCtXmFqTGJ/oGhHn6BruDF8acHhEQEhER6+gU5e1Cc3LzdfagelABvWgiVHukdEObkRbF38/Sg0ChBTN/ACC8/hgclxM0n2NGTZuvq6+oT7OTq9eSxura+rqtvkHcA05edNeFGdfEO8qAywqJTJg+FBwcHc7Oz42Nja2v+Pkn3u6HwcK/W7WsXHhiNbX5Qn/lTxZM3Xv3QM/1PR+EigMJdYyisBVC4pDOreiCndjgqo8bIjurNigpHFnR/oOEvJA2Pn0L3Pk3iHQ1D6258ZQpdVlRPiFeexhOAwm0sFO4FKGxnTslIKW1rHG6tg6LCL41DU3Kqx3b9H6qxrtbbN0jbimlLLXAPrXQNqfgWu4VUeDAqH1lmbrgRBs0Yux+rbZ8cTGemJiVMpBDVt2hguLekKYldKBeQH2ApAFIAH99PlauAZsuNcXAN5PjGzpyRyV3CdwyFm1ko/IGGQft1GgZPhEXDjVGlLdHgqXkGmMirnDlxbu+5K4de6d+JyvAsbY0pbopkE+RnaXgMhYHH0XBu7V9Q+Geppac6qyYkp26sAjTYWzYHQ28f+zKGFdF/x8GQ6zsyhkYmqYJEV0dHcWkVM6kqKL6WnsiaLPhtZqbUG3kV7lCK2HIr/KxGgrlXdnxiekVZ6c9FYfAs9NwKegcm/5rng4LTGoobf/hK7P+LSq3o0fYua+r9mUfIpwKY4Rla+so+Gxw5324Dj8Jn1lkH70avucLYeIN51yTL2CHo5BX5A2euHrskf1FBVVn9nvKd2wqqSvJqqjfuqt96pHFO4e68ZRu9fGi2Du6ConMRKBEYjCDAj+blRfHwAALGAOpFInHsChKoMeNRKDzgY1ERSQqFClB49cpVFhaWVFogKyrsBzjYw9vPlxZEC2IEh4QzwqIY4TGh4VGhYZGRUXEOrtRt+y9KzlwuJCyLw5LhMAQ/Lx8vNw8Hx/R5EnOCNKiDZQMjDaOdvm0WN0ylRKSIJOL8+fN37tx18tQZdfUH9vaOwcEhERGR8fEJqSlpCQlJzPAIHwrN0NhCWVHtxOGjR/bsvnjkkOq5E/cvnnh86fiza6efXD9/Zf+2ZbOkJUTF8WgsDzcXCsazYdHM5bOk+bn/QKGRXl4+fhSapKQ0Lw8vCoFGIFBYHBaLRaoqqJRmVfS2DXQ0tedFpecyU/PC0/KZqfnMFLZzw5KB88OScunRmbSQTFpgdmBQTjA9JzC4Lie3r6OjMis3MSI2MDjMNzDUjeLvRQ0KCI1iRCUFhcW4+fi7eFCgCYUh0Jw5P3qUo5e/sY2TsY2Dmb2Tmb2LkbWTrrHNax2TF1pGD569Vn/8QlPf/I2u4fVrFxVUlF/omOiZ2emb2+uZ2ukY25pYuxpZurhTJnG1OXBWBZDR19v7LROTv19UeNDqleqxW88//rwOd2jevnjxjsHY5o/Rr4XCZs5JpZ1ZNYPZNUORGbVG9jSAwtFsFGbRMCtp+Bum0H1+Fbq/Jg2zV6FjTaGDUNg7H0Lh9LaS7F7ghPByW3MKlCDRONxSN9BcM6Bhnhqf9S9XIujv66utrS+rbKpr6m5q629s7fsWg1u2dQ46BJZtV4xQMUgLSWpISs23s7ah+nj/6AoSQHUd+exf/NlEyOZIiKVYHDwWUxzj4ETQdvRVjt1zssRC4SQW3sWyCY+Nv5+j4c9UGgYjFW1xJnYaMjPEURiEAIwPGEdAb9+7LjzFrbwtFuLgv00abvhAw7l1YQX1ET8dhbsH2vLqw9nvHdhPsNtsDgZP/FMOBu9ddVt8S3fhyOikpk4Oj7wF55y+gTF/qT/e4F7hqQ1nnsU/tc5Kzmvp6hkcGhz80UnzX9fA0AjYmcP3YnLLflr9jZaOAVWDNBtaydj2lMbJmlqyUzkiLusnTI/7kgBd5GZnUwMio5JK8is6gd8l+n/N+eUdxdVdwfG1xx/G7lGNvGeWEZXR4hsYfvDMlb2nLh+/onT6utIVRdWbKreV7t5XfvBY4dFzxWdvzivdn7l4rYW1gy8lUEruTzhKGAYnwWA4ARiOjx/Lz49BwHEoFBGFJqLRBMgoqMWgiTABtKiIlB+FamZmtnbtWmvwpUML9PKmePv4elGoXhQaQGHAef5BjJDQiJCwqAA6I4QRHh0dR/EPO3RaQWr2ChHRGQS8EByG5OPh4+bi4eLkurHrapdny3DRyHDz0FDaSD2z1sHSwdrG2s/PLyY6Ojk5OSUlNS0tPSYm1s3dU/ON9k35W0ePnlizZqPcjAWi4jPxBFEEDAPnR6AEECQkShSLkSISZISFpUTFJYRFZSQlpWTkSAQytKAyD5cgin8mASmK5MWjBFxcnV3dPaSkZLFoPBqFQaHQGAxaWkrSzNCiurAJoHBnS0d+XE5BTE5RbF5RdHZBRDoA4jxmcl54cmFkWiEzOcufmeRJSfbwSqdQMqh+mVRqbU5OT1tbSUpaPDMqiM6k0sO8/IO8/emBYTFRiZkR8WnetFBPSmAQIzKYCdUM9qPHWrnQXuhbPH5jcPfZG6X7z26qPbx06/apCzfOXLp55NSFA8dPX5RXuX5L5ejRQ3sPHDx44tLFm7cv3VK7qnj3mqK62sNXj17om1g7f+Pp7jug8D/Sd8wVjvI137X/TF79X/imtTr34vG9ug7BY9s/Rr8oCtcCFK6BUDgaQuGIdygMBYYhGn6XNPwRDb9PGgb+eApdJ5Qm8RENv0sazgQo7ANQWAdC4Zze0pw+CIUtxqFwLUDhlPjshrFdn0TFZjUZexUWVUG5By2NtTRfn/BQxo+uIAHU3d9c3BgNcGqMg1mz08ZPlRvPwfUd6T8uO+JLej9tDtA5G/L+EQ1XtMfF51D+XD4PiYKdvrh//qKZ0nJiG7auRKBgN5RP5dcxS1tjoMuAb6BhgMKQfw0UHhrpr2hJg+bwfW2qHPvtA+9dYk17Qld/3didf7ygImh9fcND/+YHBPApCIitqWn6VSKggE6OPohZfz3Mhf6Pp0B9L0WlN+5WiZTXTmlo/eHnhP8t1Tf33tRKXnOFoe9WMDT8qwSGoQoSAQFUb4+GmoqxoW9WVUPPM+ssW/9S0AGb0fGJJ68qH72ifPrW3WNXFU9cuXVRQU3x3jOVx6+Un2mpvDC4qPpk9pL1b3QMw8KjZy9YxYcgwxAkGBwPQxAEEAR+AayAABaBABxMRmNIKDYNAxTGEOEwjJiotB+FZmZmvm7deitrGwo1wNPL19PLB6Cwt5+/LzRbjgEVFQ5k+PgFWto6uHv4RkXFMsITLt96LDtvraj4LAJBmI3CXJzcAnwCJjcMBnx7hlL7RnpHRjqGRztG6yrq4uLjabQAa2vbFy9eyt9S3H/g0JKlK2RlZ5OExREYPL8Ako8XPn06HxcvEuwzHIGHwTHQID+Snw8B40ciEBiw20SSiLiEnJTMDDyeBBNAcnNz8XP8RwbJNx8PlyWi3V2cXd08wL9JJgrjcXg8HofFYlYsX+Fi71me19DbMdTV1lWUXFiUUFiSWARoOC88PTskMZMen8NIzGcmZQVExrr4Rtg4RNnZJ7q5pXh5pnh5VWdldbW05MXGx4SGBwYz/EPCfALontRgSnB4REJGdFK2X1C4N5UeQA/3C2L6h8aFRGX60hNt3ION7XxeGdo9eGV4V0Nb5dErhTtPbj94fkvlwcUbKrfuPFZRf6qiqgqA+MR5hfPXVC/Jq92E/qR6Q+XBfQ0dYyunyUPhkZGRhvr6yvLyb1kU8TuicHNl3uvnT+Jz/hJXqylMfvHydXbFj72i/QVQuAOgcMM7FE4s7ciqHcyuG45go3BMaXQJhMJ/oeG/TKF7X1DiwxS690nDf51Cx6Lhz02hy4z+gMLFAIVz+xIiym3NfTNSAQqPABRuqu3XME/+1ygMjuDe3t6+XnAO+Mevc0fPYFvXGDew6xNPcHmCb9To6HBDZz6LpSI+BSngdxycWNue2N5bMfp2sg+hv6DwexpuiSlreU/D41EY0PxfaLimK9HA8gkWj3704lZeDePk+X27DmyMyfA6c/mgzEzxiFT3qq4ECIIBCn8bDUNR4Yafj8JArd2VYK/A7oF9/kuK8Kcc3JbQ0Jk5ODxJ+zw6OgrOrrFRUfm5uf+inho4TQ2+yxdvrK8vzM+vq/0+Z+B/IcBWAEoABwPYuvo6qfpnpCj0DYxoO+eDHdirFuUf88Mrb/5vCVw1bZZnghfn+KPYnxi2/0idHR3M0FCqry/4IIwNfbPAF0FRdWd37xgPRcYlXVZ9dEbh3gXVx6cV1I9fV72g8lD+/nOFhy+UAAq/NLjx8OWS9dsfPHkeG5f054otPDCyAIIkAMcDDgZMDEeS4HACAkFCowVRKCIShUdC2RGAhokwAYy4mIyfH4TCGzZstLSy8aX4e3j5uHl4evhASy77BYQAzqMFMakBIc6uHhov3hgYmIXQmRFRyfefGc1buk1Mci6BJAqHo9gozM8nYHRdH0LhxN6RvpGR2qHh/BFTDZNlK1fMn79ITEwKTyATiGA3sBwcPNM5eLl4EXwwlAAcBUYQSCy/ABqJJIB9g4FBGBIGR8PhGBQKj8GR8EQhspC4uMQMaemZWCwRPCIPDzcv5x9SGPgKIfRyCZKPu4ujs6uMzCwRspggWZBEIggKko8dO+7pRilIK+9tG+hu6ylOKSlKLC5JLC6MzskOTU4LiEkLjM6ix2YFRkY5efvrm1F1DBhm5nGOjomurgmurlWZmZ1NzZkRUeH0MPBS+AWFeFEDXLxpnrTQ0OjkqKTsgNBoHyrdl0Z39vTzDYyISStNyK6PSq8OTSymhmd60hNdA6JdqBEuFKYbhengHmTpSLFyolo6eFvaulrYuusaO73Ws36la6ltaPNCy0RDy+SFrrmdq8/wpCVIDAwMgNM0OEyLCwvHhr6s74jCQB3tzR8nnA31tXb88EjbT0ZhV8/iujEUzoRQ2CWxtDOzdgBC4cxaIwead2xpdOkAG4U/S8Pj0iS+YRW6v9Dwh6ThzJh3KJwxhsKJkRW2FhQIhZsACg9OEIWbm5rAcZUYF9fZOdmh04mof6izqi2VjcIAMVmpEWMc/I6lQJvQ0l0wOPwTOACgcHUra9oca9/+QsNjKPwZGi5h03BzdG13krzKGWk5saQ8aufb3HNXDu0/urW+L8Ur0FRCWtiVZljTk8Qm4I9Q+Es0nFvH+EVQeHC4t7I1taCBOZ6Dgcc4uPV9PBhcxiR19FWzuG4yBFA4Iy3N2d4+MiysfwK/bIBLwYzUVG9396SEn1ZBoqt36IVdzj61qD23oy68SEjOm4wSHB8JEN6F5wmsItbQei4fzSv9vyxwyWTuW3zgbjR4d04/iQ+M+2mXTB8JMEZ1ZWVpScm/+C746EgPj026dPvR6Vt3L9/RuHT3+bnbTy+rP79+X+Pa3cc372soPNFUfPpq+aYd1xVVE5PS1m7azw0TFIAT+WF4ATiEwggEGYkgIZFkFJKEgOPYE+agXGEkgZ8PJSYqTWWh8ObNW21sHQAKu3p4u3l5e/pSfWlBfoGh/sHMwJCIQHqYtw/V1NTGytI+gEaPiEw0sfJctfmomPQCoqA4HIHi4+Xl5ubm4OS6vP1Sh3vTUOrIaNfIUPJItX35pvkbf/vtt99/nz7tD04eHn4cjkQkCsERGBgMA0fi+AWQAIIRKCwKjRcQQPELoFBoAhgRgELCcD4+OAwawePwgoIQCsvJSM/CYcHTwfDxAfieTkYKrBBB75wrFkL1tnd0kpWZLSYkISQkRCDgZWWlnzx94k8NyojP6ajvBChcll5WklhaHFeUy8wAHJzkx0ymhqfRIuLd/Sk6Jg73n7k/fR5kYBjjYB/v7Bzn6MRC4abUUGZoIIDgQE8/fw8fPxcPiiclKCgslhmTClpvKt3FkwKVjPAOiE4rTS1qTy7qiMlpjEivCU+rZCQXh8QXhsYV0KNz/ZkZfqFpvoGJbr7hjh7B9m6Bti7+5nY+ZjZe5rZeZjYeZvaeBlYugLMnb+Hln1RBYrS6ODeMwQgODgoYp9AwZlx8cnnVj/1R/qejcBFA4e5xKFwGUHgwq34oIqvW0IHmFVMSVTYQUdjzAYXH0/BXptC9o+HPJQ1/TMMZsb0hvvkaT3XyMtuLc3tLcvsSIitsLCEUbm0abqkHKDwwERSurKhwc3Ly9fQETDw29K80PDzc09PT1/fDK0i8V/dAc01bGosmoypa3rFU25hr2xKbu/InPzWCLQiF30+bY9Pwu6ThL9Pwu8Bwc3Rdd9KFa0eWrVqYW81oHEg7ffHA3sObqzriwxJdZs6RsnF7U9Od9GEhuq8FhsdoOK/+V4kKA3X0NZQ1x41dxrBek4q/pgizr2TAZczQ8OT9sA6O2/zcXKqPT1J8/ERWXgQonJqUBD5T8TE/rYJET/9wZnGbmU/RU+usqPTGnxIVzihqe+2Qc/llopJeqjW1pLXjx67H/j+koeHRrJJ2F3q5hm024ODSmq6xP/x/pNDImBPXlQ5cuHFWUf3KvReXgdWfnVe5d1lZ7aqK+lXV+7fUHy9es/nEmcupaVk79pzkEiALwEkCAngYhMJEBIuDgRFwAgKGhcMwSBYNI+E4Hm6EiLAkC4XNtm3b4ejoSvELcHH39qLQ2Ost+wWE0oIYgXRmKCOKwYgJCo4IDmYyGOERUfGu3vTdR65JzlhKEpKCIzFcnNwc0zl/nzZNTnSmzz2P3qjO0c7RdkaLyWUDAprEycXNxysA0BaBwBAIggSikAAMDUdgEVC1BzgMjgbsyyJgHB8/EgbHoDFEJBIPIJiPF6p/DIej0RgCWVBMXEJWRgZCYTQKC4PBODk5kHzcCwWRR5bIxNKptnZ2sjKzxEUkhUSEsDjsosULHRwcAv2DYkJjG8sbezt6q7KrS1MqiuKKskPTkvwi4rxD4r1C4tyDgs0c7e5rWCjdcb7/2F9XL8reNtbJMdreoSozq7OpOSWEEUQLdPelunpTPLwpLh6+7t7+1EBmMDMukBHj7htg7ehqYmlj7eTFTMhPLmxJLGqPzm6IzKiNzKgOSykNSSiix+QFR+XQwjL8QlK9AxKcvcNsXYMsnajmdt4m1u6mLJtYu1rYe5rbe9Imc+Hln1NB4u1woK3e5g3r12/YuImlzVu27t6zd/OGlcKC0qrPrMdu9WP0C6BwO0Dhxh4Ihd+YOUMoXDeYVQdQuMbQ0R+gcGRpf3hRz19oOB/yN0+h+2TdDdYqdOOThjPiekMoEArnZrYV5/WV5AEULn+Pws31AxNE4bra2mB/f3CV1dY2oUKzTQ0NEWFhyQk/vILEePUOtjV25le1JVW2xFe2xFa2xrIgOKm+I621p3hg+Kd9x7BRmL1qGht/xweGv07D7KjwNYWTS1fOz66gNw2mAxQ+dGJHQ3+qnYeWkCjRnWZc25P8AYX/hoZ/ORQeHR1p66ksb45jLUwdzU6NeMfBwPE17YlNXbmTv95yR3t7dVUVuCacSJIPwN+qigpAw+WlP62CBPthI9Mbbf3/8bJh30s9fcMNrX1GnoUhiXVtnYNTUeGPlFveYR9Yyl6R5xcROFyHhoYAaUw8yS0whLHv1Llth44eOn/18u2H1+5rnFNWPyuvfOvO3VtqatcUFeRVVBctX7/3wMnsnILDxy9z8pNhCDK04ByMAIcDpmTFgxFQmgQcDqHwO2O5ueDCQuJ+flRTU9PNm7faOzhTacEe3n5+/nT/YIZ/cJh/EIMWFBoQFMoIi46MSoqKTYmKSwpmMF08fV/qmB84eWvmwvUEYRl+OE4A8CoCzsnJzcnBvUx2meMLh476Dvs39jOFZ3JwcvPywqCSxlDWLxaHJwMDFAb4C2hYAIbi40eg0HgkCg8IGIyzA8NYnCAKBWgYzSqEjIDD0Di8oCgLhfE4MhqNQyJR3Fyc/FycMljYseUzkpmBttaWcjKyEuLSIqIiBAJ+x47tAQGBVAotxI9eV1zV39Vfk1tfllpVnFCUE5aWSImI9giOdvVn2nn7aJlY3XlkoXzX+cFjmq4uQOE4F6doB8eq7Jyu5haAwgF+NHcfiqunr5unj5Orp6Orp6dvgH8w04dGd3D3sbBzMjCzMLJyDIrKSCpoSixsi81tjMmui8qoCk8pYyQUBkdlBzAzqCEpvoGJnrQYJy+GrWugpRPFzM7D0NLJwNzBwMxe39TW0NzeyNKRGsz8xnPdd0Bh8Eg/o4LEaElWspuri4urq6ubm6e3D9WPYm2qt3v9wt9+47j+0GLsVj9Gk4zCA+Me6H1UuI6NwiX1b8xd4ss6MwAKQ1FhFgrHlkaU9jOLuiM+oeF3U+je0fCHpOG/TqH7zCp049IkWDScEdczDoV7S/P7E6JYUeE0gMIjEArXTWjaXH9fX0NdXWNDAzgDjg39K1WUldlbW/t5T0YFifEaGu7r6m9o6S5u7Mxp6Mxq6spr6yntGWgaHpk8Iv9U71EYcN47Av40TYJNw2MoDPyehmu6k55qKknJijISnJuHMs5cOrhl5xrvYLPNO1bLzZKITves7kr8GIW/SsPQtLlfBoWBRkaH2nqratpSK1riWBH9mKrWWNbacgl1HSkt3YWTz8HfUcPDw+DT9I3zSH6cfi4Ks2UfUAZO42MbUxqnPDYKv5tr8SuIXUEiLTl5gr8QAvkHBx84cWLXoUNHz1+6rvbghvqTi0p3b6k/fPzy5YOn9+4+ULmtrjp/8eoNWw4WFpaevag8nVcIgRJGoITgSEHAxAgUYEpBKCqMIEAJEjAM4EuAwgg4locbLiQIUJhmYmK6es06C0sb/wC6t28gLTAshBHNCIuhh0YEBocFBjIYYVHRMUnh0Yme1ODn2kanL91au/Xwuu2nl6zdSxSdKT135s4TGw5d3rZ4zVw+AYHffvtj09qt2ZnZVy5d+c9/f+fm5mPjLGBcwL5YHBmLIwnA0QgUDo7AIBAYHl4YYF9ggMKgBTeDwXFYvBCeIARoGEAwjB8lwI8EfSERSWnpmQQcGYPGY7A4Hm4eXk4uAh/3waUz0iLprnaWc2QlxSVlRMXEREWEbty8zghj+vn60X1p1QVlA90DdfmNpSlVxYnFucyMJGpktEdQpCOFYeUOUNju/jMbtQfOj5/S9HQj7W0T3F0T3T1qCwq7W1uTQhi+np4uHp7O7l7Obh6Ozi629o5Obp4ePjQnN29Le2czG3t9E3NdEys/RmJcTh3EwTkQB0ekloUnF4fG5gVGZPqHpfrRE739Y9woEfbuwdbONAtHHxNbN11TW20jSx1DC20DUy09Ey0DUy9qwORFhf+Rvm+u8HiV5STfu3Vh6Z+LT1xWTSv8sUlO4BxqTS0Z/rYclIkro6gtPLWhuw/6AsvJzoBQuB5C4QYIhesACseVdaZDKDwMJUg4+XvFlUaUDYQVdUOBYYiGv5g0zJpCB6Ewm4Y/nzQMaJgdGP7rFLr0+B66HwuFs9qL8npLAApHV9hY+aWnlbc2QyjcWMfKFf63xdS+l2qrq6k+PuGMyagg8YlGAXoODvcODvcMDfeCPhgZ+8tP0ngU/ic0PJY0XNWZ4Eu3mDNfVt/icfNQ+ulLBzg4pguLkTBY5J1HVwH1lrVEF7NWpPuUhtko/BENAxQubIj8dVAYaHR0pHewraW7pL4jq649ra4jrb4jo7m7oKu/dtIKCX+koaEhcPQOTiA74pdSRFqjDa3k534SbGilSbk/IVP511dOWbtdQOkvlTfCriDh5eZWVjLR+nchYWEnzp49cPTYxZu3lB9q3FR/cl3tkfpzrZfaei/fPH/95vH9R/fmL1u768ip8qpKtQcveOASMKQYRMNoYTgAYsDBaGEkEtAwCYHAAwJmczASgePlGUuQACi8aNGf+gbG/gEhnj7+FBqUthkRkRDKiAoKDg8OjmBGxIZFxNo6el1Xfrhmy6FZC9eLyCyds2Tb8g2HF65er/Tmkl3kC+f4Nzped9ftWvbf/06Xkp4RFxd//caN33/n4OWBAZAVADjLSorAYEloLBGGwCLRBDgSi0RB6cLsccDBoIXBMQIwDBJDIpBEcHhBNArsM8B3JAKJIQuJQihMEEKjcQQCmY9PgIeLh59j+pZ50qkRdKqz9aq50gCWxcTFZ8pKv9Z8xQyP8PXxDfL1q8gtGuwZrMtvKk2tLk+tyGGhcIxncKSTX5i1B03f0k1Dy/nRc/fnLwMMDaMc7ONcXdMplNaKiu7WtrggurO9vQ2wg5O9k7ODk6OVtY2NnYO9k5u1vbORubWBmYWOobGOkblvSFx0ZmVkZnVEegUjqYSRUBAWnw+FhMPTaaEplOB4L/8oZ+9QGxd/Kyc/CwdvYytnbWNrTX2TN3qGmjr6r7X1NXUNPXwo3/hLwndA4eHh4bra2rLS0vZv+CH7R6DwQHeLt53RlrXLFy1f+9LQrrqxfewPP0wAhc18i8B188DgyNgiwD/M4OGc6eXHHsSa+xQX1/QmJqUCFC4EKNzzDoUtXOLKu9LqBjMbhiOyaw2c/D3jSsIhFO5hvqdhFgp/TMOfSZP4B6vQZST0jqFwdkdRPkDhvvco3AJQuGGwEYoKT6CCxNBQd3d3T0/PBH8Ug6LL9fUtzc0T/3Ht/wN9hMJjNPzNU+jKW2Pza5jPNJUsnV7V9SQ/ea0IOHjmHCnV+5dSi/zL22KLofILUAWGb6ThPKh42S8UFX6vkdEhcA3TP9TRN9g+MNQ1PNI/+eU+2BodHS0vK4uOiMjNzp7ILyTg36mvq8vPzQUXh2NDP0m/BApTp1D48/oFUbizoyOCwfCnUCor/nExtY8UHhV15sKlvYeOXLyppPLo5TW1J5dVHio9ePn4uc4bHV0tXS2lO+pHrp3Sdnne1FXrHeKzdPNmLGkGCiuOxIohsGJwtAhgYiRKCIUiI5EEQMCAg1kojOflQYqJStOo/sbGJjNnzn7xUhOgsJsHxd2LSg8Bux8dFMQMCo4IZcT6BzH1jW0Onbg6c9FGMblV4rLLyeLzRWQWL1mz78aTmy4pb6iFJtQCk4ASixcOSngyQVhIIiYm9sGDR9OnQ9kRAgIoAQE0YFwEEgeFfjFEOAqPZLUIFJ6VJgHlP4A/QVnCKLwAHJpRhycIkcmieLwgComDw9EwBBpPEpKWmUkiCaPQOBJZGAzycvPzcHAslhFLCA8NcbHYv3zGbDlJSUmJJQsXWFtbMphML0+vQF9aRX7pYO9QU2lbQ0FLaXJZalBiAiUixose5eLPtPMJNnOk6pr5vNGn6hoyrKxjXV2jnV3i3D1aKip62tpjg+mONlaW1paWtnZ2jk629rYmZqYWllbWtvbG5lY6BibaBiZaega6Rha+wTGRaWXh6RVQinB8IT02LzQ2lx6VFcBM9aMn+AbFetIinbzoNq40KyeKuYOXibWLnqmNpr7xizc6LzS1NHX1tfQM3bx8Jw+FB/r7YyIj/by9iya9ggRQaXbCQ8WL8+fO3XnkPI2ZwCoq/8OVUdT21DrLL6qanlAXFF/7Qx2WUg8ea+1Vxi6VSHmdNB3rEFtnr+IGFgr3jmaW1mmyUbh+KL1+OJyFwh7xJRHlA2HFPR9o+B9MofuYht+nSYxbha4d0HB6Yh+dWvDsqXZOdntRQV9xQV98dIWttV96OoTCTe9R+N9GhZsaG8HXf3xMDDgPjg1NacL6FIUr2NPmvpmGK9picypDs8rpFa2xKYW0oGh7ZpJbbjWjsiO+pDkKEPB7FB5PwwB/33fG03A+hMK/VlT4VxNA2Iy0NCc7u4lXkEhPTfVyc0uMj//G/LkfpCkU/pX1C6Lw4MBATXV1RVlZV9dEZ1lERUddunLl8LHjV24qKt/XuKz84PytuzduP77/RPPFa33Vu88uKCnq+T7xz7Vs6K4obEp7bHV79p8rSYIzCWRZDEEShRNHYURRGICPUDE1FBIPzZlDgJbAx8tCYZq/kZGxlJTM4yca/v50ZzdvBxcPwMRBwWH+AYwgeqQvjaHxUn/rrhOSs1YISi4miS0iiswjiMwiiM6av2LLS8cHQUWWUSXeQbm2lDwjh+jXf65fgETiIyIjNTW1pnPw8PDCAekCDgbIi0QTUFgSQGEEmoDEkEEfDKLRBHADLI4MZUSwMiVgCIDCUCqFoJC4kJA4FkdCILEwJAZLIEtLzxAkiyJRWEFhMQwaPAUYDxfPDAnRKGYo08Xsysa5q+ZKyUhJbFy3zs3dlR5K9/D0DPFn1JbUDfWPdNb1VWXVJQcnx1Gi4ikRsV4hUW6B4U5Uho0H3dwh2MSGYWEX5eSa6O2T6OUT6eRSW1Tc19mZzAx3dbC1tbO2dXBwcHG1sbcxMTOxtLICNGxgZKJjYAxo+I2Ovq6huXdgVGRqaUR6BTO1LDShKARC4ZyQ6OxAZholOH4Mhb3pdu7+1i5+lo4+prZuBuZ2mnrGz19rabzS1NY30DYwcveeRBSGKkiEhEx6BYm3w/1dTH/3M4d3r1m/9aWhXdNHuU0/8lyfWdx2zyzDhlbqQi93Cv6xdmdUqJtmbLgOLSN8/FHCfb0gKycIhWt7RushFK7XtHSJr+iEULhhmJlda+gMULg0HKBwSQ+Lhru/RMMTWIUOmkKXnthLpxU8e6aTnd1eWNBbXNDPRuGM9yhcz8oV/rcozK4g4ePhMcH8sKGhIXAC7enp+blf/7+IPoPC4/D3Awp/IOCPabiyLa5pIK2hL7W6MwF0Ot7mtL3NbuhPA5ulLdHvUPgzNMwODL+n4Q8o3Bg5MIXCXxY4bgvy8vwpFGjq5wQrSCQnuzs7x8fGTqHwFAp/Sb8gCn9HxcZEKt66cvniaWUVxTv3HymoPQS++0jj2cs3qnee7Nh/6dpjRY90PWqucXlrbk13gUuCzvZjO4kkWSGRGQAdsXgJNFbsHQqTUEgCq5IahMLsBAk/P5q+voGIiNi9+49o/nRXDx97Z1dfin9QMCMgiOnpE/TwqdbqjXuFJBcRxOYQxGZhBWcQhecIis/HCc1etHqLJdWwtC2tvbcpoTzQO8fAOf7Nhn0reXgRjDCmiYnpdC5ebl4YO8oLDAgYjWMRMIqIxglhCcIAgjFsIMYQiSQRDCtdGAEtE43FYIlkQTFRMRkyWQzKpkDhMHiSlPQMIWExOBIjKCxOBHfng6pMyEiIM0LpES6majsWHFw5c4aM5KH9h/z8KEEhQR5eXhGhcc1V7SODbxtKW5ND0qJ8IhOosQl+kbHejGgPeqRrQISjb7idR7itW6SDe6ybTwrFP8M/KMmHUl9S1tvRmR4V5epgY2tjYetg5+jqZuNgZ2pmYmFhYQr+Z2puamapb2SmpauvrW/qTmVGppRAWcJp5QyAwjEQCtOjsvwZUHaEHz3eOyDKxTfU0TvIwTPQzo1m4eAJRYV1jV9p6b7S0tY1MtY1MvH09Zs8FAaokZ+TkxQf/y2V278jCsdQLKXIGJywjOpjzYiYhNzcnNSU5KSkxPj45LLKH5ufmlHUZuRZWNnQ09w+0NDa90Pd0TNkQSneoRRxxzg9NLkxLDLJzsWr6C8o7BoHULgBoPAIQGGDdyjMKOlhlvSGFY3R8Jen0P2TVegS39FwfEd60jsUzmkvAChc2BcfU2nDRuGWkabGwYaJoXB9XR09MJAZGvotiTdfUWNDA/hHEuPiJrOCxC+r4ZHBGoDCLX9B4XE0PC4w/DkarmiNjU7zdPbRd/TWc/IZs7Mve1M/rSSwrCW6hEXAn6ZJAPz9lIbz68OKAApP1nIV/6Pq7OysralpBZ+rCST5APytrqrKSE2tKCubQuEpFP6SfkEUBofrILgK7O+fyPHPVkJsxB2lKwrXzqrfvvX0ycOnT589f/HqjbbWS83XR05enr987x3DOz45BrQc85r24obuCp9Mo+MK+5FIQQJRHIcXRaKFEEgyDEEUgOMEBDBQooIAilWZATV9Gh+JKEKhUHV0dHE4gqKiCo0W7OsX4OLm6e1LCwgOc3H3u3P/xYp1O4miszGCM7CArcVmCUouEJJYJCm3fPW6/Y80tEqqisBOtvTUhRd7UvKNrJnPF65eyM2HDAwKtrG14+Th5+FDwBBYOAoPRxHYKAyMQJMweGECSQygMGBfLI6MROEJRDAigmYFhgENgxaMiIrKABrG4AUhFMaRJKXkhEXEYeD+QmJCQhJIBA48HVkZ6dDQEIajyeN9f97YsXjhLJnLFy4FBgUG0gPcPTzjotI7mgaGB0YKUovjAxIzGOnpwSnxflExvmExPqHRnsHR7gExLn7Rzj4xrt7xnpRUv8DMgJCswJCWqpru1ra0iGhnW2sLc2MrWxsnNw87RycDQwNjYyNzMzNzcwtjUwtDE3N9I1MtfVNn35DI1NKYrOrItApGQiE9Jjc0Jic4IpMWmkwJTqAExXn5R7pSGC4+Ia6+oa6+dBsXXwNze2i2nJ6hjoGRkZmFoakleOEnb9ocdJgODvb393/LxOTvh8IjrpqqPBzTcGSxNes3btq4YdVKtpYvWLjmtbHn2K1+jNIL28CpfGzjxysspd7Uu6isFsKF/NxMBzfPgvoxFM4orXuHwoMZjcPMnDp9Z3/3+FJmxSADcPBYYBii4b+dQscKDH/7KnQdacl9wf4QCufkdhQU9hUBFI6ttLGBEiRavwcKgyOqqbGxuanpX6ywNV4/q4LErykWCqdUfILCwCwU/isN/3UKHQDoup7kh8/lceAMjEcTiFgCCYsnYsCmoDBh1jwZP4ZVVWcCQGGIhsfFht/T8Puk4fc0DFB4Kio8aQIkMTQ0NHGemKCmUPhX1i+Iwr29vTmZmSmJic2NjWND/1YAhdWUryhcP3dXRf6VxhOdN5r6ujpGBnoPHz1esWGfzKJdtw3uemXrhRe7dw909I90MfM9DlzaDuPHYKHqvDgePiQXD5yLR4CLW4CTix+Ym0uAhxvy7//lJhKE/ShUPV09BAx59swFql9gcDCDSgvypQbZOnneVLn/5+otBNGZCKIUTmimkNQiYdnl4jNXL1qx8+IVVYofvaOj6+3I29rWiqhyT78CE98cw0emt8SkpLl5Ed4+vo5OLrz8SD4BFAKJR6CJSAwJmI3CSAwRoDBRUAKBJMARUFowEkPAEAQFRSRAnx0bBnCMwZIEhcQlJOXIgmJINB6NIYhBtdIk4QgMgSQsKiaFxZLgcMzs2XPCwsICbI0ACqsfXrHhz1mqireCQ4KoAVQXF7ekuJzONnBlMlyWWZ4TmZ8bmZcWnAxQONaXGesbGu1Nh0pJOFGYdu7hju6xHj4p1MDMIEY2Pay1uqazqQWgsJOdjZmZsbWtvYu7j629k7a2tqGhnoW5mZGh8RttfX1jMyMzK21DSydvOkDh2JzayPSKsMSikJhc4ODI7ABmOi0k2TcozsMvwsUn1NGLDmjYwy/UwZ1qau1sYGqtZ2yub2phbmNvYeNACwqZvCU2/pG+HwqP5iYy32i+fvHi+eNHD+9/0L3bdx5TQhLGbvVjNMnF1Nq6Btu7x3AwIz3d3tUzv769pvcdClsBFO4CKJwOUDgXoHCA2zsUHk/Dfz+F7uNV6P5mCl1aal9wAAuF8zoLi/qB4wAK21KhXOHvgcLfS3U1Nf4USsTE8iz/v9HwyAALhWMqWz9GYeAv0zAUGAb9uu6k13pqs+ZIz5wtNXuuzOx5MrPny6LQ8N9++23txqXhKW6VHfFs/H1Hw6D9appEPRP0p1D46xocHOzp6QEXhxON5oL7/wJpQlMo/CvrF0Thjvb2YH9/T1dHlMrQAAD/9ElEQVTXiVeQiI+NVFO5oXTryj015dfPNfR19Qz09fX19K/fVJ65eJPEvJ0nlW84Jb6JqfLuHYLykmvbyi9cPyvAj0CjsEgUhpsXNp2Lj5OLD6AwNw+chxfJxw9FhWECGC5OAWEhCRrV38jQCKDwjh27Pdx9mMxo/6BQMyuHi9eV5yxZSxCbhSRKYwRnCUsvFZ+xZvaf2w+euGlm6ZSXDwWDgQID6I+01I1oj+yjXuu7Pjh26qCosDR4ICcnFzc3DzgSzyeAYXEwGYUTAviLwQuyciTIWIIISUgKgxOGwSEUhhAZTxYSlRQWlsTjhTBYMhIFhYeJJBGAwmLiMlhwAwwBcLComDQCCdUnBlhMIAoBYl60+M+IqChXw9fquxY+O776wPpFTx6qh4TRffx83Fw9MlKK2puHhwZGqnOqMhlZqcGpibT4RGpskl90om94rGdImKOfv7mLn5FNoIVdhJtnsn9QVmh4LjOyra6us7klIzrGyc7azNzY3sHJxcXT0sJKV+eNqYmBhbm5oaGpjp6xvrG5oYWVobmdizc9Irk4OhtaZy4ssYwRV0CPy6XH5tCjs4MiM6lhKZ4B0S6UMHvPIAevIHe/EGdPfysHD3NbZ2MrW2MrOxtnNxsnt8AQ5uRFhX9iBQlwWh/5VD/+ZD/JKDxe41G4ru9tRlk9hMKVAIWH0huHwnJr9V0C3BNKGRWDoaUQCjMACn9L0vCXp9B9NmkY0DCEwoGFzzR0svM6CosHior6Y9konDGGwo0NAxoW/x6FpypI/AiNoXAzQOG4jziY7Q80/GE63V9oOL04MDzJjZnoykx0A53odC9jm2cyM8SuK54srGeyikhEfTsNs1B4Kir8NYHz2VQFie+uKRT+kn5BFP6OFSTi46Pv3L51W/nWo3t3X794YaivD6yp+ebwyYuS8zZIzt+5dNMBdRMFWpZ1Wx9U0r6hsYFOD1mxYhUcjkBjcAgUDgbHwOBYGBwPH1t8joRCEoH5eBFiotL+tACAwnA4ct68hbq6RlR/ura+2eETF2YuWIkTmYkiz8AIziGJL5Ges2njjtMvNE2iY5MrK6G1yQaHhlxc3VevWi8lJ3vk7GHlO9cvnTu5c/02WcmZ3LxwS0trHx8KjiDMB8MCDkbjhbEEMRxRDBAwC4iFQZ8sLI0niSMQeCxeEEcUBihMFhIXF5cREpQgEEQxgJ7RRChHQlyaFRgWZQeJARaj0HgsjgSYGGzicILLV6yOjY83eHZPYfNs7fMbzu1apa35ghEW4unt4ePll5dZ2VLfPzgwXJ5ZnhKckhKalhSckuCfFOsTE+4SwrCnBdt6+1u60swcgm2cIz18klgoXBAV19HY1NnUkpuQ6OHsYG5ubOfgYG/vaGpsZKCnbWFham1ta2XjaG7laGJhZ2xtb2br6uLLYCYUxkAVhSuZyZWhCcUhCXmhCfmM+PyQ2NygKEDDyZ6B0U6+oU4+dA8qw9Un0M7Vx8bJy8rR3dbF08nLz97Dlx7+rStrfgcU/lkVJAb7ezs62js+UVd3z8DQj60h/wugcEf1OBSOrexMbQQoPMxGYbeEMkblYAhA4fE0/C5p+GMa/ker0I1bdwNC4aD3KNxfVDwQG1dlbesHoXDrSFPTRFF4qoLEj9AHFG6J+ywNvw8Gjw8Ml7e8S5Noiq7uTGjsTwNu6EsFbhpMr2yPU1A7v3zNorgsn+quxJImFgo3Qig8noY/oPA4Gp5C4b8VOJVPVZD47ppC4S/pF0Th71hBIjEhVv224h0Vhcf31TVfPDc2MDDS13v+/PmWvcfF5m6UnL9VTG7VgtWrbj66lFea1dTcZGhklJ6eQaH4LV26FAYoGEvA4QUBViKRgIOJMBhBQAAvIIAVEMBwTOcVEhSjUf31dPV4ePiwWOKxY6fV1J9u3L5fTHYBQXQORnAWgjiLLLl0yZqDN5Q0/IPCc3ILsrNzq6trW1rbbGwdFi5aCoNWy8AuXbz68O4jR7fv371+22zZOTy8cLCbNP9AQRFpPhgOjRPGEsXwJAkCWQJHFAf4CzgYTxQnCUmThaTQGDIWS4bmzOEFCSQRMTFpcTE5ISFpIuBmrCAeLwR4V1xCFhAwuA2JLCosIgmYGIsjg46IiBSJJLZ6zYaklNSnaoqnloqb3dimeGyTkZ5WSAjdw8Od5hdcnNvQWjcwODBSV9xQEF+SEp4d6BnpbEExfm3zUl3r+e3n+o+1KJbOEW5+kW6UGE+/OF//FH96fkRsZ2NLR0NzUUomneZva2Nja+9oY+doYW5hampqYW1j6+jm4OJj4+BpZu1sYedu5uBt4xEUyEyNi8+PTciLSMwLicsBZkLVhQvp0TmBERmB4enU0ETvgCgvWqSPf7ibT7CTO9XZzc/Jg+rmE+jpF+zmG8iMTpo8FP5JFSRGkkK8VVWUlJVVVN/pttqdBw8fKKvc8wyIGbvVj9EvgsK1faPpEAq7xVR0pTQOpjcBFK6DUDgRoPAQvbT3XWCYRcPvkobfTaH7JE1iPA1/wyp0qWl9QUEFEArndxQAFC4ZjIuHUDjtHQo3TAyFpypI/AhBKNzOSpAAKPw3NPzFpOH3ZpVXiwY07OxrICJGcqcZ1fUkAxQeT8NfTxoubAgHf5pC4a8IHLdTFSS+u6ZQ+Ev6BVH4Oyo5MeHR/Tv376g8eXDv9fNnxvp6xga6T58+Xb3loMjsdWIz15JFZqOxwqISUp5enuBbY8eOHZcuXqioqPD09Jw7by4/vwAWS8BgAARj+fhQvLwobh4kNzecmxv2+385SEQhKpWmra09nYOLhxcmJTNr9vzlJPGZeNGZgIPh+BmSs9YcPiXv7EbNzS+JT0xxdfWKiU2oqqo2MjKZPXsBjB+BggngkOhF85Yd3HXk2K5DuzZsmzdrAR8v4vVrLTo9VEJqtgCCgMGL4iAOliKQJYHxJGAJPFGCKChFFpbGEUQB8hKJoliCEI4gJCwsIS4uJyYmJyIiQyKJAxQGBCwiKiUhKQdagMXABKIwGAQoLComTSKLrV2/JS0jU03hxmZZrMnVLQ/Pbzc31g2mB3v7eAcFhpfltzZX9w/2D9eVtYQHplqb+dy/p3/+gvrOnWdWrdixatnmzWu2PFK8E+5OTfYNSfAKiPOmJVACcphRHY3N7XUNZanZ8WFRLg7O0DIbdo72oLGzt3FwdHT1cHDxsrRxNrd0tLZzN7Zx0bdy9PIKiAE0609nBofQgpjeAZFUegItJMnHP8bdl+lJCfPyC/OiMDx9Q9w8/e2cvGzs3e2dPB1cvJ3cfJzdfRzdvEPCoicvQQKgBjhTpyQm1tfVjQ19Wd8RhaN87c+ePnXq9OmzLF26fPXa5YsbVi74/TeuS7eNxm71Y/RzUdjO1TOvvp2Nwmllda+t3aIru5IbhtIACudBKOyaWMqoGqKX9YWMo+FvnkI3RsPjptB9fhW61PT+IFZUOKegs6Ckv7BkIC6hysaOClC4mY3CjRNC4Ya6utDg4IiwsPb2Ca2Z0lBfzwgOjo+JAddsY0P/hzWGwq1QgsS/oGEAvjWdieyoMOS+1KaBNGAj66doLNLJWw+gcGkzhMJjNPx3U+hYKBw1hcJfF7iWAx+Htra2iST5APytqa7OSk+vqqiYQmEbWllK/tTCy5/RL4jC4HAFZ+++vr6Jrxmempz04umjh/fUABCDjr62ppGe9oMHD5et3SUst1JI8k8CURyNxPFy8+to64DH3bVrJz8ft5qaallZqYuLs5ycLB8fLxqNgcPRPDwwQMDcPHD2FLr//mc6iSgIUFhLS5uDk4cfhkHjhDEkCayQHJIoixWas2rTQS09i8zs/MzsXDMrW9U7D/0D6Hl5+S9fvJSSlOXl4UfB4FApCn7Y3FmL9u86cnj34R0bty+Ys4ifD/n4ybPwiMiZcxbDUWQsURwvKEUSlCEKSgMTyNIEkhQw6JOFZYhkSRxOhEgSw5NE8ERhQUFxqGqEqCywiAggXVFAvULCEoB6gQENgz4wFBIWlQIjBJLIug3bsrJzleRvzCfxvzq5SvPqLgdz3RA6uBj3jgxPrCzqrC/tGuwbTorLUVPTP3FKbfeB68tXH5o5Z7XszEWSUnNEhSQ2rVpvpKEV6uwT7xucDNDVLygzlNne0NhSVV2SmJoaHulhY+Vgoutsru9kaehkZeRiY+rhaOloaWSup2mlr2NpZGhmZGRvbcn09Ur0caMYaLobajnb2JhbOZlbOZtbuRibOegbWesbWegbmOkbmuvqG7/R0nn56vXLVy9fa2q+0dbW1NZ6o6P16s1rR2enbzxnfp9pc4CGBwcHRya1gsTbrrbmMpbKKyobGhrKinIcTN6sWCCDF5pp5Bw6dqMfo18MhV2jq7qSG4dSIRSu1XeFUDi0ahCg8F9p+H3S8MdT6N4nDX+YQve5pOGPaBhC4WBWgkRBZ2HpADC4voVQOBNC4cYJo/DAwEBLc3NrSws4usaG/pXKy8rsrKymKkiwNW7a3DsUhmj4YxQGZuPvRzQMGDqlwD8kxikkxjEk1gmYHuNo76G9Yu1iFAYRHO0AoTDEwYCAP6VhKE78EQ1DKNwUNVVMbXIEvhXAWXr0ZyfNJxV0u4R+txkj/052AeVphVOZV59RQVWvY3BV589ZYvzzAqducAmXFB/fNOEKEmlJ8S8fqD1SlX+spqBxT0Xr2X39V0/VVZX+XLaeLLGIJDwbhxVEI1C83Nzqd+/09vZevHCOc/p/8Vjkg/vqgIYtLcwlJUT5+XmQSBQMhuQXQPLxI3h44dw8Ar//dzqBQKb4UTU1tbh4YEiMIIYgjiJIIokyM+avvXLzTlh4bFFJGc2ffvrctYNHT/v40bKzc+7duy8sJMLHw4cQgCMFBNC83Che3hkyc3fvOHRg1+Gdm3ctXrAUDN+9ez82Ln7Bn6sQaCE8WZIoJE0SkiULy4KWKCgDsJgkKEuGRmRIUC6EBIksQRSUIJLFyGRxYWFpYWEp0LLBlywoNh5/xxuM4PCC6zfuyM3NV7ilIIHiubN7kf7NPZ5mryL83QO9nZNi02rL+xsqewb7h23sfbbvubZr/831m8+sWLd376kD51SPHzy3e9HihXPk5uzatEP56i2jl9oUK8doT7/04LDW+ob6krL86JhMZoiflb6LtpqblqKLloqrtpq3/j2KyQM33dt2LxTtX952M3gRZG+REuBdzKTFOOobKJ7WU7nkaGJgYWlnZGKlZ2ihrWfyRlsfvM5Pnj65f19d/e7t26oKKiryyso3lJVvqqjeUla5eVtNQUHxmr6+1jdePn0fFP52fUcUHq++9jo7w+frV6/Ytv+4g09IV9+E4Olv9QugcEdV72gNhML1AIWjqrqSmoZSmkYYAIXdAAqXQShc3gvRcOlXafgfTqGDaJidNJzSmZLeF0QvfPpcJ4uFwkWlgxAK21PTsypa2kYamyeKwt9LdbW1gVRqVHj4VAUJoHfF1FgoPD4w/CkNv8Pf9zQMBgHpamipSEqLAEvLiknLiYmIkeEIAS5uzmNn9mRVhFS2x7GyJiD8/RYaHosKT6HwVwUuC7u6uvr6+iYYzQV3BzA8+SHhqqqq+LjY6KiomOio5MQYIzu6urZ/dHRUbEw0GAGd7KzMH1r2e3BwMDkllUqj+VIofiypvgnWswqgUv3Ati8FaiOjotom9gPU/656enrbwJNv7+jp7krIrDb1zCqrau7p7mxr7wDDg4M/9vv0b8WuIOHh4jLxChJpceEvVS4+lT/1SuWCptplnXs3jR4rPZC/uGjhMoLQTDxRCovGoxFwbs5p58+daW5qUle7Defj5uX8g4RHv3rxrKa6Wl9PR0xUkJ+PB43GIpFYfgEULx+gYdhvv/1OJAkGBAbo6unz8GP44UQ4WlRMZtHuA2ccnL0LCktSUjMfPdVcsHjtrr1HA4Loubl5CorKeDyJn5cfBYchYQJIaIkLTiQPl7T4jB1bDhzYeXj3lt3Ll6yCI7CKSqpJyanLVm5EYoRZ0V+Ig8nCcsCAht9hMeBgCIVJJElBsiRZUBIAMZksISQk+Z59gQWFxEFfUAgYyo5gj7NuAP4qgcWRN2zamZ9fpKioLAjnvbJ+lsnNPQFmT+J8zBkeVtmJmQ1Vg8UFzS3NHXom9pt3X9594Pr6zUeuql+2DH3ukvjGNuKF4rPzixYunCE9e/GCJVvWb758/LTho2dR3n7tDY01BUWpgcFpgVSGnZ635nXv56e9np/zfXmJpnmFpnXF5/VFV42LPtqqSR6WBSF+pcyAHH8XH+07WjcPm9y76WxmbGXlqGdoqaVr+uqNwcvXOq9eaz598vjBvbvqd1XvqCmoqt5UUrqmoHBVQeGakuJ1VRV5+ZuXAQqDq/+x9/6r+g4oDKC7trq6pLi4rbV1bOjL+gEoPJQeS79yct/MWfNOXVOLz/z7qXsT109GYTfPvIYPKPzK2i2qqhNC4eaR0Lw6PYDCSWUhVYPB5b0h5VBgmE3D46bQ9X6loMT4pOGYv5RX+3gKXXJGXyC9AELhws6CsoGissHYxGrrMRQeBShc3zjwbAIoDOX4dnYCApjgj2LgK7a5sREcnBP5cfn/GwEUrh2PwuNp+CMUHhcMfofCMfU9yc/eKItJColLCklIiwCLigvKzpQ4eX4fI96loi2utDka4O+7wDBEw+/xF/K7NIn3NDyVIPG3AuRaVloaERaWk5k5wQoS4LIwJyuruqpqMmk4LS31zl31I8dOHjx87NCR44cPH9l//Mbek7dB/9ARMHLs0OFjFy5ednV1nUgm9FcEniyF4nvi2JFTRw+eP3Xs/MmjwBcU9S9cuXP+xOFzYPPUsTPHwc4d0NPT7ezsHLvb/w319w8wmeEWFubGRoYmxkbmpsbPXuhcU3mprWtgZmoMRoyNDe3t7TMzs8fu8DMEvggimUxw4VI14QoSaVH01zcPvbi6V0fpmOHtU8Z3zlk9uKxx4/iiufPxJEk0RhgFR6Lg/Fwcv+/eua2iolxX+w1CgAfBz83D+YeoMMnQQK+mpvrVq+eCJAI/Hz+ClSbBxSMwbTovFy/Ppr1rU7MTfQI9Zy+biyGKr1iz46WmQWp6dl5+sYOz1+Ydh0kiM/fsOx4bm5Cdk3PlynU8nszPD0ejUWg44GBeJHggPg4cnG/x3MU7Nu/bt23/ns27Vy1fh0ITr1y9mZqavnbDDhRWBPCuoIicoLAcaMnCM9hAzDIbhWXIZCk2CgMLCkL11FikKyEkwmqFIfwFBhxMFhRj0TBgYihULCwsgcWTN27eWVhUrKikTELCDv8prXd5e4ipWrTzc6a7cVlWYVlBu5c7Mysr39LOY8feyxs3nTpw8pR5sIZfgSElz4haaGIb9Xznkc0ykjNnyc6ZM2POgjnztq1dZ6Ch0VhVVZNXmODtl0bzS3Czomnd8tE47ffyfMCbK8C+ry86PzvjqHE53s2wOj6kPIaRQfOMdjR2falionZB5/aN+0oqN26onT534/ipK0dPXDp28uKZc5cvXLxy5fK1GzduKNy6qaJ8S+22kqqKgoqywt07qvfuqSkp3jQ01J08FAaoER0R4evpWVRQMDb0ZX1fFO5qqXWz0t2+YeXqDTuN7X3aeifETN+uXwmF61go3JXUNJjcPAyhsHuQe1pFWN0ooOHQqgFG1SCjcohRORhWMcisGAwvHwCOAC4biCjtjyztjyodiC7pjy7uj3nvor64ov64gr64/L64vL74vN6E3N6EnN7EnN6E7N6ErN7EzN7EjN60nOHg0JJnL/RyS3uLq0ZLq0YTU+vtnAKy8qo7ukfbOkYBED+3TI3Pqh/b9X+oxoaGyLCw2KiojqkKEt9PEAq3p1a2xLLx95tpeCwwXNkam1YUwIh3Bg5LgOqpMRNdI1M90osDK1rBbWJKmsZQ+OM0CTYBf5I0PIXCfytAcuwKEhOsjQ0uBdNSUjxdXRPi4iYNhVuam+VvKVyXv+3g5u/qE+LiRXf2CvKkRvgExrh4BUP2poPx1zpmR4+fDgoKHLvbd1VBQeGp40fN3jxNY1CAUxm+wOnRkakR9NRQX8gMSnqYn5+D6YnD+wMDf8g+/LLy8fE9c+qQmvKFJ/dvPL5/A7SP1G/cv3v98T2oD/z0wU3566euX72Ynp4xdp9JF7gCBFdxVZWV3d0TPVEkh/trXN755NzmV1f36MgfMFA8bKZ24vm1g4vnzMTihBFwrAAfL5yPh3v678v+XJidleXs5IAQ4BXg44LDeHm4pkmJCVtbWVSUl99Xv4tDo3g4ubi5eKZxcP8+nXvljsXmAc9rO0qyq+Nf2d+9fe8+PSSioKg0ODTyqry69OxVcKzksZOXUtPSE5OST5+9gMWSEXAMBotBo+EoGC+ajxfFx4PgnbZYTvDy0YO7t+7ZuWnHrk0716zaiMMLnz57KSU1fduOgyisqKDwDCGRGSwUnsG2kOhMFhazQ8WyZEFpQMMQELPiwexECHZImB0AFhaBzA4Js5kYoDCrhaLCW7buLC4pVVBUJqKRm+eIPj6+JlD7OsPidqSnRW1RDSM47erVB+ERsU6ulG07zixfvu/KnSteWXr+uRbRJb5x5TR6kYP8o7NS4tJS4jIzZGbMkJ0hIy51ZM+e3MzM+tyCVB9ahn9AmIOZ8e3TOvJ7DFUOGaseNVQ59vz6frVzO+zf3C2LC6pPjykMD4x3s2Ha6Dk+V1Y5uWfv+jV/zl82c+ZSaekFEpLzJKTmSckukJu1eObsP+fOXbZkyZr16zfv33/g4qWLgImVlJTU7qip3Fa6eOncy1fPJi9Bor+/nxkS4ubklJ+bOzb0ZX1HFG6syLt365yctOTW/ad8Q2Lau/tGhoe6u7o6Ojv7Bia0Ptnf6ueisK2rZ+47FE79gMJDEArn12k5Ua0js/0KWn1zmnxzGoEp2U3AflmNVOBM4AZaZoN/RqN/OnBDQHpDYFpDYGo9cFBqfXBqQ3BKPTA9uY6eVEdPrAtJqAuNrw2Nq2WwHBpTGxoNXBMR1+RNSXv4RDM2pTIpsyE5szEkPNfU0iMqLqukvKWotLGgqOGhcVxC9s+uIDE4CGAaHBiTGQn7ZfUehata4v8BDb9LkyhriqntSmwdzup5m9f7Nr9tJLvzbS7oNA+mg9uwCBig8Bdo+EOaxAcansoV/luB47YwPz+QSk1JTJxgBYm05GQPF5eESawgERwUdOb8FXpkWk3raE3LSHXLcDVomwermgZYfcjgT0XVXfcev1ZWVh6723eVp6fXtXMnusszRrtrRjoqx9xSPNJW9mGzs2qktUxP496rly8GJra85f+QauvqL108a2f6qKogsKogqDI/EBj0awqhlr1ZXRhUnEm5r3peQ+PZ/wen0LgQ3zsn1iodXHbn2NpHpzdonN/y5trOB+d3LJ4rh0IReXkE+Lg5YTzcPBx/iAqSoiIi/Gk0Pk5Obo4/EDA+wMRgXE5awtHevrS4WFnhFlKAf/off/z+x3QMCatueoVRbt/QXdHQU1HWll1ZWxEbl6TxWn/V5gNYobkY8qzr8mrZOXkxMfF7DxxDYclINBFa5wKNRsH5kQI8SF5eFC+nLJlf5cQqswfXDmzbsXX9tp0bt69fs4lIkjh05FRyStqBQ6fRWDHAwUKigIDHUBhwMBuF2YYyJQSlBSFLCbGyIADsjs8G/isTsztQpgQbhTFY8tZtu0pKSuUVlAgY9IqZIrd2L3F+cDLYRDU1yLu2pFlH12ndhoMhIRGu7n7rNx1dtHjHlfuXgoqs8hsSW7rrWnsa6rvLaEzPs2fOrF29dvasOWIi4iJkke1bN6emJpelp6X5UMIcHB/duLR12eyNf0pvWzlr28o5m5fNWTJb6tD2DcGudnUZ8eVxYVlBXjEuZn5Gz5VO7ls8U1ZMSIJEkCDgJfF4CRxOHE+QAMbixIVEZWfMnyc3Z46QkJS4hNzS5auOHD12A1y7yd88ff7M7r07ARAPD39Tes93QOGfVEFi1Ev/zm+//TZdAH/w5MUHjx4/uKd++7bqbVWVWwp3/v8upgZQOGdcrvArmw8ozChq1HKkvHLwM6REGPowDb2Bw4y8gJmGnmHARp5hxh7ADBNgd4aJG8PULdTUjWHiGmriEmrqEmrGtnOIObBTqLljiLkDZAt7OmS7EAs7uoUNZGtbhr6Bh4KimqWth62Dj62Dr5GJ44PHr80tHdw9qW4eFGdXH4WXgcl5/7Ji0VQFiR+hMRRuja1qZaHwexp+j8Jfn0LXHAtuU9UeH5fl404zNrHRsHF5ExztkF8dVt0Rz8ZfiIbfpUl8mYbH4sSFjVMo/Pf6360gYW5mdlv9aVldb1ldX3F115fc0Dnq4B5w9tz5H5EjYWZmqvVEfaSxaKg2f6Amj+3B2rzBd322R1pKA12tHqjfaW//v/IzFDM84ua1Mw1loW/fFo8O5YwO5X5iaPDt21JmgJnirau1tX//FT9eHw6yiR1u4HD9XhUkYujeyoeW3dy9QPnQcvVjqx6dWqNxfsPdM5uXLZyFQOAA9PJwTQcozDvtd5QAb1Cgf2JynNwMMV7OaQK83ChAw/w8XNP+O2eGrK+3V0lx8ZVLF/m4uQGHzF4iZx76JLDIqqwld2C4v+9te0pR7J5Dp0VklyDJM8XklqnceVpcUsZkRu3YeQiJEUZiBHEEMhaNRQsgULz8CD4eBB+njBDyyr4lgboX/XQVju3euXHt9p0bdmxau0VIUHrHzgPJKamnz15FY0XJgIBFIRpmQzCLg8fgGGqF5UhkGTJZmkyWFCRLCAlCNAx49yMO/tTsCDEWK7h9+57S0rIb8kp4LG7ZLIlzmxfqye9lWD8rTYwvzW+6cPnegkUbAwMZzm6UpSv3zpi17urt60XNKd2D7cVNmUVNac291VW15QBUfH18nz5+um/3vk2b1z14rVhaXZCezQxwt7h/7coMcTEkTACHQhGwKCwGAYfziwmSNdRuF8dHVyVGFoT6pfg40K21n986u2CGNJ4gRiDJkEmyZKgFT02GRJLGYcVEJWccOL/7jt5VhRfn1+1YgyMIY/FCi5YsP3Ls+P5DBzdu3bR4ycJz505/47T77zNt7mdUkBjxM3smJyOzZMWabdu2bVi3bs2YVv355/o3pt5jt/ox+sko7AZFhSt7R6sBCpdDKBxR1ZnYNAQcVdnhGpFmQY2w8o+29o+yAaYBR0KmRtn4RdqybAdMiWDb3mfMDsDe4ZC9WPZkOnqGQ/ZgOroznYDdmE6uTGfIYU4uYa5ukWbmPioqd13cKB6e/p5eAdY2Lk+eaDg5OQXT6UGBgf7+Aff0makF/7Ji0VQFiR+hD1Hh1vh/QcMVrXG1nYkB4bbbdq9DoRFwhAACCZOZIX7v6Y2scnrV2Jy5aChjmFVS7R0KQzT8Hn/H03AR6Eyh8GQJkPQkV5AwMjK8++A5QOHimu6iqs4vubZ11Nkz6OKlyz093/lIAAxmYmKs/+Ih4ODxKPypRxqLA92sAQr/iMlzH117fLr50cjkyMPD89kjha766NG+zJHO1C95dDAnM95dTeVGbl7+2D2/opHh6vKSytqxUg/tzc319ROt3/wdK0gkMCj3T69TO7Li/qm1T86tf35x46urW+5f3LZyyTyAwhwcXNwc02BcXDAujiWrZOJSw5s6azXMlRavlIHzcQMDGgYtoOElC+cHBwXm5+UdP3L0t9/+M2eZrG3Ec79c49gyWmlLVmod3djvuajsIgGMjMy8NS/fGJaWVgQEhm7eug+NEcFgRHBYQQwSjhbgQfHyIbn4UDycM0SR14+u9tG7len2iGl97+LxQ6tXbN6xYee2DTvFxWat37AjOTn12g0VNFaMhcIzRcRnCYsBzwYWEgUddmAYSh0mkqWJJCkySYKNwkLv5sa95933fXaHbXZgGIMh79yxD6DwtRsKOBzxz9kyxzYseHJxe7SbUXNRZWxs0cr1h2bNXRkQRPemBM5funXZyt32zm7dAx0lrenBxbaBRZau4UaabzTcXdySk5LA65OUkBQURk0pCW/rbUypDLWmPVu1aj64ePjPf6dxTufh4ODm4uYk49EXjuyLp3rXJccWMahp3raRdtoGdy6tXSAHF8CgsFJYvAweL43HSeJxUFQYjRJGoQT3X9hhGfbUI0PHK0NP1+vekjV/whEEsoj4gvlz586WkZIWJuDhu3dsmlQU/nZ9xwSJjpbGosKC3NyczMzMjAzQsJWRlpZZVds8dqMfo18HhVMhFHYPr+pKaBpMaBiIr++Pqe6OqmiPKm+PLmuPKWuLLWsHjilti4XcHlvSFlfSFl/SllAM3AqcWNSWUNgKnFjYllTQCjm/NTm/NSUPuCUVtLmtKTktqWxnt6RlQU7PbMnO7WSE5Tx/oV1U2lhe1V5Z3ZmUnG9paQveEXAFD77Suru7jDzykvP+fjLlD9VUBYnx+giFx2iYjcLjafizKNwcU92RkFEStH3P+unTp4mIkRctnTN/0UwMDoVAwV7oqLJYOfYDDf8lMPyBhtmbbBoGKFzSFD2Fwl/X/24FCWNjozv3NUpre0u+isI1raNOHoEXL13p+d7Xq+CpmpqY6D1/8C0oHOBi9eDe3bbvFxWuqqpKT09/P9uhrKwMfEWBtxL0h4eHS0pKwOb7p9zZ2Qk2K97NDGttbU1LS6t791trY2NjSkpK08SyxT6Sq5v788eKnXXRo71fReGB7Ix4t9sq13Ny88bu+WUN9rQ9UzxzVulZTSf0/Rjo4mpg5D7BA+47VpBIDqdpXNr28PS6pxc2Pb+4+fXVbdryu9UvbFuyYBYSSeDi4uHi+INn2nQ5OeEnVtcKGlN6htrDS92eWF6TkRPi45iOFOAFNAzj5eKe/t81K5aFM8PAe7Rj204hcYKWx23/IjOfTCP/XAu/AqO7RvKCEgtnzFuvZ2RVXlHlSwlYsXorGiuMw4lgMYJoGArFx4XgnSbAzYnk410gSVI+vZFmppLq8zrL51Wip9bdm5dWL9u4bcOuXVv2y8ksWr5yU1JyqqraQzSUFTBTSGzWGAqLz4EMMTE7XViOJCQDrb5BkiASxcnAJDEymT037sNUOXb/g4WgljWFTgyNIe7etQ8cqFeuyWNwxAWzZPeuXXD33LZEqkNPXYerW/isxet3Hd+bmBEbmx5zTuGqkaltRVmNN8VTy/G+adB9LXeVQxc2zZAR/3PBQnCRoPnqdXxybNtAfd9I5/DoUEV7LiPf/YLCCSkZWVlZuRkyM2fNmLNmxYo78lcjKO4NWSm1qXFF4YHZ/i5RzoZPb51dvWTxokWrV63asXrN9tWrt61cvmX5so1LlqxbuGDl6vUbTXxfMCscA/It/fKM/fKMrtw7QSQIAXyXEETPEOaXE+YVx/73+O5Vk4fCP7uCxBf0I0/3v0KuMBuFU8rrX9q4ARSObxqKb+hPaBxgZ0okN4+kNI2kNg2nNQ2nN42kNw6nNw5lgLZhJL1hOKNhOKue5bqhbMjD2bVDOcA1wzk1oB3KrR7Kq4KcXzVYUDVUUDmUXzGYXz5YAFw2WFg2WFQ6WF41GhtfrvnGoKF5sK1ztL1rNC+/1tHRtbq6amxf3761oJT86wQJcARPVZD47voUhb+dhoHrupPNHV4ikLD9R7cFRzkkZFPis3ytnF/LzZKct2hmUh61rjuJjb+fo+FP0ySgmXNTKPx1AXL9360g8Q6Fe34mCpv+NBRWVVUF3/n+/v6gD85jp0+fnjFjBntmXn19/f79++Xk5JKTk1m3fUulUsHmjRs32JsODg4SEhJPnz5lb75580ZYWNjMzIy9+V3k5u6h8Vih4xtQOD3O9bbKjdzcv48Kd7fWXTm4kii33D0kFWzaa2opKulO8Gj7jhUkUiICXlzZ8fjchueXt726sv3N9Z16SgdUT2+ZLSeFRJG5efg5pv/O9ce03cfWeGToZDVEdfe3x1b6eaTr7Di8ko9zOpyPm1XqgQ/Gw8nD8fuWDesT4uMTExK3bdt6XH6Ha7KWf7GJX6GhdcTLrcf2/Ll8p4W1S2VVjbWN0+Kl62BIEgZDQqNxCBgSzssP5+WE800j4wXW/Cn7+OaRENvHGTT9dKpBkq9ejLfJy/t3N6zavnX9rt07Ds+etWzB4jVJyWmPnrxCYcXYKAwsKDZbSHyusMQ8YfHZAIWFRGewS6oRBaUIZAkCUYxIFCUSREgkUbIgMHtiHITC7wpHsCbMCUpAhsZFAQqj0Pg9u/eXlpZdunodgyPMkpXctnKO+qWdUT428cxEBaVXmw7uMqQ8LmnOqGwtSCwIr66pcXby+HPx8jmL5LbsXbZ09SwRQRwRixYk4kWFhWbIysnfvxhfRStqTu3qbxseHe4d7swrzgIfAW8vL3dXNxcnlyAqrSwvt7elaaC1uaexrr26oqW8qKYgM4ZJp/j4UqnBgQEhAf50Gi2Y6hdE8fX38PBxsHf2pfoW12fVdZfk1MVRs8G1h/FzG0UpKQkiFr9mnsipjRJnN4mdWE14fmvP5OUKA9SICg/38fAozP/7z8nkofCPFAuFi39iVDjnPQqX1b+0dQ+vhlA4tqEfouGGAZahTmL9QFLdQHJ9f0rdQEodaPtTQad2IKWmP62mPx24Grgvo7o/o6o/vbIvo7I/E7iiDziroi+7vC+7rC+nrC8XtKV9OSW9wLnAxb25Rb3F5cNRsaWvNPVqGnqaW0da2kazcirt7JzLy8rG9vXtWzOfon+fK1xfHx4aGh0R0fF/tdjnj9DwyMDnUfg9DbP7LZ9H4YbelJvKp0XFBcOT3NtHspsH0oGbBtJe66nh8GhKiGVDX+q7BZkhGh43hY4dDP6YhosawqdQ+OsC2JqRmupoaxvBYEy0ggRr4eXJnDb3zSg84vj/XVQYXLesXbv2t99+MzKCVj8dGBgQFRUFmyYmJmCzsLAQBoOBzfc1K/T09MDmokWL2Jt3794FmwCXQX9oaOjw4cNgU1FRceL5su/1DoWjviMKtzdUPL5zdcuOfddvvwQM4qZvoHrbcIJHG3glwddBTXV1z4QrSKRFBWre3PPi8rY3N/fqKhzQVzpkpHZC6eRWaQlxBFqYhx8+bdrvXBzTzt/eRysyjSjxKmvJYRS6UgtNz6vsAfjLz8UB5+NF8PMh+Vk0PH3azq1bk5OSoqNijpw4ePb2AT3fB5ru9w9eO7p511FP74Dq6hptHUNpmXm8fGg4HI2AIwX4+QX4+AR4wT/CNVda8NLh9Vavbka66yXTrKN8LIJczdxtjWxMDR89frll27FNG/bt231iztyVM+csAyisqWXARmHBcRwMtRAKzxISmUGGagxLsxZkFseTRHBEYfaiyiSyyKc0LARxMLQGB0Bh1gi4jQgShd2zZ19JSQn4MGJweBkJ0fVLZj25ccjd9OXF01cXLN5wW/c6rcCktDVj+O3Q8NveuIyIdZs2C/ChEaxVo+E83Ag+XiIWJSpMEhMREiKTD17Y5pWl55djmlgRXNtR1txbU1VXXlNdM9APzQoAJ6XmhqaqmrLajtKqtoKa9uK6ztKSqtyoqGh6SERoaBSDEclggE44nc6gUgNotICYmLjy8gp2YGtoZLCiNc8/15qSb/TSTklGWpKMxx/fMt9AaZuh0mada6s8dW6MTBoKQxUkQkPBGXaSK0j8RP0KKFwBULj/bXJZ/Ss7z5imwZTO0aT20eT20dT20TTgtrE2vXU0o3U0s3U0C7gFcjZomyHnNI/mNo05r3E0rwFyPnD9mAvrRgtrIRfVjhYD14wWVY8AF1eNFFeOVNaNJiRXv9E2bO4Y7ep7293/trC40cHRtew7oXBlebmro+PEK0iA02hba2tnR8dUVBho/LS58R4j4K/SMPhTY2/qhWtH/lw2r7ghsr4nhZ010dSf5hVoCvjYwVO3vjelvCX6rzQM4S/os1D4YxqeQuG/FcDWooKCIH//1OTkiVaQYBVTS5zEYmr/l1EYSENDY9WqVQwGA/QBwsrLy69fvz4qKgpsNjU1XbhwAWxmZ4+V7A0NDV2zZs29e/fYm56ensuWLTM0NGRvAoBevHixi4sLe/O76EegcFt92ZOHarrGFqePn4rJKvQyMVNTM5qko+0blB4TpKN08M2N3XpKh4zVjpmqnzJ/cF7x1HYJMQkERoQXhp7GMZ1j2h8nbu6gFZpSckwD8mwp2SbUApOTN7cDDublnA7jBbTHB4APKcADRvg4OQ7s2ZOZnhEawti1b9+cpUtmL121fd8pWgCjuLjkyeNnwiJS3Nwwfn44gGB+Pl5+Ph5eXh4ebi5pMbLihSNuxk+DnfWpTqYuViYWRgY6WvqPX+jdf2pw56n+vhPym7Yc27712IyZy2RnLY1PSDE0tkBiRYXEIBQe42AWCgtBGcMzBYXlAAoTBaXxZAkcWRRLEsYQBLFEITxRmEQSZtMwq36wOJlVOg1CYbIYsJCgOBlal06ERBKCIzAAhYuLi89fuITFEqTERNYunPXi1nHLV3eWLVo5Y+4iA8oD8Mpk1EQODPdXdeXbhGgtXLkABcdgkUgEPy+CjwcpwItBCJBwaGESUYhE3rx7tUPMK988Q58sI3qRfWCujfpzZSVFVVdXz4z0rMbGppbmloLS7Jz6+Nhy/5ACx5BiB2emwbFTJ1et3rN2/d51a3esWr3lzyXrlixZu3XrHkWF2z7elLra+uamlqrG0oLmZGaxm1+uiV+BifyT04LgmRJI53YvcX1x0vPVaZfHBxk2D0Ynra4wuGAFZ2pwkgXXbWNDX9YUCk9Q41G4qv9takWDur6VfWS6e2K+a3wusFt8jntcLrBHXA6wZ1yuVyxwDrB3DHA21EbneEfl+ERl+wJHAmeBlgIcngXsF57tx8wCpjKzaGGQqYxMGiMLckgmLSTDH5ieEczIdXVnqt9/EhIWHx6VEhGd5ukdpKdv9L1QGBxOYSEhUeHhE4wK19fVhQQGxkZFTVWQAAIoXNeeWvUJCgN/hob/miYBALqhN0X13uU582WT86gsFIZyiBv7Uu3ctQWFCW5UY3ADwMFss1GYnSbxEQ1DKMyiYVaCxNS0ub9Rd3d3U0MD+CBM5HIO4G9dTQ2UIFFZOYXCn/pHoHBbW1tVVVXfu1h+c3NzTU0N+3oGvAWAhsHm+6QXcIKqrq5uaRk7YYKXorKy8v2SH11dXRUVFRMvrDtePyQqXF92/46if1yewYPrj7TN7PRM1dVNJni0gcO+r7cXvCDfmPf5FWXE0g1UD+so7DVUPWx697jFg7PWTy4rn9kpLioJR4vwwXEcXFy//+f3ZRvnOUS/BsznlaVHLTS1j3q1auNcnml/8HFx8PNww3h44LzQuhsA+3inT+f8zx+H9h5IT0mn+PqvWrNzw+bDwfTIPGjtYmUMmsAxjYuPl4+Pl5ePl4ePl5uPm5OLYzoCxrdr42qjl49sDLT1NDWfPH2t/uj1nfuaKvd1FB7qKz80uqthcvHW0/VbTi1cvEVMcpHs7GURUXHmlvZonDgUABb/EBKGOmKzoQlzQnIkMhQSxpPEIA4mCqLxZGAcQZBAEiayaFhQCNAwBMQQEwtCHCxIFiWTREkkiIOJJDIMjtq9e19hYdHZsxewGIDCYivnzXh564TxU5XF85fOWbjYiPoIXBgE5dmlVDEYpc6mwY+Wrl2E4EOg4TAUjJ9dZAO8LFgkjITFkLA4OTlJVc3z7qm6lHwj93QdDVvFJasXS0hIL12y4uC+ow/uPfH28S2rLWzpratqKwrOt3fP0LKNfL5ux3o0ShKFFufnx6OQ5JkzF+/ff8LAwCQyMjohIdHPz//unfuqz246xWhRiyAONqQ9XLV5JQZJJpHIN46spepe8de/5vX6bJjj88lbYgMIXO+CY/RbTtNTKDxBjUfhyt5R0NG0dX1u6aRp6wb8xoZtV8jWoHUBrZa1m5a1q7aVi7YVq7V00bZg2dwZWOe9zYCddEyddEw+WNeYbUc9I5YNgR10DSAbGDm/fGV6/bq8traurp6Brp7+69dvrG1sGsdN8p0ICg8ODLS1tra3tU3w9FdeWsquIPHdv2X/FzU8MsBC4biq1oTxHMz2BxT+Ag3Xdyc7eemJiJGfaSpVtsVBZt3guuIpkiAuPMmtHkLhmG+nYYDC4AZTKDw5Avg1+nOmzf0fReH3+ug1/3Tzo5Hx+sqfJqgfhMLqt+WD0+sLYiinzpy9q/T4wQPzCT4BcFRkpKbGx8Q0Nkx0Gf/shBCze8eNVQ+aqR+zfHDK9sk5hxdX1S/ukxSTFkAI8sGwXNy8f/w+HYVFXLh70C7ypXuKDuDgC2r7cTgY77Q/+Lm4eLm5+bm5YDxccD4uAW5ONJxfUoo8f4nsgydq+fm5jLBIBjMyOSn1zIkL/DyI3377nfMPDl5uTh4eTm4AwZwcAKaJKPiq+XPUr5631Hxu9EpT66Xu4yeaCnee31R7pXBPV+GBofIDQ7UnBgp336zaeFxMZoWI5J+ys1cG0cPsHN2IZFlh0dlCUHbEHCHxeUJic6CQsOhMKDUCcDBJAk+SwBFFMQQhDI6MwRIxOBIWomEhAkS6QiQyAGLQQh1WGFiESBQmEgUhk8hEEkkAjtyzd29BQcHpk6dxGJyEmMSyeXJaqmd1H6ksnLdEWmbuI1MFWpGpT7aBT5Yh6LxyUpGSlYJxC8B5eOF8vHB+PhgfuEjgxSIEiGgkEYsm4XGLls29+eT0aydVNZ2rG3eul5SUExOTFBERlxCTlZaYvWrNWtdQ86LW5IrWvNACZ3Dt4ZyotWnvZhRcVEx01ooVG27eUPTypGRkZGVn53h7+9y8qbB40TI8liQqKXJcfs8Le+VHpje2Hl6LI4pgkKS5MuIvFQ4GGN3y1pW3eHzezVTjGxOKvg8Kf7umUHiCAihs4+aZDVC4b7Ssa7ikYzC9ujmhuDqhuCaxpCYJuBi4Gji5uCYZaqtTimtSgYuqgdPYLhxzakEVcFpBVTpo86vS86syQJsHOeOdM9nOrcrMqWS5KjO7Ejgvv44ZnvLy5avystKamuqqyoqqqsqWlpbxR95EUPh7qb62NtjfPyYycqqCBNAYCrfEVn8OhYE/0DAbhf9Kw9XtCbkVoUdO7Tp35VBhbXhNV2JNZ2J2Gf3yzeNgJLcytLojfjwKl7Hw969Jw2wahsh4DIWbp1D4b9Tf39/R0dHb0zNBKhqBaqkNTyS0/E81hcK/sn4QCt+9LU9Lrn073PVC9cz8eVvu3LcY+9u/VXt7exCN5u7sXFpcPDb0b5WfEmavcdb64TH7Z2ccn5930bzioSP/UvGknLQcP5zAL4Dm4ubn4OD64/dpeDJm54l1lx8e2XNmHVkIzcf1B4zjdwGOaYBrebk4+EHLzSnAy71++6Knljf1/e48NLnOjKKDhxgYGLh//4HsbKmFq2bNXCCNQyPg/Jy8PNP4eLiwCMQ8Gcnj2ze9VLjhqvUiwNIo0tsz1t8/wM3tzSvtW6pP5dU0Fe7qKKhrK91/o3xPa+uei5Kz1kjILZedtcrbN8DNw1dUbJ6QCIBgYCgeLCg6C1poQ1iOKChNJElCKEwUw+GFsVgyBkPEoAlYLBHHCgzjiYIEIoBdQQKJzDYgY2AwyBoHLZFIJsGQyP379+fn5588dgyHwYqLSSxdMNPw4bVX95Tmz1sqJjJrz8k91swXlDwj31xDh5hXhy/tgAkg+bj4+Ll4BLghwyAm5sMg4AQMmojDEPE4MuB3OZn5i+fNnjdXXExOXFxWWmqGlOQMScmZwtA6IGJKr84HFVsG5Fn7ZBoBFDakPdywfcuGtbvu33saEhJWkF+QmJhsa2t/6tTZGXJzwZPi5YHx88HhMDR4OpKyUuKSohgMFonASIkKn9+/0f71TQctJa0H129dPvni6eOhSUNhcG6trqoqKihofffLzlc0hcITFBuFsxo6yntHS7uGyrqHawbe1g6+rQUty3WDb+tAO/C2fpwbBt42Ave/bXrvvrfNLDf1jgI3v3ML2z2QW9nuHm3rHm1ntW1dUKUI4LbO0Z6+t/kFtYaGxmN79jlNKCo8ONjR3t7Z0fGNV3VfEjgxtTY3g5PpZBLAL6uxBAkWCle3fSEw/CkNj8sbrmqPj8/yjU71qutKqmqLq+1OqutOjsv0TcqlstZeZkPwR4FhNg2PBYbH0/BUrvDfCuBvaUkJMzQ0KyPj/Y/p/0Lg36mtrgb/SOUkLrExhcK/sn5IrnBdqZL8Ja+YctBPCrTk/Q12UVGP/ad/ra7OzqjwcEDDVZWVY0P/VoVpEU6vr9g+O+fw8rLT66tOWjdcdRS17l6cO28eDxzHK4Dm5hbg5OSeNm3af/77H0C7cCyMj3c6GcE3R5woiRPA8fwO4/4vH9fvvFzTOaZNk50tpumi4ldgElRiGVtKK6sGHJRXWlzq6uGiYaViFHD/lbPSiYtbly+QmicjtGLBrGM7tjy+edlZWyPU2ijMWi/a0bg0Mrg2gVnCpHpbGqso37uh8PCWsoa88jOAxUpqL05fvLNo5W7p2atkZ69ycvX2oQRIySwhi8wWkpgjLDZXSHSOoMhMsrAskZUXQSCKE4hieIIoFiuIBRyMIqBReAyGgMMBGibhgQkkQL14ItRCBjRMhiAYaskkPDRMgiORBw8dzMvLO370CA6LEReXWLZwlukzhae3b82Zu0xUdI6E9JyDF3c/trj5zFr+2NXtJDKeYzoXFzc3Dw8PHzcvPxcwD4yXD4NE4vE4AgFHIhKESIKCoMELCxHFxYSlJcRlJSXkJCUACs8CrSBJbO2O5caB913TNd3TtKk55nY0I0Njs5johJycvOioGH19o10794G7wARQ3FwCoEUhceCpIZFYAT4kL7ge4YTD+FGiQoJ7t659qX5d96niHZUbFy9f2XPolMo9jclD4akKEpMpdoIEQOGyvtGSriFAw2yXAXdCLu9iGXQ6Bys6h1gGncHKzqEq4I5B4OqOoer2QeCa9qHqNuBB4Jp3rgVuHaxtgVzXOlgH2pbBerabB4AbmgcbmgZb20czssr19AyGhr749TzBXOGpChLfXRAKd0C5wtWt8f+Ihj+gcFt8c39aamGAG8VQz/Shuf1LeowTGG8eSAftOwIGKPw1Gh6XJjFVTO1vBLD1/4MKEl9fYgOg8C9RV9jVioXC/1dOOG7u7iwU/p7F1LpaakwMtcIzoW/57uayc7v3vjLyZP/pX+s7VpBIT4jUVL/2TPn8c9Urz29f0bhz7cXdm3cVri1cuoobQeThx3Jzwzk5eKZzcP7BMe23/0777bffEBy/71827+qetQdXzlougRVFTsfyTRPg/p3j9//sO7vRK1OfkmecWBHUN9pZ2VKoev/6yxdP2rqaS9rSgottfPONbEM1Xjw4//zWWaNHqu4Gr5kOpkkelrHORv6Gj7211SMdDOKdDRPstT11n6jIK1y+rHj12u2r19Su3lC7Lq9++fq9zbvOzliwUXbuGitbF/+A0JmzVxKFZ5DFZ0McLDyLLDyDJCSLZ+VFAAjG40WwOEE0mohC4lFILDAajcVgcFgsxKV4AmBiIguI2RFiEoBgEsTBZJIgmSgItUgU6uiRI7m5uceOHsHjsBKSUisWz7V6qaqhpjB7zkoZ2SVEkiyWICw9U1J6pigCwc85jYOHk5OLi5OLB9AwL6BhXi5OPh4uNApBJOJJJAL450WEhMRExERFJIRI4kSsMBEnLEQWFxKShCpXkEQF+JHg/rtPbbRjaMWWU8vasksrizMzskNDwzQ0Xm7YsE1YSBImgObjRcBhaDYEgxYYARmNQCBgcAyeILxy5ZprVy+rqipdunrj+Llrh05f3XbgnNK9l5OHwlMVJCZTAIXtPLzzWnurht6W941W9L+tBO4bBa5im1VkDTLosPo1wL2Q69jugVzPdvdoA8v1XZAb3rkRuHPMTcAdkJvZbofc0j7a2TOak1etp/+jUHiqgsSP0Ltpc3EsFIZo+ItJwx/R8Ls0ibqupJBox137NiJQ4OKfh5+fV26mxNPXSgU1zOr2+PIPBPw5Gv4kaRigcGlTzBQKf0UAW/93K0gYGRlCKFzXW1z9dRR+t9rcd50WBsRGYWi1ubq/R+GgH7ba3K+pb19iIz3ODaDwtyyxMTI82NzU2NXL/lIYbaipqm/6lwuO/ggxmZGnz149ePjs0eOXjp64ePTU5eNnbhw7q7Bk3V4BrBgXL2YMhadzT+MAtMv1xx//kcXBX547bK509sWFPQp7V2yYgZ9DhomhuLBc/731+KhfoalPplF+Q1J9d3lIvtOB8xv2HFlf0JgyMNqdXhPhnW3gk20Q4K8T42SU7G2bTnVM87WOc9ClGzx0fHjV7v4lmu6DAG1132fyhkoXL548fvL4mdMnL5w6denU6csnz1w5dfbG7oNX5y3bK7tgg6GpbWho5LwFa4lCcmTR2YIis9kcDJWMYOUHY/FCWCwZjSEiASNChdtQSAQKiUShUWgsGoPH4fCAh3EEKEJMFMSTBIkkQXbeMJGdOgwVUxNGoTEnTpwAKHz06GFAzlLSMquXLHTSvaf9WG3hok0LFmwWFp3JD8PycAtwTOP4/Y8/uDk5+AAGc3JyQ6FhLm7AxFzTubk54TABPBYHJWQIkoVFhIVFRElEITFxyTkL58yYI4fHE/n5kJycfBzTedEo3KyZc27cvBkRF1bbWJWbl+fl6aukpLZs6WohQQkEHAs4GJA9Bg0FuSEORuGgJ8gyHInhRyAxBMEFf646cvLihWtKJy/cOnxa/tAp+UMnb+zYd0717vPJyxWeqiAxmUpPSzO2sY/KL0urbk6uqE+pqE/9yOX1aeX16RX16aBlOWPMdZnl9VnlDVll9R9cWpdVWp9dCjpQO+aS+pySuhzQFkPOHee84rq8ojGXlLVERKW9eaM9/OVpbROMCk9VkPjuYqFwGoTCbQnjaPhjFAb+gMLjaLimIyG3krHv8BZOTg4JKeHlqxYuWjIHgwWnW4SBxROAwpWtUE2JdwQ8nobHAsMfrbsBoXAzQOGpGY1f0/9uBQkzM1O1e89KoQSJno/wd7zr2kcd3APOnrswMPD9P6RWVpbP7iqONBWPNBQN1RV8ySPtFb72pg/vq3d2QqvB/V+QuweUIAGh8N8svAytNvdtCy9/OK7+coxN7HgDh/33qiARQI/cfujaul0Xt+y/vvXg9a1Hbu44rrjn9J0N+64QxBcAMOPg5uPg5AIoPH067+/TOHk5/9gyR9JO+bz7/ctWaif15A9c3jRn73yRjVKYxQTe248OUfJMvDMNQgucGSUurila+85u2HduAy3HoqQ5I7c+wTvT0DNTLyTCMM3VNN3DMsXDLNJG009T2fHuWXPFo66Pr0Zavooye+GrofDk3MG9W9bv2r51786d+3bv3rdn7969+/bvP7xn3/mla0/ILNz2Stc0Kipu8Z8bCCRZQWHAwTMBBxPIgIMlcUQxLE4QgyUBDkah8Ag4Bg5DImAIyHAEGonCoNAATAl4PIGVIwFlCUPz59jFhkVJZFEyMGu1OSwOf+bMqZycnCPHDpPIRBlZuTVLF7gZPrTSfrxuzd4FC7YKCskgMTg4HMXDzc/BDeiXi4+Lm5eDg2va73yc02Dc03k5p0N8zMUN5xPAIlBQqjKRSMTjJWXFj17d89RS4Y7+5bW7l/Dy8cAEEEuWLFe7fTc4mJ6VmRMVEWNuanXs6Jm5cxcLCokjocA2Hoch4bBkLIYEUBg8NVYwGAsMR2DgCCwMicWQhRYuWXno6LmzlxVPXJA/fuba8ZPXjp28fvj4tR17Tt2++2zyUBgIPNhUBYnJEbhc0zEwNHdys/ei2Hn6sG3Ptse71sPHwdPXwQPYB9jRw9cRaiE7efg6QS3bvk7uPsDO7r7OUPuJ3Xyc3KD2Y7tCdvPwMzWzNTQ0/MrX6oRyhacqSPwA/QWF39Pw36ZJsGm4Na6hJ8XRSw+LR+09tCUk2jGtMCA5j2ZioyEmIbRizaKMkuC6riSo2HDLeBpmdz6kSYyn4SkUnkyBj+okV5Dw9/c/f+lGTHJRU9dobdtobetnXN8xWtM6+vi5rrz8rbG7fVcFBwefOrK/KD50pLFopL5w+J3f90EHUHJ7ceo9hasGBvqT+fr8XIWGMpTkz7dWhb99WwBoeLQv64P72R1o8O3b4pgQa8VbV6qqqsfuObkCp+7vVUHCPyRq+1GFDfuubzmksPWwwvbjijtPKO85fXfPabVZy7ZzIQi/c3JP4+SYPp1z+jTu3/87TRiLUD242UnppI3CIYPre15f3KGyZ8mVtXKn/hTZK4u5cmqVS9wbWpGZT7YBrcjULPjxnMXSO49DK9UF5FkF5dl5ZRp4ZRlQ/V4wDR+HGT6had9xfnTV+OahNxd26F7d6/rkWpTly0jTZ073L1/csXrdsgXrVy/ftG7llg1rtm5ct23zup3bNu3edXjNppNyi3Y9fmWYkJiybMUWHF6aLDgWDyaQAQeLY/AiaIiDCSg0AYqVwtAAheECcLgADAGgFYlkozCRNTOOXUcCELAga8UN9nIbrA5Ubxig8LlzZ7Jzso8cPSooJCgrJ7d26TwvkwdUmzcHdx+fM3eTkLAcBkdAwFF8vAhubkC8PICGeTimYXk5Zwri5osQxNEwLC8Hgmsagmc6HsEvLUiYKyn652yZ84r77KNe+eSAF0TPjPb0/NWTTx4/jYyMzs7OiYqK1tM12LP7gIzMLAD04ClAAWWCMAEvBDgYGAMQH41HgqeGwMDgaLYRCAyBLLx42crDR86cP3/z5LnrR85cOnry3MmjZ44cOXvg8NltOw+pqD2cVBT+dk2h8ATV0tISGRkZTKeH/nwxgoKCkpKSxvbsc5oICn8vNdTVhQQFxUVHT0WFgQAK13eksfF3HA1/U9JwVUtcU1+a6v3LYhKCjDiX9pHslsEM4Ma+1EcvFHB4VGCkfWNfGoTCf0fD7wtKlExVkPgG9fX1tbW1dXd3T5DSRlgxiwnOQ/1HamxsVFBQkle6Z+3oa+vsZ+NE+dR2LlRNXYuTZy7QaLSxu31XNTU1q91WvS1/2dZQE7LB609tb/Tm5QPV82dOpadnjN3t/4Cqqmsunj/tZPm0oTS0oYxRXxr6qRvLGRV5/o/uXnzy5PFkHjnj1d7W9r0qSAQyYnefvr3lsNL2Y6o7jqnuPAF8e/epu3vO3lu37zJRZuF/eOD/mcb532nTf/9j+u+//WeeuKDWlaPWNw4ZXdn1+Pj6W9sXnl834/RK6RNLxHfPIm6aTVZ6fMSaqeGSpGUR8uTY9W3cHBwz5oiZ+D/wKzTxztanFJh4Juk46Ny0VjljrHTi+YVdyntXXdy44NymhTf3rHp1ab/jw+uO9y49Obll22LZpQtmrVgyf82yBZtWLt62dtn29ct2b15xaNfWLVsPzVmy74GGQXJq+qq1O9BYSZKgDDCBLIUnSWAJYmisEBpDAhyMROHhcAwMhoLDEAICMAHwHwzQMAKFRKNRaBwWj8cDHmbTsAgLgj+gMMtiGBz2/IXzWdnZR44cExYSlZWR3b1xRajjywSKqfyFSwsWbZaQWgggFQlH8/Ei+bhh3Ny83NwcM8WFTm5Zo3Ro6629a4+snrN1vuTa2aLrF0of2LDk+tEd6hcOv3pw2ZHxklJg7J9nkVQdVNmZV1FbmpGRSaXS7t9/tGbNBkGyOApFRCKhrAo8QRgyTpAFwe9C3SgsDInhh6EEICPhCDQA+iVLlh84eOTkiXNHD5/Zf/jE3sNH9x48sm/voT27D+7ctX/9pm3yiirghDf23n9V3wGFwcejsqIiPy+vpbl5bOjLmkLhCQp8Fw4NDvb39f0o939DH3wzswQGv35ynAgKQ5UfvkdUeHBwEPwjnZ2dU7nCQONQOPEdDY+lSfwtDYPNpt60C1ePLFk+r6guoqE3hZ09DGjYg2oiLEpy9NJr6Estb4FyJMbRMGg/S8NQ0jBAYfDXKRT+isBHvqSoiEGnZ6alDU4gVxj8O9VVVRmpqRXl5ZMZ+ExMTLh5U/7I0RMnTp0FPslqx/ePnTh9/PhJOzu7iWRCf12FhYXPnj27evnS5UsXr3zBAJeZTObYHf7PyMXF5dyZIy+e3NJ5dVv7peqn1n19W/02eNnOJiQkjt1n0tXV2RkdEREcEAAO4LGhf6ug8IR9Fx9uO6G++8zDPWcf7D13f++5B7vP3t957sGOM+pLNp/EiM7jhBP/4Ib/zsHzn//8PkeE+PDETt0Lu5+f3Xpl68LdC0TWyGCXSWCWiKHnCSLEENySIpjtR1edUzuw6eAKNA7x39+mw/n4T17dZhH82CXhjTXj2Uudi6rHNl3etuTkxnm7l8uumyuyaobQ6tlimxdIHls9W2Xf6rsHV53fMHPLn5JbV8zZtXbBwU1/ntq+4tK+ddcPb1A6te3ulQOnj534c/XRu48NUlIzN2w5gMSJEwWlgMc4GCeCxAgiAUcCXkTiYACFBVACAnCAwfyAhSHBABnD4UgkAGKAujgingBoWBhaYW4cCgsJQ8bicZevXM7Myj506LiosISslMzVkwfSAizzGfZaj9RXrd4pI7dckCSFQmB5eZG83DBOTm4yAfP41pVA4zeOD66b3z5tpHLCVO2Cyd1L5g9uuGjdo5pqBFu8DPbR9ks3dMvQYRa6NXfV9oy05VWlXb95c+aMuUKC4miw8wgCHi9MIIrgCUI4vBAOJ8hKimA/KSgdgh+O5oej+GBIPgGkABxJJJHnL1i4Y8fOffsO7N61Z8f2ndu2bduydeumzds2bNy6YcOm9es3LFu+4uq1a9/ID99n2lxkWJiXm1vBVAWJKf1VE80VptMjmcz2qQoS30/vUDiuBqAwC38hQyj89zRc1RoHUPjyjeOLlswpqGG+R+HmgXQ3PyMhEeIYCrPShd+ZTcCf0vBYmkTxGApPJUh8UQBbv1cFidTkZDcnp/iYmMlEYaDa2pqUlOSE+PiEhPjEhA9i9aFB8N3xo3+0aW1tKyuv+LLL6+sn+uP7/6L6+vqoNJrm61cvXmi8fPn81Qe/AC0YAeN6ero/kYOBhgYHmxob62preyec5EaPTD5y/eWeC08PXn1x+PqrQ9dfHrz+ct9Vjb2Xn+27rLHr7MMVO6/MWr5HdNZKgvg8BF5UVkz83LZ1yvs3nN6wcN0s4dlCKFEsHwnBTYTzEJF8GBifADcPBo0RFBXG4gkAEPFYEhlHnDtD7MSZjapPTly4vGXP2tm7lkhvXyyxfYnEzuVSu1bK7ls14+DqmUfXzji3XvbmttmqexbcObLszrkNj6/seC2/z+D2UYv7Z+yeXXJ5fcNbV4lqpq71RG3zznO3br9OTc/etf8shiBFEpJhpUZIYPBiKKwwEk2GQqpQSBgLg2EE+FEC/AgBAWA4PwBiflZoGI5EINEoFA6DIeKg+WzC7AQJKDWClSssJCQuLCyOw5OuXruemZV16PAxCVGZWbKz7ipeiw1wjKE6mOvqbN9+VG7GSiGhmeDheHjh3NywP6ZzrFo4N8HHpcDfzV1D0eLOafvHl4OMH0faakXZacc46sW6GCS4GUUGGPmmGrhn6ATn2WfVxiTXBxn7PhMUEeXmhgPkJeBFiMAEUQJBBIcTwmLIaDYEI7ACgIAB/sIQPAJwbn44jwACADGOQJo1Z87atas3bdywft36NWtWr1mzcvWq5atXLFu5fMVy4GXLli9bOn/e3PPnz00qCoeHhnq4uExVkJjSR5oICn+vChIDAwMtzc3tbW1TUWGg8Sj8WRr+iIPZHgsMt8U19qY+01QRFiX5BJm3j2Q396e1DKQDJr7/7CYGi/Rn2jayo8LvA8MfCPgDCgO/p+HipsgpFP66ALYWFxbSAwPTU1ImEhUGx396aqqXm1tifPwko/CUfmWBQ6G1tb2pGZwmWz9xCxj//2keYWh06knFN4dvvDyuoH1KSfeUiv4JFb1jSjpHFXWOKGgfvaVz6Jrm3guPtx27vW7v9aWbj69YvW3v5m371q5cMUtaTpQkJkwQFSKKCZEkhAWlxUXkJMVnSkvOkpaeIyu3YMaMJXNmrZg/Z83C2avnSm9ZJLtn5exdS6X3L5c5tWnOlV2LVI4sf3R69fOL67WvbdK/sdn41jYLpR02Kjsd7+7zfX0+2ESJYaUWaXsv1ulpvNvLRE/NVF+dDD+9vEDDQEf9fQfOnbx4Oy0j99ip6wRBORGxWUIiM4iCMgSSFJYghsEKjc2ZQ+KRCBwCjoXDMHA4BoHEsJNrQQeJgjYBCmMxRAJekEgUhpaaI4sSSKJ4vCABD+BYEPAxHi8kL68EUPjwkeMSorIL5yxSUVKwtDDV1zd6qqFz+OiN2XPXk4VmoNBEPj4YFw//79M4dqxakennke3nTNG5b6Z25vWVPaYqp1yf3/LVUqPpqjMsXsQ5GcR5m9DiDDxZa9T55ZpQ8o3kn59GoggAecGegAfF4QQhCMZCleCQUGwbzSeA4AEPwcvPwcPLycMDWtAXQKAJJGFpuZmLly5dvmLZ0qVL/vxzyeI/Fy9ZCgYWAS9b+icYXbp02dJly+bNnXfu3CSiMHikkqKijLS0b0lpn0Lh/1OaCAqDwymcwYiJjJxoBYna2iAaLToiYiIRtf9vNA6FEz5Pw58LDFe1jNFwfXdycKSDqLjgmg1LPWgmjDiX0FhnQ4snktIiC/+cnVYYUNedBBHweBr+m6ThyKlpc3+rnp4ewCmdnZ0TQVhw3/q6uvycnNrq6ikUntL/kMBVHPgIdHd1DU1giRm2GHH/j723AGtr2/a+7/u99733yN67gru7taXu7u4tBUqBUtzd3d3dNRCcEA8haJAkuLtLaSnUe765EppNu7vlnLa0+xz+z3jWM9dMRrIka67fnBlzLIqaZaCSsc99s0A1syA1s2BVk8B7hn53DLxv6rpd03K+pu5w9b71FWWzK0pGl27rXLyqfPn8pfNHDxzdu+3gHoXD+7Yc37/t9KEd5w/vvHB4x+UjO64d3X7zuOJtYEe33D4id/eovMpxObVjMurHpB8ekzY8J+94Z6/fw+ORhufSbK/lutws8riN8FVCBapiQzSJEbrVMXo18YZN6fbthf4dZcEd5WGdqKguTFwPLmGgImmImDRWlVqZH3f+3PUbd7WaqW0PNA04eCQEhOX4hWTex0gIsXNA6SOgxyyz8bBBT9bgpucV5mJj5waVAJEZoMzCygmFDbOws7FxsLFxsrNxsbJxbYaWnOzsHOycnOxcgE25DAxNaLSWm7eURIQld+85pGdkZecRYmgfoGcVqKbrduCEkqCoIgsr3w8/bPjv//3rX/72N7UrF0mpcVWpkfh4vzQXfbt7x8yu7bVTOuqhfi5Q91qKg3Z5qGNlom95sX8eORjeEgKjBAbn2x46vefHn9hYoIeAQBsGRUGwsG+gE/DfIMjeAKXy+Nvf//dvgIN//PuPGzZsZAG7IygoLi27bev2Pdt379++e++OPft27jmwc+/+Pfv37z944ODBg4cPHT565PjRY6eOHD+9b/9hXT3DtcsrDAR+qW/fvPkjzes6Cv9H6XNQ+Es9bW49g8RqMWOF6SgM0fDoPDNo+Pen0I0+qRuaqzYwe/D3H/7GwcUK8HeLojSUXXjDDz7B1iOPa0YeQ6mFAf7+QRoGKDwwW72Owuta17p+Tc+ePWtqaKiurPz8DBIoYv3NR/YXVc2uqpldVTG6pqx/RUn78p2HV++oX7mpeuX6vevXb9++cePe7Wtqd69qKt/UVblhrHbFXOOi5cPz1lpnbTVO2dw/bqV2zPr+Uct7By3u7LG4tdP8+jbTKwrGl2SMLkiaXJQyvyRpe1Xa9baCj8rOsIcHU0zP5dpfL3K7g/JTxYeoV0ZokKK1quP06hKNGtMsWmG2rXkOHfnuA+VhA4S4AWLiIClloCqjn5TVV53TU5XTVQVHwjPPXbx7U0mLSmvV0jbaxCbAwSvGziXEyiHAwsa7mZV702bAuBybgW1if2+gDNUA1mRh5WFj44VGW9m4Nm5i/QmKmti0ceMmlo2bNm9i2bSJlZWFHaJgaKhYANRra+tQqS3Xrt8WFpHYd+iUobW3lWe8gUOkjm3EI6vQm5oO+04qickf5OSX+nETNy8vv4+ZETkjhhjnh4xwDTS4rXZS9uZ+4dv7RVQPS2if3mJ391iYsVK2u1FesHVklJldqLaBu+rRi3tZWFn/8veNP24A28NKD/9l/WkzZBtY2DaycGxm42bj5OPkEeLhFxMQkhIVU5CU2S6vuG/7nqO7D5zce+jM/iNnDx0/f+TUpWOnrxw/c/Xk+WtnLtw4e+Hm+Uu3L125d+W6ytUbKucu3ra0/qNzPb8MCv9xraPwf5Q+B4W/lNYzSKwWHYUpdBSu/5mGmSj8B2h48llDU1eprrGqhLQwvwA3nwC3/FZJK0edtkH0GB2Umfj7AQ2vQDBYfkDDfdPrKPz7ev6FMkiAS2Dh8ePFp09X1te1rj+D5ufmSgoKYJmZw0NDK1X/qkrLys6ev3T8xMnLF89dv3JG6ebZ+3fPad+/ZPrwutWjGw66N92Nbvqb3wyzvRPtcCfRVTXdQwPm9yg/RCc/RDs/QCPfUzXb8Vay7ZV4y/ORxidCdA8GaO31Ud/hdX+ru7K8+z05z3tyXspy/mpbI3X2Jpkcz7Y5X+h6E+GjjA54QAjTIkZqV8boVMboVcUZ1ibQUTjPsb3ApaPIsw8Z0VeR0lmR3krIqkfn4BGwsuLcgsLcPHhealaRmp7rXQ0zWku7vr7pjxs5WbkEWDh4AS9uYuXauJljw0a2DRsBU7IA2/ATZBs3sG7cwLYBmj8HXmLfuJFzEyBjFkDGnNAYMAc3JxcPPccwPw+vkACfsCCfkDC/qIiQBAsLq6GhEYVCu3rlhqCw6MHjFyzdou39Mi3cE41d4vQcYrRswpUNvC+qWh29rLXr8LVTJy5m+HvTYDGVib7pHmbq53bvEdukKPTjdsEf94ltOi7HdWW3mOrJ7aZ3T9uqXVU6f0RSRpRLSGgTNAzMzsrBy8EtyMUrzCcoLiwmIyopJym7VXbLDgXFXVt37N2x5/CeAycPHD1/5MSVY6duHD9/6+Tlu2euqZ67oXb+pvqlWw+v3NG6rqR9Q1nvpore7fv6d9UM76oZKz0wvadmeg8sHxhfu6Nt5+i3dii8nkFiXb+mz0Hh9QwSX0Nv39FR+DEDhT9Fw4wwiV9/Ct0IPUyiZ7wCV5OZmOGXnB1AIGf3TRHHntYzZtF9TMMfoPDHNEzPILGOwr8lgL9fJIME0EBfH6qsrK66ev1aWNefSE8WFipwOPDTnZyYWKn6V0XCFJmonDRSOuKkc8HX9Fq47Z1YJ+UUT7UsvwdZPvezvVVyfZTzfZSK/ZVKApRKAu+VhzwgROkS4wwrYg3wUTroEA2Er3Kh+02Y85UMu3NJlsdiTA9GGu0N098Vprs7XHdXuM5OYJF6e5ItTsBdrhZ73i7zvYcOeoAL06oAnxOjS4zVI0TrVUTrV8Xo1yeYNmfak/PcSDA/AiwWmZdemJOemZEZm5wdHJ/rEwv3iSvyiy8MSUVb+2ZrmHi2dfSYm1pv2MjFxsUPOJKFnWczGw+g4U2boegC+lMnIFsZD97MCY0TgyULJwsrFysbFCwBPe+Cl55UmD5hTkBIQkhYQlxEfPuWLccOHJSVkGJhYTO3tG5uply5fFVIWPTE2WtOfqlOgTAb7zQLjyRjtwR951g9x+hHtuEa5kGq+p6Gxs7FySm45IhoRxPz+7cOK8qIC3CK8rKL8XLICPMoSgnvlZc4tF329P6dFw7v37d9h4iYgrDEdhmF3Yo7D+zed3TvwZMHDp8+fOz8sZOXjp+6fOrstTPnb567dOvCldsXripduqF69c7DG/d0bqkY3L5vdEfNSEnd9N5DC2UtS9VH1ipaVqpaVmqPrNW0LFQ1TVU1jJUf6Cup6ty6q3Hr7oObd1UvXblhZW27dii8nkFiXb+mz0Hh9QwSX0OMUeGxn1GYQcNQ4eOg4V9BYcbA8OM3LQPTpMaOEmpP+fRy05N3baMLtUwUXkXDvzaFbqXcvz5t7vcEUPiLZJAA6mhtTY6PLy0s/EykXte61lKvX7+enZkZGx39/AwSPc14mL9mtrdqQfDD0nCd8ih9VJQ+OlofEaZR4Hc31/MGzO1qjstlmMulHNeL2S4X8t2vIQPVsKFa2DBgGqjQ+8hgFUTA3RLfm0XeVwu8ruR7Xsp1O5/tcjbH5RzM+VyO01m6nS/wvAHeD9i3Mk6vOsGwNtmUnGbRlGndkGFdl2ZFzrBuTLdsTHeozgpAZIanJMRERKf7R8K8wnPdI/JdIopcospco5Ge8VjvRHRgaoVHLNLQMaKrd8je1mXTZj5OXmE2TkDDfKwc/CzsEBOzsvOwsHEDA8jLRjdWVm4WFi4WVlADRQOzsXO+R2EhPj567ghBMUEhCQEBUXFh0esXzpvr6+1Q2MLCwm5pbdfU1HzxwkUBAaGLV5R8I2BuYfmOgTm2/hlWvqnm3snmnknm9EFiA6doW/eY+Ki0AFcvfU1tldv3L1+8eeb8jTPnbgI7d/H2xSt3r1xXvnpD+fpNlRs3VK/dvH/ltuaVO1rXAODe0bylpHX7ntZt5Yd37mndufdISVmHbrr3VHVV1PRU1PRV1A3vPzR98MhSQ8dGU8dOU9tOU8dWU8dG/ZHZfU2De6qPbt65f+Om0s0bt69fvXrjyoWbV87euXrm9qXjyteOatw5oXHriJ+T4Zs36xkkvpqoPQCF+9ZR+Hf1OSi8nkHia4gRKwxQePwDFP4VGv5UmMT4E3LHMC4VFmhsqXFP7aqG9m2fYGtsTSaUbe3x79HwL4KGAQoPro8K/6YACn+RDBJAw0ND4HPqamq+Xgbfda3re9ZoRxU2wRQVrY+OM0THG2ETjHEJxtg4g9IQ9Tyv2zlu17KdL2c6ns90PJsFuNYFIlpkkDo29BEu7CEuXB0bqY6JUsNGquAilPGRKhXRDyqiHhAi7uPCVfGR90ENNlwZG6aMj1CrjNNuzrZuK3JpK3LrKHZvL/boKPHsRvh0IXy6y3wGUH69aN+GkvCirPTYpFzf2EL3aIRTFNIxqtw5utw1ptw9FuUZj/ZLJQSkVYRmVvmn4i29Ejp7h9zdfFlYBbn5JTh5RNi5BNk4gQkAJmbj5KOHTEBMDBnjcRubOejGvnET9FiKjZvYQJmVlYuTkxfKniYkLigsKSAoLiIsceroydvXbsuIy3Fy8tvaOTY2Np0+fYaXV/DGXY3QxGK/2BLv6ELPqHy3iDznMJhTCMwxMNvWL93SL9XWN83FO8nOLcLKOdzKMdLKOcLKJcLSOcLMMczEPtTEPsTEPtjYLtDIxlffwlPXzF3X1F3HxFXbyFnb0Enb0FHb0F7HyF7bwFZbz0ZH30ZbzwosdfTtHunaaulaPdSx0NI2e6Rt8lDLSFPDQPOBnsb9hw9UVNWUbqjcPKd89fidC/vvnd+tfnXfo+v7DO8csHpwzFXvrI/xhWCLKzEOtxMcb5YnOb17u1ajwqDH1t/X10qlzkxPr1T9uv49ULi1byG2cH1U+Pf1OSgMfk5EPP7zM0hMjI2VFRWBj/q2GSTevH398s3zV2+ev337WfEenykIhRegaXOrUPhDGmag8AoNQ4XVHDy2UD84U+3gZsjKtunvP/yNm5eDk5vtb3/7y47dCsWYhNEndQCIP6bhT0+hWxkYhlB4bh2Ff0fPl5cfz88/W1z8zFhhxuesB82v68+l5aWl+bm551+iAR9ur0LFmZRF6ZXHGqLiDbFJRsAw8fpFIQ9gXrdzPaBRYZjLRZjLhVy3i3DPKyX+SpiIR/govYponYqYR8Q4bUKsFi5GgxCrWRH3kBinRYzVJMZoACPFa1YlgKV6ZZx6daI2OdO0tdC5q9y7qxzgr3cnwrvjvfWU+w2iA7uxgWhYWFhYont4gWcC0jsJ75mA80jEeiZivBIxPklYvxRcYAYxOJsUnlsXmlPlHJHX2jXo5xfKxiEKUJgLmjknzMohSDd++vAwHz1eAgoghkaIoSe0cQKjR0dw0dNHQEy8cQMLMGi2HDsnF48Ar4CooJC4iIiUkIi0gKAUP7+ok6NLQ0PjqbPnhEQllR7oRaShAhMRAYkl/omlPnHFnjGF7pGFLqH5jsE59sFZDgHZjr7pjkGZLmH5riEFLiF5rmEwtzCYS0g2qIQsMMPBP83eL8XOJ8nWM87WLdLWJczGJczKOczcPtjM1tfMxtvYwtXAxF7P0Epbz1RHz0QPmI6R3iMdfS1NE+0H5tqqZlp3LR5et9G6bKN5zv7hKRftk166J/z0jwcZnYgyP5NsfyXT9Vaet3JxkEZ5hDY6Rg8Xa0hINCHGG9DKQtYOhf8p/dlReO7Jy7aBhZSyAQB5r9+sJyT6HX0OCoMu1tMnTwAHf2as8DfPIPH89dL80uTYQs/QXOvIfNvkk74nz6dfvlleeXlt9YYRK0xH4fEF8i9p+EMU/piGp5YaAfIKCvPx8HLc17zh7GVs46x3+vzhv/39r5eun2ofxk4sklej8Ac0zEDhD2n4z4LC7969fQclYF3Xuta1pnr79m1XRwemvLy9tfUz7wVAI53VmGQLZJwRJskUm2JGSLWoSLeoSDUrj9YuCFIpClQuCbhXEnC3NFCpNOguIkQZE6VBTDSsTbesz7CsTzcnZ5hXpxhXJhmSkg1JSfqViboV8Vr4GHV8zANirDoxDpgGsMqER1UpBrUZZuRsK3K2dSPMthFm15hr25Br2wS3o8CdaPnODXDH4njH6ACvoNDYgKgMn6g83/jywDSCX0qFf2plYDopKIMUklUVCqsJhzeGw8k+yeVNrb0h4XFcfNI8AsCkOCEahp42B4yVQ4CVnQ9KKMHGw8LOw8bOyw49iplnMxs3K7TKA8qQsXOzsHBs3Mj6008bfvyJnqKMhZ2Dg5uDk5udi4ebl5+bi9fRwZnc0Hjm/CUJGQWVh4bRWZigFFRAMsIvscybjsJukQUu4fku4XkuYXmuIXkeoXluYXmgxjUUGCjnukfkuoXDXENzIAvJdA5KcwpIc/RLsfeMtXEJtbb3s7LztrDxMLVwNDaxNDY0NtLTM9bTNNVVs9BTtTFQcTVR8bVQ9bdSDrW9F+eonOx6L9VNKdP9Vo7H9RyPq3meV/K9rhR5XSn2ulzsfaXU7wY6RIUYo1OVZFKdZlGVbl2VblWdZlmdZkVKMmlBhK8pCoMf6PPl5T/y592fEYXfvn03ObdM7XmMrp/IwQxF5nUHZnbiGydXXl7Xr+tzUPhLaWpyEotC1VZXr/1gGOCnx0vT3dNNzSOYpmFk8zCSMopuGcN0TBJHHrcuvphde7paPW3uX6Dh2ecUW2c9VvbNkQnuA9OkwRkIbam95Srq19jYN6Or0meeN38ChX9Jw+/DJPpnKr9nFH715sXTF/Nzz8amnw7OLA4vLE8tv3oKTuvKy2ul5eXlubm5xc/LK8zQ0rNnE2Njf2SK87rW9T3o5cuXVURiWlJSXXX156PwaFdtRYYtNskMl2pVkWFDyrSryrKvybInJJkio3XKI7XKIzSRkeqoKA1klDomWrMiQb8qxaQhx6Yx1w5a5tjUZlpUp5tXp5nWpBrXpBgQE7Rx0Rq4SDV85H1ClBohWp0+YPyoMkGvKtmwOtW4Ot0UuNRlWtRmmtdlWZBzrOhmDcqVqZb4NJfy9EBYUlhMRFhMQkZsepl/XElAEjY4rTI0syY8pz4iry4ivymygBqUiq9r7ohLShcQU+QVVuATlucRlObmk+TiEefgEuXgFAIGDQ9z8nFwCnByCnJxCXFyAUoWYOfkZwVYzM7LRk8/DCUbZuNiY+diYefezMK+cdNmAMQbofiJnzhZNghzsnk4O9bX1Z0+fkJUWOzaHbWQxOKwNGwwoOFEhG9CmU9cqWdMsUd0kUd0oUdUoWdUgWdkvkdEvnt4nmtwjmtghpN/qr13gq1HtLVrmJVjoLmNp6mVi6m5g6mZnamJpbmxsYWRjrXRQ0czTVeLB96Wqn6WysHW92IcVFLcVHO878N975cE3S8PVUOHqWHD7mNDlbEhStjg25igm6iAa+X+VxB+l0p9LpZ6XyzxAnapxOdKeaASIUanOtW8JsO6JtO2NsOmNt2mOs2amGBCKQ1dUxTu7empJpEG+vtX1n9dfyIUfvX67ej0UlPnPKJmLAM1EJ7bHQbrzsMPN3bMATIGr668b12/rs9B4aWlpdmZmc/P/PD61aunT558fiKqf1YAc6eeDlFGCLUDJXUDJQ1DpU0jiOYRJHUUSRtDtY1j+2ZqF5YnoDeuod68fTW1atocRMM/ozCDhqHCx0HD9Cl0AKBnX1A0de5s2ynXPUZYfNc++axxaqnx6T/a0/OCuXk4MvPDZl40D81/gMK/RsN0FIbyqQ3O1XyfKPz0+fzgXFvreCVlBA36MLQxTNs4vn+2ce7Z0Ks3axdsw4gVRpeXU5qaPn+6W0dbWxEc3kgmr6yva13ft8BvnkahIEtLuzo6Pr8NH2qvQsSZFkfqlcWZopOt8Gm2FWn2FSk2yGiD4pCHRUHqhUFqRcH3i0Pvl4TeLw1TR0ZqYWJ1iSkmpDQzsKxMMSEmGxGTDQmJBhUJepWJuoS4R4RYLXyMJh5AMFjGPqxI0CYlG9SkmtZlWNRnWdVnQ+BLZ1/L+ixLsKxJN6tJM69JNatPN2+E2TcWeNUXhRLywrH58WV5aclJybHJsJi00uhMXHxeXWwBOTq/MaawNSKnro7Sk5WTLyW7j09YkU9kK68QoGEZLj5J6IFzXEIcXELsXAJsXHyMGXVgycEjxMkrxMUrzMEjyA09Xk6Ql4ePj4efj0eAmxtCZE5Obl5ODiFuDjkhnn0yIpf3KehcO1qcGd/VRjPVVD2ya/uJkxc8gjIiMypCUzAhyWhgQYlIv9hSr8hCt9Bcx8A0e594a/cIS9dgCyd/E2s3IzN7fSNTXX19Az1dMwMda8NHtgbqTkYPPM3uB1irhTmpx7prpPpq5Pg/KAzRQEY+xEMxJzqkOJ3qeN3aBN3a+EdVMRqkmPvEKOXKCKWKsDvYoOto/yvlPhcR3ufLvM+Xep+j2/kynwul3hfKfC4j/K4hAu6gwx4QEwyqU82g8ft0m7o0m+pUG2KCKaVkbQMkaquqQKetvqZmZf3X9f2j8POXbwYnntW1zZZWjaaWD4TCuqLgPSWkUVrv46n550vP/9BhXRdDn4PCoOErLy1tbmj4k4Y2zj2baBrGVPcX1Q+WNg6V0yEYBXCqdRwLOLh9ArL+2dpnL9Z0fI6Owh9Mm/ujNEwvzzxvNrfV2r1vW+cwbvY5hT5UXPP4NS0zP1RQmDevNHruBWXkcS30lOYPgfg9DX8cNPzdovDc0kTrGKlusKx+sIQ8VNo4XNY8gqCOlreOoTsnK8YX2l+8XqOn0YLb/5fKIAEEeDolIaECh1ufObeuP4tevngxNTHx9MmTlfXPUC+VkBWgleajlhumVxRtXBZnjoyzKIsyzvNXz/ZSgfurFQSoARouDnkArCRUvSxcEx2ji080IiSb4BIMsfH62HgdbJw2KloLE62Fj3tUEa9NTNR5b7qVyfpVaca1GeZ1mVb1mTb1Wda1mZb12QCFbeqyrAAHg1Xwak2aGSnJEBg505JW5N6FCe3Bx/YQkjrwqY3otIqytLLC7Pz8QngRLru4OgFeHZvfEAWvr2sdLCwu37LtML+QIr+IIp/wVh5BWS4+6NnLbJwCgH2BMSbPbWTl2sjCsWEzB1hugoKGOVjZudjZuTk5ebg5eTnYuDhZWOWFeY9tlbi8V+7BmX329y9FWWnmB1hWJnuNtZAWZycay2Axnk56Wtpu/km+MUUe4dluweku/kkOntE2TkHmtl6mli6GRlaG+sZGerrG+lrmRpqWhiq2hnddzZT8bFUjXTXT/fUKwoyQsSaERID+UIRJQ5ZZY5ZJc7YhJUufmqHTkqFHy9SnZRlQM/UpGfrN6frk5Ec1CRrVcfdJAIUj7wKrCLuJD76OCbiCCbiMDbqKC7mGDb6KDbmOD71BCLtFCL+DD7uLDLyNCFLCRWlUJurVpJjUpVrWplpVp1gRE0ya1xiFQfNaCIeD5e+mcPtuUXhx+XXPyNMq2kxx5WhSaX9wdldsQW957Xj7wMLck5eAj1fet65/Rv8yCoPbP+hZpSYmVhIIoB1cqf2XBEh6Znp6fn5+LTNILL9abBkjVfcXMjmY8gEH4zon8V2Q4cYeU16tYdzwm3cro8Kr8fcXKEwPk/jUU+imlppgJZGKO+VTcgKfvmubf0UDHDz2pN7IQn3vwe20PuSzf3TMvqDMvGgeX5Vm+Gca/sUUuv7Z7xGFF5anqaME0I2pGyhhdmNaoFFhxunDdEzixxfa1ubEgWuhr6cHg0RSPzuvMNDQ4CDg4M729s//qHWt60+nPhoxN0Q3y1+jIMKgJMakNNq0LNq0ONwgP/BhQZBmWbg2MlIbGaVdHvmoPFILGamFjtHGJxhWJJsQU0wrkgzxCfr4eF1crBYm5iFYEuK1K5N0q1L0q1INqtMBAZvUZZnXZ1vV51jXZ9uQs+3IObZ1YJWOwgwaboDZNufbNcNtarNMwZupBU6dCJ8BXNgwMWaElDhWmzpGzhisS+skpVPxOfWYAgKqrKSkLAuOSIShGtv7UVjitp1H+YS2Cort5BfdBlCYE6Awjyg7PRaCjUOAlY2PhZWHBXq2HPdmFs5Nm9g3bNj804aNP25g+eEn1h9/Yvnxbz+wb9x4XFHGQfVchIlSsq16ka8ZKc69OSOgDRbcVRA511n/enFuqaehEZHlZGloYGyub2ptYGJqYGhgpK9tpv/QxkjTwUTD3VzT10o91OFhnLt2ZqBhQYQpIsYEl2hanWHVkOtAK3TpKnPvQ3kMYDwG0G795c7dxfYdBZatucatMIM2mH5rll5LlgGFblQ6HNOyDSjZek0Z2k1pjxpTHjYkazQkqdfH36+JU6mKVa6OU6mLv1+fqAYKpBgVUqxKTbxadbxaZYwqJkwJFaqEjbxPiH1YlWhATrNqyLSrz7CtSjajlYWtKQpPT00N9Pc/np//3f8vvjcUfvz0VcfgEyJlOr9iJL6oNyirK6G4D9cwCbD48dOX6zkiPlOfg8JdnZ3g9t/a0vKZCDux5hkk3v3j3dB8R3V/MYODm6AQYRSNHiXM4GAAUl1ThO4pQs90Rd8M8fHS5z5C6Y8LQuGnAIXrVg8GQ+VPBg0zB4bf0/DkYgOlG3HizMHjp/fDy2KQlWmYqozQGGcxCcFrd86W4hKRxNRSXFIZPqmhoxjg7+qxYSb+rqbhAQiFv69Y4aWXT9vGq6v66N2Y4ffdmFE06MaAc8foxnRO4sC5m13sXZu44S+VQQIIfNTnP8x8XetaG4HGf2R4eHxs7Ev9iTHcUY1OtCiPNcImW+KSrfApNhVptrgkS2S0QVmEDiJSGx2jg4oGKKyFjHqIidHCxelUJBoQEg3BsiJRD7IEbULcQ3ysBj5WkxCvRUzQrkrWq0k1qEk3rss0rc82J+dAKAwgmJxj1wADZgusEWbblGcPmLgBZkPJtwc03JBn3Vzg0FLi2lrm2Y0O6MUG9WGCBgjhQ6TYkaqEkaqkoaqUXlJaBzGThs+qKc9CFmR2dXVWVJG37D3FK6YoKr1LUEyRT0iehx+KGObkEePgEuHgFObkFGKnA/FKXuFNHJsACv+04YcfNvztrz9s/uEHOQGu++eP5vjbNWUFklM8G1I8W3KCOvMjO+ER7bBQWmbwWDPp5cL00/bKhsIYe+3LZg+vOJso+duqRLioJ3k/ygnWR8SZ4tMsq7JsyLm21AKH9mLnAbTPENZvoNxzsNx1AOnSj3QBS8hQrv0o126EY0ehTUuuGQ1mTIMZtuYZtsKNqDmGVJgxNdeYlmfckmsMVttyjdvzACgb0nIAHOtTM/Wa03UAFjekajWkPWrK1AHWkP6oJkmzOhGYBsMq4x4QolVxUSrYKBVMlCo2WpOUaFwHaDjLrjbVoqU8Yk1R+I830N8JCs8tvGztX8A3TubihmIKegABp5UPkKjTg+OLjxdfvXq9puGb/8b6nACJ169fM54St7L+r2rtM0gsvligjBJqB4obhhBNw+UApKgfcfAk/j0HVwIUpg8Mr9G2vY8VXkFhJgH/Kg1/gMK1M0vNiZn+0nJi//Vf/yUozKuwTRoYGzsLWBUQ4tmyXUZui6SMnLisgoSbr9nI41/LNEyn4fcoPPQ9jQqDbszgXDvoxjDGg5uYHDyGYXJw1yTUjQE2OFez/PLxiuefTS9evHi7DsTr+r717NkzDBJZWlQEgHil6vM00lmDT7HCJZlVpFkDq8ywrcpyIGXYouMMS8MfloVrIiMfoqIeoiI10NGMcV+digSAv3r4OCj+gZSkW5nwqAJwcPQDfIx6RbwmKUm7OkWvLt2wNt2oPtOYnG3aALMEvNuQa9uYZ98Md2jOf29wh5pMi+oMMzLMpgmCY5tG8J58u6YCB1qpayvCraXUrQ3h3YMJGa6IHauk03B16lBV6mBl6gAxpROXPDXQVtfYsn3/WX6RLWKSWwVF5Hj5pbh5xbm4xTg5RTk4hDk4BNk5+FlYuTduYv/xx01//+uPP/zl7z/99S8sP/yVd/NP20T47hzfE2b1kJwdPlkFH6vIbs0Lp2YFdsAjugtju4vjuoti2mCho83E53Pjc83I9rLIwlB9TIIFGebSWuLZVe7bg/TuLnfrRbn0oZ370E79aOdBjPMwxmW6yn+S5DeIdhtEuQ4iHXvLbHtL7XpK7PvKHPsRTqDQWWgFeJcGM2rJM2qFm7TCTamAjPPMaXkmAIUBE1NywKsmrXkWNJg5Jdu4KcugKdOgMQOYPrCmTEMKzLgx27A2TYeUqAWsMvEhMUGTGK9eEXsfD1AYMoDCKsiI+9gYbVKSaX2GdU2qWQtibUeFgUCnDcr8t/w7/xh+QxT+OBEEvDskuwsU6tpmR6aePVl6/ebtOgF/YX3OqPBK6bO19hkkJp8M1A6UkAfLAAfT/1uHOJgxpgg4GLAUk4P7ZyoHZkmACBdfTK04f2X9jMIL5BUafo+/n6JhqLB6bHh2udkn2GbbDjlpWVEJaWFpWTEpWVFJaRHAvvJbpRirElLCkjIiTp7GgINHPnwE3SoaXhkYHpiBUHjNegK/q2cvF5qGsTX9RfRuDJIy8uluTO80EfRhgM0sdn/B3+qv6QtmkAACncyhgQFqc/Pjx39Wjl/Xf4imp6aK4PCcjIyhwcGVqs/TcAcJk2iKiTPEJkD/5hOSzStSLfHJ5qgYXcDBZeHq5REPUBHqKCiJhDo25iEhTpuQoEuExoMBB0Ojv9XJuqTER8T4h8CqkrRr0yAOZlh9plEDQOEci4Zc68Y8O8C+1AInSoFzc4FTE9yxIde+PsemPscaUHIzzLYpx6Yhxwq8k1Jg31ri0oZwb0d6d6D9e3Bhw5Vx49XJ47XpY7UZozXpI9XpI1WpI5XJ84O09o6e+0pqF4+funb23JkjR4/s3rtvm+Ju+S3bpWS3iEnICgtJCvKJ8nIIc7GIcm6W4mZVEOQ8KCN0dZ+c9qWDQcYqmGjXvrLEmcrcuZqCYVwmNSe0Kd2/LS+iqyShF5k6gErvLYufbK9Zmh2baUSMVaYP4qMmSdHTNfGTtQljpKghQnAfyrMH4dRVatNRbN1RYtdVYtdbZj+MdR/CuneV2vaU2fWV2XcX23YV2YIlKPeVOQAs7i62boOb0mkYQt6WXDMqDDJQpsJMKDkmzTkmjTmmzbkWTTDzBtCjyDBoyDAEBgpkQMNZRk05xuRMgML6Vck6hDhNfKw63UCHRI3OwaoMIMZEP6hI1K/LsKzPtK5ONqEh1jaDxNu3bzvb2zHl5X09PStVv6K1R+FXr9+OTC81rkoEAQxKBNE5Nz6z9Gz59de/kf3n6l9G4fn5+ZHh4ScLC59/71/jDBJv373pm6WujjH9KEQY4uCpDzh4YLZy7lnfu3+sxV/t72OF6Sj8noaZ+Pvh6srAMDAmCo8v1NfRCvNKowvKY/MRMfCyaHhZTH55LFjNK4vOLYnKLY2ClUSCZS2tcGQemj/3EQoDW03D3xsKjy/0gW5MPdSNQa4OEf64GzNNBKevf4Y4+rjxxevP/ePitwV+t18wgwTQwsJCeWlpblZWZ0fHStW61vVdCnT/mhsaaquqnnyJOXNAQ+2VqHhjVIw+KsYAHWOIiTMCTIyON0JG65SGqZeEqJWGqJaGqJSFqpaH30dHaeLjdSqS9CtTjUipRlWp0NBvXcaK1dKX9Vkm9ZnGjBpylnFDjhkZwBxAYbh9U75jM8TBLs35kFEKXWnF7sBAZRPcARo5zrFqyrOlFjq1I7w6y3270YHdmJAubFgvIXqwMn60JnW8DqBw2mht+nBNGkDhuZ6G6YlxVFZKaXx4aUIELNQv0csp0tE82MrAx/ihy6N71vevmdw5q3f1qPalAwaXD1rcOOykcibU8G6Wqx4yxLo+2bMDHjZQljiCyZggwvoQSU0Z/uRUHyostKM0oQ+TOYTPGcRnzfQ0ARSeJiMma3Mn6jMm65LGa5JGqxMHKiJ7scE9KL/ucs/OUpfWQtuWfCtKDgBcs85Cm84iWxrMlJJjRMs1oeVClW355pAVmLXAQY0RJdugKUOvIV23MV13pQAtDchp+pBlGNQD/M0yArxbl25Ql65fB/oYqbq1KY+qk7SIceqEGLWK2AfEOE1C9ANUmDIyVAkddg8VpoQMUyoPVSoPulMedLss+G5JsBImWqs61bQq1awiwaC5JGhNUfjNmzdVRGJqYiL4yf52Y71mKLy4/Lpz6AmhaQpQb1JpXxisO76oD3x119CThWev1lOhrY3+ZRSmNjfnZWfX19R8fiLJNdbLN8/bJ6rIg6UrMaYfTZWjc3AvHaQYHAxwcGiuavpp25t3X2sa05u37x4vvmKU3zFjhRkozKThDyIlfp2G52snn5JnlykLr1sW/9H+7B8dwJ6+bZt7QZ1eapp61jgJbAnKsDb+lDw8VwPsIw5mGH3HoX0HB2E1Cr989Zba87h75OvC5a8JdGM6J+ugbszKcP4qDmZ2Y6YremeYpw86g0+ef90GDaAwgOAvlUEC6Pnz51gUCp6T09HevpZzSde1rn9B4Baw9OUGMsa6a4gZNoRUS2KadWW6bVWWfXW2fVW2HTHNAhNvgIrRQcdoo2O0gOEABCcaEJONK1NNqjPMa4Clm1SnGVWnGdakG9VmmtRnm9PNgpxj0QCzaM61oORaNuZaNuVZUwodaMUuLaUeLaVeLSXeLaU+bQj/DhQg3WBgnajADpR/O9KvC+XXhfbrxgT04IJ7sMG9mNA+bEQPMFxkHyF6uDpppC59pDZ1pC5jqC5jmJQ000l8tbT4bKBrhlo7Sa4Yq8YMVpb34Ys70bltiPTm/Pjq9BBUpFuBr2Wup1Gum16eq3ahh165nwkhzJoU7VCb4NaU7tcOjxxApo7iM9sLImoSXGsTXClZAZ3Fsf2o1GF8NqDhuV7q0vTIWHXBaHX2aF0G+PaR2rSR6tThqqTBytiBiogBXEg30q+txJUGt6pJ1qlN1mnONKZkmTSmGzak6jakazdkajdmgoIOOe1RXcrD2mTNuuSHdcmatYmaNQmatQmadUlg9VF9qk5dmm51sjYp8VFNsl49cIfGg43r0o1qUvWrkh5VJWnVJGmR4jWQIXeLfa+W+l0r879Z6nejzP9WacBNRODNsoAbJf7AbhX73iryvVngexPucwsRql6RaAhOHDZGm1zgu9axwh1tbSgEor219dui8OOnUBAwun4Chh1KKO4LhXUllvTjGyf7xxcBHK8/H26N9a+hMLg911ZXg55VDYkEflsrtf+qni8vj4+NzUxNrc1Uoeevn9FG8Y3DCDoH/9p/66s5GPBi9fRTyj/+8bXiN168epuJHiQ2Ty+/AAfzzfQi9QMUZtDwHw4aHn9SP/ec0jaAgSNiUnIC0/NCcDVZI/O1M8+bxwBDL9SNAntSB6VUo6Pwb9MwHYVr37yDAqtGppay0IPGQY1FlaOMLV9jvXi9BM7dz92YD6fKQd0YwMGrTx9kpLlnfaAJXPmIryDQug709QEObqXRXr1a6dJ8jsAHDg0OAg4GhLFSta51fWcCd4HZ2dkvNRjM1GRfY12+V22ea2ORD6UssA0d2oYObkEGNBV71uU61sIcanPsa7NtqzMsazIsa7NsajKtqzOs6rKhrMA1GWY16aYAiGszTOuyLJgJg8nZVo0wKyrcjpZvT4HbUvPtW4ud2xGenSj/bnRQNzq4Bxfei4vow0f2V0T1EyIHKiIHiFGDlTGDxOhBqBA9QIzoxYf24cMHoFejBipATcxwdeIo4ODa9NG6rOHajCFS4lQb5vXzxaWR/mlKzVgtbqQKOVyFHKpCDhBLByoKulCZlLxoYpwnMtimxNe0wEMPcHCpj3GZjwnCxxgZYIYPt61L8mzNDetHJA2ikynZ/oRoO1Kcc1NmQEdRbD8yZRiXNYDJnO2mLE4ODlZkDhGTh6tShmrSoDHp6rRRUCAl9ONDe9H+PSifrjL3jhIHCsyckmPWmmfZBrdohZu35pm35JnQcg2bs/TIqdoN6brkNADEAIv1G9L1GjP0G1L1GtJAwaA5y7g5x4ySB3UhGrLNmnIsW+B2bQVOwGj5ds0w83ooNMKgEcBxmgEpQQsffR8broILV8GEKWPCVNBhSuiwO6iQu+VBdxFBd0uD7pQE3ikKuFPodxcdoQVQGHRjMFGa9XDPtY4VfvrkydTk5O+21F8JhacfP6d0z5fXjoNbaVxhb2gONA2uijYzMvVs6fl6EPA307+MwsODg4CGAQGsVH2GxkdHi+BwPAbzRR5h/7t6/mqRNoppGin/9RjTyv4ZJgdXj9BDCPqnmx4vLj5bfru49JppT1eVmQYqf63+oxqGvXj5dnJuWdeXrOJck1w22Dk0N/6YMr7AiBX+YGD492mYHjQ8vdRU1ZinoX1HSISPjX0zNw/7voM7fIJtesYqJp42QEERDAKGUguvoPBvhEkApgSfufDsKa5h2j6aesWCeM6E4J3SVtMyW90yAxkNstrWWXL7HLljDlq2zzV2zoHrndL9mG7ztN7HoAPcNrDQ1r8ACu2DT7qHnzKtZ+TpwPgi04Ymno1OLzFtfGZ5au75zOMXr17/Y/HFPHUUu9KNoU+V+2A4/xccTN+RqtnFrrfvvu5/Fy9fvFh8+nR5eflLjY2BS4zxUaB/uD4wvK7vUPPz8xU4XDWJtPBFI9ohFIZ71+a6NRb5UsuC2pAhbcjglvKApiKv+jyX+lyn+hyHuiy7ukyb2gzrumzb+hy7+mxbco5NfZYVPVuweW2GGVhCT1SG2TTmQY9TroeyBUOJI0BlA8BietgDpdC5DeHbjQoC1okK6kKHdGNC6RbSjYXGgHuwoX0QH0f04sK7McFdmMAebFAfPrifENpPCOsnRAwAUCbFj1Qnj1SnD1alDRATJmjlr5aePBnum2gmDdUg+4hFPbi8bmxuNya3C5XTUpRUlxqECXcs8jaBOT/KtHsAc3pY6GlY7GVU5GlY4mWIDrKojnOh5QT3lib0lMbVJXtiwmwrYl0aMgLbi2J7UamD+CywnO5sfDo5OEBMHaiIGyTGDVUmDpEAEyePVif348Jbi91ai+w7Sxw7iuw6im3bC206imy7iu06QbnYpr3Iur3Qsq0AAmIqzBiKlAB8nG9ByzOj5pqCQmse4GZzWp4FNc+Clm/dUmDfWuTYUuDYWuDSWerZA45YmU9niXtLvm0zzIycaVSfalibol+TrENKfISP08DFPsBEqmDC7wEIRgTfLg26WeJ3vdDnar73lQKfqwU+1/N9biPDNNBR2qjIR2XBKjUwt7VGYaZevXz5G+31F0Rh8CVjM0uAtMqqxzJQg4C6wmDdOdghcKecmF1efvHmC9011vWv618OkAB6QdfKymdojTNIPH/9rHUc1zyCBBz8W1Pl6DgIMHHscW33RE0BqS4T3ZeHH8nDD/9suFVlpoHKX6v/qIZuBRUjmajBe47VR3WwZ40rzEMbKlvqp56SJ54wIPjXafhnFGbQMFQAsNs/RbqveeNvf/ur/BZJdg6WzSwbhIT5uHk5IuLdxp+QxxbqRuboNMxAYSYNf2psmI7ClWNP6qYePw6D9V4yrzikhQYobBrcFJnX84HBe8BviWkxBT2xhb2g0wsMFOKL+hKKf7bEkr6k0n6mJZf2pyB+tlTEQDpykGkZwFCD+RUj4zPPn76Yov5+N2YFhcH2g50CPZmZxc43b79WcMtX1ZOFhc729qnJ9WfIr+u7U39vb05GBjwnZ3RkZKXqS2ikoxqXZImOM8YlWhCSrStSrYlp1qQMG2KqJSbOCBmpjQjTRISol4eqo8I10JEP0VFaqMiHqAgNYMjwByhoUt0DdJQ6Ie4RKVm/KtWQlGxATNCpiNcmJGjj4x7hYx+BlyoSdUipxuQcB0q+azPcrSHXhVrs1Ybway31aSv1aSnxpBW70orcupB+3aiAjnLfdoQPWNKDJfx7sQG9WIDFwb34sP6KqEFS3CBg0OrUPmL8eHPxq6XHi6P9k5SaoTp0b0VBOzKdVpxIKUxoyo2tSQkpD3HMczNKt9VMtb6fZq2a7aCZ76ZX4K4PloUe+gh/U1KMEzUrqKckvqMgihTjggq2JkQ7kTP824tje9FpA/jMHmTSVGfj4tTAADF5kBAzVBE9XBEzTIwdrowdJcX2oQOocFsa3KIt3xJAbVu+WUeBZVexbXepfVepXWepQ3uJPUTDRdYdRTbtBVZtBZZthVYdxTZtRVathZYrLxXbdhTbt5c4dJS5dCE82opdm3Ltm3MdaQWu7eAoFXm2FLg0w6zImSZVybqEGE1clDo2UhUdoYoIVykNUyoJuFHid63A+zrc61qux4Usl7MZThdTHa4kO15PcLiR4Hwrzet+ho9Gpu/DVA+V8hSXt2uPwm/evBkbHW2j0RZ/PQHW56Pw6zfvhiae1bbOFleOppUPROZ1R+R2g/s9pXt+5vHz5y/XRzi+I/0LKMxIRfIFR6pmpqaIeHxDff1nPqrjD+rlm+XOSRJ1FLX6v3WIg6dW5lrROZgJUrWAHXsmauHE+hREbyZqCGAr0wCirV5lGKj8tfqPahiWjRlKLuu/51R9RBt7wZRoG9WMaSJPMlH4lzT8Hn8/ScMzy5SC8lhePs7zV46jiKl3lC+du3QsLtVnx+4th4/vaelDTS81QY+gW03Dv47CwAAKg/e8eP2sf2wJEK2GR91FswpAt+Ozy/1ji32jkIFCz8jTjsEnTGvrX6D2PAZG64WsuXse/MyYVtc2W02bqXpvlZRpfOMkrnESLIFhyRPI2nGmIWrGiypHQaM0Pf/iyYtJ2hiGQu/G/DJE+AMOnl05fQCFZ78+Cj9bXATAuvD48Re8LkBzTWlqysvOhiZ4fIm4i3Wt6wtqcmICj0ZX4HDgx79S9SXUR8HmBahle93J9VUpCHhQGAQZI3dEvt/dbPdrmS6XMp0vZrtcyvO4Ave4CnO7lOl4NsPhTJbTOZjrxVy3i3kelwq8rhT73SgPvosMUUJAE7buoEKVUOHKqFBlVMg9ZMg9UMbGPKxKNqnLtK3LdiDnOreW+XaigtoRfu1lPq0AhYtcgAEU7kEHgGVnuU8HwrsT6QOsC+nTjYLq+3Ah/YSIwcro4ZrEgdqU/sq48Ub4q2dzS+MDk5TawWpsL76wHZHWmBddnR5MiPUu8bdNtX0UZ6oSa3w3yVw53UYt2+FhrpM2zEk7x+lRnqtOia9xZYwzNTuopziuFR5BjHFEhVhVRDs2ZgZ0lcb306fN9aNTZnsoS9NDQ6TkIULkSEXUCDF6pDJmjBQ7UR07hAvuLHHuKrbrKrLuLLTqKrLqKbHpLbPvRTj2ACt37kY4dZY4dJbYd5c4gLdBQEzH3/ZiYDbtxbbtRXbtxfZdZa49KM8etE8vxret1L0BZlefbdcAc2iGuwCj5Dk3ZduSM8yqk/WJsVq4KA10uAoKWJgqKkQZEXi7xP9mvs+tPM8bOW7Xk1zuhNsqe5lpOJnqWhlqmeqrmRmomuurWRlqmGorh/q5/sHAyC+JwgsLC4zZGN2dnStVv9C/jMLLL970jS2SqDOAesGtPTy3G2BWadUYuCnOP3n5cv1ZGN+l/gUUHhkeJmCxXR0dX2rCHLgSlpeWnj9//qX+XP5tvX77sn+2kTaG+uC/9V9MlXsPUrUAMcFyfL5zcfnF8ou3Sy/efFkDXcfp+ed6fg0PPetSy4cGJh6PPaaOL9RPLDSsEPBqY+Dv+2lzH6PwQv38C6q9q6GAEA+qMu35P7of6SvdVr44+5wSleghJMKHIKTMvaQy9muFgH9/YBiaNvf6LTRg//L1WxJ12julDV0/wTiY/5rAeX4L2bsVe/sOHARgb+gGCq9ev331mrF8C74UtB5gCbyePJ9pHccxuzEfTJVbdfqYHAwFT0Mo3PX27VdESfC7BS0qori4kUz+gk9LBlTd1NCQlZYGOorLv5cEc13rWnvNTE8DIP6y7XYfBZPrr5zteTPXV6kgULUwCLKS0PvFwSp5PrdyPK5kuV7MdDmf4wZQ+HKB55V8j0u5rudhLufg7hcLva4WeV8t8rla4ncdmsXlfwMZfBsZDEWsYsKUcJHK2AhVXMR9fNR9fIxaZYJ2faZFE9yZWuLTjQkdgAKCo3pxYT3o4C5UQCfAX6RfNzqgBxPYjfLvLPftKPfqQHh2Ijw6yz06oKVXD8avFxc0RIgYIcUPVSX1V8SMk3NePptdmhycaK4eJGF6sQUdZRlNubGVyQGIEMcsZ71YE+VoI6V4k3splvcz7TSy7R9m2T/MBObwEOasXeZnWh3v3gIL7SmOp+WFk5JccTEOlQluFFhIT3nyIA42UgEfrch5Mty5ODUIULgfFzaAjxyqiBkiRg9Xxg1Xxvaig9qLXFvzbWi5FpQcU2oOPSFannkr3KoFbkHNNafAzJuzzZqyTN+bSUOmUUOWcWO2SWO2Kd0smmA2LQXObSXuLcVutGI3aqFrc74zOFDN+a6UAjdgVLgrNdepGdBwullNslFlvA40NhytAZgYG65WHqJcHKBUHKAM81WLcdfzcra2tLB5ZGCvbuh2T8fxtobFdVWDK0qPrtzRPH1F2cTa5Rug8NLSEkDh7PT0tpaWlapf6J9FYUYiCFzDZB5uJRFEXGEvqn49EcSfQ/8sCoPbM7muLiMlBYVAPPtzzul59493E0+6WsZQTA5m/rc+8HOI8AoHA5aij7bWLr36kn8CfiQAf9mYodrW2ddQm/Bm9hk0be7TKAwMEPCvBA0DgJ5/SdU2UN65Z0vfZOXyPzrVtW/fuHMOVJIa8+QUJFJhQbMvKKOPoV37gzQMUHh4rnZ1MrXJueWx6W+TW2351dP2CSLoxoBz93M35lMhwtC5m2fET9fML/WDk77yEV9BAAW+bAYJpgBq1NXUfKmHF6xrXV9Ec7OzE+NffkIRQ6OdJFySKTpWDxtvhEs0xiUYYxMM0XF66Bjd8ghNRJhaWej98rD7iBDV0iDl8lBVJLAwVVyUemW8dnWSXk2yflWSLj21sHZVkk5tqn59mkF9miG0zDAiZxo3ZpmCJTnblJJv21HuDdgXIOxYXfIYOWW4OmGIFNdPiIAig9FBUB4JpH97uU9bmUdbmXt7mVt7qXMHZE5tJU7tpS5d5R7dKO8+bNBQRdQAMQY4jtVlvHg6/QygcFPFEKm0F5vbWZZOyYurSglChDnmuOknW6knWT5ItVbPtHuY4/gox1Enx0knx0UX5qaX72GIDbdrygjoKozpLUtqK4ypzwqoTvGuz/BrK4gaxKSPEuGT1UVTdWVPRnrnR3v7CQm92NBeXEQvLrqPENNfEd+LjelABDXnOZMzLerTTWpSDWpTDevAvoMdzzCuT9WvSdKuTnhUlaBTGadNjH1Ego6YLrCaFL3aVIOaFPBmAMTW1FzH1gK3tmLP1mLPliJgXi3FXq0l3u2lfp2IgM6ygPYSv7ZCz5Z8l+YcW3KGRW2qcXWyfmWyXkWSHi5eCx2tiYjQKgwziQ10dHT1MXaOMnCO13aI1XaM1bSJUDMLUtHzuavtclPT5pySkYljwOu1R2Gg7s7OJjL5N+Lc/yAKP376sqVvVSKInK6k0vVEEH8+/dMo/OYN6EcV5+c31NV9qVHhNc4gAbSwPNkxgQcsxeDgVf+tr0yVA0S4ioPrJxYal1/Nrzh/Bb199275xcq+v/vHq6knKxkkfmNgGBRW8Hd1mT4q/Mjg3qGju/smKp++bVN/dPvm3fOPX9LQlekSUsKZ+aFzLygMDmbYCgGvpuEPp9BBKDz/AQp/Q715+7pvpr51HL3CwZMrw/nQ6VvNwXMQB9NPHzgsdU+/fjK1L5tBginwycwO55P1pzGv6zvQ0tJSDYmEKisDjfZK1RfV7GATrdSXWuRJLfKmFkPWXOhBznNphLs05jo15TlR4E5NMLvqVDNCvAEpxaQ2w4L+qGSHtmKXToR7N9KzC+HRVe7ehXDrLHMB1o1070F69KA8etFe/RjffqxvL9obQlhC8Fhd/FRD+kxT1iwVNtWcPVaXOlqbPEyK7cOF9WBDu9DBnUg/gMv0MWB38OGdZW7d4CsQrt0It16kVx8afJRfHyFkqCp2uDp2iBQx0Zj14unMs/HBiQbcQGVBFzqrvTSVCo+rzQjBxLgV+JplO+lmOmhnO+rAXfULPU1KfMxLfS3K/C0RAVbIYJvqBI+2vIg+RNIAMq27NKklL4KaE9oGj+wvT56ohE/VlPTj4JWZiW31dY/H+geIqUOEiAFCdD8B4HtsPyF+sCJxABfdUerTku9Iy7Ol5FhTYTZt+fZthfYteVaUbLPmDOPGdMPGDGNymnF9ikEdsFRDcrpRY5YJNdeSmmvTmu/YVebVU+7Xiwrsx4T0Y0J70aE96NAuZDCwblRINyq0GwkspAvh317i0VboTM21a8yyIGdAWTtImaakdIPqVAN8klValJ93SKq5b6apT45lANzCD2bmk2HmlWbokqhrF6tjG6llHXrPyMcpIPkbjAoDvXjxgpFMDXz9J7fgt1F4av7nRBCxqxJBjE0vLT1/s54I4k+nfyFA4sXz5/19fbMzMyvrn601ziAB9OrN8tBcU8cE9uMY01UhwkwOBss1CDZl6i30tDkqA3ABCn9Aw78RNPyehudeUK0ddRV3yLX0ohbftTNQuGMIa2H3SECIF1eTBd6wGoWBfYzCH9Lwd4XCQJNPutrGMR2gG/NbU+UYpw86d5NPKC9eLaw4fzV98QwSH2lsdLSKSBwZGlpZX9e6vpFmpqdLCguz0tI62tpWqr6oHo/QutChnchAsOzGACQNg1I6QOl+Q7rRwb3ooB50QEeZVzPckZxj15zv1F7q2Y306YSiF6C4hR60Tx/Wrxfj04fx7kN59iI9+lBefRifAbz/ICFgqCJwiBA4iA8cJAQPV0VONqTMNGfNU2Hz1NzZZtgkOWOiPnW8JnG0MhpQZi8G8J9fV7lPNzCkdw/Sq7vMA1hPmXt3qXsPAny4bx8moL8ibLgqfqw6caQqdrA2+8nMxFh3bzOipLk0l1qcSS1MaoJF1aYHVyZ6I0Ns8z2Mcl30CzyMEf5W2FAHQpRrRbQbMdq1MtqVFOvWmObfWRg9iEodxmYNoNJ7SpO6ihM6ihJbCpPqchLKYkNDbMx1birBs+BTg93dmJRhQuQAPqoPF9OLjehBh/ehI/pRYR1FHi1wOxrMsjnLjJpt2VfmNgC6AQjnnlLH7mL7riK77hKH7lLHrmL79gKbtgKb9kK79kL7zmLHnjLXfpT3IC5oEB8yVBE2RAgfJIT3QSnkwnrxoT1Y6Cx0IAPay/07y+nx0wjPjhLX1kJHSp51c455Ez2Lc12WSV2GMSHVPjU21Dsc5hhe4hBR7BheZBsEt/HPsQuAWfpmmnmlm3kDS9N2jPWOgQNwXDn3v6kvjMIMvXz5sren55O9ul+iMGjbJ+eWAQGDekDAAJ5CsrtAobZ1dnRqaXF5PRXan1j/FApDN3t6kocve79f4wwSDM0vjQCQYrDUJ0CKHiLMgEsAo8/W6qnLQG+hp82toDCw36ZhxhYCY5Znn1MSM/2FRfmz8sOe/aMDoLD8VsmHunf5BLguXz/VNUKYetbIhGCGMfH3Axp+HynxvaHw4ovZrskKgMIr3ZhPnj4ovzLjgNTNPev+2pnUvrZevXxZQyIB+KiqrFyfP7eub6vnz5831Nfj0egvOBqyWjMDTZQizybo2W8e0NhwsQel0J1WDMruTXDnhlx7co5tfZZ1VappZZIxWNamm9dlWNSkmVSlGFWlGFanGlenGpGSDUlJBlVJ+lVJesBAmZQMKt/XpxjWZ1k0wh260AFDUPLgmF5cZC82sg8f2YeL6MeEdiP924s9mvMcyNnWjTnWTWCZZdmQBUUd1KUa16caQ8s0k4ZMy+Zcu5ZCl+5S/wFkWB82uqYoeaC7F4+scTfzDrH1SHD3zPbzKAhyLQpxLg1zLg6whXmYwNyNi32tsOGAfb2r4n1JCT6kOC9SrHtVnGd9ij81N6qjJKmzNKUJHo9PDikI9Uh0tfE20Te5p3rn9KVDCvt2y+zLTM1ta6yrg4UOYsJ6USE9yLBeZFA3IgBsRmeRFxVm15huTE7Rr0/UqU/S6y6yH0C69iNd+lHQcgAY2g3YMNZjCOsxiPEYQHv0I937yt1BzwEaNccF9OGD+gkhfbhgYN3YQGBdGP92lB9kSN/2ciiZBjSDEOHeUeLcVuRAhVtT4JbUPMvmHCgDcW2GOSHVMj/OKcjfyyso3ic6xze2yD2yyCE03z60wDoQBswmMNcmEGbsmRaYWPotUbi/txeQBwGL/WUqiTTkALYBmhPz9u270WlmKrSBiLzuMFg3HD/c2AGlQlt6vp4K7d9BfxyFX7x40VBX10gmf9n5wkDTU1MEDKa+tvaLpGb7g3r99sX4QmvPFKFvhrgSY/oepBgsxSBLwFLzSz0AT1fcvr4gFH4KoTCTgD+mYaZ9RMP0geGpxabmzrJrt88GRjg8fdNmYqX517/9hY198579ivmImKnFRrBHjH1cbSv4+yENf58o/Obt69HHrZ2TuI+mygED2wz2ZXVoBDhuSy+/yg37Iy09ezYzPf3kyZOvkQP4zftUEl8wKmld6/pn9WRhgfHMl8XFRdBof6U/QIbbiOg4fWSUFv3BcrqYWMiwsbro6IdlYfdLQ1RKgpQLfO9kuV5Nc7iU7nAxw/FiptOlTOdLWc4Xs5zOZzicTrMDdhZYuv2ZDIczGfZn0+zPpdmfT7U7l2JzBtRnO18o9LuNjFCvzbKkFblR8l3qsuxqM20bcx2bgMEcyZnWVcmGhLhHmEh1bOQDTLgaOkwVHaqCDlVFhaqgglVQIWD1Pi5KszJRvz7TjAZz6iry6UHGlKQkUZrac7JxZ49oHdtx5dTOs1cOn1M9e0n76lXjOzfNlW9bq95w0rjjo/sg3Ew31tooxtog0lI3wlwrzFQz1FQzxFQr1NIwxMrEz9TA4ZGm1o1bV4+ePLJt93aJbXLCW2UEt0vy79whewyWVdRYSahM8xtAhnaVBXaVBfSU+vaU+vSU+HQWeVBhtg1pRvXJerUJ2jWJOm1w6z6Ecy/Cua8cMgDEvUiw6jSC8x6p8B0k+AzgvIcxvkNY3wHI/PvxQT24wB5cUA82qBcbBM0axAR0Qijs24Hy60L5d6P86YPl3l1l7p2lzu0lDrRCG1qBTUuBDSXPipxjWZdhVZNmiks2ygoxTAi0i48IjI9PDo3L9YrOd46A2wXDbIMgDrYOyDHxyghJQ735Y23mV0Hhro6O7PT0suLi+dnZlar3AiicVNrf3DVfQhpNRfSH5XZH5vUUV45Sex7TU6GtB6v9W+mPo/D42FhBbm4+DDb8pf+lBXd3gNfLS0tfqW39NS2/Whh93ExH4cpPhQgDq51ZbH/5ek0fMryCwvQMEr9Dw3QUBoUVFKaXGYarziTW58wsNeUWR2kbKFvaayMIKRNPwUv0x3DM/zEapodJfG8oDPTs5fzAbC29G/N+puOqqXKruzFrMyQMfrc9XV3IsrLmhoYvmEFitQCFtNFo83PQpfp8eXlt0g6ua11MPZ6fryaRQJfsK/3CmeqloPMClGE+t/L87+YH3CsMVCkKUi0JUS0KvAf3vZHncy3f53qO2+U0hwup9hdS7M6m2J5KtTuT6Qgg+Gya7clUm2MpVkdSrMDyKCikWoMl3ayPp4JXbU9mO54t8LyECLqFjVGvTjOqy7KABpXTzYDVZljUppnXpJpVpxgTE3VxMZroCDVUmAoy5B4yVAlAMCbiATZCAxuhjo/UJERrkRIN6jLMG3KtWwrdelABtJL4YJdgUiWltKz22JFHYsLH+DkVBTjkBDmlRbmlpfjl5EQUtohJK4pL75KUOyi77ajC9iMKWw/LKxySkz8oI7dPRm6PtPxuqS2K4lsVhOWl+GXEuKWEOCUEOSVFeOUkBBWlhXdLC+3ZIXsCll3cVEWszQ4Zxcf0YcL7sdDw8AA6bBAbNoQN7Srxast36ihwpEcJOwxgvEeJQcNQZIjfSIX/CNG/H+fRj/EYwYNy8BApaKgyaLQyeLwyaBwsSRFjVVHDlRGDxPChyqihiugBfMQgIXyAENaPDx0gRAxUQJVDhKhBXPgAJqgX7d+L8e5GASZ26ih2bCl0bMi3a4DZN8Nsm3Kt62D2pEwnfLo7OiO4MD0+IzU1Ki7FLzzNIzTbOQjmEJBj5pURmob5ligMGtbaqqrOT+XDghOGnWJpEXk9Mfm9iJqx9oGF+acvX6ynQvs31R9H4enpaRQCAe738/NfcQLZGmvp5dzEk9bBWTrwARSeZ6AwFCAx/pg8u9ixBmGmH+kjFGYQ8KdRGNgvaBgspxabHr9smV2mABSeXGwANRNPGx6/bpl7Cc3GA7tGR+FfoWEGCq+i4cG57w6FAXw+XhoZmK1mjugzujEfcnDt9NO2F6+/8FNhPymAwl8pg8RqMdpqsGyhUoH9SfO3rOtPqq6ODlhmZklBwfTU140WG2qrQMTolkZolsfooGL1gKFj9fAJRvgEQ1S0NiJCAx39kBCnWxGnV5lgWJmoT4zXqYjTqU4yrErUJ8Ro4SLV0aH3EIE3S/yulvhdLvW/WhZwvTTgeknAjbLg24jg29iIe8TY+5XxatWpOg055s15ds1wB1qBM63QlVrgQoE7U/OdKXCnxjx7co5NXaZFbbppbYYpOcu8IcuyEWZHyXWk5jnR8p1bClzbij06ED7tSL9ubFh/ZTw8OlRTyQmDbi5H1ijuuMsvdEJQ6DA//24e7q3c7LLc7NKcrOKbNwhs+BvXT//LvvEvHJuB/ZWd5e8cm//OufkHLtYfedg38HFuEuJmEeNhleRnlxbikhXlVRABxqcgLrBNSmiHjPDu3fKn4blltQRCDSxytj59oi55qj51ui59qj5ztjF7riFrkBA9iAubIEVNkCLGqyOn6mNnGhOmyHGTddGTdVHjtZHD1YCA/ccA+1aFjVaHj1SHjVaFjlaFgOVYddRETdwYKQayqrhhEpSgbaQyZqQyepQUP1yZ0IWOaSoMI2UEIWO9C8Nc88McS2Lsq7KcO8o8ess92kucm4scGwvsKQVOrUWu7SWebaVe7SVetELfxvyAWngIJiskLzkiMSYmLDLBLzTR0TshMvmbBkgALS4uMsLOABYDY1QCtQ0slNeO9448fbz46uV6KrR/d/0RFAa/E3CzBxoZHh7s7weFlRe+kAA9TE1MzM7Ofo0/l39XL18/m3vWP/mENv64EQowXSBPLjRPP21fWB56+foLh4L8EUEo/OQDFGZAMLPwHoJ/hYYXyANTVe2D2NZ+DK0X2TGI7Rmr6B6taO1Ht/ShhmarV9Hwp1D4FzQ8NFf1/aEwOEpvHi8NDc/X988QGd0YsP0MFAY9mbGF+pnF9uevvuTzYH9D4Iro7+3FodEtFApjUvLX0/jYWGFeXkFu7tDg4ErVutb1NcVolsEPr7SoqAKHW1hFC19DY101lRm2lamW1Vl2dTCnOpgzWDbA3RrhHmSYS02mbXWGFTkbChcmZ1k35QCzaoJZ0eB21FybhkyzunST2hSDyoSHxHj1qqSHdWk6tel6Nen6NekG5Gzj+gwDcoZBY6YBOVO/IduUArdrK/HoQQcO4MP68WF92NA+bNgALrwfF96LDevFhfVCNcH9uBDwhkFC2CAxcrgyeoQEuDBmGCyr48brkyfJ6ZONOUPk/CjfaK0HsQR8F6mKeviovozCHVmFq9IyZ8REDwsL7hHg3cbNIcPOKsW+WYpzsxQXixQ3ZBI8bJLc7JKc7OJc7GI87OK8HBL8nJKCXNKC3DJ0kxXkkRfm2yIptFNObP8WicNH91xDlBLwCDQ6NXSmMWuSnDbdmDndmD1LyZ2n5s00ZQ+R4gcqosAWjpEix6oixqojR6rCh4mhQxXBQ4SgoYqQIWLwYIX/CMFnGO8ziPUZwPr2Y/x6sX69+IB+QsgwMWq4IhLYUEXEENjZqoQebGxTYQQ2JTA72CPM3srh0SPt28p3z964fvzatRPX7p67bHL/VpSTHjHNtRPh3lJo15BrWQ/6GDDbFijdhwPoaTDCTpphjg0wl6ocd1S6e2GyV2asV4S/e3Z6xh+8738tFGbo6dOnNSRSbVUVIGNGzdt379anwf3n6HdReHl5uaOtbXR0lLH6xTkYCDSyJQUFoJFdswwSHwlw1YvXT56+mFhYHgb27MXky9dPwXWw8vLaanWsMNMY1Lu6DNmnptDNLDVn5Yc90Lqt9vDmg0e3Hzy6BUxT966G9h0tPaWqZvjUs0YGCgNqXM3BDPuAht+j8Mh83feGwkDgrD15PjH5pHX0cQN946uhsfwF8tRT2vxS39qMBzMFunNQJOXXD/KZGBsrKyoqLSycnIBmdKyHDq/r6wn8usZGRwcHBsCv+s2bN309PWvwDPChtkpUjBE6xpCUblOb41gPUDjXpT7PrSHPoyHXtTbTlphsik8wQEVrlYerISPuoyJV0VH3sTEPsNEPMFFquBh1YpwWPloDH/2gMkGLlKhNTNSpStGvzzJvL3Wh5duRM00bskwABzfCrCj5Ti1F7p3IgH58eB8+ogcLCDi8Hx8JZSiriO4nRPXhI/vxEX24MGCgMECMGqyMHiJGAUYcrIwaroqbIKdMNWTPUOE9dWVh/jAt7TQUur2J0nvxsv327cpbFK5ISpwQEdwvxL9bkG+7IP9WIX5FYf6dIgLAdgjzbxfk2yrAu4WfV56XR4aXW4qXW5KXS5KHQ5yLTZSTVRgYN7u4EJ+CtPheRdljuxTO7FY4e/rgbXR5JaIAkRfpP0nOnKgHG5Ax05Q505w525w5SU4dqYkbIEYMVoQOEUKG8EEDuMB+rH8fxrcb6dWL9B7E+A9jgwbRvn1Ilx6kUzfCubvcvRfp1QNeRQEmDhwihI1URg0RozvREfXw4NJ4n3gvBydDE41bGueP3D6geHGHzGl5yWNSYkekxI5KiR2WFjsgL777yNZDJsp3EbGW1HxbcqZJdYZhXYZZQ6ZlbZoRKcmAmGhYkWhIiNerSNCvSDLAJRqgYw0QUYbZ/jrlGUHf4MHLvxT4ledlZ+fl5Kxncf/P1O+icG9PT0FuLg6NXv3XwZfVN8kg8SmBBh96CBoorFR8C9FRmLYyKvzkN2mYicJQeWVseP4FzcHNkJVtMxs7CwcnKycXGzsHy//97//7//7ff29RlEFVps0sN0NDp3+Yhodmv1MUZujlm2egDzO/1D/3rGfuWS/oySy/mnvzNZ8t92315vXrgf7+7s5OgCmM0eihwcG1nG+6rv8cTU1MIEtLy0tLZ6enwerX7uYx1E/Fwf3VsjxvFYVolkVoo2IM8Elm+CTziiSziiRTTKxeeYR6aYhycfCdooAbRQHXS4JulAbdLA2+WRZ8GxWmVBGrXpXwsDJWgxClhgm7hwi+gwpVBnDcmG3ehfCkwh3r0k3r0o3rMoya4TYtxS60IldakXs7wre9PKAN4d9S4tuOCOhCM3K3BXWhArvK/dtLfYB1lvt1oQO7IQvoRgWAQi8uFMDxCADihszKYriDXcZd1Uh4fl1DY9fxk4aSkhckxI8LC+zl49rGzS7HwSrBwSrMziLMwQowV4yTTQwU2FmE2FiE6JViXOwSgIP5eWQEeGUF+eSEBeTFhLdJiu2UltwrJ3Vwq8yxHXKndsqdOn3gJhZZWZxbFuniOERKnianTNYlT4IlGSpP1SdN1MSNVEaMEMNGCMFD2MB+tN8Axm8Q69eH8ulH+QyifAeRvv3l7p1ljp3ljt1IFwYK9yO9RrD+YxURfdgYSnEEKtk/0cvR9pHe7fN3j+y9sEP+tKzUKQmxk2LCx8WEj4mKHBMVPioqeEiIb5+owD4Job2SgrvlRbdrXT+FiTduzDarSjWsTDWpTDbExWigglXLg1XLglVKgpRKAm6XBt4uDbhd7HO70Pt2jts1TIr9d4HCszMzWBQK2Nf+12Nd36d+F4U729tzMjKg1vDrpM4B+iYZJL5bvY8VXqHeT9Iwo/Cx0VF4+llTPiLWwu6RhZ22pb22lYOOlb3O0RN7f/zp7xo6d9oHsZOLDe9R+A/R8Hc7KrxK9D7MuzeQQT2Zb6CvmkHiIzGG6EABfGN5SQmipGTs/Z8261rX5wv0shhN8fjYWD4MlpedPdjfz3hpDTRAw+cHqKW5XM3yuJPjrVTgr1YWpl0eqYuO1kVHaZeHa5QEKRf63ywOuA4IuCzoBjLkFjrsDib8Lljio5SrEtQrY9XwUar4KBVs+F1Qj4tUIcZp1KbqNWSb16aZ0NOraVenajfkmjXn2zbnOzTnO1ML3WnQM9V8aIVetCLP1hKvtlKvthLPjlIvYG1F7i2Fbq3FHm2lkLWXenSUenYifHrQAf34kGFi+FBNWlpEpq5u0pWbAalp+IbGzlNnAApfkpI6Jy52VERwvwDvdm5OGQ42wL4iAHzBkoNVBAAxFxsgYDEeDkk+ThkBbjlh3i3iwtulxQH7HgD4Kyt5QFpin5T4XmAyEvu3SB1SlD5y6sA1PJpUCENYaehQSyKnoCzIMcOkGCgioip2nBQ7URUzURk5ggseQvsPIL37yjz6Ee4D5e59CLe+Mle6OXeXObaV2LeXOoBCT5nLINpnlBDaURpESPFJ9nF2NjJXu6FxYv/VbTInJUWOiPDvlxQ6qCBxXEHimLzkURnxg9JiB0X59ojx7ZGROCgiuFdEYI+0+D5xkZ1bRWV9Da6QM0yrkvVxCdqYGA1E6L1i/5slATehkxVwvczvGgKy66U+N4q8r8PdL1SkWb37HlAYtKoT4+OMlvTly5dDAwOAidem87eu70G/hsJLS0vgvg4KT588IdfW9nR1fb17PGh5v0kGie9Tn4wVZhqAXWbhY3s/MAwAd3i2hm61YAk+itSQd+b84TMXjrT2oaeX6KPCq2j4t6fQ/RlQ+BsL/G6/dgaJT2pkeBhgSkFuLuMpuItPn87Pza0nHl7X5+jx/Hwrjdbd2QnKAIhB499IJj/9RdLVr6fhtkpklF5J8IPSUK3yCG1UhC4mUh8dpY+J0cPF6KAjtcpD7pcGKpUF3kYG30EG30IG3UQBGg6FDBl8Axl8ExN2FxNxDx+tQoy9D82Qi7tfGf+AEA3gWLUyTh3U4CPvEePu16Rq12eaNOXaUvOcqfmutAKPtmLvtmIvQL3UfBcq4GO4U0u+c3uBS9v7ckuBY0uBQ2uhc3uxa2epZw/SbwAXNEaKbMNm+Tin3n8Qe+6SR1R0WVNT56VLJlKSl6WlLkqInxQTOSIstE9AcAc//xYBPgV+HgU+bnlg/Nxb+HmAKQjybRXm3yYqsF1UcLu48E5Jkd1SonslhHeLCe4Q4QeVO0CltNgeBckD26QOndh3lYCrLshBal1Tq8wIniDFj1REDBIihggRIxWRI4TI0YrIMUL4ICqwt9Srt8Stt9i5t8Spp9i+u8iuqxCyzhKH9mLH1kKHjkLbnhKHXoRXU65XYYSzr4XRw5uqpw/e2LX1kpzMWVGRYyKCh6WFD22XPnp817nTe84e2Xpkv9yBvdL79ssdOrTr2C3lm9pWmrfU7yruOCYpuldWcr+UgOKtw7sQETo1qfq42AfoSBV0mEp5mBI2RoUQd58QpUyIuIcLVwL9E0yYEirkHoDjepjjd4HCqzU0OFhSUFBTVcXIHbiu/wR9EoUBBDfU1UHDtPTg3WfPnn3ywYTr+hoCKDz9i1jh1QZgl1l4D8Ef0PDUYuPsc8ocZFTIXlCf/aPDP8yOlW1TMTrh8UvaCgp/QMMfo/DI3AoND82vo/DvCKDwGmSQ+KXAhUltamqsr2d8KY1CwaHRgwMDjFe/ySTUdf3pBH69L1+8AG3+G3rceVtra05GBrq8nIEBT588WcufNNBIeyU2zggdqY2J0cfG6mPBMkoPE6WLitJCRWqWhz0oC1IpCbhT4n+z1P9mid/1Ep+rxb5X6ZkiQOEyKKND7+KjVPAAvKKUcWF3gBEiAYHdRQZdp5fvooKuYUJvV8SoVSfrNWZZUHKhvBA0uAsV7kzLd6YVOFHz7IDR4HYtefbAaLl2VIiY7ahwqLK1wLG90Lmz2L2rzLsPHTBWHVeRm2FiGHf9jt/h4+Y+ftlNTR1XrhiKipyUED8hInxISGCfIP8eAYGdAgLbBQV2CArsBCYsuEtYYJcQ/w5BPkUBvi2CfApCfFvopiDEIy/ELS/IDc2WExHYJiqoKC4EobC8xAEF8UNHd1+pwFfnZJTeOH6nMMxjsiphlBg5CD0ZLozxiLghfNgQNmQQHdiP8O0r9e4r8+wtc+spde4pdeouduoqduouc+lBeAwgfQdRfrR897xAC/tHD64cvbR367ltsuelJS9ISp4VFzsqLXpkp+yJCwcuKV+8d+v0tWOKh/ZIKO4VVzwqu/fqgdO2LjoplT45TUFp1f5Grjpb5Y/Iih6UFz+0R2ZbtJMyBW5Vn2FYm6pfm6xfk6JLzjJoyDEkZ+rXp+rWpOoQEx8SEjQJ8VrYSLXmIu/vC4XB9UBrbs5ISUGVlTG6gOujC/8J+iQKD/b352Vnw3Nyhtckgnx5eXl0eHhyYmIduIHoKPw+Vni1vY+UYFDv6jIdgj+gYcYUOmCM8pPXrbEp3lzcbAXlsQCFQc2HNAwVPkbh92ES0Kjw43UU/i2BxhNcMpUEQntLyxo3m4B3GZNNl5aWMEhkenJyU0MDWH2+vNzd2dnf2wsuLmj1+fPH8/OLi4tgU4GeLS6CVcZL4KIDuLPw+DFjPBts/5OFBVDDACPguLDKEeDR7zq+/tARbOHPjvTJAD870v+FZzgCYzpCm/r06Yrj0tJqR1D/a44vXrxgOoLV1Y6ghuHI+N//jziC5S8dwfs/cgQv/exI38ffdgRf/auOdPRccZyf/6Qj2PHVjmDv/llHRhsLOlEz09NgFfqQ5eUWKpWIxzOmxIHfDGj5CVgseD9YXXuNdlRh4oxRkToYxvM1YvQw0XpgiYzRRUZrowEQh2sgw9TKQ5XLQ5WQoUrocGVshCo2UhVDL+CiHxBi1Sui1fFR9/GRKvhIZXwEYGIVYoxqRfR9IsDfRE1SnHplnAYpQQugMDnTtDHHshlmQ4M7UqBcB3aUPHsqIGC4Q0u+Q0ueAy0PULITMGqBI63Qua3IFVhHiSvg4G6Ufz8ueKQmLSMq5YF62JUb7geOGDq5JjU3d968YSgkdFhc/KiIyEFh4f3CQvuEBPcICewR5N8lwL+Dn28bH68CL7csD6cMN4cUJ7sYB5soF5s4L6eUEK+8mMBWcSFFCeHtwEBBVHCbmOB2CaGdMqJ7ZYT3Hdp5sZJQnZoIP6x4NsbVeqImYYwUNQRomBgxSIoYAuXKmGFizCgwQuQILmwIFwo2sg8X2I8PGMQFDuGCxyrDx6uiO8vCC8KcbDRVju86DChWRuy0nOQ5aalz4pJnREWPCvPvlhLaeXbfOc0b96+fuKworijOJSnHJ7dHfMdhyT0ayjdSiV5F3eGF7WElPVGZ9YHXlG5LCx3cInVMTnynhfqFliLHlgJbSr4NJdeyOdeiAWZWn21cm2lQnaZXlaZfmaZXmapHSjUgxD1qLvb77kaFwbVRXVnJeLA4aI9AYaC/f417hOtaY0Xl95LpKAzunWMjI4zWcGFhAYtCgdZw6iunkGQIfC80Mw+FAr+6lar/YDFQeOLnWOHG94UVFAbGoN7V5Q/sIxpeIM8sNZdgEq0cdCjdiJmlJkb9xzT8K2ES6yj8R/Tq5UtAJKC1BPS2UrW2AgRGaWzEIpEDfX1gdWpiojg/v6SggHFFDw0OAtYBxAPACMAQQHYiDtfX2ws5PnvWWF9fRSQyUlKAu0AtidRQV8cYEBkeGgKIz3QENwWmI+CwJjKZ6Tg3O8twBPi12hGAMqA0hmNvd/fPjhUVE/Qn/zMc62tqfnbE42nNzQxHAPSrHZsbG5mO83NztVVVTEfQnWY4AhZ8t8oRnBHQsDAcGaGADMe66urHdNpjOFKamlY7giXDEdST3juC9wMvpiOoBC+tOL5719XRseL49i3Y+E86Mh6SwnAEmwQ+n+kIlkxHcOjAVjEcwQ6CrWU4gh0He8F0BHvHdATbAPad6QiOyScdGX0DsJGg4wR+Box3IsvKstLS2ltbwSo4753t7eD9DGhee3U2k/IjrXFJtuRsJ3IOlEytJtupLselPtelDuZYl21Xn2lTn2FTk25RTc/425Bj0QizaoLZAJyl5toCrq3PNCWnm0IPRs4wbc62aMo2a84BZk6BWdHybFsK7NsKHduLHJtzrchZ5lTAu0Uu7aUenQjvdoRPa6lXOz0+uKPUGxTaSjxbS3w6EAGdyMAOVEAnJrAHE9yLCe7DBw3gwweJ0WO1Ca2YdHtz/ytXXc+ft9m3X9vSJpxK7bqnZMbPd0BI6ICAwE4+PkUeni1cXHJcHHIcbFJsLGKsm4VZNwmybBJg2yTMwSLKySbGzQFxsAC3nBCvgqjANlEhYIqgIEwfKhbh3youuENOfJ+c2P5Du86TKmoT43J2yZ0MsDWfJKdN1CWO1qSM1aaN16eM16dOkLMmydmT0DJtvC55tCZppDp+pCpmvCZmui5htj6lFxtbGuvpoq9zas9xcf6twnx7JETPyEldlJM6JyN9VlziFNhsfp7tfGySu+QOHNpxTJxXVohDSpp/i6Lozr0Su/aJbbey08b3ZzeOYkh9RfmtoQXtYUYuOjLCB7ZKHZMW33fn1OHKFAtqoQ05z6ohxwxwcF22cXW6fmWqDjFFG1hFig4DhSvitb9HFAaCQjbpPX5wrRbC4aAlBS0jWAUNPaOvua5/M8UXdjd1Qm1lf28vuHeCGxtjwAO0p7MzM6C1pb/r64qZQYLRTP+Hi4nCk0wIZtofpGE6CoPCCgrTyyNzUMwDvZLxID2AwkwaXgmT+CQN05/ito7CfwIBEAfgyxgjHB4czM/NBVc0Y7YrQNKMlBQ8BgPeA6AWABNYZYwfLzx+XFZcDMvMBC0A5Dg0BM/JKS0snKU/iLStpQVAEtMRtA9Mx6dPnpSXlDAdR0dGGI6MRzAwHcENBUAVwxGAMtMxJyODAbgMR3D5Mx0zU1NBV5zhCHguPTmZ6YgqK2M6MiZ1MR0BbTMcwREADM10BI3Y4uIiwxFQI9OxMC8PdBiYjoyQAKYjWDIcQX12ejrDEbwfeDEdQSV4ieEI3gy4k+EIPgRs/CcdwVczHcEmMYbbGY5gyXQEhw5sFcMR7CDYWoYj2HGwF0xHsHdMR7ANYN+ZjuCYfNIRHEOw2lBfn5aUBHActPaAqqsrK5GlpeA94DPBq4zltxIOh7MyMwzxtCpL8W0oCm9HRbUhw9rKw9oQoS2l/tQiD2qBO63Ak1rgSckHZTdasUdLiQdA2C6Ef095QHuxB63QpaXAmZbv2FLo3FXm0V7i2l7s0lro3Fro2lHq2Vrs1lLsDsCXUuBCKXTtQgf04EJ78WE9wAjhfRUR/RURfYTIPkJUf0VUHzBi7AApYbA6cbAmYagmgQGdo/XJ43VpUBq15tyCmLALJ7R27VFT3HZLQuycmrptM6VdWcmIlUWeg12OnV2KlUWMZbMwyyZhdhYxTjZxHg4pfh5ZAV55AR4FPi4FXg55Hg5Zbg5pbnYpXg5paP4cr5wQv7yo0FZxoe0SwjskhXdKie6SFtstJ75XSmjnXsXTVcS6uOj0bZJHnQ1NJhrg87S8KUr+DK1wjlYwSy2YoxXPt5TOtxbNtRbM0PJnmmEzTRlzlMzJ+gxaaQwm2T/UzvTa8QtifDvYN8vxce+UEDshI31RTgbYeVmZ89JSZ0RFjvBwKm7+SRQKVubbzsMqI8a7dZvY7pO7Tj+4dt/V3B6DL5l7Mf767auBubb8lrD8tlALbz1p4T1bJI/ISx45uXN/frBBF9KlpdiRlm9DybNozDFpyDFuyDYiZxvVZxvVZRnWZxuTs0zqUg3aUcHfIwozNdDXV5CbW5Sfz+j+Dg0OguZvYnwcXCRAz4AWFxnMBFqr58vLoKEEVyNYBc0lABoAzozLCRTAKqgEZfAGcNWBNzP+emM4gopPO758udoRfD7UMn7K8fWvO0J95d9zZHTov5jju3f/tCP970Wm4+qjCuxnR3CQ/wXH168/cgQnDqzSyy/A/TK7kNTeC40bgQYUtKqI4mJGW7mWmp2eBo0y+IGBzVup+g/W+wAJCIUn34PvxMKnx4aZhRX7MEwCFBg0DGxqsWnuOXX8CROF6TRMD414PzAM0fAvp9DRn16xjsK/o8WnT0HzOD8/z7juvrkez88DpgQXNWgKwCqgHBQCAQAIXGKgTaA0NgL06WxvBy+BLQckhEUix0ZGwOrkxATgV3A9Mv4fB5i72pHa3Mx0BDeBGhKJ6QjYi+EIvnq1I2htQKPEcOygjzsyHDHl5SP0R7gzHIl4/M+OZWVkej4Z4Aj2onyVI8A+puPM9DQBi2U6DvT3MxwZ7S3TETR9oLVkODKeS8JwrMDh5ui4z3Csq6lZ7QiWDEdQD9iU4QjeD7yYjqASvMRwBG8G/Q2GI/gQsPGfdGQMLTEcwSaBz2c6giXTERw6sFUMR7CDYGsZjmDHwV4wHcHeMR3BNoB9ZzqCY/JJR3AMwSpgcQwSSaNQwG0L3OwYU+cZP5VvrmIU8cwd3Qu3H+oYmPi62cHjfWoKQlvLw1tKg6iFvhS4e1OeazPctSnfrTHfvTHfrbnQjVIMzJ1W4tVa4t1a7NVa5N5S5ErJdwTWWuzaVuLeWuLeAhGwW1upJw16p0drmXdbuW8XNnigKna4NnmkLmWYbqMNaeON6RMN6eMNGZN0m2jIGidnjTVkjjdmTkAPswBkCZ+g5k1T4U9aCkYbCn3tPE8c1T9wRGv79juiIqdv3jICKHxf1XjTBknogRqsEmybGYkjRDlZxbnYJXkh2N0izL9dRGCnsMAOIf7tQnyKgnzbQKUgj4Iwn4KI4BZR4S3iIlvFhRXFBBXFBLZB48QC2yQEtwtxK+zedrKmuiE+Ok1R+qS+sk4nNvtxS8EULW+aBp+lweeo+Y9pRQutxU9ai562Fz/pKAGFKXIepTg2zc/RQv3hrTM3dysc4WOXZdkgxcu9S0z0uLTUOTnZiwpylxXkLwMUFhc5ys+7h4ttK9smGQXZU7sUL22XO3Hnspq9qWNKTGoVrnJmEvpFAc0/n6wfRha0h+fRQlT17orxb5OXPLBV+vgemX1BlipdCCdqvjXg4KYck8Zso2aYMd1MmmDGjQCLcwAKG9em6rYhA75rFAY81N7aCgw0f+BqqamqAt1NRg8bXDDUpiZyXR2jOZifmwMNa3tLC7g4wSpoFhvr60HLCxxBQwYKwIvxlw14QyuN1tzYOEcfqGA4tlKpjGEM4Aje2dPVxXAEDeJqR9C+NDc0MBzBRb7aEfR6mY6gRVjtCDaV4choDpiOAAeZjt0dHWAPmY7D9EYWOIJ9byKTmY5gl1soFIYjaDjAPjIcwQHp6+lhOgIiX+0IbicMR0ZUFsOxq70dUCzTcYg+zYXh2EgmM0Y4IMfmZtBU/exIJjMdB/v7wfFnOAIOBjc8puOThYXVjlMTEwxHRlcB9HBAa8j4CxXctkEzmg/L6emBVsEtkEQkglaVcR7XUm/fvAF7Ae61YAtXqv6DxURhgL8fDAz/cpD4n6FhenkFi+kGoTCwT9Hwxyg8Ol+/jsK/IfC7BWxRWlgILufvpzsHtop5QS0uLoILHLTYoKEDlaBBA00Ko9MLmk3QWIFXGRc+IKqpyUlQw+jbgxZvcnz81xxBJ5bpCC5hhiPjCPzs+OYN05ERycB0ZITVMhxB88V0BO9kOi4sLEyMjf3sODPzs+OLFx84PnvGcAS3j48cQc2KI/2W8bMjfVCA4QjesNoRLH92HB9nOIL3Ay+mI6gELzEcwSpotBmO4EPAxn/SEXz1rzo+frzakcGsDEewtSuOS0tgL5iOYO+YjqAG7PvPji9fftIRHEOwCg4yKDOGlsHqdyUEsfGCpsvhm2ZHbxqcva2t/sjAzdEyO8q1Li+4pSSEWujdBHdpzHWsz3Ug5zs1FrhSC1xpRW7UYhdKsVNzkSOt0LWlyJ1W6EItdAZA3F7q0Y3y7UH79+MCB3BBfbigXlxgHyGknxg+UkN/Vlxj+iwtd64FPtsC6DZ3mgqbpuZMN2dNktOHqxIGibEDxLghUtJINRR7MNmUMd2YM9OUO0GFTVGyF2i57RVwG5OgG7f8LlxzO3zMTGGL6q07NhRql7a2PcvmLeys8mybJVk2irJsEmKhR0SwbAQmyLoyQizJxSHJwyXJyy3DzyMHTIBXTpBHlp9bmodTgptDnJsdmAQPhwQfp7QQr4KE8A5Rvq17FE/UVjckxGbu3Xb59lnVkpiAmabsCXIKw6Ya0mebYQu0wrnmwiFSRkNhRHmcd4KLnd71O4fk98uLHZAQOcLFsY11I/jMLeLCRyQlzsjLXVaUv7JF9rKU5FlRwcNC3LvE+XcrSBzZueWMxn1LH4/o6PAkck3DQE9/J61tbAjq9756/qqjj4LrzS7oCC/qigiG2e/ZexBAvJzUPkWZ41vE9xreOUdKNsDHqVUkaBDjISMlalYlaVYnP6xKfkhK0oJmzsVpYCOVwRn8rlEYCLR9jNYQNI511dW5WVkAsMAqaNRKi4rysrOH6f1dsITn5IAeP7ggwSp4T3Z6eiWBAK5h4A4KgKEpTU0MR0RxMXgzI0khwxF8FCOYCTiCd4J+M8OxhkTKTE1lOpaXlIANYDgCzM2HwZiObTQa0xFc56sdQUvBcGQEtzEdQSvAdMRjMGAHmY6AYhmOqLIyWGYm07EgN7ekoIDhCPriYB8ZjqDlqiISmY6gGVrtOD46ynAErRLTEYdCgZsH0xHcPkF7xHDMycgATM9wLMzLK87PZzqCl5iO9dXVTEeAvKC7z3QETeFqRwDBDEfw+aD3DxwzUlIY//2B+xk4a9kZmR1t0F944JiDGkZDua5vqFWxwhAK/0zDH2DxSoFBvavLkP0WDTPHiVcGhoGtoPAKDUOFj1H48ToK/5bA1QQanLXPILGudf37CVfbrmwZcUnb65KW1/kHzqeVTM4raWvqmoT6eRSlBBLz/JuL/ZoK3BpyHRuhqWxOLflOrXAXGtyhEW5JzjNvzLNqyrNphttSC+zbSpw7EW7dSM8etHc/1ncA5zeA8+/H+w8Sg4Yqw0aqo8brE6eb0mYpWXPU7Flq9nRT5mQDFHE7UpMwRIrpw4V1oYI6ygN7sOFDlbGjNXFjdYljtSljdSkjDYlj5DjgW1WSZ2IYfUs5/Oa98IuXvQ4cNFa659rc3GVp5cfPf1CAfx8fzw4eri3cXLIc7BJsrCKsmwVYARBvENj0E//GH/k2/Mjz009cG37i2fQT3+YNfACU2aAAYiHWTYIcrCIAggV4ZIT45YT5FUT4t4gKbBPgkt299Wh9XXN0ZKqiwoUTe68G25jNNWdNkROmG5LnKZnzFNgwKaMuNxIW5OJl9Ejv5rVrh07sFt8pxiEnxrtdWvSQEN9u9k1S3OwyosL7JcROSUte2Cp7VUHiorjwKUG+g6K8e/dvPaN+W9fTMSAlIae+ljLYP9rT2dtYVYeEFRJL0ePDo4CG4YkZbramgUmWCQiX8EyHqzfOC3HLighslZXYv03mmLz4vnunjpWHPyLEqQEaxkXfLw+9WxJ4szz0NjZKGRejio25j42+j45URYbdIcOdv3cUZgp0N6cmJgCNMegKgCkBhwPcBnANrI6NjADAJWCxjDS0HW1tRXA4gC3QK3396hUoADJrex+ej0OjwZsZT7ZjOGJRKAZDMxxrq6oYjo1k8mpHAgZTVlTEcAT9YAC4TMeujg6mI9jU1Y5Pnz5lODIGUJmODIZmOFZXVoJ+M9OxhUplOAK2Li0sZDoCasQgkQxHAJ2ANRmOoMNArqtjOoLO92pH0BFnODIYmuEICBhi6PeO1OZmcCtlOAJoZvy9BRzBEUYjEExH8BLTsbmxkekI+vegv8F0BOdotWNfTw/DETA0eDNwBBtAbWoC+/vmzav+nq7ELDytAzqq31DL30cGibfv3rx+Cw3zr6x/I62g8BNGrPCv0PBvBw0zURgqfxAmsbr8KRpeGRheTcPrKPy7Ar+Zb5VBYl3r+jdTRUO3umPSbbNIFZv4exYx1/X9r2p7XNJwUH5kZWpp7+/tkpvgU5nj25TnQYE5NmSb1WUakdNMyGmmtRlGtfQg1Pos44Yc08Zc82a4Ba3ApqUQMmq+FQVuSQPlYrvWUofWUqcOpEcfLnCoMnyYFD1cBUg3frQmYbg6DkDwYCWoiR6BLGqoMmqoKmakJnakJmakOmakKg4q1EWPk6PHG7PyknP0DZLUNBM0tFLuKoeeOmN7T9mjsbHLwzNBWuayrMxlaamzYqJHhAT3cXNt42CX5WSXpD92TpyDRYxtswgrixCAYzYWQXZWIU42IS42YW52UT4uSUEeWUFeOUFeWYDCYCnEB9GwMP8WXg7p3duONTZQw0IT5aRO7lM8b/FAfYiUutCSM1GfTiuJLgp39zPVU7tw4YDcdnlBGTl+OTEuWT5WOSGeHdLiByWF93JtluHcJCUssEtC4pioyHFxkdOiAicEeI7ISJ46e0LZTN8pJS6zoaZxbGhsqH+I0tiakZLnZuXuYWKfERHf39nbQWuPcPdXP3317qHTj25fszF8oHnrljSPHB+blKigoozIgS1SAIX33zp6AhVtRINbNGQa16frkxIfoiNVMFEqxASNqpRHwEjJWqRk7Yo4NUqJx58GhT/SyxcvJsbHAZUy/vpZXFwEtwGAMox/qQAs9vf2Aqx5++YNgLbJ8XFAY3P0UArgODI0BN7M+O+e4QhqGP/gMBzBJzMcpyYnP3AcHh7o62M4Pnv27APH+XmmI7gnfeD48iXDkfGP3s+O9JEbhuM4faos05FBkMBxdGTkA8eBgeHBQYYj6AysdgQ7y3R8/fr1akcAuAxHRhjWiuPo6Bv6Q1MZjoxQCoZjf18fo0cBHAFM/5oj4F2mI7j1gk4F0xEg72pH0FtgOILPZziCIwAODjjCgPjAMgrezcgg8Q0JEGz/N8wg8fbd26cv5mcWh8cXesYed0097X+8NPH8NRQJ8030EQr/UzTMKHxsvzow/DMN/zJMgknD6yj8R/TqW2eQWNe6/j1EbO7XcstStknWdE7Tck27bxOnYh5xS9vjsrL5JSX9G8q6evomnvbm6QEWyFhzfKoRIVWnMlGzOkGrOlWvJsOwPsuQnGXYmGPalGvWnGtGzbcARsu3aM4za8gxbso1peRbUPItKQXWrSVOneVevRjoiXEDxPABYuQAMWqQbv0VkcPVsWN18XRLGK2NBwYIeLgqZrg6fqQ2ZqwuZrYppasq38stUU0jTv1h4iPddBX1qCvXvdU0Qpuae6Oii3btfqi4TUVB9oas9CUpydMiIodERQ+KiRwUFdwnzL9LkE+Rn0eBl0uGm0OSi12Cm0OCl1OCj0uCHyw5JIR55QH7CkAQLCsisEVEYKsI/1Yhvi28nDJ7tp9sbmoJD0mQlTiyXfbklaMXy5ICqIjEeDdTvZtXDm/ZJcElxrORX4hdVEZAXopPXpBTVphvh7jofgnxg9zsCqw/gW9RFBY8KMB3gI97t4TIsUMHlFRVLby9IlAIwvDA6PT4VG97NwlT6WHn+ei+2ZE9V++eVy7LLhwbHCFX1niY2isdPndr7zGlQ6dVj57RPHn+2p4T4hzS/BzSovxbpYX2y0sclRXdq3zqFD7RorXAmpJr1Qz6JLmmDdlGdZkG9ZkGYAlNm6NbTapWa7nPnxWF1/Xvoe7hp7n4UfMwqn9mJ4Y88eTZNxvNAnT+rTJIPH/1bGyht228qnEY2TRcThlBUUfRbeP4oTnKwvLEm7ff4JgwAyTeE/AvaPhTBgCXWVixj8Ik3uPv6jLd3g8MfyJoGKLh7xyFl14+XViemn02OvtsBPRhnr18/ObtepDPutb1ZxWxsUfHOVXVPELFNFDJ0OuWlsN1VfNLN7VPXVQ5dvb2kdM3Tp69eeb0pXvXL1o8uuVrpZQVqF6R9IiU+KAqRbsqzbAmTa86VRtgcW06YGLjhmxjcrZRI8y4CUZPYkCnYYDFzXALCty2tdilB+3bhw/uI4RBSSRwYX2ESMDB/RVRA5VxIzUAfwEHJ43UJNGXCaAwVpsyUp84WZ/yuBmOh+caGMbe00zQ1ssyMsvVMcl6oJ2sb5JOoQ3BcknnztscOqCze4eK4pYbslJnJSWOiYsfFhE6IMi3m497Kw+XHD2psDQXuzQ3hwwvhywvhxQPmzg3izDbT/zcLCJCvLLCAgriworiQtuFeLfQM0vIC/FvPXrwcmtLh79PuCDXFjnRQztkjmjdUdK+dXO7qAzvRgH2nwRZf+QX5BDbIbldUWyrjOAWcf5tYgK7JEQPCPDv3PCjGDuLLCBgYYFDe3deU7ql7+oUXFyM7ejsGx4Y6e/sa6ysL0zNi3QLsnposVvqwPG9V3TULRFFqJmJSWxBsfV9nbv7T93df1T5yEnVY6fVT5zRPH767La9IpwSgpzSonzyMkK75CUPyYrsNLh1sSbHBhz8xixzej410waYSX22UW2mYXWGfnWmfm2WQU2GPjHxAaXUcx2F1/UtVUmZvmlDOmtMOG2It4uiLi5/M4D4Vhkkll8+7Z5uqh9E1A2UkAdLG4cRzSOAhstpo8jWcUz3NGlmsW/taRig8MxiC4BaBv6upmEG7EL2KSwG1MssvIfgz6dhJgp/F7PLV+vVmxdTT4e6phpoo/jmERToxrSO4Xqn6yef9Cy/guY8raUWv7MMEuta159UmIr62+rWZ288On5Bed+Ri7v2Htu+ba+C7HZJ6W2SMlslJBUkJOUlpeSlpeQVZBUObZezUjuJjNWtSXtUk65fk2Fak2YImJiUoluTYUjOMSXnmACDUCzHrCnXojnPoglwcJ4ZBW5Jgdu0Fjv3YPz6CCEAhfuhNGqRg6SYIVLsEClusDIW2HBV/Gh14kh10mgNlKB3rDZ1jJw21pA815g1RS6NC07R0El6ZJxraJ6va5LzQDfltmq0inp4fUNPQQHxxFHtI/vUjux/sH/X3W0yp8WE9gvw7uTl3sbLpcDDKcvDKc0LjEsGADFkEAdLcG0W4dgoyPIjD8cmQX5uKUE+aCIdH5cM4GBh/q3SEnvEhHfs23O2paXDw9Wf9UdBEZ6tYnzbhTjFuTbxs/0oyLFBjGODkDCH6E6ZHTukdyiIbpMW3ibCpyjMt1uIb/emnyS4OOT37r5077aho51/Xm5Zc1N7T88wuY5agakqg5UmBsR6m7jaaZpa3TdUO6d8aNsJP4/Q7s7+od6BnJgE3cs3b+0+eu/AcdXDx1SOHFM9ekLz2AmNY8cPSinwsQkLckuL8cnJCO+QFd+3XXqXt6lqS6kjBRzwXMuGPAtyrlkDdBagbgkjmVpdllFNhgEuXq15HYXX9W01Nb9sEdp8+BHmnDEhEw3NgPxWAvQAIPgVPR3HStXX14vXy4CiavqLa/uLGwbLmkeQgKVoo+jWMUzbOLZ9Ahima4owu9j/7t2awg0dhVuZo8KrUfhnGv4VFAZGL/wqDTPx9xcozAiT+AQND8/XjD1u+N5QePnVs74ZKnmovBZ0Y4ZAN6asaRhBGUHSRlFt45iBWfLT51D40NoI/G4729uL8/PJtbUv6VFb61rXuv415cELFXfsFxKVFhAQ4+LgY9vMtvHHDT/89W9//d+//O1//ufv//M/P/31f9h+/Csvy0/C3JsVxdk1L27J8b1Xlw4NA9ekG1enGlSn6lSl6lZnGNRmQshFzjZpyDYlZ5s2wgAKWzblmjXBTJtggIwtaUUO7Qj3doRXa5l3a5lPe7l/R7l/e5lvW5lve7lfJyqgGxPSjQntwYb1E6IGKqIHiDEDVdBQ8VxzdhMi38os/MGjJB1jmIZOyt37kZev+xw/ZXf0uDGB0FSYj9olf3af3JmDiteP7r11cMd5BcnDEkL7xQT3iPDvFOLdxs8tx8shyckmxrZZmGUD/+af+Fh/4mffKMTNIsbHISXEqyAmpCgiuE1EUFFUSFFMeLuE6A5xEUUuDontWw+3tXa4OHj+8N9s7BsE2H4S2Px3PnAwuFiluDaL87GI7pDYtkt6h7zoNlnRXQJcchxs8sKC+3cqnrt66b67axAcjqBSOoaGxpubu3LhBGeXmHu3DM0eWrubOLvoOzg8tDZX1Te5o2X10CQxJnV4aLSmstbX2kXt+Lnb+47c3X9c+eDxe4cOKx0+pHzosNqhQ0oHDmwRFOdhExbmk5XgV5AV3iklvPPikcNF0WbdCKe2AltqoQ2lwBqKSIFb0PItaQVWtAIbaoE1Ld+6Jd+mKcekGx/67t06Cq/rm6qgYvS0IUHbhzw6/d2N+X1VvX33dmiuo6a/pHaguGEI0TwMcTB1FN0CRUcACMZ1TOI7IcP1z1Qtvlg7qAJ6++71zGL7ahSmE/AvaJhpvx00/DMK/4KGfzto+P0UupH5mvHHja+/JxQG3ZjuqSZw+moGislDAIKhyBZGN6Z1HPRkMKAbMzBTt/gCmjCwBgIovJ5BYl3r+iJKTkriYGX5n//vv/723//n7//9Xz/9z39t/st/cfz4fwQ3/R9x9v+W5/vrbtGfjspuvrKH58EZSXOlXYFGJ2Be1/BRytgoFWSECipSBRTQUcqYaBVg2BhVbLQqPvZBRbwmIR5K4FURp14R94AQ+wAfq16RoF2ValCTZlqVakpKNa9Ks6xMsSAmmVWmmFdn2FRn2tbmONTDnJrgbq2lvm0Iv1ZEQBsqbIiU3InPiPKN0tIM19ZP1QIcrBx68Yrb0ePmu3c/3Lb1LhJZjULgj+84fXr7iWPbTh/deX7f1mPb5Y7ulD8pIbyHm12GY7M4+2YR9s1CHCxCXGyiXOxiPBySfJwy/FyygjwKIgKKkqK7pMV3S4jtEBPZLiq0TUgAihvm5ZJiZxHeveNIR3uXjaXj//uvHzf/nYflB36OjaI8bNKcLBIcGwVlBRW2S2xXENoqJ7xDQnDnVrljN29o29n5pqfBm5pofX3D9fUthYWEAP8MdXX3I0d1pCQvHNp1zV7f0d3EwUXPzk7DwvKBYaiLX11FzeTEVH5O4Y0z16/sOnp3/xGlQ0fvHjh2Z9+h2/v33TmwT2n/vnsHDlzcuVuMU4iPXUSUX15SUEFGaLuiqKKD9o3mQqf2IgC7Vs1wK0qeOYUet03JtaDCrahwa2C0POvWfBsazKyXELY+Kryub6zRmRfavo0RcCiv8DcUNEdwcJAxDXGl6itrYXmmcQhd018EOLhpFQcDlmJycPcUoWe6omeaMLHQ8ubt2kVuABSee9b1EQrTCfg9DTMh+FPGAF9m4QOjozCThj8i40/QMH1gGKDw5ALl9dvvhfAAdw7OtVf3FwMOpp8+wMFI6PStDOevnL6uSdzIfPOL19DU3q8tsEngB1xFJHa0ta1nkFjXuj5HyfFRgiz/y/P3/yPF8T/bBf56WPLHs9tYb+7nVj/Jb3hFzEZJ3k1zV5DRoSSHC/l+d8sjNDDRmpjw+6gQpdKQ26Whd8vDldGRKqiIe5hIFXQEKEOGiVLBRqtholRBGRuljI9WwQM+jgF8/JCUpFeXblqXYVGfbV2fY0fOsW+AOTTmOTXlu5LznGthjvW5zk0F7i2lPq1l/q2IwA5sRB8xMyUoVPuht9ajRB3dZDW1iCtX3I4fN92zW32L/A1J8XNFRTgijnT54Pk7h86e2nbooPyhHRJ7t0gcUJQ9KiWym32zOOsmIU42EW52UV5OcT4uSV4uRqSENABiTlZRDlZhLg4RLnYRTmBswuwsQozEavzcMmybhBVkdrW1dpib2Pyf//o764/87BtFuVgkuTaJs/zIz88hKi+soCCieHLvOXUlHVtL9/QUOJXS0dXVV1vXnJVV4u4eq6npdu6M0e4dalu3KMvK3hAVOnb3ipanpbubkZ2dpqmTjlVWbOpgb39PR3dcePylY5dlecSv7DqgdODgnQP7b+07cGP33ht7dt3cs/POnp039+09ILOFj4VfiFNCnG+LhJC8tICM0rHD5dHGHaUOFLh5E8y0MdOYnK5fn6ZXm6pXm6JXAyxZtyZJh5TwqDJeixCt2lzkvo7C6/pmgnhieKy6nmbsjQhLI7R2dM8vQAkovokYT17FIJGMrBdfW2/fve2boVb3F5EHVwYUqe/HFH8GKToH980QgQ3MVi0+h9IIro3evXszv9QLWHY1BzOMScNM8P2kMcCXWaBD8Ac0zMTfP0LDI3PVU0/a1rIz8NtaWJ5tGEJBp+9DDmaevq5JfM9URQ84g1B8C5Teew0ECHh5efnl+mNi1rWuz1N5XtKdQwLKh3gNL0vZKW3xergzxHB/rMWRVPsTOW7nC3yulofew8dqkpJ0a9IMazOMalKNapKNSYmGhEQ9QrJeVZpRXbpJTapBdYp+daoBKNRlGNVnGtemGUJRE2m6demAffXqMgzqs4yb8ixbCh07Sty6yr260f492MB+etDwABFYJCh040Lpj2UOp0+nixkixY+R09EZsQbqrmoa4dr6aQ/UIm9f9zx70uLAXk3FLTekJU6JCB7Mziquqqi5deSy1tkrV3cfPb7l8CH544qSR6QEd8uI7hXgUdi8UZBlEz9kG/lYNvBt+olnw4/cP/3A+ePf2H/4KxtY/vQDB6jZvIGffbMglF6NG3CwOBeb8P/8fz/Jym7t6emxs3P6v//10+YfBNk3ibP+JPrj/+Pk2CBwcNexe9dV3R288rIKmxto3Z19FEo7HF7u6hpy+47R3j1K27bd27lTffs2VcDB27bek5W+pCB10lDdwtXI3kbDyMfMCZ1fNjM5RWuiWupZntx5bLvolt2iMtf37r+1Z/eNPTuv7dp1ZceOqzsVb+zYemvXtos7tssLSPCxCIjzSEvybZXglz2+c3uKu1ZPmXNbgU1Djhk4zjVJj0jxGsQ4DUKcOg70RiJU0aH3QL8FFapcHqpcGnCjKsvu7ToKr+ubqH9oNBOOsHUPNbDy0jAN1LX0NbHz8Q1LwhLrnzz9BknEBvv7UxISiuDwpTXJILH08mnTCLZ2oLhxqLwZGg9eHSJMZymIoiAO7p+p7J8h9c8QZ552/8F4ps/Xu3dvF5aHJn7BwQz7IzTMoN7VZToE/yoNM8tMFAbGpGGAwrOLXW/ffRdpGd79413fDA1wcP1gWeMwOH2Ag1EMDganb/Vwfu80sXe6Yniu/uWaDAyva13r+iJqqS6MtTmdYHMi2/1yod9NRPBdZKgSOlwZG3GfEKNZmaANoLY2w6Q6zaQ63bQ2y6wuy4KcZdcAcyTn2pFzbZrz7Kh59hS4XVOudTPcuhFmQYFb0QpsAfU2wMwac80ocItmyCyb861pRQ4dCI8etG8vJoD+ILrg/gpGVrX3Vhk5SIqmW8xwVcIkOb0dl+Jm4aqq5P9QO+GBRuyt6z4XT1sfP6izR/HuFpnz0mJHhHl3JcRnVhPr7p64YXrjnsapi1d2nzqx7fReuZPyIvvkxQ4AGubllOXikOTkkOBkF+NkE+VgFWJjEWDdzA+Mk12Uh1OSh0sKGirmlOblkORhF+VkFWDbzP3T31kExfj0bB/MLExmFCVtP6DIskGI5SdRYb6tR/efNTOwykjOqq2qHx0Zo1Jac2ElXh4RykpGCvKn+fkPiIufV1RU3blTc+dOjZ3bVRQVlbZsuSUhdmqf4nkzDQt7LbMwF38ysWZuapqEwRve194iKH9Ifu9WQeljsgpXd+++umP71e3bLituu6S49fK2LVe3yV3foXBUVk6EQ0iYU0yKV0aCR36H9C57fU0S3Lul1I2Wb0/NtWjI0K9JBCgMztpDQrwGJuZ+eYQyIlSpPEQJcHBZiHKh/3Vipu06Cq9rrfXu3TtiTaOVS7C6vrOje0xENDwtA5mUWuYflmlkHfDAwCk0NnN4dHLl3Wul2ZmZGhKJ0tS0NhkkZhZHAUitTJWjc/Dq/9YZHAxAis7BlQOzVYOzpIkF6qs3X4uoXr95Ozq99Or1z5PzFp9P/BoKA1sJk2DS8KewmEG9q8v0WOF/hobfT6Ebma+ZX/p47uDrN++Wnq9R32C1nr96Rh3FQ90YOgd/ONMRt5qD+6AzSByYJS0sQ08C+tp68uTJ2MgII2P3StW61rWuf14DNHRp2L3i4JvlESroqAe4mIdYyB4R4vSISYakVOOqVOPKFMOKRL2KRF1SqmF1ukl1hkVdtnV9tmVdlml9pkldugk507Qxx7wRZkbONoISqMFMyVBWNZOGbBMo33AeQGEojLUZbtNa4tKB8OxEeHUhfTrLfTqQfp3owHakfycqoAcD4DioDx/STwgbJEYOVUaP1KRlhweq3rVTuR/14EHEzZve50/bHT+gf2Cn6g75q1ukTsuIHRbm3REWHF9dWX/r5A2rO6rWd1XVTl65BGh4+6ljiiePbDu9R/64rOgeKdHdwoLbeDiluDnEeTgl+HikBXjlhPjlGZPkgAnxb+HlkuRkE2HbzLfxJ46//WUDCyublsPt4pa4pddPmsZwXukWN27d07hvGBwYVYEnDfYPdrX3IMuwMeGJyrc1t8gfFxY8yMmxnYN9p4T4+V17NPcd0Nu9W2vn9vvbt93ZsuW6rOxlEYH9x/dctNG2TA9P7OvomRwdzU9JM7ilekB6pwyv5LHtB7fwiZ5RULi0fdvFrVsubVW4uFXhAlhukbu0RebCNvntQqKCrALivNISvNJyYjtuX1MN8PGOCfNKDLErTbAnw53aiu1ocCtKjhklB5wOY3KWQXW6XlWqHjFZl5CoXZGoDeC4vvC7f/Dyuv79hCbUaBg5G1oFwOGVfe0zM6MvZsdfzEy8GB9dqm/oD4uFqxu4OHpHjoyvXTwA0BpnkBiaa/s5NOIXU+UAS0EgRR8SHpghDc5WDc9VTyzUv3oLPdrwa+jFqzexhb1Z6MGZxyvxuMsv59/zbhODelcbc2B4ctWEuV8ag3pXlz9C4d+lYQiF6dPmnj6fYGwYQwPji7m44Za+tU5YBjS3NN4wjCQP/TLjx/tuzM8cDLoxpIHZyunFzq+dAwT8btczSKxrXV9E/VRUUfC9woBbZeGqyEh1dLQWKuYROk4bm6iPTzIgJOnj4rWx0Q8BJWMiHwBKxifogMqKZD1c/KOKBO3KRF1CvHZFvDYpGYqCqE7VqU7Vrs3Qq0nXrUnTr003rM8yIeeYNcIsGnKAWVELnNpK3FuBFbu1lni0FHu0lnlTiz2pRR4d5b6dwJB+XeiAXmzIIDGqsTTB1tDp2i13JdXI27e8z521OnrQ4MBO9Z0KN7dJX1SQOCUjekSEd5ePZ2hNFfnaiSsmN+76aOta3VVWP3Pp5tFzNw6dv37o4oX9F3ZJH5QT2ysrtleIV4GLRZRlIz/rJiggmJNNlJNNjINVlJdLip9XmotDhBVw8AaOH37Y9N//56/b9ivE4pzxQ+nPXj4ZW+xun6qltFBaaO0ttDZkGTopNs3JxuvCyVvbpPaz/Ci0ebMMF+dOXr598nJXD+zXPnBQf+eeh9u331fccneL/DV52cviYqf4OBRUrqmVwYomRyd62jpiff3UTp+5vOPAdlF5RXGF/fI7tvELn1WQO7NF9oyC7Dk5mbPy0mcVZM7JSZ+Xlz4iLSnGwSfMLgRQWIxf7uSJ6xZ2ntaugYYW7gZGVo62ltEBtoVJzjUF7h0ov36cfx/aswvh1F3m2FXm3F7s3Frs2FbiQC2w6q6IWEfhda2pGqkdD41d7F1jmur6H0+8fjL99snM2yez7xbm3j0GNv9uYvJ5QWnNfX1nr5CEZ0v/njklABX1TDc2DJWtxJj+YqpcLz1EeDUHD8zUUAaqukdGx6ZfjE4vjU7RjVn4qPxJ+7U3Ty9NzC73jDzV9SVfNKvwTWuvos0sLr99/ebZ1BMKHXYBCv8WDTPB95PGAF9mgQ7Bn6bh1WW6raLhx3XLr+YZR+/x4it846RDDO2adWVp1VqMtn6ksYVueogwlDfto6lyUIjwag6mn77BOdLkE9rXfj4IQOH1DBLrWtcX0VArHhHxsDT0Pjr6ETZWFxdvgEs0JKQYEdNMSWlmlammhEQDQrwePk6bEK9DiNcnJpuQ0kyq0o0ICbqkFOPaLMuaTGBWtVnWjXm2DXlWDXmWTfnWzfk2wCgF9rRip5ZS17Yy93aEZ0e5D8DcHlxwFyawE4oVDurBhvTiw/oIET248P6KqH5iZF9FRD8xarAydrIhqzQ15r6yw7V7ATfuBF666HzsiMH+Peq7tyltk7kqL35eRviUlNBREd49jna+tbVNV05ee3T2WpC+oa/2IztVZb1rNzXPXlE/c0Xj/K1ze85sEdm9Q/bYPsUzsmK7udkADfOxbOAFtuHvnD/8lXXjj5wsm3k3b+L+8QfWv/7vT3/53x/+v//6y4V7x3NpQaUdsVNPh5+/Wlp6+7hrsDU0JEpf2+TkwfMn91/au+2EvPhefk451o0SPNzb+QT2y8pfPnBA++ABg527H27bobJt2+2tctdlpS6KiZwS5D947uStkvzS+ZnZOkKlm6Hp7UOHruzcfmHrTjl+ib0K27eKSO4UFgHse0Je6qSc1CkZiZMy4qdkxU9Jix2VEJXl4ePdzCPKJSrCJSknve+Rvp2zX7S5S5i+ZZCWsesDPSt1bTMjE0tvD5uUSLeyTP/qwoCWMu8+tHcfyr2n3KW73K2z3K292GGgKnYdhde1dpp//MTOI8zEJqiteWxx5t3jydez4y+nx1+MDD4ZG12amno5M/P68cK7mbk3sGKSkpZtYRl+xfPray0zSLx997Zz8v9n7yzgosz2///737trYqJImiiidHd3d9fMUDN0d3d3NyggDZLSXQpISXeqlKggIP7PA6zr7t2913uvut5dPq/Pnj3PeQKcGWbez3e+53uan0wVf0wR/sjBH6fKAZAa2wOpyaWm2ZXmgdmW1Mr62ILee6WT90rHU/b8sfOr/m/69w4G/ftlEwmFoypOzezISg5UpaZH65NnK7u775ZeDy5AM+f2Ufi/ouGPnZ8g+Hdp+CcO3jdEwzPLLQurXe92oOSQpbV38QWjcraN3LpVvPrVrgk9uTVTmVWTH51VNZldPfXRubXTD+tnCn5yUeNsSfPcnkFn9lHrXOXj+QO3zwPCru96XveTwS1BS+/Lj27vX3oysNw9stY73dExXfJpivBHDt4P5wN/5GDw9E0uN86tdmx85RU3AAofVpA41KG+iBZHH7dkOtUmmzel2TSlWjen2jzOdHqc5dye5did79aV59aR49xd4Nbz0B20T7KdWx/YtWXaPs6B2o4cp64Ct84Cj64Cr64Czy5wTJFnb4lXf5n3wCOfwQq/ocqAwQr/gQoIf0erQ8Zqw/fygGPG6yDqHa+PnGyMmW5JnGu/P9d2f7b13mx7ymzbvZm2+3OPUxeePkwMiZaRd5VQChSVdOPjs2Zl1mGghVGTKVCTyXJyKXHzyJET8V/Dojc1dGxvfyrOJyfLxO+vYxBiqO+ng3JCaFrIK5tKy5krqmhLKrKSslETsvCziPAz8VPcpr2BS4RzEf/8aZxTJy+inTiLduLUiWPAEAQDHz+G9v/+74igAkfW06C83tDGscKBxfbOhQrPKNubV4ivYBKS32biYBAjImAgukWPi0mCi0115TIj4V0hRmZNFmY9OlptSmo1MgoZEhJxotvC+Fe4bt/iUVUxqaluXJidK87IMJRTkqRlFKelEael4rhDQoh9nYmE/C7eVSaCm3wkBFzEt7iJb/Pcvcl95wYX4Q0OghsUeNi4Zy7gXrx6HevWVWxCfkEle89IB79YM+cIE/sIXStfmKGdGspRWcNWCWGspm6gq2fsameeEGBVmmz/ON+pu9Cuv8iur8C2J9titDbiEIUP9e1UUFqjhrKvKn/6+sXu8vzWy1koL+L5/GZjQ19P9/Tz51sLz98tvHi3tLY79+KdV8h9PUvP2flvVE/3W1aQeL+7M7jY3DFV8ttT5Q5ShBvG90Bqaql5dqVleL6lqKUxv36wrHWhrHWurGXPHzu/6v+mf+/g1rnytvnChhm4awtAYUWHJv+0gf7xNQBXrzam9uqp/S4KA38ODe9T76f9PQj+BQ1/xN9f0vA+Cje/XB/anzO3sPQ2Jn9E0b4JILuAUY1LXE9S8RgYic4bic4fAZ3InOHQrKHQzCGozRoKyxoKzz5wWPZQRPZwRM6e9/rg4KjckY+Ozh2OzgPeu1reMLhabMHPjoM8mlo22TzY3D1b8quZjr9IEf706Vtunlpumlt58vbdQVT76+mwgsShDvVFtDTV3VMc+DjHtSPP/XG2y5Mc155CH4C2YLO/xK+70Kcz36O/FFr8YrAiuLfY93GWU3u24+McqO3Ide3Mc3+S696RA+z8JMepM8+tu9Crr8Snv8S7D7jUp6fYC/Bxf6nPs1LfwUd+QxVBI1Whw5XBQ5VBgxVBw1WhY3XREw3xEw0J4/VxEw1xU40pU00ps4/vDTZmejiESMv7SCj4CYjYcnCYMDHo0FMjGOlgKtoGrjH2non2KEsUGSm7Fsy040m3lLAKOxGTqwYy3Ngo1NAkQN/YQ1vbBaHhoqnhqI2U45agI2QSZuWDi8tIcooykbISXiHDOHfl+NFze+x74viRE8ePQjR84sTpY8dPAhQmoyOOq3QrHIrM7Q0tHIjMehogpymGff7mVUxiNnpRcmLO61coL+OQ4WBTXr3CfItAkJ4RwcKqR0enTUUFo6RUICWRILwtcO0KOwWpsJ2tX0/PwNDAYEJwiLqgoBgllRg1jRgNlRAVOeW1m3fxblDfukOEd4WTiICX+CYP0S0eYgLuuzcBB7MT3KC7fuUaOjrWWYxr2LfwL5PQ0/Hrm7o7+sbZ+MSZuUWbuUQZ2AehrLxRFj7qBu6qKDslDQs5uKmKuommpp6NqV6Mv2VOjFXlPevHuU5P8+xGaqMOUfhQ30hLK6sWTgHe/veWZt+tLOxAHDy3+WLhHXBOdlVDQ+/zF9uAgxdevltY2lp+vdveO6Vu5JqRV3Zw/lfW5Ph4cnz8w9zct1+/gsQuVEmto2u6bP+79WcL/8jB0FS5fQ6ehqrqQstMzCy3vN58AZ0OXWHPHzu/6v+mf+/gvV9p/e22bdRTq/Cu2s7F1xsHhRrebC39RMAfafgjEH/sHKDwXmePfX8HiwH1fuz8BMGfRcMzKy1rG7P7vxIQ+PVqO5/bRXXL2zU+ap0DI1s7uz97+z3wu70WeOPdDvinffTq663lNXCvBRl0XqxsLLx8e+Clt7PP30zOvz7wwuux2fXh6VdDewadgcm13rG1wcn1vpnHXTMl3bOfycHgGWwGD9HHBI+/pja236y8WXj+anJxbezl66m1jRdbO4dZHIf6TrU41tGW7dmQateYZtf0wL4lw6El0wm4OcMRtGCwIdW2Lct5zy7N6XbNaTb7brpv3ZJu257h0J5h3/bApvWBxZMs684cx65cl+4C96cFrk+y7Z9k2XXk2HXmQCUmunPtevId+h66DpR4PyvxAh6A+NhnsDxgsByQcchQdchITdhEffxkY/xMe3xbSay5oae8cpCojBcnrykLsy4DPZKeWkNWxTC+0jd/ICyvP+TBE3+EiQpcVb/rSa+CpDoJLomZnEqYoVGksXmYiUWwsZG/voGPnp6fsampijY/Ha8AA7e2tKKRiqaSkBwXLQ/5HcYrOHdPHL9w9MeTJ46hnTp5Bu3E2ePHTh87hvbjDyfOnb2IMJNLa/UrGo3M7gmyj9CnpKS7hkVy8zIl0W1m7EvEmJeIsbEpr1xmIiQUpqWDs7Dqg9+QhgZBSalMRiJ1+5bAjWucvDxqSYlZU1OzZSUVhhq6UizsYlQUYtRUIlQUgmQk3MREd7Cvkl2/RXoFn/LadX7Su3wkhHwkd3hJCAEKc9zGZ7p5/Q7mJewz6JcxrhHgU3DxyOmZeTr6xlt7xtr6JFl7J1h5xll5xJm7Rho7hupa+WmauCMMnOGGrso6DvIaFgoIA7iGnqmRib2FYaC76b1Qi8ai6M+cy3GIwof6b9XU1qWmY19T1b3+Ynefg5/Pb75Y3Hq+uJWdXdnQ2Pv85fbiy62Fl1vzS1uLqzuLa+/dglKsXUPevfsW3/YuvXzZ2tTU3dX1bSpIzK4865551AutSbaXGrF48N36Txy8nyL8MwcDInz+6snO7tequ7y9876u6/no7C/K2G2/f7u35tzjxd9AYeDfp+GP/q1V6D72f+FPCBjq/7QKHeDg+dXOje1f/8MBrWZUTPaP/wHT5qaWe8BtzKcc/A9T5RonXn7kYGiJkIW1zq+dIAG0uro6NTn54vnz76qCxM777RfrM8PPO3pmazqmyjqmSrtnygcWmmZW+tY2Ft9/2xXFD3Woz9Hc8OP6NOeqRPPqJIvaFMvae1bA9am2AIsb0uxq71nX3bMGNAza2mTLyjijmkTDmkSj2kTj2gSj+mTT5lTLFihX2LQp1ag906Iz1/5xlsOTbIfHWXbtD6zaAR9nWj7OMG9LN3nywLQz06I73+FZsWd/sftgqddQmc9gqTc0W+6R/1BV8EhN6Fhd+ER9zHh99GRrTFVmMErdUU45mFfUiYEFSU+vSUutycykae1v/3AwLL8vtOhZTOFQpF+Wpa6BUUd7D0wRdeUMPpxXIkBHL9zIOMLMLMrcPMzUNNjYLMTMytPAVEVAmoeaR1lI1kHXyEpTV1VUgZNBkIVWmBCf/iwa9sljZ06ePHX8GJQgsedTx4+cwccnUELKWAfp6zioU9Mz4Fy8fRmT+DIWCRYGERYmMS4u9ZUrzLdvC9HtczADioZWnZJShYxE9hY+P+Ftfg2EdUVF48zMfGx0Chs9Ly0BmQgNrSgVhTAFOT8pCS/xHebbBARYV6hu3bmLe5Xx1k1BMiIBUiIAxLzEhByEN5kJbpBdxsM7ex4XHY+UkF5UXN3UPtg9LN01NN0x4L5TQJpjYCroOPrfs3SPAShsYBuoZxWAMvPVNvVS03NU0bNX0bWTRZgoa1oowowVVZHqGsiI8NDPfM88ROFD/bdKySzUNfMeffZibfE9FA+e33y+8A5C4edbObnVDc39L5a3F5e2F5a25pe35le2X77ezS1vQ5q5j4xPH1zia2p3d3d7e/ubLTW38maud66yb67iEw7+xVS5yaWmTzl4dqXl+au+r7fy8O5+cPgftFdd+NN04X3vg+/HDoTC/xYNH+Dvr/xbNAw4cq+i8G88L9s7u283v9Hz9anm1wZ79kLCP6e1/LJkxP7T9xMHQ+H8xbWerZ2vWy0bvICf9fXl5+S0NTd/PxUktnY2p5YHn0yVt4wXtk8UPYZWqIamG3bPPgKP4fDzxqXXU9+sWvahDvWZmh1qr09zqb1nU3fftu4+aG32ONixNcsZuCXDsQGMpFjVJllUJZhWxhvWJhnW3zNpSrdqzbBtz7J/nO3wJMf+cY5tW5Z1R55Db7FnT6FHN/BD9+5C975iz75ij6cPnZ8WOPUVuTwrcR+q9B+pDh6tDhqrDh6rDhmrCR2pCh2pDp1oiAYEPFYXNVkfP9EQN92WWJzsryxnKSUfwMZrSUGtSkmlTE6mxs6p4Z/ulvcspKAnqm++uW+xqbArITDWe2BowCfIi/QOuQA5q7OqerChXoSZcaylZbSlZYS5RZiZZbC5uZ60ojC9oBSnhK2Wno+JuQ1SX4Zfhp1WlJdVgfwux8VzeEePnDh69MSxoydB58Sx06dPXDh7DAvrPP71a2R4OERn0a5dOHMD8yIhNiYJDjb55cs0V64w3bolSEsLY2HVY2DUoaZRp6JUISaSunmDn4JM3Mba71n/yMCzEUdH/7u3GLDP4rERk4tQUQqSkfCSEPMQA969Q4uPfwfvBi0hyV2cK+x3CARI7/IR3+EhIuS8c4vl1nWqq5evol/EPI9NfIdOScXI1S/ZNybfKybfJ67AMzLPPSzHJfiBg3+KnU+SmUuUkX2okX2IsUOYkV2ogU2QjoWPtomHhpGbopaVCtJWRctWTs1cWEbHyilwZ+cQhQ/19bW1te0VkuDhm/x8emN5Yevl/LuXz7chv9wBzs2vaWodeLm6+3z1/eLqzsLqDkDh5693Hw/MIM09q+rbDq7yJ9LG9vrgYkP/fMXH79Z/xcH7IeGfOBjQYcvK2/GvXZDrH7WxvfL81dO9AsMdvxUb/g0a/gi+P/uXNPyx8xMB/wKFP6Xh/X/7+uYvyqj94Vp+M9s3V7V/G7O/qhzg4P2I/j9yMJTgAeU6f/X1QQAKP/3OKkjs7O5MLve3ThQ3Q6sqFv1cOnC2oneuAjyA4PU//Lx+5c0fUAbkUIf6J5obftKU4VGf6tCY7tT0wKk5A7SOLRkQBDdnODSl2QEIrk00q00yq0s2b0y1fpzt1FPsM1QZMloTMVoTudeGD1eFDlYEg3a8PmqsLnK8PnK8IWq8IWayKQ4w7k8z5KJmWmLn2hJnWhNmWuKmm+OmmmKnmxMA+ALPtiRPNydONibMttyfa72/2PmgKClYXNhQSMyNmcOIlEKBjFyOhESRlR3ue98lbyC4sC9mfm0CvBU8fzXTP9m18vpl42CJkTuCm5ZJR1DCW1s91Fgv1sIi1soy0tw03MQ01MzUWg0uxyEuwy2rKwvzM7PwMTc309ThYxTmoJMS4FQlJ2I7dwb7yI9oR48cP3b02Mnjp86cxDh/Au/88avn0W6cRbtx7tT1S+gEWJeIcXAocXFp8PAYr1/npqJSZmHVpWdE0tBqUFIoE92RuHWDn4sDFhWZPjkxU1lRrw4zuXKZ+sLpawRYN7jJyflI7/IQ3eEmJuIhIeIjI6a8fpP0+m1aQmJi3CvcxIS8RLe5CG+yE9xgunmV6ioe/gWMS2cvERHRKcNNvUPSQhKKfWIK3MKzIIdluYZkOgWm23glWXnEW3vGW3vEW7rFGDuFGjmFmLlEmjtFGVqG6Jj6axi4axi6qeray2lYiigYWDgCFD7MFT7U19fqq3Vr1+CY+IdrL3ZfzL+bm349PflqeurV9Mw6cGZ2RU3D0+kFKFNzAnjh9dTLTYDCz6aXDe0DcgorD67yNfX69evx0dGZ6elvExgGUDu/+qx/vvJXJSM+TRH+hINbAYy+3Vo6OPkbCgDcTyswQwT8O5kSB/7iNLwXC+/5erHw/0wbW69+dRsDhYT3nr6PHAz809MHee3t1MHJX03g829qcrK5oWHw2bPt76OCxOL6VNtEycfVxfdWVdzn4IN5ouABHFioGn/Z8ubrzyk81KE+X4tjnW05Pg1pjgCFG9IcGlPtG+7Z1aVY1ySZVyWYVMUbV8cZ1yebt6TZPM506CrwfPYoZKw2dqopebIpZaIxeaIxaaolZbo1dar5PrTZlDTaEDfWGDfelAD6k01JE41Q7u90c/xUU9xMS/xsa8Jsazxg4qmmmMlG0MaP1cUCAp5qSgQoPNOSNNOcDNqFjrSipFABbm12bnMmVl1ycnkyClkyckU6WmUTN/P8gdCHzyJap4qfv/n5e9Tld3Ntk48sLQ3UeIRt5KR8tdTCDPQjTI3DTQ1DjQyDjA09UCgVXjFFQUUlPmk3PSN/S1Nvc1N1SWUmEm4uRjk+diVyYtazp7B+/OH4sWPHTp44dfokBjraVYzT1y+exUc/i4+BfhsHkxgHixwbiwoXl+HaNS6iuxKMjJoMjNpUNHByckXC26K3bwkqK5kVF9XMzMynp+Zyc0hjXiC+iE6Ic+E66dWbXKTEgHe5ie/wkJLwkpPwkJMQX75OeZOQ/AYBCd5ljru32AAE41+mu45HfgUH/8JFzDMXSYjoYJoWHqH3fWNzPcOznALTbLzirTzjbH2SbLyTAAdbeyaYu8bYeCXYeEIobOocbugYYuEWbeEaa2gTijT10zbzhhu7KOraSmuai6kYmDv4bh+i8KG+gV4srRjb+aakPnqzsrsw+7a1+VleXs3Dh/UPCxsKihoCQ5PjknMfljUVlDXmlTXmlzc/GZh+8XZ3cG7NzCUsNav44CpfUzNTU9kPHpSXlHyDChL7evNuZfRF0+BC9U8c/A9T5Q44GELhpdcHJRS+vd6+W1pc6/ytwDDwPgR/7Byg8F5nn4A/dg44GHifej/t/8I/oTD0z19uXdv47kKGu7s7Mys9H29jPk0R/sjB4Onbg2CI5ufXOjb/Idf5a2h7e3tzcxNaJuZg4I/U2631ntn6fQ7eW10cqsHcvcfBn8413LuXqFlY69t5f1gA7lDfi6aeNZdGmxZFGJRGGz8M1XkYgiwO0ykI0ioIUi8K1SoNR1ZE6dUmmDTft2i+b9WW6dCW7fokz6u3NKS7JLSrOKi7NPhZZcSzquj+8pju0oju0rCesuC+8lDg/kfBvSX+3UW+vSXAPn3FwN79UGaw7yBkaIW5/rKA/rLA0ZrwkZrIkdqo0bqosTrQRky2xBcmBvFxqtPQazIyaVKSy5NTyFLSKJGTyQmIKgfnOOf0BGf3BJX0JQ0/79p+D016ebu9vrG71t3fkR6bEOPiFKin5auJ8EdpBehqB+gh/fV1fAx01YXF5HilJTnELGBavmbGfmaG9to6/PS89CS8PCxKvOwK5EQsZ05hHjly4uSJ02fQLp1Dw7tw6uqFMzcunr+FefEOFgYRJgYpNjbtjeucd24LU1LK09HDqKnVSEnlbuELkJGKmZp4dnb2j45OeHuFUZHzYKLfAWdhXyLEu3iVCv82NxkxNwngYCJeclI+SjJWYiJCnCvUt24TYl+muHKZ6dZ1uhtXKK/g3MXBvoJ+8dKZS+SkDJooa8/gVPfwB45B9+x9kx18kuy94m08Yyw9Yizc0iSP5QAA6dtJREFUY608oDlzJk4RUIKEQ5iBbbCpY4SZU5Sla4yRXYi2uQ/MwFVV30nVwElRx15O21pMzcjcyf8QhQ/1LbSPwsmpZa8BCs9ttDQPFOTXFRY1FBY3FpY0BoWnxN/LLyxvLnjUlP+oqaACoPDUi40Peygc+m1Q+FtWkNjX7ofdpdcTeyBV+/G79V9Oldvn4JaFV13fYNLV72kvMDy88FNVtX8+he5jYHivvw/B/yENz6zsp9h+o6fj39LaxuLw87q9DO9Pp8o1Ti1/mhoB3cOApw/cxnz7zJY/XHOroy3jhW0TRRAHT+0ty/dLDt4vmbIXUK8DD91+dZRDHep70EB7ebyDTLy9VKKzXLKL3D1XuXR3+VQX6XQ3mXw/FeCiIHhhEOJhECLbV6UgEF4aqVsWY9SY7tCa5dac6fo436OjwLMt170t2+tJnt+TPO/2HLd9t2U7tQJ0hmzXlmHTnmn7OAvY5jFUaMKuO9+p+6Hb04eevSV+g5XBw9URw9VRAIjH6iLHGiAULkkJE+DSICVXoKVVpSSTJyWTIqdWICWVIrrDLyQlbeKpY+qlZROMyu4I75mvH1vqbZ961LfYNL44MD46Oj0y0vaoJD8mPMbW0hep4akF99RGeOpo6kpJirMKSHFLaErKexoa+BjqeBvqqQpJ0d5lYaUR52aW52OXJ7nDdPY01rGjZ9BOXjhz8hI6Gt7FszcwzhNcPE+AgQ5omBwfn5PojjDxXVFqKnkKcjkiIimCm0LMTMoB/gljY9PNTR26KJubN+gxL97FxSTCwybGvnTrKsYVBkIiCIVJiXjISHhBh4yYEv/WbWw8SvybNy9eorqGR30VlxQPm+ASJs7Zi1jouPR0XLpGjh5B91yD0228E2y8Emy9Ehx9kpx8E208YkycwkxdoizdYy3cYgAEAxoGrb5NkLF9mKFNqKFtKNLcB27oqqbvooJyhOm5wnRd1XScpeBWth4RhwkSh/oWWlt/beseEhmXv/Ly/fPFrYX5zfn5jbmFjfnnm/MvNrMfVte19s0tb80svZte2pxe3pxeebf4drdvelnf1j+3qOrgKl9T37iCxL62328+fzUw9gKiAYDCv8ox3Qep+bWOV5tQybA/UBtby5+uPPeZScM/r8n8kYY/8T71fuzsEfBPHajfCtrvLUv4o8Dtwfxa/x4K/3wb8w9P395tzBq4jflay2X/St9PBYmtnc1n8y0t4wWAgzv2OXimvOcnDt6fa3hwIwG98uvBn8DL9ZF/Y/7c7tZg79Oa6qrq6qrKyqq27oHtz5v18lX1fG6ioa66sqq6vqGhsaG+uqqqqq5x9vlv5H6MDzxOz0h/vvI93uYdCuhZe1mMrWS4OX+UlXCys8x9V5lUV6kMd9mCANXyCO19l4RpPgxRzwtQKwnVqEsyqb9n1Zrl/CTP43Gue2eB5+Ncl9ZsKFrcmecNBluznJozHFoyHAEHP8l16sp37spz6sy178p3eFrg1JXnCPqg01vs3lfq3V/mP1QZOlIdOVobM1IbO1YfN9WUMNUSP/P4fkV6tAifFhGxFDmpNDmJNBGRGBGpOBGxyK0b7Ndwae7cprmNT8rByxpT7lw0GFX8LD6rO7BgIDwoycna1KI4K3dqeOjF3PTgk7bKjPRkbzdvPS1HmJKhjKQQHbskl5g0l6Cjtra3rraPgY6xkgoLOTM9GS8brSQ3sxwXqyzxHaYzpwANnz11Ev08Gvb501fPnbmBfu429iXym9dZqSklycBvcleYklz6NoHwTXwBGWnjnJxHMzMLebmlYsJwXCwKTAwAwSR42ES4WHexL17Hx7rGTETCSQpQmJiLhIiDiJDlDgEhNh4wyeXLBJcwyK/gQMeho186dR4H4yoHl4SJlbdbQIqDb4qNd5Kla4y1S6yde4KdR5yFS4SRfZChXbCBbbCxU4SJU7iBHZQcDDoAhYG1zfy0zf00TLwQRu4aRh4IfVcNA3cEoGEdFyk1Kzv36MNpc4f6FgK3XH7hSU5e8XOzb1++2HnxYvvl0vuXy+9fru2+WNvNLa5rfDL4Yn13cX13Yf098NyrnYWND039k0hzj/rmJwdX+Zr6xhUkPmpr5+2L9aHJpRYABD+xVPMM9PU65MVXT19tzP5m/YRvKcAoq28mFiDAPSDgf3sK3e/Ehj92fiLgg85eMHVwZ+87vu9TG1tr08sdP6HwP3Lw3m3M6hNwG/NT7eavq++qgsSrjeUnU2V7U+UgDv7H5an38iIOUuTBXQS4l5hb7QR/Cwfn/yvtvp4z01UXEJGEwWAKSso2frGvNz87fWj3/fjowMzil8+8f1xToKMFU1FWEhUW4BcWVwHSNqxo6zvY/YkePQhk5WJ9OrpwsH2o70zjPTW5QZoZXoq5gYiCIER+AKwwGF4WoV2fZNKaatkC5UWYNd0zq082bbxv3p5h05nj9LTAvbvYu6vQ+2mhz9NC766HHp0P3Z4WevQU+fQW+fQUeUOrbBT79Jf4PCvz7S8DLeSBMt/hcv+hMr8BYNCpChquDgEcPFYTOVEXO9mYONWcPN4QP14fB9qJlnuVmdFSojp374rfuS1ARChw5zbfbQIeglucN68zXMEiwUYnwEC7evMyoU0gKn8wLO9ZaMFQeGqbj7AUJ+bJi/zU9MG2ltXZ6RMDfUsL85ODAw3FD9OCfDwMUMJ0rIL0PDzULCaKih5aGu5ITTsNhAADB/VdNhZqMRZaMU5mGUDDdwmYzqDhnDh27vTxC6eOY507g4+DQ33jOhs9jTQ3uxoFqeid24K3bvIT3xXRQTm3tHRNTs6EhSYw0IpgXSTBwSTe42BiwME4mIRY6Ffu4t1kISJlJyFiJyJku32LkeAG1fWr+BcvEeNduXnx4q2LGDcxLuCeOXPh5NkbVwgFhRVtnEO9gtMdvJKs3eNtPRMsnKJsXOJtnGLNHSAONrALMLEPNbQJBhCsZxMAbGQfomPpp2XipW3qrQls4q0OINjQDa7nDNd10jB0VdK2U9SykVIxt3EJP4wKH+obKT23VNvY49nA85WV3YVFaFW5hZdbC8vb88vb2YU1de3P5ld3Zle3gGfWtmde7cxvfkgtadSx8Jya+U5Dg19K2+83V9/OLr7qn1vpACAFOBgQ4eJa99Lr4Tfvlr6T79a3dl6/XO9f+KSw2r+k4Z86+wT8KRb/1PkdGgYcuQgFU/+wnJDP1Jt3y3OrT8chmGuYXIKyIz69jVlY61zbmPlmGd7fVQWJF+vTe6kRJb85Ve5TDoYC6ktNE0uN08utn/+M7zzvYWWgEEaYJiWnxMTE5pTVbm597u3i+3frPu7mcbnlB9tfTuODXfdTEpNjgnkYSYnYxMJiEhPupT+b+A3eLU31Y2Cl7xr5k7+z/e9qqr+hMt6oIkanKt6gLFK7OEzzURSyKlavLsm4IdmkPtGwIcmwMdmoJk4fjAAshpxm2Zxu83ifiSG7PS1wBW1vkWd/sXd/MaBev5GKwGFoMbmQwb1V5UZrwoarwkaqIkaqwkdrIvvLgvrKAkH7rCJ0qDpitDZqoiF2qilhqCqirxQcHz7emFSXF6sgYwhQ+NZNrls32G9dZ8W/xnzjCt01PPIrWIQ4F29gnb+KdRqPjYPBOcogodo9stgJaalAcA3/wpFzBBewJKjJDYU4QiwNytIS+9qaFmenF6YmOupqQty8dJS0BOi54fwCdsoKdmrKDhpwOW5+akImJiohZmohZlpRTig2LE9CyHrh3BW0IxfQT1/Fw6G4eZPjNgEvO6saHyec8Cb3jWucdHSy7m4RQ4PjTx53m5k63yFgwbxAjHMJQDAJLhYRLtYdwMHYGDdxzuNSXiNgu3uX5c5NplvXGa5fpb5+5TbWpesXLt3EwsVGO4uNdubC8VMY57DIyFiU1UzsXSI8/FOcvRPtPOLsPONAa+0aZe0SY+UYaWYXauIYYuIYbO4QZmIL0bCedYCBbaCulZ+WsYeGvhvSxFvD2FPd0B1h4KqGclDRtlNF2iP0nJWQdvKa1hLKJjYuoYcofKhvpMdd/XA9x+JHj1dXdyEOBl46QOHMh9W17f1zaxAKz6xuTQMUfv1+4tWWrW+co3fkt4nUrq+vjwwNTU1Obm//MbPTtnbevN588Wpjdu3t9PrG/Nt3y99bTPTt1tLzV90LP1WT+KUPwPejPwaG9/o/s++vvI+/n/YBB8+tPAaPwMFP/b61ubX2cn1kdqVjarl5CioFDaHw/GrHy/XBN++ef8twPkDh6amptubmoYGBP7aCxO6H3dmVofaJos6pMmh56l+lCC8cpAh/mlgCPL3csr65eHCJf6Wt+S5Odkbv7OaD7Z+0ND9RXV5a1/xk9e3BI/9+a6O3o7WkuKT1Se/+H/bj2jIWOkJxhEFtbeuLtdev1170Pu1f34CO33y99rSn5+Xqmw8770aGh+YWF/o62xtbuvarV0+P9j8qLW550vuvilmvmWmIi+h5vD7Y/LC0MFlbVf6osm7uxav9kUcPgth5WHvGX4L+SN+Tzmej0BcHOxs9T5pKSh/1jxxMFX2z9rz/2cjaq9WO1vqquuZXPy0DeaivrfHu6pII7cJgtfwAlRxfhTx/JdAvCUUUhcCKQ+BlofDycPWKSI2yUERFhFZ1NKoqFlUdr1cRq1d/z/xJtmNXrnNXjlPnnp/muXTnu/UVej8r8RsuDxquDB2sCB2tiQKwO9YQAzxQFdYPRupinxYGdOb7dhcG9JUG95WF9JcFDzwKfvYopLcksLvQ91lZyHRjYktBvKqcMTGRxE18zmtXGK/i0V7GocLDIsXDIMS9iI+FfgUb+Bwe9llcClIiYUlOLl4mfNyrOGdxcM9iX0fH5L5DoERHoslJYyXD74mCJ/t51BXmjj3rXZiZ6WzrSIpK9LG2cdKEmctJWirKwoWEqG/R0pLyABRmgmhYgotFkYdNiZKE6/Y1Ki4WKZK73DdvcNzE56YklyAjErqCwygqrJnxoGh2dvFhfpmkKAwPmxzrIhH062GR4GAS4Vy6i3OJAAfjFtb5a5fRcajxbzER3KTDv0JzDY8CD5cQCxP79Fms0xfOnzh/+tgZ7Au4JHepBYUVDMw9XbziXdxiHJ3DbZ3DrN3CbdyjbNyiLJxCTOwCLexDjW0DjewB+/obWgUYWAYYWAUY2QbpW/kjzbyQJp7axh6ahm4IPReEvitM12kfhVW0baEO0kFR00ZE3sDaOeQQhQ/1jbS5ueHiG+noFTO78HZp9f3+qnKAg+dWtrOKauseP4NQeG0bcPDUq+3FrQ+VHYOqAJ3L6/dP/9oCGJGZlvaouPjtt6og8T8ngDjrm/NQMPjr0PA+B8+utq28Gf+jymX8B9p5v/Xm3dLq2+nlN2PAq28nX28+39r5CELfTuDdfOvdO3Av98dWkAA3AFPL/U8mS/ZThH+1LN9+ijDEwXu15yaWoJIp+7NFX218bk48QGEebtagws6D7T1ND3TameojUUgEAuEaHL+yufth521ucihcTU1DHSEtq5DysPrDh3exvi5XcdCJaVn0DZ2eTT8f7Cg31rF+Ngel7c6Pdmsb6FV0DO9urAR6OBiZG6srKegYuL3Y2H72pMZIF4lCIeFwjYh7Bf8USl9bIqXFjbz3I/Ozzx5b6mmqqMGVFOS1jayeTUOzA8sfBHHwcgzPry2NtakryScWNIJnLysuQFNDHYXS1kQZlzX1751br6ut6+LhqoPUEBMV84pM2fgeioP8BTT2tDI/QC3TUybTSxa02d5yub4K+f6KeX4KBf6KxUFKpcEqe1atjNAAKFwdp1eXaFSTaFyTaNJ4z7Ij27EbEHCBW89DKELc89Czv8SvvzRgqCJktCZypDpyqilxoiFhqilhsjl+uDZiqC5iuDayu8ivr8h/sDR4sDSovzSwp9i/p9ivrzTgWVnQMGDlqsjphoTHhQkayhbEdyVu3eS6jEeHi0WOfYkI6wIB1vnr2OevYJ2/jIOOh3fhCs453EunMC+euIhx/CLmSQzcc9i457Evn8NguHFdnJJIkZFUk5NOi5tJm4fFUlY4zNY0JzG+qbZ+dmp2fHi4paIsNSTA21hPX0aC9iYFKT4jPTkvI6UAI5UoM60UJ5MiF7M8L5ssQtmMnkrs+hXW69fYr+Ax37jKClczq69rn5qciQxPZGEUARCMAyUHE+NhEeFi3sXBuINz4TbOhZs46NcvncG5ehGX/Do+xbUrpHjYd7Awb6CjY5xEO/3j8XMnzuFgXKGiZBaThGnp2Fg7Bjl6Rtm6hFrZBVnYBJnZB5s6BJs5hprZhxpYeutbeBlY+Opb+Ohb+ehYeOlb+BlY+utZ+BpYB2gbu2saumgZgdYNrusI03FQ07FXRdoBAlbStFXWslXRslPSsJVHWInI6Vs6Bh2i8KG+rt7v7j5f3lh9DYWp6pvbYbp2BWVtq6+h1TTml7fmlrdml7ean472jC0CDp4BKPxqe37zw9DLNzY+sab2fi+WvtGUo8nx8ZSEhC9bQeL9+/cArHd/byW3L6Gtra2Nb/htOKAcgHr/EQ3/moA/9T4Nz61ChSOWXg9vv/8uVoj4d7W7u7NH8H91WgEvkunl/o6p0l+nCB9wMJQaMb6XYA04GMqxhji4eWbl30Dh7YVubg5Gi9DM6dn5iYnxpbUNAK8+1jpicuoFj6oTg1042TnTKzt3NtcTgzy9gqJKSwuNNKS5xOFTq+/aq4tY6e+KqekUF9esvdtqKUngoBdpG4PitWPd9ZTMjPcqOnbfLmmIMmPhExlZOj7IKpubGzfWlFdDmZdX1wY5GXNwi9Y+Hd//TX5DuyvmmpKi+h7Q6oLv1930VVi4xaOSM+7FBjHSkBi6RYLh6uxQXiHeJ087PS3UOYXUnk0tjT0pE+TmtvePra4oNoBLSSEsVt9/mO7Mx796hUtUKTY51cFEjYKOq6H/+d7PONTX1UR3NUDhbE+ZPF/FXB8FwMHAgINzfGRzvWXyfGTyvWVyPaXyvGSKA1XKQxGVUciqaN2qGP3KaL3ySJ3aOIO6eIO6RMPGZNOGJJP6ZNOmVKjmWneh50BF4Eh16Fht5HhdzER93ER97GRj3ExL4nBVWH+p/3B50GhF8FhlyGhl2BC0PEfwWG34REP0THPCbGvywuP73VXpRjquJMTSN25wXL5Mi41JjHWRAPPcDcyzVzDP4GKdw8O9cBnv4tU9GsbBPIWJfQoT5/QlnLOYOOewsE9fIMLC5rp7S4jstgw1sRIDpQoLtTo7jQ4vM4ybHS4hFxscW5ZfND0+ujA92d3SVHAvSVdZk4GUg/IOGxURFy0ZPz2FMDONBAejDC+bgggfnI5S7PplFjwcRnISQTtbv76+od6eAWsL1zu3GDEv3sXDIrmCTXwZ6w4u5m3cSwQ4F29ho9/EAch+Fg8D7dJ1DJy7eHi3MTGunTuLc+os5hl0nIs4t28SMTFySUipGhjZOzgH2TsGWtn6mFh6mdj6GNsGGNgE61sF6lv5GVoF6FsE6Jv76Jp66wCbeeuae+mZe5naBhlZB+xtemsYOsN1HRC6jmpIO7iOA0zXXknbSknTSkXbVknTWkXLFnCworq1grqlhJKxrcthVPhQX1/5ddNBDwYzKqcfPBpRNAjVNPVrezq+DGj41e786vbsytbMyhbgYMjr7wEHT77ajsooVUTaVtS2Hlzi6+vF8+f1tbVP2tu/4JSj1dXV5OTkpaWvuDRGc3Pzo0ePDja+iXbev1uDaLhzj4YB434uDf8Kfz/1fjwYtICD//DqaeDW4unTp8+ff0XmmJmZ6evrA7cxB9tfQuCOa2V5eWJ8/Pni4rfJKfo9gd9kfnWoc7rsUw7+hxThxomXHzkYqh4IXgCvPztB4v3Lfg5GSiJ6LgQCoaqqlpjbMNbdKMpF5x2XC+5FXs32woVZ1O3CwB3V21dre7cmu9W5EdRknPXP1na319TkuSwCEveu9KHpYRwXo1j7AQo3ULMy36/s3N1cUuKkZZNCza1D4d/+pgIWOvoHpdCyl7O9NbwM5B6Jv1/h8ScUBj/95XAzKz25f0rZ/p5QRx0qFsnpV7uNhdFc3PRmxnrikkoljQNgV4y7IZeU5sAcuPPfqcwIoqFkrhtYnevKvX3njk9yBTjg+WgTBzVV+IPGvSsd6utqqq+uPEqvJARRGqZZGqr5KFwLuDRUvSBAKc9HPsdLNstNMs1RNN1JLNtdusBXsSQIVhKs/ihcuypapyISWRQIy/WWLwxQfhSGKA2Bg+uURyJrE41bHth05Lv2FHv1lfgOlQPkDZ+oiZppjJttShqpih6riZqqj55uiJptiF5oSnjekvCiPXHx8f2FjsyFrryZJwWTHYVtlfnWluHUtKqXr7Lj4VFjY93FvHDj0tkrl07hXThxCeM0NtZ5XGz0y7joV3DO4+Gew8U9h4N7FhP7DAbWWUzMUxeun79Ad+MKO8E1vjs3xMnvytJTABrW5GRQYmFgI6YSYxOS4RIMdrGrKcga7n26tDjf9bgzLiLRVM9WkEuelpyPkpibhoyPgVqYhU6SmU6Sgkjg1jU2Xm7lmOi06em5kpJqWWnNy1gkWBdu42ERX8YhuYx1F+8SAS4GPu7FG9jo17HPX8U+d/nSaaxLaBeunL945fx5nLPn8HGuUJPRc3OLScrANVGWppYedg5+Lo6ege7e0X6BwV5+zvYeFtbuhpYe+hYeJhbuJqbOOgYOWoauhuZ+RhYBeub+uma+eube+uZeRpY+emZeYBdCz1FdzwmGslODVle2UdWyVUXZKmtaKWlYqGhZK6pbAhoG7Z4tJBQNbBwDDusKH+qrK/7hKK9+Na9etaRVs0VQo46lv6lDUHULNE/uxWuoasTzt7uLb3cXgDc/DL5YT8itVNaxj4h/sPnu26U8bm9vr6+vv3nz5gsGcQEEy8nJAeg52P4KCgkJCQgIONj4VgI0/Ort1OJaFwSyEO/+MxQG/ldpEnuFhFfbl9+Mfn4Nga+nlZUVPz+/2trag+2voPz8/IiIiC+bivP+/fve7u68rKy2lpZvWRDwN/Xy9fTTmUe90NLKlc9+Wk3jF1PlPllNZgZ4pQW8Bt5ufe6ac+9f9HEwUzMJKdjZ21tZ2zys6WqryqYmwJJS1nCwt7e3NmYgu61qFQQehbWF4Yggbzt7W7iiyF1C1prOFx/er8MUuCwDk/Yv1VwYz8si9XgciuGO9zbSc7KnVXXtbrxQ5OM1cj/A5bq8aIJLeHCkEXRtExQRwVXn2Lz9Xb+hn1AYQHRPbSY1A01D78Fag83ZMdQkPB2z6y1libRkOLhYlzWtwvf2vDfXFL1Lw2plY+vg6KCpIklGyljRsTjTkc3Izlr+DEop3lgdk+Wg94st2Tv+UF9Xc4PN9ckWVbGGNfHGwPVJps33LFvuWzalmDQkQXPmauMA8mpVRiFrYnQbEo2b71s0JJu3pdt257l2Zju2pVo3JJmAg9vTLZ5kWD3JtOnIsu/IcewEHFzoPVAaMPQocLQyeLw6DHiiNnKiPmaiLna8JmKiJmymKXa6KWm2NX26LXukOaerMr2+KK0870FB5oPsjMyE5Gwzmzh2HoOr17kxMSmwMG5fOn/14mm8C2hYZ4+eP3cCHf0UxsXTmFhncbHO4mBDxsY5c2kPhS9hnr6Ad+YcCS42w/XLbPhXeAlviJATSdOSqbLQKrMycZBS8tIysxFTKnGz2KjKBFmb5SXGdTQ2jA6NjAyNFReUe7gGKcho0VPxU5Fy05ILUJMK0lGIaCEsKysaZ2bmkxLSOVglzqJdO38SFw/j1hXsu3jYRHiYt3Ev4uMADr5wFev8Zezz4LfCRD9x/sKJc9hnLty8coOelllOFqFr4GBmH2jtGu7oFubs5Ofv6p3i712ZGN6Rm9yWc6/ifmJ6ZHS4T0CQh1d8QEC8v5+rrRNK11JH38kAigr7aBt56Bi7Iw2dtfSd1HUcAAEraVgCCIbr2MG0bWDatiqa1mpIaxUtSwWEmbImaM3l4WZyMBNZNWMJRT1uYVUDU+ft7UMUPtRX09vNnZ7RFffEXoDCrFrlivZNDd1LbR29+laeSDOvhIxHLT0Towvrc+vvp9e2e6aXipq63cNTVfUcAiJSXi5/7wUE/qU2NzdRKFRhYeHB9pfWq1evrKyscnJyDra/od6/33q9ufBivW8fZ/dI9/dp+HeThg/yIgBVr76d+k7yIjY2Ntzd3aOjow+2v4K8vLyCgoIONr6Q3u/stLe23ktMbKqv3/qjUfjV5lLfXFUfhMK/WE3jIwfvpwjvx4P3lhVsAa+Bz0+w3pp/ysPF4pXZcLD94UN7RSr5dSwldT03FycArA6OzhVtfeuvXvhYavEJSto6OSHhUqR32Wu7Xu6+W1GR47AM/CkqXBjHyyLdOQXdlkw/a2X4iMKCgqbeKfvHVGaEEmBcRxlZuTg7Avh2dvVo7R3d3/Ub+gSFOyru0zFRtw/O7u/pLEimJ+N7PPOquTSBiY6AjZ5GVMVw4Q249941gPOTM3M7O7s4ODjY2dmFRCXNr2yNt2Uyc7A96oO+oFh7MSTLwRAQX7p/qUN9Vc0NttQkW1TGGtYmmjekWDbds25Nt3uc4dCZ7dSRDVrHrmzHzmyHp7nOPXmuPQXuPYWeXfkePUW+g+XBg48Chx8FjpQHPCv26it07y92HyrzHin3Hyjz7S/xGSkPnqoOn6gKGasOGa2G5s+N1MSM1CU8q4xpzwuoz/CtyQotTo/KuRd7LyEhOjIpIDjOxSvayjnCxi3WxjvZJTjX0DZFUML2DrHYJQxyjPM3Mc5eRkfDPnvi4pljZ08fO33m+Jmzx89joGFinsLEPA2MhX0KA/v0RawzGJdOX8A6debmxQsUeFj0l7FYb+Bx37kpREYIaFiGnoblzl0eKmoOMioJZnpdUV5dMUEDKVE7pKarjeODe5nPegfHx6aaGtsjwhKQ6mZcLNKczNIuDgHdT58NDoz4eYdTk/NcxiK5gkmIdRrn4slLuBdv4mESXr50C+/iDRz0a4CDsc5jY57DOH/yzOkjJ3HPY9OQ0UlKKSH1LGzt/a3s/I0t3a1sPbxdvRODAkuTop/mp4w/SpsqfzBTlT1TWzBS+fBpYXZHXnrvw9TugpTq1IRQdy+Utqm2rj3S0BWh6wBH2mroOmjoOQIChiEB+1qpaFiqau7bGhhwsJKGuTzCFLQAgmVUDKSV9cXlUCIyWmx88nrGDodR4UN9FS2vvavvep5UPBZXMBKZM6xo3yhuXpdePrEfce3sGXALiIUbOBnaBXqGpQYn5gXE59j7JWiYuKPM3JPSC15+qxThj3r37t3C3Nzc7OyXjaglJSVZWFh8pS+sGxoaEAjE9PTPK85/U+3ubmytQAvRrXX+RMPAv03DH9Mk9vo/JQevtIH+y9cDrzeff1fz5NLS0gwNDb9SkjdAbSQS+cVvkN6/fz8xPt7a1DQyPPzHJkgAvdt5O/qirW+ufK9kBBQS/hUHT/40Ve5gdfFlaE2+97uf+y3Q1nwXNxeLf377wfaHD71NBYyUpKklP48AjXVXMhMSR6TVgX5vQxYTEXtVxyJAYSUZDouPUeHiBBZqnrZh6Ma7v6WYgJI6vXofhfmNPBL2j2kqTqQmZ2roOSDaA/3ey2N3xUxTQkTfA/xjhtuKKMnv5tX37O/JCXOmZRIfXdlueBjFK8STmhQlzEbrGA7dyroYKIhrWv7qa4KhpnRGNpaSbqiaytrzQWl2hoCEg1yLQ31VTQ80VSdbVsUb1wMOTrVuTrVtvGfTdM+m8Z41GKlPsWi8Z9Wabtv2wK4l3bb1gV1bhmN7pnNHrsfTAs/uh549D937Cz37Ctx78126su2AnxV5Dpb49Bd5A0oeLg/pKw3qfRTR+jCsPjfqUXpEwf2o5IigIB8fT88AW6cAE7tAI7sgXesgpGWotnUE0i5W2ybG0DnJ1DPdMbTExClDRNaZml4FF5sW4yz+hVM4505gnjl+AXDwqWNoaEfRTh07feEUBuapS5fQMDDQLmGiYWCioV86deHSKXSMk6evnD17F/MCBdYF+suYLPiXuQivC5LeFqYgpb1xg42EiI2EnJ+KUoOPU0uQDynCryHMx0pKQ03IgISZhgcnNja0T4xPtbd0pCRkpN7LmZtdaGvuQKmbERMw38CjvHmZgomcVZJLiBDrKsbJS5jnruCcv4pz7jL2WTzMM5iYZ85jnTtz9dLFO1evslDRykspIJGGOrpmpkY2VqZ2Lrb2Mf4+pUlRnQVpo9WFU/VFo48eDBQkDhclT5RnTFbnTdeWzNQUTz16MJwf35+T0JKW5Ofoqq5lqqFnC9OxhWnbQCFhXUeEjoOGriMcaaOmZaUINwVWgpspI8wUECbycBMFdVN5uLGMir6Usp6Uoq6YnLa4PJJbRNnQ3PkQhQ/1JQU+HGYW35Q0z8Y/HAUEDFD48cDS8+UN/7RnQQ8G1978/FG3+GLpYWmNV3C8mYO/roWHgbW3rXtodFJm65OerT+inNny0lJFaemjkhLQORj6EhofH1dRUcnPzz/Y/nJaXl42Njb29vY+2P6DtL3zdn1zbun14D4QQwawCxnA8Uc+hvoHe/eDwVBGxOMX631rb6fe7UDfTX9XGh4eVlNT+0pJ2NnZ2ZqamuC262D7ywnQMLiR23r37qvO1Pwc7X7YXXw13D9f+RMH702VewmtpvHJVLl9Dt5D4ZW215v/xnoTAIXZWek9HvxcXubV/KCaGIe8puWzsemZydG6yoaF1dcTz2qZbhMGp1RubawE2Wvj36AHKPxh9w1KgU9Cw7y3f2xz58Pg4zKGu3dcQ9MH+zqcDJVP4N5+UPN09+1zWR5ufde4/YsvjLTzs1AbOQZPzMyPjwxU1TQtLK3t7/oN7S4bw0UEUa5vwC3By1Epbjo5LcvOZ6O97TWyAmyqplC0uDoziI2H/dn4zD0fMxJqzqb+qaoHodT0rAn5lfMLC72dra1tj8En83BTGg0jQ/FT6KWytjggzkzte5gg8U00/ayxJtmyJtG08b5V0z3LphSLhiSLxmTrmkTz2kSzmniT2gSTRjCYbFYdZ1SbYAyOaU2zffzAoSvb+WkusF1npt2TdLvuXOfOXGil5fYcj5Ysn9pU75Ikr7RQx7gA56hgHy8PL0dnH0tbbyMrX20Tb7iRv5pJiJJRqIpJuLpVlKZNpLZdNNI+DmmfYOCSYuSaYuqVbh/80NI9R0LJk4FVC/8GxyV0AnQ0nLMnME4dRz957PSJY2h7PnXuFPqlM5cw0DAunryIcfLCpZPoGCfOY5w8f+HEaezTZ25ePEeEcZ4C6yLNZUyGGzhst69x3rlNfvkK4+3bTHeJWe/eUWBhUOViV+fjQvDz8FDQ4WMS3MVnoiTmFRNUcXf2Hxn+ec7ow9wyWTF1ckK2u/gMVzGIWciYIp3svAxRnBTk1y9i4Z7GxDuDiXv6wo2LF8mu4fDQEiOk+Cy1lK2QMBNNmJ2xvre9daSXW2qof1lKTFdRxmhVwWRd8UxT+WRd0dPcxJaUoNZ7wd3ZMX15Sf2FD3oL7nWkhdZFuJb62HakRFbeS7AwtlFDWcB0bdRR9kh9JwTKDo6y09Cxh0EcbCKrog8so6QrpYgE4C2pgJRS1AHsKyarJSaPlFTW3Te/JNzExv0QhQ/1ZbS1/b5vfDW7eiombyQieyijcrJ/fPXd1sHyEKA//fw3pkOtrL56NjTe3tnX2TMwOT33R9X0BXrx/Hn2gwdZ6ekLX5pR0tLSFBUVs7KyXr/+YgW2BgcH7ezsUCgUQO2DoT9Qu7vvttcBEC+/GXn+qmcvHeIAefe811+D+vuLyT1/9RSg89rG9Mb26r+x1u63VWxsLAwG6+joONj+QqqrqwMvhvT09IPtP6/evFsZfdE0sFD1i6lyv+BgaKrcHge3Pn/VC+6pDs78DG0t9gjysvtnNx1sQ9qpyY0T4uGWkFGAw2FKynp1XSPbm0uupnBBUUlTcwtpcX5WbpGaLijZIDPcjYqSWkHNrHt0cWdz2UlfhZ6RTVVdS0NJipqVP7uuZ3fjBUxcxNQ7ef/SH3Y30iPcebh4ZBWVYWpqcF3rp6MHpX9/Q7urVtoyMsZe+3d4pWmh/NxckrIKclKiEvLwmq4xMFiRGcItwDO88ObdyxFJLgY9+5DF+XFzlBIHN5+qGlxZUd7VJww8HOPtOawcnBW90E3C2vMheV7WoCRoCt2hvram+uur4owqIvUqY40qYwyrYvWBa+NNahJM6hNNa+MMa6ACEcZ1Cca1AIVjjVruWbWnAxS2g1ImCjy68z2aH7iUJzkUJzhkxzgnBDuF+rp6ubnZWNsbm9mro8yVNCyUUC4KKA9ZlJeSYaCCQYCSYRDCMhphCQg4Fm4RCVBY1zFR3znZxCPN0PWeufcDc+90C58HdoG5dr75sgg/BjYU4R1BXEzSi2evnD15Ce0E+onjZ44fO3XsKBrwyeOnL5zBuHgKoDA6MAZoT5zDOHHu4okzl9BOX0c/c+vi2TsY54mxzpPhXaC5js2Af+0uFhbNdXwaAgJa/Oui1OTSDDTyTHSKbKwIORENfXktczivqAgVKaelidPU5Mz01Fx+XmljQ9vQ4GhrU0dcZIqBtiUdOe/ty3cCLAzGq3Myg12s4PKaIgIIQV45dkYVHiYzJeFgC83ccOfypICCaO+MUI+ylMiWgtSu0sz+ityR2sKJ+uKJ+pKJ+tLJ+qL+4rSqWJ+HAXbFgfaVES7Vke7V0V4lQfYPHJHhegrR5vrdDwtGauoCXAJUNEzVUFZwLWuEto2qpqUCzERF3UwRZgwIWEJOS0JWU1xWQ0QaLiyhJiIFF5ZECEtriMpqSSiiZNQM5ODGoBWW0bRw8PmzVZB4s7EzMLnWMbjc+ddw19Dyfp2yP1Arr7YankK5EBE5w/EFo6Utc7O/Rb3fuV6trbU1Nz9ubV1d+cK5Ge/evQNcpaGhYWpq6u3tHRwcHPSfCpwbGBjo4OCARCKNjY07O39RV/UP1/b7jY2t5fWN2dW3E8tvhpdeD7xY79tz/8vXz5ZeD628GVvbmHm7tbRXJuIPjlz+c7169crd3R2BQISHh1dVVTU2Njb8d6qoqADPHcBrPz+/za+wMDJ4K58YHx8dHl5/dbCOwx+updcTw4u1AIUPUiN+Ox7cOr/65PNrR+zr/Zul3KwHHcO/yljYbK0p8fF08/DyS80qmnsJPQ6L04MRQT5+wZFlVbUV5aVTi9Dt6KuXMwkRgb6BcePz0B/7wvizyEDvoKjEjs6uikdlg9MvPuy8LSvIq23/ecLr+4218sIsdzdnL9/A3NKaldf/JK99s778YVFN28Ft/fab+kcFXh6uPgGh7X0HGcYTAx2ZOdkvX0Fv3Y9rCxPToELFS7PDKbHhzs7OweGxje1QTsXawsiDB5mTS9Db6bu3y0XZ6R19k9D5h/rKmuytLYvQKQpQLw7SLg1BPgrTLgvXLI/UrorVrQUQDHGwUV2SSUOyedM9y7pEq6ZUp7r7TmWJDvmxLmnh7nEBngHuzi7OjpY2DgYmjhr6roraLlKaTqIIB0ltd2kdbymUj7x+oJJRiIpxiIZllJpZONw8UtsuXtMmTss2FmEZCbeIQNknGLmlmnkBCM6w8Mkw906z9M2wCch2Ci6E6YczcOoQEYtdvUyLgX7j7CmsUycunDx+9vix0/soDHzqxFn0UxcuoEG+ePL8hZNnAQpfOH7m4olTV86eun7+9A30swCICS+dJcG5QIaLiX8BnezqNbIbN0iv4HHevc1LRiRARqImyRt83zqzIyCjyz+6wCc5JW14YLyxod1Iz46JVkhaHOHpFlxeVjc6MjHQP5SbU2Jlbh9gbznfUrzYWjRUdv9xZkRbelhlvFdBtFNpnFt9kl9PQUJvUUpfSepQVd5YQ+l4fdk4aBtKJhqKx+sfTtQ+HK3M7Su635gSnONtmWKvk2itGW6s7K0l6YuUDtVVcJLl1uOnt9dS767rnmwf9bHzk1NAKcONlOHG8qqGMsr60op6UBhYQUdSXhugsLispoS8NugDIJaQAy0Y0ZZW1pNVNZBWgVoZZT1haU1zW68/W1R4dGYdAFlU7nBi0VhC4ej/rpOKx//lPwEc4J7Y+2Tgc6ddf1nt7n6Yef4GgG9cwWhkzhBA4aaeFyvrfzCX/8cCJAEYYnPjq9SwBxcHJAQQ1szMzPK/k8WeAJ9NTEwcXP370877LQC777ZfbW6vbmytbG6tbm6vbe283ls/77sm4E/18uXLqKgocMthbm7+8ZH/zwROBxcxMTFJTk4GkH3wA76o3r59W1JYmP3gwdTk90JL2zubC2vP9kLCdZ+WjPgEhaEKeuDG6X9oRZUvoH8/feUPz3j5q2myp7o4RLMwQK04SKM0ROtRuHZZqFZZKGiRZWE6xWF6D8ONs0OMHwSZJ/mahrqbBLhbuzlaWppZ6BlYwZHWqtp2Mur20tquskhPWaSvtLaflLaPJNJTRs9XwShE0ThMyQQ4VMEwUNs2VtMqCmEeqWkdh7CMVreKAVgMM4+AmYVrWEUaut4zcrsPaNjYPVXXMR7QsH1wnlNooYHDPRY+IyJSqRvXWbAwbp87g3vqJAba8XMAhaHA8LEDGkY7fuYc2nkIiE+hX0Q7d/HkOfTjpwEKXz516srpU5fPnLl29iz+uTO3zp+9eeE83tkzRHhXCC9fIcDGpLlxlf4mPsud2zZuiNy+0IKB8NrxzJn1gbU3Kw8LHslIaBIRsN2+wUh4g4H4Nhsfl7ypsWNSQnpXZ+/I8Hhvx5OV8f71obal9uKFhszJhvsdLXE1bZGVjeHNjyKHH6UPl2dN1hXMNpVM1hWPVj0cqykery0ary0Yrc4eepTek5fQfC+4NNQpxV43SFfOU13UWY3fRpHLQVXQB6lgJy9oIc1lpSZ/PzC6KeeRo7GtmIiCpIy6pKymqBRCVBouJa8lIashKq0uKo2AUFhOS1JOS1oRJSmPBIgsKY8CrCylqCshjwS7JOSQwHyiMEOzP10Fic7B5YD0gbrO590jK59GT3/lrqGV3vHX/ZNv+ife9I2vPx3+Zwf/yl3Dq30Tb7pHVj9z/D8x+PXGXveMrv16/JfuG1/1TOoraf7ySYf/XFvb759NrOVUT0XmDANnVk72jq1uvDvIhTjU7wkAMaCr2T3N/YM+Dv5q76fjQPPz82++3CIgh/qXGhkZKSsrKygoePhfCJxeXl4++TUhdW11NT87Oy0lZfq7QWGgdzuvF18NAA4ef9kwudR4sDb1CrSgxiy0QvXjlTfjO++/fIz8z6ZDFP62muqtKY9EPQrTLA5DFkXoFkWZ5IYYp/kaRTrpBdnquppoWRtoG+vpobQNNbRNlDTNpTWspRB2AgqWIqoO4hqu4pouUigvaZ0AaR1/GR0/KaS3nK6vvH6AnF6gsnGEknGYrL6/vGGAnGGAunWMqmmkqgkUFYaZRQAU1rKNh1tEqVtFo+zjAQrvZUdkgBZlH2fh8wCgsF1grrVPjoC03R1S6Zv4nLhYJOjnLp9Gu4R2/PxBjsQeDR89evLYkZMnj506c/Ls+VPnLpw6fwHtHPoJCIVxT57CRUPDPnUK99Tpy2jAoHMKE+0UATbutUtYVy6gE+FgkuJgs5DeDkwzzx8ML36WML0yvPlh/fFIrYIK/AYu3Z2bLCS32YhusdzBZyG4wXT7JhMZMbuCDCLAO7SlvmljffX9m7WNubG1wfbezqyCzpDM7sDsnuDCx2FPa++PV+WN1+SNVecMPcocLM0aq3o4Uf1wuCKrtyjpcWZ4Xbx3cZBdqrN+sL6Ci5qgkyq/r45kjA38vpvxA0+rVBfTdFejZEeDOBujksiQeF8/KXEFHn4pXgE50AqJyolKKAmJKQmKKYtIqopJwYXFYRIyGiJScBEpQMbaAIhllfVBKyajsWdNUWlNTgFFQ9M/3bS5jsHl5OKxtTf/6zGG9wszk4sv/3U1sZTS8eKmX35L+DX16s12e//SvZJxQMAx+SOFDTPjc+t/mojF+vr65MQEQM6D7UMd6n9HGxsb/b29T9rbV5b/mK+Jfk9bO2+X30zMrz6dXm6ZWt5D4eWWuZXHL9b7X23MHnLwob5Djfe2ZIeY3fPWj3HX97FFOlnq2JgZGuoba+hYKCBMJZUNBeUMBOXNBBSthFQcROEuYupuEtpeIgg3UYS7FMpXEukjqxMgqxMkjfKR0naX1HaTQXnL6wXK6wUrG0UgLGJkdHylUd5KxkGqpmEwsyh1y1gAwdp2CSiHJCP3dB3HZD2nFCO3VEDAxu5QjoSJR5qR230LnwxL30wzzzTH4Idq+iF3yWWvX+e8jEN5Ef36Xo7ExZN7gWGAwkePoR05ehLQ8NEjJ44fPYF2HO3MidPnTp45f/IM+snTl04C8EXDQDt56SQa5gk00KKfOHEB7dSNS5hYZ89jnTmDf+H8TfTzDKT4odlW+QPhxf0JA4vtXQvVyXV+vMICN3BpiAhYiAiYiQlYiAlY7xKw3rnFDIj8EvpNfDxiQ5j6ZEfD5svZnddr79+uL66Md8zWlA+lZveEZnQH1rTGjldlDZWl9hUl9xWmDJSkj1Xkjj7K7i1IakoJKA6xy3A3TLTVDDJUcEUI2yvz2ihyOyOEAg0VE+x1Ut2MMzxMH7gaJdpoR5goJdvplSTeU1FAMDBzs3IIsXMJcvOJ8PCL8/BLAizmF5LlF5LjE5QTFlPmE5bnFVbgF1URloAJS8IFxdWExNUERFWgjgSMS1DByMzpz4bCnYPLKSXjgNgOtn9H794uZyVEuDg5OLu4xt/LGJ/7Nz48VhbGAn08atqhNeI/1er8WISfZ80n6WX/sXa3Xof7O8Rn/+u6OffLJr5NVHhx6W1l+3z8w1EoIfjhaG3H8xcr/5Or4/4TjQwN5Wdn11VX/4Gz9w51qP9Gm5ubO9/fq3d3d2dja3VtY3b5zdjym9GVtxPrG/Ob22u7u4dfJR3qe1RbS4uZoSlKywChaSKpiBJW0BVSNORXMhOCO/Ao2fCp2nEpWvOq2IuoewjBIPwV13CX0IKQVxThLAx3kdL2ldcNltDwkUX6yaJ8pJFeCvoBcrr+CoYhMPMoDas4VZNQOV1fFeNgDesYdctoDetYAMGAgEGr53xP1ynF0DVV3xm09yx8MgEHfwRiU890E/dU++ACc89MBm4U7lV2XGwqzAu3zp/GPX0C4+RxdChH4vhPKHzs5JGjx48cOXb0x+PHfjx+4siJU8dOnD5+cg+LT50/dvzC0WPoR4+dO3b8DOicPIl3/vy542jnTqBhnjqNdfLUNcyLzpG6RSPR+X3hRf2xoBP60IGajvEqFjHBVcpb16hu36C5g09/5yYjwOKreORYl25hnr+swMvZlRc73VDw4mnDi+nel6/n1t+tjL7sLuyLAShc9CS8vzihJyvqSVZkZ07M09y4p7nxTzKiamI8s71M42zUgw0VvJGSDqr8pjJsuqL0OsL0hlJslkoC7tpyXkgZV4SItSyPgRC9vgCFo4JInGuguLAsOTU9PQMrAwMrExMHEzM3EwsvEysfC7sAK7sAG4TIIuzcohy8Ehy8khy8Uuw8kmzcoC8N+hx80lyCcqzckjqG1n/CBInkkrF/mbG6OPGEj5iQlolXWUmBn5tDw9hh/PnnVnSaHelUlpdOLoQWwJzsa898WP56Lyo6P9SpLS+TXPBz1ff/WFtvltXkOHRdQw+2f18ppeNfFYXfv98dnVnPr5uOzoVyIVLLJjoGll9v/DlJcaC/PzU5ubSw8PuZeHSoQ32mPnMG9B+r97s7O+/ffbdlQw51qH2VVDYLKZhySxnzK1hzypjzq9oJwh0E1BxEEM5CcEdBNQc+ZTsBVXtRhJu4hqeklpc00lsa5SWF9JTU9pTQ9JTS9pbX85fQ8JDW9lHWD5HTCVQ0CAYjSkZBamZhambhmtax6pZRMPMIbbu4gzlztgkIy2gwomkTp2kTC8Z1HRON3O6be2cA/DVwgbAYGNCwmVeGdUCeQ1ixskHgLVLRS5gUly7cPn/m8umTmCePXzh+bD9HAu3oUbR9FP7xyDFAw8A//njsyI9Hjh45evToydPHTlw+jXb11AmsY3/HPnnk6tmTV8+funz25LljR08fO3bm2LHTR46f+PGoiCJnWptf4XBU/mB4dk8Qyl4F8+JVwLt4GPiXMW9dxSG8gUdy8woFwXXqyzh3sS7gX8W8ZqIk0Z0T2ZUd3psd8bg8pnYwo2ehqWeu8WFvdGZPUH5bcOM9n8ZYz/oEn9p4n4pIt3x/m0wv0xQHZKiRgpe2uJ0Kn4k0K1KYRo2bVI6FUIH1rioXha44qz1M1E5ZwEScWYubEsZKpM5O7KCm5Gpkz0jLRkxCSkFBTkZKRkFOQ05GS05OS0XFSEXNTEPLSkvHRkPPTsfIxcTGz8wuyMwuxMQmyMIuzMopysEtzs4jwcknycguhNK3+BNGhT8HhacHazluU7iH5M5OjaWEudwhvBuYWnmw719p4/XqkyftU4tQ9kJejCurpPrQChTb2Hy92tX+eHr+96rS/htZBACF4fJc+m5hB9u/r6+Hwm82d7qGltPKJwABR+UO59ZMDU+/2t75M6esLczPtzY1Pevr29j4swW8D/Xn1vudnaGBgf7e3i9e/+RQh/oLqqy+UxLpyaNsLwR34lOxFVCzF0G4iCLcxdTdRODOgmqOInAXYZiTsJqTuLqbhKYHsAzKB1hC010GBcjYU1zTRUzDWULTTRrpK4Pyk9P1l9PzUzIOlDcIUDAMVDONQFhEQ/nB5uFwi3BAw4CD4RZRqiZhCMsoDetobdtYPackQ7f7wACIQR9pF2fskQo42NIv1yqwwCas0DIgW1DeAu8G8/lzN8+dxjt18tLJY+gnjp45fhTt+FGoqhpA3h/3UPgnH/3xxyN/P/Lj3374+6XTx7mIbgiQXGO4eo7xOjrPHRx2gktUuGcIME7inTuGifYj+vEfT/zwN2zM86qGogFZliH5tvouKreJCMD1z55EP4+GceE0JsZZsP8yLgY+3qVbWBeuo6PhEl25GWKh3fUguDbevSLCsTTWKbPOO6c/7GF/dE5vWHZfaHaZZ4GfVYG3Zb63ZZa7SaKNpr+OtLe2mDdS1F6Fx1CCCc5LKctyR4z2Bh8pLjcRFi8JrhDlVQXWOyghWmNxZn1henUuMh1BBgNxHpiwsIyIJDkxGREhAQnxnbtEd+7eJb57h5ToLhkZKRU5OTUFBS0VNSMVDSMdAxsDMxcDMw8TKx8zmwAbpwgwO5coMBuXCC0jD0rP7DNDCX9CFBYgZ71f2Av626/mpFjo4KaBUP/N8sOMFDcn55DIhL6xA8TcWFvMTo13cnRJzCxa29h+t/6ysLjo2cTCdFetvCDjucu3tU3tCypaN9ZePCoo6h45WPprpLc9LNDX1d0rr6zuzTuIIJcWJkpLCjs6u9Piw7z8wzqfHVQAWJwZzU1PdHd1ColJGZiCyl4CFNZQ4jX0iNg/4J/oa6Dw/kJxCYWjEdlDsfkjj9rm51/+GyU//3f17t27N1+u9O+hDvXN9Hp9vayoKCM1dXR4+GDoUIc61H+qsrouBYMAQbiLiLqriDrEvkKAejU8RBCuwnBnIZizuAbAYldRhIuYOkBkF0FVBxGYMwBlQVV7cQ1XKW13UQ1nQZiDqLqLhJaHtI6PNMpbCuUlb+AvZ+AvbwiAOEhePwi2h8Ia1pFqpqFKhsEqxmFw8yiYWYSOfaKxe5qeU7KuU5K2XRyA4L284ftm3plW/rnmPtnmfjmmAXmm/tkou2gGblX0S3fRTmKhHb+Aduz8ySOnT+yVjzh6FO3I0RNHIfw99vcjx478+APw33889v+OHPl/f/s/4ssYRhLsdnJcZhKMhmK0hkKUCJZbEqRY/HcvMd84R4N3mhTz5C30o7gnf7yBi87ORczJT4l/Hef4D8ePHz1x8hg0Gw/t2Bm042dPnzh/Fg3j/ClM4DNHzrIQEyU7G1VHOBT6W2R5GKU76iXFm6a3++T0hwIgzmzxT400v2+nm2yNjDNTjzBU8tQQsZJjM5Vh0ZdgUuYi46e8znAbk/zqeSKc04SYJ0lwTlNfv8hIgMlNiitIcVWC7rYME5E4/W1lHia4iDD5bUJCAnyiO7dJ7hISE90lJiYmISYhISInIaakpKQBpqKipaFlpAEoTM9GT89BC4WH2emYOBiYuemYuAEB0zPx0DHykJAzwzX0/6IJEgCFBSnZUougfN/dN4tynAxa1lAINiHQmZ+PX15WXoifSxVp3jO+BPanR7jLKSrrIHUNHbynlt4sT3YLiAnH5deNNj7kpiM6femygKRibGbVy+FOJS7hiKxa6PoDLQglSRExKXlpCR5ewcg0KOt3vLdegodCQgGhpa7CxcasqGU5tbTx4f3b1BBPeQUluJoSPy+Xro3XyvaH3c3Vb4/Cu7sfphbfFDfNxuSPRGQPJ5eMt/S+XP1Xj+SfUu/fvz+sXnSo/yEtvXxZWlSU/eDB5Pew5MqhDvU/rsKKVlGEM7+yvYCKvRAUA3YGHVGEK+iAVhgG8NdNQtNTRsdXQhNiYmBAw8IwJ3CwuIabNFQ+wlsE4mB3QMCS2h7imu6S2p6KhoFqZmGKRkFKRsHAqqbhcItIhGWEmmmYgn6QnG6Amkm4umUMyj5J3/m+jkOSpk0MQGGUQ4Kxe7qlb46ZdxawsccDQ490Q88sPfd0I89UJV3Pu9QCZ89fPXEM/fiR08d/RDt25MSRI8d/PHriRyhL+AhA4b/9ePTIj//vyA9/+/uPJ/7vhyM//N//8VHcDdFVSjaHFXjoZ3nohBpI2EgzaHISqDDdkKHCkyTDFiXG5CG4SId3+s6Fo9fP/nj57AkMtOOnjx07fvToUXDZIyeOHT15fK9q2/Gjp04cO3Pi6OmLJ9FEGSm8deSjzZRizJVjLVQTzFQiLOUjIlAJ+bZJ+XbJMaaJdloxJmohuvJe6qKOyrym0iwIAUo5DjIucnwivAu4505eQDty7sQP50/8eBHtKPbZk5fRT129cPom5plbmOfu4GKQ3cCjvnODiYKCk4Gd9C4JMRExBQUlFRUNOQU1MCUFNRkpFRQMpqEnp6KloGKgpGaiomKkoGCgoGCipGIip6Ino6Inp2akoGYhp2KhpmWjZ+Aio2CGIXQ/c4LQnzIqzByX1Q6wpzg1mIyMJDq/Zb63ho2O2so3ZnB4tDgzhoaM1DowY3d7UZKXwcArcfXVev/A4JutD3NDj2lZGYMeVHz4sBHurEPMLlbc0v/q7faL/hZBUqbg9Bpw/SBbJCOPWGn946G+DlO4JB2P7OTq7txgMzX+CT45ZMOTnpx4PxJimvuPOj582K7Mz8otqZ6cmrgX6kBHxVLd/fLDzvq3ROHNLWihuKyqycicYQDBGXvF0T4uFPeX0ps3b0aGhgb6+w8Llh3qf0ibGxtjIyO9T5+urf7rsjOHOtSh/rkKK1t5Fa25FawBAUMRX7izMMwR8C7AXDF1NzEEZEktLxmUj6SWBxgH7T40i0J5FC6AemX1fKSQHrI63lLanuAsWR1fYAX9ALh55E9ZELGAegEKwy0iVIxDVIxDwbiiQZCWbTwKgmBovWW4ZQTSPl7fJUXf5Z6JRwbgYBPPDFPvTH3X+4YeGSa+OUbeGSiHGFEV87uUnBcwrx1Hu3Dk2ClAwD8cOfL3H3/8+49Hfvjh6N8hFD4GtgAU/+2HY3/7v//DRDuqJ8ad6WT4yMeyPzWkJyMo1VHTBc5tKk2rJ0yuxXUHznxThf6aJBkOF/45xssnyTGPE1w6iXX2ONoxQNbgmseO/HjsyBHAxMeP7GH30R9PAOOgn+eguC3PRqLFS2YoSmsly+oO4/VR53dT53XSFfAyEo80VYq1gPnpyNgq8moL0SqwkwhR32AgxCa5egnvwll0NLSzp06dO3vmAjr6JQwMbEwsPBzcKzi4l7Gxr+Li4V+9fucWIRHhXTISMmoqeno6dkZ6diYGNmYmThYWbgZGTjp6NmpaJnJKOioaRkpaJgpaRko6Nmo6NipaFkpqBkoaBmo6Zmo6Fhp6Nlp6dgYGTkYmblY2fhY2flpaNm1t/b8oCs+PNrPduMXGLamL0uDmYNEwcX7xZjcj1JaBV2pydf/cTStVUX4Z/c13KzAxTnENi8GZl3vjH6aftTFysgZnVIF+XowTrSh8eA2KIL581ipKyRaV2/x2fUaGl9YpImPv8A99tTnkhFQlTyYWxtqZqa/H5UKs/HqqV5SexS06f/+Yleezj9takiM9aIkoMx71fdh987koXAJQ+D8v/rX86l1d1/P4h6NhWUOgLW2Zm/kfXCjuC2phfv5hbm5ORsbczO8vrHqoQ32X2tnZef/+sCbDoQ7136qk+rEwzAGgsLgGlAWxHxgGvAvFgDXchGHOAsqOQqpOfIq2e9FiF8C7AIjBYZBhTsIIF3EtN0ltNwlNV0DDcrp+sjp+AHMV9AJBq2wUAjODpsfpOqUA/FUwCAAoDEYQltFgF2j3UVjLNg7lmIByiNe2i0U5JOo4Jhu6pZn7ZBt6PNBxSjb0SDPwSNd1T9V1SdG2iRCHmVOziV4lpLmAc/M0OtbxU2d/PH7ihyPH//7jib8dPf4DMJQrfPzvf//7VfRTqrz0sdZaFcH2LZGek7nJAzmxMRYwZzi3E5zDRoHJSIRCm5MQoLAYORY3ITrT9XOk2GhX0Y+ePgbFlX8E+gHKOYY6R0B79McfjvwAMPuHo6dPnMQ+d+oG+glS7NOM+Bd5SS7LMBBo8VFq8pLB2Ah1BajtlPns4aIawoxCtAS0hJeJrmLexLlwDeviZSysq3hXbly/dYvgDsEdkrsk5MSkVGQUdDT0rAxMHMDMrLysHAKc3CLcvKJcvMJ7FdMk+AUk+PnF+fglePkluHnF2blE2DiFmFn5mFh4mFi5mdh4mTkEmNkFGdh4GVi5mNm42Tj4WdkFODgE2dn42Fm52dk42Vg5WBiZaMhIUOpqn1l450+Iwuz4BDxCilbW1v7hCUPTL8Cgty2CT01n66cvxuPtkFx8yqsfPtQVpIjyc4tIKcZnlu6Ac4cfM3GxhWRCKJwdaU8jovbsJZRlAlBYjIo9pqD1xUIfPxtJUuFBKYkX/S1cRExp1X2zY228HOR5tV1g8M3MM0UmLpeIPNDvqC4wRGnpGZqoq8rcJSTPeNT7tVH4/e7u/kJxUbnDETlD4BFrfPpi5dW7g91/Ya2urBTm5QEU/q7WKTjUof6J3r59u7y0dJjSc6hDfSmV1j6R0vbgUbQVhjmJa7jtJz8AFAZADGgY9AVVHASU7fmUbPmV7UQQzlJIT0DD0LiqAz9UZM1FCO4kCHcQ03SR0/VVNgqS1fFXMgwGpCun6y+D8lUxDlUwCISbRyoaBcvo+mlYx2hYx4LNvcU1QD8GAdWXCNd1TkLax6mZh8EtIsEBYFDP5b6Oy30Nu3h91xQdl2Rd9zRd93RjrwxtuyhpTQdWUQ0SRuHrRHTY1+5cwLx65jzWydMXjp86cxztzLETZ44cPYmFfkZDhC3N1bAizLEmwqU50nMkM/FpZmyIqZIzjNNBmcVckk6Hl0SNCV+CEpfr7kVa/PMEl05jnT566tjfjh75GxQT3uPfvf9AC/3vhx9+/AHKQz7y978f/eFvPxz7+/87c/TvGKeOXkZHu4t7no4Ah54Qh/kOLg85vggDiSADOSMJAcnNq4T4+IQ3CUnuklNR0NPRszGycLFy8rJxAt4V5eGX5BeSFRRREBFXlpCGQ5bZsyxCQgYmJqUCLC6jJiatIiyhKCyhJCimKCCiICAsxy8kwycozScoxc0rxs0jxssnycMnxc0vycUvxs0nyssrws3Ow8PGxstCy89CKcxBIcRCLMtNIcN6ywEl/RdF4f0EiXgoQeJn+doiOGQ1Pi68G2IC5xSE71fV6misMNKQp2Ngr+iceD7Rw/ITCmeG2dKIwodWD6LC+yj8crGfh5koai/6CzTXXctEwpDdMDgHUJidLKvyMRh8Ndkrz8jlFVey835FS4pHRE7zUU3Dg8RAJgqqB2U9Xw+F3+0tFJdZORmWNRSZc5ALsfnusxLG/wra3t4e6O8fGhw8nD93qP8VTU5MVFdU9Pf0bG39i/e9Qx3qUJ8jgMLSSE8+JTteRRsAuIB3QSum7roPxHsxYJe9vAgo+UECjCC9PmZKCMIcJVFeolquQhpOEjqeEnsV1pSMghUMgxQNgxQMAqRQ3koAjnV9ZfX8VMzDVcwjNGxiNW3jVEzD1K1jUI6JCCuozhrMIgJhE6VhE61qFqpqFoawila3ikY6Juq43te0j9dzSTL2SjfyzdbzzDT2zTPxyUI5xsvpefPKGzHwKZLS898iZrxyixznym0M7CsXMHDPo+OePoMuxEz7wNOiOdGnKdG/IsSp0se6KdijLi7I10BeT4RSg4dYkYlAnOoaHzEuPf6F21gnsc8dPXP8yHHAuVD89+iRo8eg/44cOQbp+PHjJ06cQDt5Eg0N7fTp02fPnrtw/sKlixcxLl3CwMbCvIyLffMa7t1bV4luX6e4e5OW+DYjGTE9OQU9FR0DHQsbCxcXhyAfr4SwiIKYpKq4LExCAS6toC6viFRU1lWGGSqpGcir6CuqGsor60oraEnLa0rJa4lJq0tIq0MrLcsgxGURwpKqwlJqwhKqgmJKwuJKQqLyQmKKIhIqAIsFhGSEhKQF+cSF+IT4ODkE2ZkkuJmUBBm0JeiMZGlslOlc1ZkdVWk8Neg9VEgSHOR3tj/rzfPPhsJTAzW8pEwpBT0H23vKiXAko+NsG4EixO9WZ5QFORRQru+2N19vQuHSlZFGdvLr7skV85PPmDiYgx5AxdeyIxzIueR7FqAf96K/RZicJSKnaWtzUZGPWtv+oBRaaWoACRVb28jy3GATBwtRRjnE368memTo2L0Ty14+72Mkue2X9AgMPmvOZSQlTyt5ClAYrsCl7x6+d4F/ps9H4dXXW239L5OKxwAEx+4tFDc2u/7+MJD0S+3u7m4f8sSh/ne0s7PzpK0tJSHhUXHxYRHAQx3qi6iosk0E7sSjaMMtb8UlZ8klb8GvYsunbCMKlYxwE9dwFVS1AzQspe0loekuoe0hresto+MDgFjewF8K5SWB9BBDeUjoeoEWsra7vGEAoGE5fT95Q3/Qyuj6SOv5KBgHqliGq1lFKZuFyxkGyhoEqFhEaNjFAcOsotRtY+A2Ueo20dr2cdr28QCFNW1jUU7JAIV1Xe/puSTruibreqTpe2cZeGWZBeSZ+mVpO8YpGHiJqFmwiSJo2CVI6fjvkLHgE5JfuXEX5/LtK3jXrOEKbSnBnekRVVHe1aHOxa6G94zUI8z0tERYuUnwGG5jkV1Fv4t99ibGaewzx04f//HEkR9PHv0R7fixEydPnkA7hXbq9JkzZ8+dPY9+/iLGRUxAvDg4Vy5fuX7tGv4NfILbt+8Sk5CSkpKTkJJTUlHT0NDS0dEyMNAxMTGwMDOxMTHxsHHycfHz84kLC8uIictJSCpKy8AVlFCAfQH1yqvqK8L0lREGyggjJYSRAtxQVk1fTlVfVllXRhEpA2hYQUtSVktKBikhpS4mqSYuDROVVBWVVAEkLSquDDhYUERWQFhGQEiSn19IkJdHlJddkodBnpcKIURmLEHqqEIToMMSZcIaZ8Z6z5orw1EgzY73gT1vqgXzo3Dt9zt/yagwQGFuIvq4rCcH23ua728UYaaR1TDNzi/wsDWgZ+F8UNm19fKZt7dbVHJGUpgrPTX1/fLuxYleelbGgLRycEp9XsxdQlJb7/C27uHFvhZhMuag9GownuxtRUrBFBCdkpWWIMHPombovPnhw0R3DRsj4YNHbeAAgMJSNKwesSUbb2clWMnltSwqKx+ZaEjfJCB7sJcg8blLbHwGCi8uva1on4/OGwEQnFA4Wv1k4c+3UNwX1+ry8ovnzw+/dD7Udy6AwoP9/Q9zcx+3tv5PrLJxqEN9/wIoLKTmwKNgI6Bszylrzq1oKahmx6NoKYJwltByF4Y7CMHsRRBOQjBHYYQzVHBN000C6SWq4SauBZWVENV0F0V6iOt47rcAeWX1fWX1fCW0PMQ0XGV0vKRQnuJId3kjfxk9H3njQDmjIFFtT2l9f2XzcAWTEADEqlZRMOsoCIXtYvRck1GOiWrmEUrGIQibWKTzPR3nZG2HeIRdrLp9gq5HuoHnA2OfLGPfTD2Pexp2UUpG3uLqNrwyuswCatQsEqTUXIRkTPh36Ehuk/roqHfeD2+J889zM22O9cpxMTIX4xWjp7qDdwnn3PFLZ46dP3X0HNrR82jHzp48fhrtzLmz5zDOn7l08Twm5iVsLJzLl6/fwL99i+DunTukRETkxMQUZOTUFJR0wFTUDHR0TIyMLEyMrEyMbOysnFwc3FycPFxcPNzcvLw8/PzcgsL84qLCMhISilLSqpIyqtLycFklLXlllIKSjoKynrKaoTIcImAF0CJM5BHGcnBjWTVDeTVDGSVdaUWUjLKOjJKetIK+uIwm4GApWZiYlLKgiIyQsKSYsAQfFw8XG6sAB7MED70iPyVSgtJKkcYVQeuDpAszYEgwZUyzYcu058ywY0+1Zk2z5c5w5M90Esh3F8lz4mlIMNp9/5dE4edTXaZwneLaoYPtA70vS49RkpVRhSMUFJTDEjPf7H54vz7j42yuqKyqrKRk5Rz0Yn1ndW5Yx1AvqwoK7r6cGTRHKotLyyfm1i5PDthqGWZVQPkPK7OjdoZIBSUlGExNS8/8yQCUeDoz3GGkD6t5PAD6bxbGHFGGyQXQknU5CQEqSgo6BkYGevq6hqa1TyY+7G54ORsF34Myif+5/gkKb+/sDk+/yqudDs8eisgeSns08fjZ0vrbz3q+/+JamJ+vqaxsrK3dePuXKKh8qP9pbW5uTk1OLr08mNf7XenV+vr4xOTY+MT4P3hsbHxhcfEvO8/v8Db7e1Zx9WNRhBOPoq24pruIupMwwgG0gjAHIbiThLa7oJq9oKodaHmUrMGIsIYbrxpgYlcRdTchuIuEtpeMnr8Y0lNEy01U211Yw0Vax0tcyw2cLgx3FIY5isAdxTScRDWdRTUgsJbW9ZHR95c3DlYyCwMQLGMQAGgY9BVNQ1Qtw4EBEKtZhAMOVjIJAYisbhsPt4yCW0Ui7OIACms5Juo4J+pCQeL7eu6pOm4pWvbRMIsQOZS7qKoVpziKlVeJjl2CiJqbk54jycGy50FMdaj7Q3ezDHcTbTFuimu4mKePnzz6w9kTxy+eOXURgC/GeVysi1dwMK9evnztGmDf6wQEt+5CdXvJoYXcaBhp6FnoGdjo6dkYGNjpGdihlpGdgZmDhZWLjY2LlZWLnY2Hl5NXAOAvLz+AYAE+IX5eYUEBCVERWRFROQlJZXEpFQlpNWk5DWkFTRlFLVklpJwSShlmpAw3kVc1UoCZyMNNFeAmCnBTWRVDBZixjLKejJKOlIK2hJy2hIyWhDRcTFJRTFRKREhYgIdDmJNBmptKiY9cQ4TcVI7WSY3eR5Mm2ogxzZoj24Enz5k/x4kvw5En05Enw54nw5b7gQ1Pui1PpoNguh1fmh3vfUu28kj9v2hU+N3btd7Op4sv/zEfdGfgaXtJSUnLk57tn96plhamaqvKK2qaXqxC1RXevX3V3dsz9+JgVaeZ8YHyR4+GJhe3N14/e9oz9/xg/NXSXF11eVlFzeT88v7IxuvV3p7OFyvQD93ZfDPQ3TM1B3167Wy+7mqrL6+qHZ+eHx8bfbHy5sPuzvBgz9j0/N55/0y/icJvNnfA43CvdDwsawhaKK52anDy1c5hMsRna3R4OC0lJT87+8VzaMWTQx3q+xSUz/N5sz3+EAHY6+npbm6o6Giv7miv+ZUft1Y1N9Wurq0dHP296/D98y+kwqpWATV7DgVLcS13KR0vwMECAGQRThD4IpzBLn5lW14lG0GYo6iGq4iGq7C6K0BhMU0PcS0PSZS3pK6vsKa7IMJZCOEEEbOCBZecGbe8uYCqnZiG815Q2UEcoLC6kxTSXVbXR07fX8kkVMU8XN4oSM4oCHCwnEGgAoBjkxAFI9AJUjQOhllFadjHI2zj1CyiABarmkfAbeMQdvFq1lFaDrFIpwQ9t1Q9tzRdt/sG7vd1nRJgZsFyKA8JuJ2QvBG3CIKaWVRBRK46LmIgJ6UtNeG+q7U8J+3li2fOnzl58RwaNgb6VVycW9evEdy8QUgIwJeAmOgOKQkxOQUFBTUtNS0DLT0zPQMrRMCMbPRM7IzMnAyMHIxMnIyMnMwsXCxsPMxsPOwcfJycfBwcvBzsPDycfPw8fDzcPNxcPHy8Anw8QoKC4qJisiLi8mIQCquJScIkpTWk5QEHowDmSivpyKsZKMKMFdSMFQEKAyBWNVRQ0ZdT0ZVVRkopaErJIYRF5IQEJMUExcT4uSX4mBX46TXEGA1kGS2V6J3UaPyR9LEmrCmWnGk2XGm2nBl23Jn2fBm2vFm2/Fl2/Bn2fAB/71ty3jPnTDFjTzZjf2DLm2rDk2jOnmjKUhqh+1dE4S91O/671/mt8U8P/oLxAIDCZa0/1xV+ubpZ07EY/3A0NHMormC0om1+7sVhfdx/W8tLS5VlZS2Nja9e7U+bPNShvkeBW7Xurq7ni4sH29+Z1tbW62sfTQ1Wrcw2LU03Ls00Qu2el6F+Q1tj0cjIyMHR/6a2Nl51tDZP/FTj8h/1dmU2OyOj49l/WBVx9cVMRWlhdk5ee9ezjY9xkf9a2xvrGWkJ2RXNoN/dXJ2TX/b637mX+WK/x6H+qQoqm3mUrTiVrPYI2AXgL7+avRDMUQRwsIqdCBiBOwuoOohquEFkDHOURHqKaboBMpbQ9hDVchfRchdFeopAe52E1OzZpIyYxfUBDQuq2Uoi3aQAoWq7SqLcJbXd5fR85PV9VU1CFQ2heXWAg2UMAuSNguUNg1TNwpWMQpSMghWNglXMwrUcErSdkxF2cTDLKIR1jJplFMwqRt0uHm4bq+2UqOOaAiAY6ZyCckmGUNglAWEZqmDgK4N0EVWx4JdGsfPKK0mpFkdGPLkfX33/PlJOBh/7Ih72BXz8q4QEN4nuEJKSkFBSUFJTUVNR0VBTA9NS09DR0TMB9mUE7MvEzsTM8bNZOFnYuFlYuFhZuNjZeTk4+dg5eDm5+Lm5BCAaZufj4hQEfS5OXg4OLi4uPj5eUSEhaRExOSFROVEJgMIIcSm4hDRCSl5rn4NllXTllHUB+0JJw8p60nJaUnIakjJqMrKqoqLSwgJCYgJ84jws0tzUWuL0BrI01kq0ngiGMF3WOBOOFEuudDveBxD7coM21Yb7nhXnPUuO+1Zc9yy5Uiw47llw3odajmQz1kQT5kQTlmRz9nRAyU6CqbY8SebMpZF/SRT+M+kjCo/PvS5qnInMGQ7LGkoqHmvuefHXeRC+uLa3txfm51/9z8SrDvVX1Pu9CXMP7t9vbWra/C4nzA0ODnW0lm2vtu++6Xy/3vErg8G50dq21obNd//JO9Xq8xETddX7BU0H2/+gpYl2BSmp+NyWg+1/0MbqwuPWx2sbv5FgPTP42FhLRUBQWFZOWlhMOjQx+0vF3rfWXyjLCag7QVOi7wXYKSIMF9/+W3y786yno3/iX39heKj/RgUVTQCFuZSsuJWseBRtxLXcgQXVHEQQjkJqDgKq9gJqjvyqDnwqdvyq9sBiWm6SSA9eFVsodVjTTVjTVVQbCg9LaLkJqNjyyFvwKFjwK9tIaLkCFAYQLKvvI4l0l0J5qpiGKBj4w8wjVIxD5fQCZA0CZA2h1GFgRZNQAMdKxiHKpqEIG0C9cZqOiQCFkY6Jeq73wQjMOhoaBIjslKTlmLhvgMJ6rslIx1hN20iYebAcylVczUpY0VhQCiUtpe5t6Zjm4+tlaUdLQn77xjUyEkJKSnJqQL40NPQMTIwMLMB09Cz09KCFYsAMewTMCOHvfvSXm5WNh42DF4AvF7fAXgCYl4sTImCwycktyM0rxMMjADa5OQUBDXMBMubi4+Li5+MTFxKWFxZVEBSRFRZTEJVUlZCGi0vDJeXUZZS0ZZWR8so68oooGTktCSk1cQlFUREJEQF+cT4OGX42RQFmdWF6IxlGWxUGNwRtEIouyoAxwYglxYw9zYY7zZo71YozHXRsuO9bcadYcidb8oA2yYIjxYrzPhi0AS0n2JthD+FyqjVHmi1XpgN/lqNgvptYtrNgijlTaTjyEIX/t5X6aCIsc6iwYSY0E0oIzqyCiqNtHBZH+0La2tpaWV4+nI10qO9Q21tbbc3N6ffutTY3f4dpEu+2tpubGmaGa36TgyG/7txcbmttLJuf/0+i2i9nngrTU/rGlhxs/4PerszlZmV2DkwfbP+DxtuKtNQ0H49BJYM+1e7GS2stWQpqVu/Q2JzsB9YmetaOnqu/+R7wO9/v/RO23d1YgSmJqDtC9YW6W6tzH5a9/p0Pq9++yM6Kh62Bc1zBYXj4qwqgML+qNZusKY+yNbeC1X48mE/Zmk/JWlDNnl/Fdo+A7bgVrUErjHAUgDmIaDgLwh1E1J3Ftd2FNFwEEE6AicW13PhUbIRhDmIazuKarhLa7uAwsb2QMOBgWT1fRcNAeX0/NdMwRYNgWV1/GT0/eeNgQMPSUMZwsLxRoJJpCMwqEmYdCbOJ0nJK0HZMQDkm6Tglq9vEIGxj4TYxwBr28WpWEBZDKOwM9iainOJ0nOI0bSJUDX1ltZwk4TaSMEtFhKUe0spSz1JGVI6MiJyKgpKOjpaenoGBgZmZhY2dnYeDgx+qusvKw8bGx8rGx8zKy8zGw8rBy8rBB5mdj42Tn51LEFrkgleYl0eIh1uImwuK/oKWh0eYi0eEm0+Uh1eQm5ufk5OPk1OQg0OAkxPQsCA3n5iAsKywqLyQqJywqJyouJKUrIaMvJaMPGgRUjJqUlLKkuLy4sJi4gK8MoLsMHEWXRlGKyVGZzhTAIot2og92QLALl+6Nc99c/YkI6ZEA4Z4Y+b7Njzpdvx74MsFCDjRnCfBjCvBjPOeNW+KFehzxJmyxpqyxpmzg0EIka144k3Z4kzYwGaiGcBivgd2fAlG9EXBWoco/L+ttEcTJkFPkorHytvmpxcPcyG+pDY2Nrq7uuqqq198r19AH+ovrpcvXoCX6ML89xgjnJmZa24oe/uiFSDv+1cdv+3XHSN9VV1dHQfn/DtamukWY6b1jy/d31xfXqgozk9Pf9D8pG9/ZGdzvevxk+nFNXDX0N/xeHRyrv9pW8aDjI6+MbD31cpCpDPqxmV8a8+Qurau9Xc/s2V7SRLJbcLwTKgWENDWm7WxkaHNvdl98xMDeVkZDzJz+8c/We5+e6OtvjwtLa2ksnF9Y/8DdWug68nAyGR7U3VJRcPrTXDxrceNldl5hV1PuxBqUtouUM34hfHhjs4BcMKb5bn29u7Fxdm68qKc/JLFlYN38ndvVlvqqx6kpxWV10HR6/fbPdXZLNR3GMURmQ/Lp+agWSjv3ixXlT1MS8/sGpjYP+tQ/70KK1v4lK2YJAx4la055cw5ZEwBBHPJm3PKmQmo2kGxYbiDEGgP7MCvaguQVxBmD7AYADGfmh0/zB60oC+s7iim6QIQWUILcLAroGRxlAewlI6XnL4foGHQyun5y6D8ZFC+snr+8kZB0vr+soZBMOsohG20mlUENG3OKgLQsDZAYYc4hFWkhk2MujWEwpoOCVqOCXCbaCWzUIQtFDnWsI/Tc0vRdU4EKIxyiNWyjoSbBKgZ+qjoe2qY+qFMvJC6DgBDefgkuXlFuHmEODj4uLiFePlFBIQkhURkBUVk+IWkBISk+AWluHjFOLhFuPhEeQXEAcvy8EMtMK+ABB+/GC+vCD/U7i1dwQ3gWIyHR4wHHM8jyMnNzw5QmEuQg12Am1MI7OIXkhYWU5SUUZOUUZWSUhUXU5SWUlGQg8nJKMlKSMiJ8iuKsKlLsBrIM9mqMXoh2SJNue/Z8ma5COa7Cz70BObLd+PNdeLPsudLs+JKMWdPNmNLtuTMdBLMcRVPtxdOtRVMsxNMseZLsuRMsmRPAcdY8UTqs/ip03qoUrmqUDoqkrmpUXshaJ0VyRzkiB3lSNxVKENQ9AmmbJG61Hl+iEMU/t/WvdLxyNzhV4d1Ib6CVldWivLzU5OT+7q7v2B696EO9d9re2tr/zW5tbX1fX5r0dHZMfD0EcTB6/9AwB/9unN1rqmlsXLt1frBaZ+tpdkeCVb6wIQy0F9/OeFsqScpLasOVxYRk0svgjJx3zwf1ENoppX3grtaf3N9hJaBjo6unJSotLz6k5GX8xN9BorcGOiYApLy/jH3F1//XMgi0BJGyyE69+bXf/Jj3Q06cEU5JVVFWSk5Fe3G7j303N1IifCVkpREaMBFREQcvaP3Fu3cCLExhqmj5OVkUOau47MvGwoTJESFVeAaGghNchoqXa84cFBRVBBKxxX8yxf6qtQUYGYWJkgtdSF+Pku3kDXoE+x9eqSvgoyclpa6mKioZ0z269WVvEg3/KtY+BRMBpbO7X0zu5svI3zsVNQQOlrqKgjdRy1QeaJD/fd6WNHMr2zFKm0M8Hd/0puAii0wt7w5n7K1EMwe4O8+5oKWfw95wTiPoiVgYkDPAItFNZyE1B0FYPZiWi6SKA8ocULDVVQLChiLartL6/uKozyldLxl9H2l9XzFoaU6PKSRPjK6fvsz5+SMgtWso5AuiZoOgG5jARPDrSLUbaLUrSFrWEdp2oHBGLBLyzEeYRMFt44EHcDBmg7xBp6pBm73DN1TjD3vG7nfM/G4Z+SSoO8Qo2sfqWMTqmMZoG3kCkNaqcAN5ZW0pWThEtJqUrIwaXm4jDwCtIBWxaWUgYVE5QRFZPlFZEAL+iLiCgLCB8u58fKLA+8TM2ih1d32UJiXR1RAQIyfX4SXT1hAUEJIUFJcTE5KWllOASErr66koqWoAFeUUZQSEVEU5dOQFTBQEbBE8Lggufz0OSLNuO/b8+W4CRX5iZf4i5UFij3yFy3zFS32Fi7wEnzoJVToLfLQSzTfQzjPQyjPU7TAW6LIV6bIV+6ht2yBl0yBt3Sup0S2u0iOu1C2G7BImqNAohVPjDlXpAl7mAFLpDFbuBFriC5j6J4jjVgTLTkynPnS7TnKonQOUfh/Wykl449aD/PGvoo2NzcbamsL8/MnxscPhg51qO9Amxsbvd3dI0ND3235iNXVtcb6iqXpht03Xb/G30+93rmz9qT7cfnQ0PDBmZ+tfRQOSqoA/ewoFxoGpqC49NqqEj1lEXZhxMDiu435Lg4aBr9UgMUbOgKs126S+0TeK8xK4aAhM/NJ3tx8kxliQ0pI6hOT1j04+vbnT4y3BnL8YqqWv35k379yNFBm5pPJKiwvyUsTZKJWM3QHtyDDrUXsTLTGzgF19bWBzoYU5IwlLaPgDsVYnAsT+5a5e1BFXetof6uKKLskwqy8ssLFRPsiFjbKMx5cMt7akJVNbeXDh5knmbevXRZQQOY8LAtx1iOlZi55MvXhw056THhUbEprW6u3hSY9u1zf5POF/johDho+NZOqps7V1+/qcqP4+QTjHxR2Pm5EKYkr6ji9Pkzm+hICKCyoasMpZwYgmE8JyovgVbQCHAzImFvBApCuENyBV8XmAIJVbUErsFd4mFfJSlDVDrCyqLqTuJaLMMJBVNNZErmHwuqu4tqeYtoe4toekjpeAIUldb1ljfyl9HwlkV4SWp5yev7KJiFKJsEq5mFqVpHKFmFw2yiAwup2MTDrSDXzMBXTEMDB2vYxSMf4vVWXU5DOiYCAUc6JBu73DAD4eqcb+zww8ko39Lhv7vfAOijHJijXIazAyu+BqUeKkXOcvkOkrm0oyjIAZeaNNHZVR9rAtSxUEaaKagYKanqyykgZJS0ZRQ1wuyglBxeXUhGTVBYSVxQSUxSVVBaXVhURVxQWU/h5MQuIjKWAefjF+QUkBUVkRMTlpGWV5WRV5WTU5OXhCoowRSU1JUVlmLIyTFFOS0nKQFXMQl3QToPHA8kZYsofZyec5iJW4CNZFiRTFSZXEyZfE65UE6lUFS5XFS5bHixVHiD1yF+6zF+6PEimMkS2IkSmPESmIlS+MlyxOlK1OgK2Z7XKcJXycKVHYQplIfLloQoVoQrlIfJlIXKlofIlofJFQTJFAdKF/lIP/SSL/KVKAmWK9zYL/SWKgyRLAkQa083e/zXrCv9plFI6XtryyVd1h/qier64ODk+fpgrfKjvSlOTk7mZmYV5efNz3+nf/vDwyOOWEoC5AHZ/jb+/9O6bztmR2va2pnf/5uS5fRQOSakC8KotzgUz8t7/Kx1qLaAkJ4svat9+0cvDyBKY3gIOQAlyisma7tfOdDBQFIAZg85obRonA3vds1+mP+0uIYS4FbU9fvU3/3KwjouWMmBvBSWgnFArCibeZ0vvkjyN2PhkJlegWPDO2pQkL52Rd8qHD++MJPjYeNTnXkOh5aeP4mio6B+2Q9Xll8a6uNjptFyjQT/RzoSTC7EKntC2NAoywtiiTjD4ev4JDw2ddwL0g95tvNvd3gJ3PnVZETQkjJW94Ol+o6MipOGeCPZ++LBtpyUjIGcwMvty4+3re37mFDQ8wy8P0+S+gB5WtvArW7FJGXHImAAa5pIzBR1mcQN2aWOAwvyqtgIwe0DAwMLqUIIE2AQoLAxz4Fe2AdDMr2IjoGIrpu4kou4kquEsruXOr2ovou4ClVpDekqhvCW0PaR0fRRNg5XNQ2UN/eWNAhWNApWMgwDpAuRVMw9VtQiTNw4EVrEIUzELhVmGI6wiIQ52iEM5xRl5pZr4ZlgE5pj5ZwH2tQjIsgrOsQTgG5pvF/4QjBv7pFn4Z9gE59qHPbQNybXyzzT3vG/oHKdnH6HvEIGyCtI290WZe2sauKjrOsG07ZQRFkoIU3mYgbyavpwKSkZRW0ZBEwCxpCxCXAa+Z5ikLExij4ZFJSADJhYUgSpCiIjJi0sqycqqKSprKCppwNRQcJgOTFkToaSioSyjriCko8QP8NdFVyjAWCDOVjjdRTjPS6TIV/xRkGxZkGx5iGJtuEp9lGpzHKwhVrUeOFq1NlIZAHFlGAS1lWGgo1gVrlgToVgbqVgbpVQTpVobDa+LRtTHqDclaDUkqFfHqNbEqtbGqlZHq9ZEw6oj1cCJAIUBHD8KVywNkS8OkgEu2RspC1MoCpQpDJB66C/x0E+swFuw6YHl7vvP+pQ/ROHvVACFS5oPUfira2V5eXFh4WDjUIf6QzU5MQFQuKig4Pt8TW5sbLa1NEwNVUMT5v6BfX/t150bL9vam8tnZv+997F9FA69B37KuBALk3sstBI+0KvFUV4Ocs+EvJ2lZ3zMbEEPAAq/1hcXMrbex8cPfs76XKqGoDNck8pBz1bT96uf+0pbjEcKbverlT+6i5MZSJjrBw8e8JGmdHIm9kf9465IOUWE08eAkilSUsHcG8C3mZwYyih8P8eiMNSRhVl8bB36rN3dXNWCS2m7RoF+kr0pN48GQOHxlvvsrLRNYxDFbr0Zk2FgdQsqAv2ZkU4vR0skSkdehPvGVeLKbvCrrqOUBRGuCXu/3ktVUU58UmaUnoGBvp4UPxsVrcDQi8NVgb6AHlY08ypaMIrpsUrqs0kZcsoac8qasogbcMia8ilbC8KgkLDgXo4E4GBRTWdo5pyqHcBfQVU7bnlzQMPQBDtVO1F1JzENVzFNCIWFEU7iWm4S2m4yut4yulCKsLJpCCBdBZMgYDXLcBXTEMDBysbBCgb+cgZ+coYBiibBMMsIAMFadrHa9nH6ril6rsn6Hilmfg9M/TJsQgvswgv38Bd08q0B8obk2kc8tA3LtwzKtgzMsgnOsQODgVnmvg/MAT27JRo6Res7RurYhGhb+KEsAQ37aBq6I3Sd1bTt1bRtVDUtVDTMVNRNleBGCqr6Ciq68iq6UN1fRW1pBQ1JWbiEjJq4tIqUrJqsAlxOAQGsoKyhqqqppqoJV9XUVFNXV1DQUpLVUZEyVBOxURdwR/H56fGEm/Im2QpkuQo99BIr85euCJEuD5GsDJGpDlWoClWsDlOui1Crj4Y1xsEBB9fFqNRGKUMoHKEM8LcyXLECAmJ50AcQXBetDO2NBsSMAG6MU29O1KhPgFVFK9XEqtTFw2rjYICGqyNVwCnlofIV4fIVEYoV4QqlQTKlQbLAJYEyJUGyBT7i+d6i+V4ieV7COW68DffN/tIovLu9OTs11v0UUldX1/D4zNbOl0wJXX8+mZOTN/Z8/cP7jbamxmdjXz6T4RCFv4FWVlYa6uqqKyqWlpYOhg51qD9O7zY3+3p6BgcGvs/vK169er2fHfHPJsx99F4m8ZOWkuGR0YPzP0+vFgck96LCu+uj/ExMXom1++PrL8b5eag8E/N/hcJGVlBOApC3gy63GhQVHqhIYadjq+3/9e2EnaYEI6/iq1+ycGd+Ah0Re9PowZo7463ZFEycFQNjTuqySggoU2JfFnryihY+AIXN5cR1jCP2ETnX346VWWZ6A/pw2X23pgGT+AcUTgUoXD8Euh/erg1JM7B6hj3a3V62N1IVllLyDQpzNEaQ3KaoeDr74cOqlqKAulvC3rUWFUXYqbjlgsMjQ4KDgoJDM3JK1t/9iuEP9Z+ooLyRT8WKSUIfoDC3vCmfkuVPa2TYCsOhchACavYiGk7C6o6gBRaCO/Cr2EIBY2UbAME8CpYAnTllzaDaEeougIZFNVwl9ioKS+t4yOn7KhoFyOr7yBr4yhr6yxr6SentRYhNg+UN/WX0fKV1vUEfZhkOt4wAEIxyTNB1StRxSgQobOCWYuKTBjDX2CfdzC/LMjDXIjDHKjgX4K9ZQIZFQJZtaD6wZSBAYcDBeVYBWRZ+WaZe6WbeqRY+9808kwwco5HWIUjrQJR1INLSX9vMF2Xmp2XormXkitBzUEPawJA2cKS1CsJEBW6kCjdUVNVVUEECyylpyitpKcNQqnBdNbgOAqGjDtNSV4NpKMury4shlcXMESIOGrxeurxBxtzR5lz3bXgyHfnzXIUK3AULPQSK3PiK3QVLIBqWLAmQeBQgVRkkB2i4MlShOkKpJhIArlpdjBoUFY4FLeir1kYrVUVAmRI1EQoAgutjoJhxTbRKVbRKfTy8OUmrKVGzPg5WE6NSHa1cDY6PgwODvTXRagCXa6JUqiKVKiMUH4XKFflJFvpIFHiJ5XmKFfpKPPQRK/AWeegrVugnluvBX3/P5C+NwvNDneYomKi4pKysrISEhLVHxNKrzYN9X0KzT8v5hUULOuZ218eRcFW/uIPJzl9Qhyj8DTQzPZ2bmZmZljY89KuVug91qG+qF8+f75e73traevdub4LW96etre3OjvbRvsrPjAqvL7a0NVcuLv66qNnvaX8K6+pctxAtdXByNegq8DBqWUPlyYBGnpTQ0dAkl3Zsv+jlZWLbS5B4rS8maGAes3+Al73OPgoPV6ew0DBW9f4ahQtinG4SkmU39O9v7m69fbW6MtxSyEROHlMIrasP9DDGiZZdeGh1I8Rag41P9fneHLud13NywmwW/qkfPmyYyooiDcP2n6HmrCAqapaqfugfuDLTz83JoOkCoXCinQk3t/p+VJiNmaZ2AFqp9O3qoBQ9i3dk1epIPTMZUVg2VDh5qimblZK+pGMaClrL88GcYsEgONZISVBS8+eY9KG+lPLKGwXgNqwyxqwShjxy5gBwueTNAeyKqjtBScCaLqKaUOk0IcTBzDnQgWpKACAGrLyXJgFQmEPGRHCv3AQ4RVrHU1rPSxLlLqPrJavnLQ8gWN9HWtdLAuUujnST0fdRNg9VtQxXMQ8FTKxoEqRuG63lEIeEHA+s55IMgBgYALGx530zn3QT73R9txR9t/tGXulGXmnmAZkmfg/M/TIs/bOsArLN/TPN/bMsQMc3w8I3C9gmKNs68IGpR7KBUxzSJhRlE6JrG6xjE4S0CtC1DkKa+WibeagbO8N07eG6Dhq6DjAtS4SWGUzdEICvCkxHFYZSUdVWU9PSQGjBlNVgSgpaKjL6quJWCBEnbQFPHe4wM95kO/5MJ4F8D+Eib+FiT4GSn1zszl/oxgdcBIDYS7jYR7TIV7TUT6I8ULoiRLYiVK4qAor+VoYrARrei/gCFIY1xgOrATclwFuTNNrvabcmazUlaTQmadYnaTSlaLel6bXc12kEg8nazSnIpmRkcwqq+R6qKQUy6DcmadXGIqpj1CqjVCrClcrDlMpDARYrgH5VpHJlhFJlpFJFpFJJkERzhvVfGoV7q3JJb92BG1iEh4f6+vqk5pa++mT2xL/QZ5QUmO8qpqRnvN84vvtuKSEmsrjm6cGOX+tfX+r3dIjC30Cv19cb6+pqKytfvvjcT+tDHeqLa3Vlpaaysq2l5e3b7/1L8Mmp6bam0nfL7Z+TKzwxUP34cdvOzueGM1/OTlaWFkUFOlKS0qSWQ8Aa7oCioOe4l1/e+bjJAiXHJ64x+nLrLTRtjn5v2txrpCA3ygSiTyB3a002RT3wnvu8t5SFhtzGL75/eOzTYmork93SXNRMAnJJGfmV5Y9CvZ3dvQIXluZ1ZHm4JbVqmh43VRVK8TJrQIkQH1qL4ilJyd1Ckp8+7Yr2s6OjZyt/Mg4g1UCcX0M3eB+FF0eaeJkoYaaejx+3BzsZYuFd1nCBuDze2oCVTRXw73hTCgMtWfUzqDjam5UBUUo6j7DyN9PtXDRktiEP5uemfM3guBj45V0z4MPCTkuKTQhW09q1/OpNbqQzDS1zZGrBs2f91ZVlpbUthwvsfxEVVDQJwmyh8hFy5lzSJtxy5tDkOWUbQMOAd4URDlAkeC9Bgl/NTgBmL6rlIoF0AxbVcBLTdBYAh6nZA0PlhLVcRNQdwS5xbVdgQMOgL63rCUWF9X3k9lpls2B1uxi4DTRJDljbKQHlkgRVCIbaRHWbaG17CIj3aVjPOdHALdnYM9XQLcXQ7Z6+SzK06ZNmFphp5vfAxCvV2POeiU+aqX+GKYBj3wfWATm2QXl7EeIME48UY7ckfacYA8cIgML69qF6dqH7qcNIM09tE1cNAyctAydtA0eYtrmauoGSqrayCkJFWU1NURGhJKulJGEEF7fUFHPREwk0E4mxFU5zEc3zkijxl6oMkqwKlqwIFC/3F6sKkqoOlgItGHwU8P/bOwvwOo6r7xfftl/xbZs2Sdu0YTYzyrLFzMzMzMxgW8xoMUtXzMzMZMskGYSWZckiy9J39u71zY0syU7exHGq83tG+8zMzpndnbsz89/R7KxA8WX+Ak9CHxd68Rf5CBb5CIMr9hUrD5Sk6lGZqgj5ijC58lC5qjDFyjCF6nDFuiiVplh1cK2J2q1JOrDtSNHvSDHoSDXsSDNqSzVoSzVsSzNuTTVuTzfpzDQD15Zq1JpsCNv2DBOI7CAcxOg3Jeo0QSYp+m0pBm3J+q1Jui2JOqSMBg1dE61cFizVlmW/q6VwT3nGkSOsNQNf+zLn4vzD6ZkHT548Gbs5cuP211Tmk6VHVwcHxu5/9anPpyuPR4b6h0Zuff2/Uk/v3Bq5Oz59qz3/2NlzCXXEP/7G79yefUR0YA9nJucWlpYfP7o2PDT5gHEJofW7t6/39PQMDAwODl59tPhSQz4ohV8NM9PTpA5eW1ubx68xIz8EN0ZGkuPjKenpr/+09cePF5saqsdv1LxgBYmFztXZ1o7m0ps3iVfKXpLpW0N2eopHDh2Q0DC/MU18Zu/OUJuWgjgrJ48gvwC/sGRWWTNEzt/rEWZnD6W0QaNuJiNs4RRHtd4IcDMW0LCEfm99acLWQP7gkeO2l8Mnvr50Wn1evBDX+VNnzrGzsfPxCbr6REH6ttIMYR5OXl4+QQE+SUWdpn7inNfmJy/bGrKycQgKCHJx8rgFxFAb7sdWCmLG1pG0fmh9OTHI5eypk0JiUrqaGiIC/BZ+SRCd4mHDJ6QLrcloeyYnO0vjNXKCxIgcG4d3VCV4A93MOHj4VDU0NZSlDx9lr+kdhQSVaeHMRw6JKWg39d1dnLphb6zKxsElIiwqLi17MSz5O53it3vJLWvklrdmlTRjk7JglTBjkzRnpbrzEqZsMpYgf3mU7flUHWkvzCnYgiYmlS6xXoS6C7+yg7CGq6iWO+FXcxJQI3aBAwUsquMB2lfSwEvK0FsKtvqXFcwCFCyCQAcrWoep2UdpOF7RcY2HLThy+QjYpWwdruVITJAgpgu7xOm5Jeq5Jui7Jhi6J+k6xRi4xRlfSjb3zTC9nKrnGqftFK3nFgs62Mw33cIvw9qfYuVDsfDJNLuUZn45zeJSqunFJFOPWEOncEPHUAOHEF1rPy1TD3UDR0V1C0UVY3klHRlZVXFxSTERYQkhXkVRLm1ZbmtNQQ9jkSAb0QQ36Uxv2cIAqbIg8cogkepg0cogscpA0Qp/YhJwWYBoRZB4Q4RcyxWlpmjFugj5SmLEV7IsUAJcRYh0VbhcTaRiTZRSTaRyQ4x6Y7xmXax6Q7xGXaxG7RW1mmj12mjNhljtliSDthSj5gS9thTDtmTDliQQsoad6aZdmead6WZd4DLNOzItOjMte7Kte3NsYNueDmrYrDXVrCPDAhwEOzII15ZmQipm2EK2zYn6jQl6TYl6jQk6tdFqVRHKRX5iDSnWu1oK91ZknjjOXtVHtC90ehoLPV0cff18FWXEBUWlkwtqyRKauN7rYqEnJCQkLa+WVUKsWzl7d9jNxkhUWFBQWNTWPWD8EfWg68s5MQGSokJyShouVnr7TpxNbxndWJ/zc3VKKyFeEC5IDLrk7efiYC0qJKCgadRx9S5htbGckxyuqqwgLSnGzsEpI6fTd4s2KW1nUAq/Ym5ev95YX4/Dw8irZ2pysrigoLaq6kfxMNbX19/XUUL9sNy2A8PrjzunR+sa66seL36DD0evLc/XleYEhUS0UT+WQXJ7uDs2MsTHN6iivoNUgyuPZ0ry8wZuQVV90lRR3NxJm93U39lYXNNMjl3cHu4ID/KjlNQ+tzL7en9bXVRYsJ9/UEFZ7eQsOQz/tKOhMtjfJzj8SufQV9p9bmqMkhrn7e2Tll38kJbRaktVSWPbEL13XZ6fyk6NDQyNbOse6m1vauklFo+73t1aWFIH3cbC9K38/PyJOUJFr63MVRTkdw8RAzSzE7dT4yMDQ6Na2jtKymrvThG/+/Kj6by0uPDohFv3Cek8ffd6WkK0l5d3XFJ679Vv8ESB7EBeaSMP8QKcGbF0GmhfWesL4qYghTnlCdULClhAw4Vfw4UPZK6GCw910jD5RWVBDRdioFfHU0TLDbakCCZGf/Uvg/aVMfaRNvaRMwftGyhv5idr7Ctn7KtkDv5ABcsQJesw2IL2BUEsZx4EQfCTH9dQsY0gPpzhHKvlFKPrGq/rlqjjkmDkkWLokazjFGvgmmDmlQFi1/xyhoFroo5jjMnFFDNiJjEhhc29QQGnW3hlWnhnWnilW15OMb+UYuIWp28XpGnqqaRjp6BuKqOoJSGtLCQoLiooKCnIISvIrC7ObKbE7qrNE2TGF+cike2vXB6pWRuj3RSr1RCtVhshXxUsAfK3gljgTAJcZYBEZbBkTbhsY7RSR4JGZ5Jme4JGa7xG4xWV+ijFhmilxivKzXHqbYk6bUm67cl6bcl6nWmGXenGHelGnenGbSn6LUm6zYkQb9iRatyTad6bZdmVZtaTYdGZagKyGCJBCrenmrQkGLaCSzRsTTIG15ZsAtvmRMOmBIOmeMOGOMOmeOPGeKOGWIP6WL26GN2aaO3qSM2qSPXKCLXKcNWqCJC/alWRahXhyiWBssWBMjkXBavizHb1Ymr9VdlH9x+xcvfLzctOSckZGSXUZ2V60Cd//yO7iKK3r4+yJO8JTolr44831h9fNFM6zcTp7u1vrq/h4R2++HTdx1r91Fk2T58ATyeLQwcPuYdTwHygNovtxBF5LTOfS6685w7886M92V3j68tjvGdOmnkRCbyMJN99511lPUs/b/fTx/apWftC5GhPJT8ni5mDZ7CX/Xv/fEtY0ZwmrF9EXCFIYVJMI987j+bmyoqLk+Liutrbnz7F11OQV8GTJ09mpqfJN+Tu3b37Y3kMm56eaawrm7vfuO2MYernlwc6S/v7ad+H+3Zs/fkbhsitvd94WtoLjrId232dhyH+qwTbJabzwgTId0J+WROXjNV5CTNOeRtORVtysQg+0LsgdjVdifFdHQ8RXU9BbXdxQ28hbXfyW8riepfoDuSvqI6HqK4nIYKNvMHJm/mDApYx8ZW3DJI185cz9ZM39Sd0sKmflJEPOVdYwtBH2sQfPODI78yBGgYHsljVLhK2Stah2k7ROs5XtByijTySjD2SNR2i1ewijC6mmHilGXkmG7onG7knW/lmWXhnmHulm3mlmngmmXkkm7glGLlc0bULVjPxUNS2l1YyFpNSExCU4ucT4OfmEOdnVxS9oCXFbKnK4WHIE2onmOQpkR8gXxWu2hir0ZKo25Ko1xyvWx+tURuhUh2qWBkiVxEoU+YnWeorVeYvUxEgW+UnWeEnXhkoVRMsUxciWwsuWKYmSKbSX7IyQLKSWDCY2FUTIlcdIl8TqlAdqlATplgbrlwdpgiOuj6afF2kcn2Ual2EWl2Een0kzdWEEUesClGsDFYoD5Iv85cr9ZMBV+YvXx4gXx4oV+InU+gjVeInW+ovX+IvX+wnV+QrU+wrW+Qjne8lkXNRNN9LPO+yWIG3eJGPBLgCL7G8SyIUD8Esd4FsD8EMZ87KK8a7+hMbgzW5Rz77+AKPsLqGqpy8VlljH0RWZwR/8vZbAYml4B9uzNmz/0Rx+425sS62E/scgzIhcvHB/WvD10YHG1hOHHCLJJa82dhYdTeUOiegOLW6EWKveoZN5NoEMciR6Gv64Zf7M9ruri/fETzPZOmbA5GXDCX3HTjTepVYzDLIQecElxwkLbjizsElRV2E8rG6BIelfwqR60uAUvhVsri42FRfT0lPvzY8TItCkO8T0MEj165VlZePjX7tn1evP0+frre2ttwcpC6ptsDg5mkeiF+cam6sK52eJubI7ja2kbQvUroohV8J+eVNXHLWLNIWAhouAlpuvOrO4BHXvQiqV0jHQwAEsZYb+EEQi+ldAk0Me/lUHUW03SX0L4vpXQQFLGngJWHgJWnkI2HkLWFIOBljH5C/smYBCpbBsqb+siZ+Mka+suCMfaWI9dT8QATLmgUSr80Z+xHf1IBk5oGgg1VsI0AWk+PE4NdyjNZxiQEFrO0Uq+0Up2IToWQTpuueoH8xUc893tAj0cg90dgjQd85Wt8xWtsqRMPEW0XPTVrZSlRGT1BMiZtPhIeTg4+dWYTrnCz/OW1pVis1Dl8L0Xg3qSxf2bIw1cZE3Y5Mwy6KYU+WYXe6UVuKQXOSXmOCVl2ManWkUnWEUnWYUlUo9cMWgbKgRCuCFMoCZfM8BHKcuXNduLMdOXMc2bMd2Cn2bFm2rOAodixZdheojjULIu3ZM+3YM+05shw5sxy50u050h04MsHElbvwkmDBRcFcd/5sV75sF15wee78eR78ue68Oa68mY6c1JRc4ChOPDkufLDNcuKmQEpXvlx3wVwPwWw3gRx3gRw3sIItX5YLN8WVNxdy8BAAR43hSrFlSbQ6nwrn4MyT5cyTYs1cFqa7q6VwfxXl6IHjLv6R1dWVhYWlt+4Sk4Ar0wOZTpxvGyZGiCeGWs+cOJ/V0H+tLf/kvlMFDV/9P649K/z4QeamW7RGvCXb/wAzd/PolKUcr5yWK1mo97oLjp1lSmm6DVJYhIXZ2j8XIi8by4rLm5ITgXPDPU+wSs0+3ciPceMXVrg3B83cnKYEi+HFGOr+F4NS+BXzYGbm6tDQ8hLxqDMzPT1HfZ0fQb4n5h89Ah2cEBMDz2Cv7bfltuP27dGWhuKl6WZiAHiu/emjdmJLOvDPt98crGhva37y5KVm6SHIKyO/soVf1ZFL2R5kLmhfTiU7HhUHMaoUJgaDqTpYUMOFXAgC1LCwjie/mpOwlruk/mVQw+Iggo19xQ28YStt6g86WBLUMHWXHHX0V8bEX0LPS8rAW8bQV87EX9rYn1TAilahoHrBA5qY8Jj4KVDHhkEZgxX5Up2aY4yGc7yaQ4yafYySdaSKTaSGAyGOtR2jNImvK4epmfrIaDmIKJgISOpwCypxcItxsvOyn7/AefYUH/MxKZ4T2pJnbNTY/S1B/srlBqjXxOp2Z5oN5loO5VkOg8u3HMw378s27aWY9GSYdKQYNMVrN8Sp18eq1kUr10Up10Wq1IAaDpGvDJEtD5WpCJMrDpBIc+JItb6Qbnk+3ZI5w+pchsXZNLMzqaan00hnRjgIppqfSTVjSjI5nWx6NsXiXIrF+VSrC+m2oJVZsx1Yc104QEznUF22E2e2I0e+M1eeM0euM3u2E3uWPWuGLUu67YVU6/NgkmnPnmJ1PhkOB/LakTPbhYfizJNhzwlCGRR2jhuoZ55sZ548N/5cN9DWoKS50u1YMx1Y0mzPp1iBFXsOYcKdYslUtss/vPxsrvDX5ldVpAUyn2JtoS42ea+/iek4SOHe/vq0E1+wVLR/tTBwTYzv8f3c/VPkB4w2+spj9p3jrh65bSLGrWngTzbtk32lJ5iYkzdJYRM5KUWLx9Rizwn1PM0iNb220VOedO7kqUuhCekxfsxnz8XkEQvovAwohV895L+qQaPU19Q01NWBOCbjEeQ7hPxX+Orqamd7e3ZGxkBf349uTs7i4nJ9XeVAZ/HY1erbw1Wb3VBlXXX+7ds4vRV57SiobhPWdudVc+bXcOFVdSQWSlOyE9RwFdO/JG3iK2HoJQoSWdOVmASsd1FEx1NIG4LuoG7BielcAgUsYeQjonuJHOsFZSyuf1lMxxOcpL6XuJ6XpL43pJQmdHCArEkAKGOQv+TsCFDAVB1MTJCAoKJ1KPlGHahhBUvCo+mSoOEUr2YXrW4ToWQeoGzmr2B4WVrLWUTRQkDagEdYjZNHio2d/9xZ5jMnTzCdPMTJtE+K85C26Alb5Qu+htxxzqL5gUrVV7Tb08x6KTaDhU4jpc4jJfZDBVZ92ea9mabgujNNOtONu8ClGrYl6DTFqDfFqDZeUa6PUqyNUKgJU6BOV5AjvgYXJEmsR+YjmmrHlmh6FgRumvnZdIuz6WanwKWanEg1Pp5mcgL8aaanUkxOJhmfTDQ+lWR6JtWcKc2COd2alWLPmQd614kjx5E1G5wToYYJQezEmeMEUpgjl4hnoTiwUOzBsVIc2TPsWbIc2CiOHBl2rGl2rOn2bBkO7FlOXCCFMx250x24Uu04MkAQO3NDDMWRJ8WGPcmKLcWWPc2OjeLEQXFiz4T0xJg0R5ota5L5mXKUwieOc9QPf+3LFxVpAedOXmgiPnG5cbev8SwhhfuvdxYd/vxAWvlXc9q6C64c2XesuIcmQwsjHY6xig7NzFsp8gkpWJCHHyiPPXDsRGrL2CYpLCFvNk8dFs4OcT/NIj3z5GlNauCF06eVtA20tbXd/a/MPn7ZYRKUwj8UN69fT09OzkxNvXXzJi0KQb4jHi8sDA8OzkwT/6d6ODsLN9sS9R8RPzquX7/e2lzX1d7Q1d74zNH8Ha31nR1tCwv4oWDktaOgqlVIy41N3oZN1hJ0sJC6i5CG6zMp7Cdp6C2i7S6s6SasRTgBTVcBLXfqkLC3uM4lQXU3EMGiepdEdYgBYxDKwjoekF6QeK/OTUTbU1jLg6qDfaRA+xr7SRr4SIFiNvYHKQyCmJglbBmiaBMOWxmzIAWrMHBKVCdvGqhoFqhhG6ZqHiCr5y6qbCkgpccppHyBS/Lsed5TJy+cOn765KEDLMf2CV84JMt1WFvspI3KOS9j9lgXYYqvbGW0dnOSQVeG8VC+1XC+zXC+7XCBw2CRS3++QxfFop14g82wM82wO8OkM82oLVm3LUmnNV6zIZpQwA3RSnWRxGcvqkJkKqifQa4Mli4LkCwLkC4LlC3yksi05UgCKWx2Ng10sMWZdPOT6RYnU82OJxkdBUGcZnYyxfRkEuFOJZufSbE4l255PtOahWLHkefEk+/Ck+vITkhhQqcSY7ppNmxptiB22XKd2bJBBDuwgku3OZ9ueyHbiZMqZzlACue4cue48WQ5caaBGnbiynThyXLhy3bjz3TmyXDiTnfkApdqyxFnfj7Rij3NgSvTCcQxbEEoE9/+yIS9NqQU1t7VUri7LO3AvtOU2q6NjXXoexaXCH1aluJ74uDphl5CX97prT+2/1RyVe/S9FWeU18qGns8WFgaaKksLC4fvdHDffaAmoXPg8dPpm73KQuzyBm4P93YiPHUP3CUqbh56OHkbWtV/rf/81E6MVeY+tqcN/HanIeehJCkwSOqFM4McD58VuzB2pOMYAc2dh7niz6BoVE9V7/BpECUwj8UU5OTlWVl5SUlsw+oq4EuLKyuvtRdhyA7AzdSX09PVlpaa1PTj/3tzJWVlZmZB1PToOofPHM0P0TOzy/ge2DIa0hBZYuQhguLtAWLpCmrtDmoWGFtD351VwENN2IMGLaa7iKaxAcy+NWd+dSd+TVdiUFi4ovKnkLqbkKaHgLqrsTSadoeVFt3IS3qGLOmm5DOJWFtT2KyhN5lMT0vCUNfcT1v4hvLhr4yxv5y5sHyFqGK1hEK1uGE3zxE3ixY3thf1sBbTv+SuJoDn7QRt4gGO5/COTahE6dZjh07efjAgWMH95478gXP6c+lOfZrCh+0Uz4daMqZ6CJaEKBUF6vdlmbUlWXaTbHoybbpzrLqzrTopVhSndVAnv1AvmNvtnV7mklzkh51DQdihYeWBK3GWLX6K8q1kQo1YXK14SCCZavDZKpCpauCpCv8JcoDxCuCJMoDpcoDZSuIV9lk8lz4suzYMmxA4J6j2DBRbM5mWZ1JsziVaHQsxfRUuuUZcLArw5op3YopzYIpzZI504aNYseZ48CT48Sd48SV68Kd58qb48KbbsuZZMGaasOW5ciRCQrY/kKWEzso4Ax78LNkOkI81Tmx5bhx5brz5Lhxg7rNcuGiuPJku/HluAlkOnKn2hHf+wDtm+XEk2bHkWrHle5AjBNnOXNRIKULV7YLT44rH8WZJ82auTJCd303ryAxVJt79PNPWfhEtbQ0lRQVrT2DHj/dqKOEnjvBMCp8jDmxjFgELd7b6uihI+IyCsJ83K4+0csbG8mBDiePHZeUVZQQ5mXlEStvG4FkY/11kpxMZ1l4lFRUpCQFjx84QXxtbvmOMMNcYQl5U/qo8CkW6Zm19YacsKP7v+AREldQkJdXVAlLzltcfakuEKXwD8jU1NTdO8SaRyvLy13t7T1dXQvzjAtFI8g3gxS+a2tr7a2tibGxlaWl5Kx0BEFeJYXVrcIaLlzyNhwyluclTXhUHHjVnLiUHHhVnIQ13fhVnARUnQlHXUiYDzSuuiu/mrOgmgufspOAmougphu3ioOApruw9kWqJvYU1SXmE4MT0bksoe8tbUh8eFlU77KEkZ+koY+UkY8UMas4QMYsSM4sWBGcSYCsrqekhqOQvAWPmA4rr/wFDvHT57gOHjmzf9+hg3v3Ht37+blDn3Kf+ESa7UsdkSMOakyBFlwJbsJ5ATIVUUoN8RqtqfpdmSa92ZY92ZadWRZdWVYdGVZtqeYdaeZdGRZdmZadmZZt6eZdmRbdGWZtqYatyXpN8dq10aq10co1UYqVYXJV4QrV1G8dU8eDZatCpSpDJCuDJCsCxMsDxcsCwEmUB0tXhclVBssWXxbOd+XJdWTLtjufa3cuz+5cji1zpjVTqtnpDMuzWdYgjply7JjBk2FxJsPibIbVhUxrNootR7YD6GBiRi91Ui/xthwo1wwHkKq8IG2znHmIoVwXbmLirytXjitntjM7bHPdufIIxw0u1434tnM2qFtXEL7cqbYcyTZs6Q7cFFe+XA+BgotCkC3EQ1Y5rryQJtedN8+dN9sNNDQk4M9yYKmK/G98bS6h+Obi8ktNMFievp0Y7mNjZW5rbW5lbuIXGb+6vj4z2puRkjIzR/RAT+bG05KSr40Rqz08mbufGRtka2Xm5Rc8MDIGMeuLUzlJ4XbWZk7OLhX1HUSOVPobit0drS76BLR0thVnZoxOL2xsLBakJzX3EG/dddXmF5XWkClH+1vSsgo31h7GXDZi5xMPiYpPSozTUxQ5cVZg+P5LvVWdVHoLpfAPzp2xMUp6Ojjw0KIQ5Juwvr4+PT3d19ND/pPh3t27DbW1t27cICemIwjyKimu7RDRdOVRsANHfFND0ZZb2Z5b0Z5flfh6HI+iHS84JXs+FUceZXsekMhKTrCXT8UJ/AJqxDgxSGcBDVdhLU8BNVdBNTcZQz9Z4wBxPS8xfW9JIx9pIx9JfS8QwdLE8hHeUiY+EoZeYrruQuqOwkrWgjImvGJaLFxSp5g4j55gPnTo+P69ew5++fHRL98/d+gjntOfSLLt0RQ64qDK7G/MmeQiUugvX3tFsy3FsCfLvC/bojPDrDXduDHVoCnVqC3NrCPDsiMTHEhhi3bqVyc6M827KKCPQQqbtaWbtiQbNBEfYNNtTtCqj1Gru0JI4ZJASXBlwZJVYVK1UbLVkTK1ETI14TLVIIiDJCpCpCpCpMuCpctDZaoiFKpCZUt8RIs8+QtdOQuc2PIdWQoJx5rvwJJjdwEUcA6hjJmybc4So8XW4M5l2bJk2rFl2XNkO3LnUEVqljN3ugNnGjGrgZviJpB3SSjHUyD/skgB4YTyPPlzPfnyL/KR323Ov8RbcJE33wMMuUEEZzlxpDuyZzhypjtyplEzyfYAW+H8S0Lg8jwF8zwEciG3i4J5F/nB5V8kcgMdDLsoTuzV0Xr/bVK4fWgmInvk/vTik7V1EMSEW6Fut3JLq08fL63Mzi08eEi4h/NLz3Y9WVx5SiRYWXsMfsLz9Mk6sY7No/mlpScba+sbS8trRMzGxvz84sIy8bHL1SfEEZefEJGLiyuPV9afbkCyJ5CSyG2J6lmGbCHzZ2e1sga5LU1fUxZiEtFyXaAOBOdFun/04cmKHuJtEqrJtg4SwMXm16MU/oEB4VKQmwuOfIVuZnp6cmLiR/e+P/IDsrS42NzYmJaU1NvVBUG4eR7NzeHkAQT5QSisbhdUd+GSJyQvubQwt5I9r7IDn7I9rxKIYEIHQ5BHxQEkMo+iPZe8LUQKqDoKqjuL6ngIabkKabmIaruJaLuLaLkJabgRE4IN/YnpEEY+ogbUVSa0L4pquolpOAkrW/LKGrIIq5/hlDrCxHP0xIWjx5gO7D+89/NP93/+/qmDH3Oc+lyUdY8K/wFj6eOuWizBFnwJrpLZfkqV0TrNSUYd6aa92ZZ9uTa92bZ92Xa9FNvuDGtiDDjTjBC+mdZdFLvuHHC2XdnW3dmWPTlWvbnWvbk2PTnWPbnWnVnmLSkGrSn6HWmG7al6rcnazYmaTQlqtdGKVeFy5aHShQEihQGi5aFStZHyjVeUG6+o1EUq1kYp1kYrVUUqgKuOUqyOkC/1Eyu6JFDswVvkwV3szlXsxlnixlXizl3kypnnxJLvxJLrcCEL1LDD+Wz7C1l25zNsz6fZXkizZUm3Y8tw4Mxy4kpz4Ii3unDF7Hy0KWu0KXu0OUe0BVuKk0C2pwjI4lyQsJeECr2Fi32FC7wE8y7x5XnyUqdGcGZQXaYLCGg+qnoWyr0kmOclXOAjWugjVugtyuBEwBX5iBb7isG2wEsU8sxyZq+K1v9vk8K912ftwns84vr90676pQwTLpW6fc75wjb1WmDmjeCsG8GwzbwRlHEdEvunXQvMuO6fepWa4CrhT/vKH0RNRo8JIGMyrwekXwNbauSwf/oImSyAnhXVFoKQICB9JDB9hEwMxwrKvOmf1MEvoXXgFKuKhq62tjYXK/tBNjP7qN6gzJFN57zJBaZf1b7UVtH+un+F9b8eEC53xsauj4yAdllbW2ttairKz79+jfigFIJsx9OnT2dmZuDOWXvyZHV1tam+Piku7se4aBqC/JeRX9nKp+LEKm1FaFwQu1QH2pdLwY5LyY5P1YlHxZFL0Y4L1LCqE5+aE4+yAy/xzTkHXlUnUV1PMb1LxLJrehfBiRt4ieleFNH2FNHyECe+QufCr2TJI2XIJqB6ll3y5Dm+oyfOHzh4cs8XB7/4+IsvPv7wwCf/OXvgfe5Tn8hw7NWXOuWqxxPhKJ3uo1IUplUdo9uYaNSaZtmeadOZZd+T49ST59SVa9cNLseukwK61gpcd5ZVD8Wml2LTTQEdbNudYw+uN9+hL98e3ECBwwD48+z68+0GCux682y6KOY9FLPebPPuTOOuTMPODIP2VN2ONN22VF1QxnWxqhXh8mWhssWBUpWhCk2xWk0xWk3xmq3JOk3xGvUxqnUxqiCLS/3Ei72Fy3yEy31EKnyESVfuI1zqJVh8kbfYk7fQnbsAlPFF3gIPnjx37hxXTooze4YjO7FUsDOhYrNceTOcuVMdeeKsuEIMLvhonvZQOuKteTrEiDnK/FyyA3u2p0CRr1ihj2juJdDHfITzAO3LD8K3wEuk2E+iNFC6LEimLESmLFimNEi6LES2PESuPEi2PEiuLBhkvXwFuDD5qnCFmkil6nCFskCZ/MvCGY6sldG6/21fm5udX63pnMyuuZNT+1IuF1zdXWL7zJGRX0uwfXDLGMZI+i7GZHQPLb7+fl5lT3RkuJOjo72Dc0BIDKXian7DfXqa7RxcZl7dnYkZnE34WkD+L3vu4cOSwsLE2Nge6vDe8tLSrRs3piYnQe5QUyG7HVDA5HDv7IMHVeXlJQUFIIghODY62tbcfP/ePRwMRpAfloLqdiFNNxYpSw4ZKxDB1AnBoHftuZQcuJQdwYEC5lC051QC7evMo+bMp+HGo+bCo+rMreLMr+EhrHNJUMtTWPeSuJ4XyF8hVXt+eQtOcT0WAZUz7OKHT3EcOHh67xcHP//os88/eG/Ph+8c/vSfZ/e/y3PqMzmuA/oSx120LoTZCKZdliuJ0G1Kse7Jc+sv8OjNcwXt255h15bh0J7l1Jnt3JHl1J7l2EFx6Miy78i06cgEiWzWnmHanmnemWXdnm7VnmHVlW3TmW3bSRXEPeByHXpzHXuy7boo1tRRYWJsmBgqBkcx78oyAddNMe3IMOpINyS2GUbdWaYd6UYNCdqVUcqVEUqV4crFAbLlYQp1MWp10argGmLV666o1IQp1IYr1EUq1kcqNUSBU66LUKiNkK+LkK8Jla0KIt63qwuXryUmFktXh8hUh0hXBkqUB0iWgIT1lygPkqqANKGy1WEK5cFy+d7iGR6CiU7c8XZcsbaccTZsqc48uV6iRf5Shb7iRX4SIHbLQ+UqQuWqIxSqI5WrI5RrolXqYuBM1Opj1OtjNcDTEKNRG6VaHa5cF61Wc0WtFuLjNBviNRviNBvjNFsSdRqi1Yp9xSlunDWxhv9tr839qFkHubT2435hHFlcXOzt7gaJc+8uMXHl7thYQU5OXXX1I+rHOGA7PTVFfxcKRc9/PfATr4P4pb4PB164Hzrb28m3LeEBKSMlJTUx8TZ1Pb7VlRW4eQgbBEF+UApr2vnVnVmkLLjliakRfNTpvzwqDiB2QQdzKtnzqrvwq7vxqjjxqjjyqjkLaHnwa7oLarkLaLgSw8OKdvxKtkKKVlzi+ud5FU+cFzx6km3fwdOffb7/4w8++vjdd758759HPvkX84F3Bc9+rMi910TmqKv2uRAr3kR38bxApaoY3eZUULTWXRTHvjzXgXz37mzn9gz7tnT7piSb5mTbzkynLopTW5pta6p1Z4ZdZ7pdZ4ZtN8W2J8emN8+2J9+uJx/ksm0Hxba/wLE3z6E3z6kvzxlcd7ZjF8WBmDJBqGGbjizQylRBnG0FgrgrC9Qw4TrBgZ7ONO/INO0CiZxt3pFl0p5h1JKiVx+rWRaiUBQok+crkXNZtNBPqjpSpSFWoy1BpzNFvyPVoD3FoCPFsCPFoCVRuyleszlBqyVesylGvRnUZ5xmU6x6Y4wquJY4teYY1aYrqo1XVOujVRpj1Zri1VsSNVqTNNuSddoSdRrjNOpiVKojFSvCFUsC5YoCZCvClOBY4OquqDcn6VCddkuybnOSbkO8Vn28Fuh18DSCS9AG1VsXowEquSpcqf6Kek2sRhWo4SuEGiZcrCbx3ZArqqX+knkXeeoTjFEKI8h3zNLS0sz0NLm22kBfX0pCQnFBwaNHj0AJQbC8uPjq0BD4l5eXr129OtjfT6rkubm54cHBGyMjEA/ByYkJ2DU2Orq2tgZCCjyDfX0T48RkGMj/+sgIJJ57+JBueP3aNXLpWdJw9PZt0vDO2BgEScOVlRXIn24IpwRnQjcEZUY3hNPb0vDh7CzdcOTq1cXHxKKwYDg0MDB66xbdEC5z/D6xXDcY3rx+HfaShvPz84yG8FTAaAgCkW4IupA0JF8j+8qQKhZJw1s3bjx58oQ0hFO9f49Y9eV5w2vDw3RD+F0YDeFx5SvD1dWbN27QDRcWFkjDxwvEZ3S+MqT+rKQh8bSzvk4aQpCcKU4agiMNoQA72tqg6MhrbGlsTIyNbWtpgV1wSvU1NU319ZA5BBEEeU3Ir2rhUrS5IGUOopZL0Y5HzZlXnRDB3CpO4LhUHPg0XEV0PKlLRtgLqjoJqjjwK9rwyZvzSBtcEFQ5ziJ6jIn/+EmOL/ce//SzvZ9+9OHnH76z76N/Hf7kH+f2/VvozIcqPHss5I5f1mcNt+FLdRcvClKsilKrj9dpTNBvSTHpyLTqzLJpzwBn15nlAK493Y5waXbNiVYtyVYd6bbdmfZdGXYd6TZd6dbdGTa92XYD+aB3Qfg6dec69uY79cI2z3Gg0KUv37knByS1y0CBO2jrjkyQ1NadFLuubPtOUM959gOFjv159r25dn25duABPd2dTUyu6KaARLbpzrHuzrXuzbPpybUmhHKmWVuKUUOcdlWkaoGfdIanUK6vZEWECsR0ppv0ZVv151j3gyKnWHWmm7alGrWlGnSkG3WkGXakGrYk6rYk6rQmabcmaXWl6nWm6D2TvGoNcRpNiZqtqdptqdqtyXotifoNsVpVEcqV4Qq1V9RqolRrotQgpjGOVLpaTYnasKV6dMCBDq6N06qJg61mXaxGbYx6VZRqeZhCKfE9PPmqCMWKCIUK2IYrVkYoVYQplocqVIYplAVLF/mIZrtz1sYZoRRGkO8RkFmgdbo7O0EJgWaqq66Ov3KlrbkZdoE6LM7Pz0pLuzNKrCQNYjc7M7OksJDUqX09PRkpKQ21tWAFug08aUlJ5KQL0GolBQWU9HQQkXRDyGqWKjdJQzgQaQhHpxuChC0tLKQbgoLMycqiG4IMJQ1XlpdBQ29pCJqPbliYlweSlDTMTE2tqaxkNOzqINZUAUOQ/nCNpCGURi6FQjcEgUg3JGUi3XBhfp40BNHPaEjOJSANK8vKQMTTDUFx0g1hL90wLzu7ICeHNASFymgIPwTdENR5RWkp3RAkLGkIjxaMhqSkJg1B0UImpCEUHaShG4IjDYcGB5Pj4+FnfUxV/2CSmpgIJwySGmxBPc9Tn5FgF4Igrwl5lU08SjZsMpZcCnZs8jZcKk58mu7cqk7cyg78as78ak58qg58ynY88pbskkZsQlpMHPInzoseOsm5/wjTnn1HPv34008+eP/zD/6958N/nTnwPvfpT+S49uqJH7VROutjxBnjJJTpJVkUolgVrVEXC1pQryXZCBRwa4ppc7Jpa6plW7otCN+2dLvWNFDDtm3g0m0g2J4Bmti+I8O2I9O2M8uuOxtUsm17hnVPjsNQketwkXtbqk1dnFlzsjVo6C6KHShjUMCdWfYtqaCt7UEHQ86taVYNiWatadaQA1UQEyPEPdnE9IneXIeBAmcQ0F0UW4jvy3MaLHDtz3cZLHIdKfe4VuY+UOBIvJyXA6LZpj/PpiPdtC5OtzpWuyZWpyZGuznZsIdiOZBnP1To3J/nCGIagj0Ui94cK9h2Z5p2ZZh0pRt1ZRh0pet3ZxiCvzlBuyZKBRRqXYw6qNumJK26OI3KSFXQ1lWRKpURylWRyrVX1KujVCBZY7xmY7wGuIZ49YZ4muSFLejgulhNkMK1cXAmmjVX1KqjVaqilCsjFasiQfsqlILkDZSoCJMtC5Iu9BUr8BEt8BYp9BHN9xbOvSSQ5cJWG2e4/vSl1upBKYwg3waQhqB1wIEftCkoS5CeoOQgCDIItB0IrDHqt2dHb98GP8Q8pErh3q6ulIQEQtGurICirauqAkXVTZWJhGFubnpyMvmhO9IQRBs5nAmGILZAX5KGoKHpho/m5ory8+mGd8bGQGvSDft7e0nDZVC0a2tbGt64fp1umEehTE1O0g2rysvphklxcaS+BENQ7aAaScN7d++CnqYbDvb30w1BDoKGphuCoiUNR65eZTQkB1BJw7LiYlLRgiGcKjnUShrCXrphdkZGblYWaQglD3nSDUGS0g1BqpYVFdENJ+7fJw3JcXG6ITmeTRrCFjIhDeHHujo0RDcERxpev3YNygrU+Rx17B/OB54cYBc8GkEQQZDXkLzKRl5la1Zpcw45a3Z5Oy5lJz4NF2KxCAVLTmljVlHts7yKxy6IHmbi23OU5fM9xz76aN97737y/r/f/fS9fx365B9M+/8jyPSJEu9eE7lTl0x4o5wkMryVi8K0axNMW9It2zMtOrMs2zPMW9Ms2tKtwLWkWzWnWzelWDUmWjSAS7JqSrZrTrFrSrJqTbVtSbUBT3OqXTvFpafAs7/QszfPrSfXpS/fpTvHqZPi2J3j0pvr0k1xrI83r4gwqI8zb0y0bEw0AwHdkWHfmmbblGzVRsyvsO3MtOvItGlJs+zIsu2i2IMaboVDJ5sTLgVOxro7x6E716GLeM3OeajEc7jk4lCRx9USzxsVl29Weg0VuoL4Hih0uVrqeq3MbajIub/Asb/AoTfPvpNi0wrKOF6PeLcvyaQtnbjGvhzrvhyrbooFiOb2NJOebIu+HIuuTKOWJJ2meK2WRN3aaLXiINnSELnGeK3mRB3QtVWRqmWhiuVhoGJBBKuBRAZpW0WVy/VxoIDVmhI0mxK0GqjzH2oJp1kXpw06uC5epz5BtzZWE3RwZaRSRSQxElwdpVQRIZ/vJ5rnI1QWJFHkI5LlxpPtwUddj4I/x5OP4s6T6cxaE2uEUhhBXhEgmx49egQyiFTGS4uLoJNAGJHa9+HsLGhKkFPkdIXx+/dB1IJmBc0EevrWjRsgo8l/5S8uLoIyo69BSxpCDDlgSRh2d9+kLkkLhrdv3aIbguKE/OmGcw8fDjAYwomRhuTkgS0NyTkApOHQwMDCszkAYAhi9yvDzk5yOiwYXhse7uvuJg1BGcP10g0nJyYYDUHT0w1XVlZIQ1LC0g3JsVXScOTaNXJslTDs6gKBTjeEvXRD0M3gSEOQ4JAn3fDO6CjdEB5UQATTDefn50lD8DAaQjIIkoZjo6OQCc2wq2uKOtS9yRB+XLhAuCg4MQgSrwPsGhEMN+DKCjyR0d0TxiC+F4G8tuRXNvEoWp0XM2CTMGaXNGEVN7wgonWaW+4Ei+i+E+yf7T/10acH3n3vk/ffff+9f7/z0Ttv7X3/zWOfv811/N/yHJ+aSB6+pMMaZS+S4S1fFqndkmLeTbHvzXXtzXfrK/IAIduX7059B865N8e5HyLzXbtzHbtynNoyHBqSresSLUEK1ydYNSaAArZpTbNrSrauj7eqi7dqSLYFNdyb79FX4NGTQxvubUoC1WtRG2tSn2BWH29WGWVQGWlQc8WoKtqwNsYEdrWkWLem23ZkOXRRHHqy7buybDoyrbpzbHty7DozrdvSLFpTLJqTTBvijZqTTDqzrHpybPrzHEDvDhW7g+unzrvoz3Pqz3WE9A2JRmDel2c3WOjQmwOnZ9aVZdWfZ9+X79CVbVmXoFsaoVYSploeoVYbq92SbNCWbNiWbNCSpN+UqNeaatieZkiMBEerVkeq1ESqlAbJ5nmLwbY2WrUyTLEsWK4iVB484CpADUcogQIuD1UAWVwerlAZqVAVIV8TqVQXrVoVrlQZrlR7Rb0uVqsuTqs+XhscVQer1kSpEOnD5EtCZMrD5EqDpfJ9hfO8BYt8hED+ZrlyZXvwgKfQh1iarcBLOP8ib0OiOUphBPnBWH/6lBSs4Ict6DNSFBJB6oQK2EsGwUMGCav1dUgGiXcwpAd3MISjb20IgW9oSAsSdlsYQnBrQ3qQsPtWhttc49cMqXxjQ/KS/8+Gu5aBgcHY2NhgIGQLIDomJqa3r5+WGkFeJ/JKalmFtY6zSBw/L3T4NPfew+e/2Hvyg4/2vvveR+/++z8f/vsfn/377wfef4Ppy7f4Trwny/aZgdgBZ/UzIZacKR6iRUHK1dG69fGmzSmW7enWHRk2XZn2PdnO3TkuXbkuPXnuffke/XlufXku1Pm7bkOF7oOFrgMFrr15bu2ZDm0Z9h2ZDq2pdh0ZTt3ZLmDYleXUmenckmzblGzblgmK07E316kr07Y1xbwhzrDmil7NFf2qKJ3aGP3aWP3SUPWCAKXSULWKcM0aOI04g8YEY1C6rakWHRlwMpadGRatqcbg2tNM2tKMW5IMmhP0wTXE6jSAeE3Sa0817Eg1ak81biPSmLakGDdBmiTDpgT9uhjtqiiNhnjdlmT91iT9xjidmij15kT9jjSTjgzTriyT1lT9emIasVpZqGJZiBzhguVqo4j36mhDvNGqFWEKxIpmEYoVIXKFvuJ5l4ULvEWL/CQKvEXyvUWK/MRLA6RK/CVKAiTLqWuigQOTshCZ4kCJkkCJ0kBJcCX+sCXiieUjosARMyhAOleGg1xWhC2kLw2GBLLlIdKlYBggUeInVuAllO3OCy7vkmBFkFRtBLHkRWWwZHumPUphBEEQ5Luko6NTQ13FzEjdwUbX3lrna86K9OiaGatraqi0UGenIMhrRVpm3v7DTP95/4v33vv4vXfe/c/bb3/0rzc/f+eNgx/8lXnvP4VPf6DI/omRyJduKsdCTVmSXflzfSXLwlRBiTYmmjcnW7ckWzcnWYOnOdmqKdG8KdGC8KfYNKfZtFLnAXdmEms+gDbtzLDpzrIlXVeWQ2uabUuqdWemfSdIXgpIYefeHKfebOceilNHun1Hml1XFthagkJtTTJqSTBojNWtjdaqvaIF25pozapI9ZJghcIA2dJghUpiETENQuDG6YJrTjRoSzVpzzAlvkWXrN+YoN2cpNOaTHxhrjFWoylesylOvSlWtTmOcE0xKo0xqrVRysSSZNQFyBrjtepjNOqi1WHbFK/dHKfRcEW1NlK5JkKpKU6zPVmvI9WgM12vJYmYugBpakCeRihUh8mVg5YNlCoBdRsgVQXKNVyxKkyuhlhnTaEyRLaMUKjihd4ioFNL/cVhSzhf0WJfkVJ/0VJ/IrKUkMUSxX5ihT5Cxb7EJzaKfISKvEWKfSGxOOwilDFVHxOeAIkif7FCP9FCP5FiMA8Ac2JbFiRZHkhMkMj24Etz5EixZy/yFWuMUamPVgIp3JZph1IYQRAE+c6Yn19wsLe1NFbsaUoZ6cm51p3N6K52EVuI72lOcbLRsLYyX1gg5q4gyOtDYnz8R//517/++ofP3/nzkY//dn7vm6Jn31Xn+cxCYp+b6gl/A+Zoa84UV758X7HyUJnqSGKVg7pY3fp44/o4s8Z4i8Y484Y4s4YEi8ZEi8YEs4Z40/p4s7p4kzpIQKQxaow3akoAZ0h1Bo3x+s2Jxm0pls1JFg0JJh3p1iB5qc6+J9u+O8uuAzR0kmVLonlHKrGAQ2OcVv0VEKMadVFqNZGq1ZGqsAXtWxosXxoEOli2IlS+OkKRWKQsRr05XquNkKqGHenGHRkmHWlGbaCAEzVbk7Tbk8FptSZotCSot4KLV2mOVWqOUWqOVW6KUa6LVACl2Jyg1pKo0RSn2kDVxyCC2xK1WuPVm2NJxazSkqDZlqzdkUL9KkeKTkuidnOCdv0V1YZoJXDUYVeZYl/xAi9hkLzlgZKgjyHnhkiFmlDZikDJikAJUL1lAWLlgSBbRUv9hEt8BMEV+wgWePGDy78MW4H8y+D4wBV48RVc4s2/yFtwiS/vIl8ueLwEyAQ0j5dA3mW+vEuQEiL5czx58i7zF/kJF/sRGrrQWxjUcKojR/5loapQmYogiWJfgeY0S5TCCIIgyHfG0PA1RTnxq12U9fWh9eXezW6ph+ZZH7p3rVhWSmhkhFhd5JuxvtLZWJOempycmlbV0D6/9N1/v2bizkhhblZKelbP0E38Os5uIz89TujUB+Jn3tER+NxW/uAl7ZORliypLjyUSwI5XkK53sJF/uLFgZKlITKloSA95cpDlarC1aojNKrCNasjtCsjwelURelVRelURmhWhGtUhKuXhamWhSpXhBGuPFi+LFiuLEi2IkSuIkQetpWhyjWRYKtVFaHZFK/flgSC1bg9xbgr3awj1bQp1qA2Urs2UrMuSr02UhHUZHWoTFWwVHmgREWQZBmoyQCJIh+xfOJrwyKFviJFfqIl/qIgLmtCpeoj5ZpilFriVBpjVeqiFWtB4EYpNETDVo5wETJ14ZJ14VK14RI1YeJVQcJVQUI1oWLVIWJVIeLVYRK1EZK14ZLVoeJVEBMMW4naMKm6MGlwkHl1mFRthFwdkaFiwxWFhhilhhjlhivKcJSqUKnKIPHKQMhTvAyUqLcASNg8T+48D67CSzyl3vzFl3mLLnKDK7jInX+Rs+AiV547e64bS67rhTzYurFku5ynuJzPcrmQ7cZGOFcWijNzluO5LKdzWQ5MFAfmTPtz6XZnMx3OZTicy3RkpjhfyHK+kOl8PtOJcBQXliwXlnRH5gznCzme7OByPTgKL4OG5snz5KI6zjxPjhzX83VxuriYGoIgCPKd0dXVIy8j/Oh+7fpiz9P5zu0caOLVB81y0oI9PX00y5fm6aNRdTnRMxc4xURF2NnZzZ0Cpue/S73aXJ4lL8bPfI7p7NmzHDxCl8NSloh54MhuYbCtJNFDLv2yTEGwanmkdk2cfkMiOL26JP3aRL3aBL2GRKOGREPY1ica1yWY1CeaNSSaU51FfZJFXbJFQ4p1Y5JVfaI57KpLMK2NNaqJ0auJ0am5Qky3LY9QLQtXqYhQq4xUq4pUB1cWql4aolEcrFEWplUZBTJau/qKXkO8YXu6RWemdQtkGG/alGDSGGfQEKdNLK8bo14fpVIXqVwfrVoTqVwVoVoZplIOOjuc6sKUKsOUqsNBnio3xak1xao1xqrVxahVR6nWRKvVx2g0xmrWRavURanURynXRyk2RCvVg0qOUqgKkwFXHS5bHSFfG61Yd0WJcFEKdZHUT8dFEpNriU/KRSnDoWtAlEcp18aoN1CX+G2I16iP16yH04vXhZOsu6JeE6lSG0EMDNcSMyJgq1QZIlviJ1boLZJ3USD/kmCJj2ipL21qRKm/GHXygyC4El/hYmIEV6DQW7DIV7jEXwxckY9w3iXeHE/uXE/uHA/OXA/uHHfubHcuihtnlisHeHI9eXI8eCgQ6cGT48mXd4k/75JA7kX+3MsCBT5CRX5ChUT+IoU+IkW+IsQLc5eJIedcD07qJzZwVBhBEAT5jugkpLAQIYUfdz991LGdW1/sXp5ugpSQnmb50qyNd58+vl/K2L20rMzL3mDPniOhGTW0fV/n+XcXX/g2473BBiG2s2wCsuFX4pMTYrTlhb/ceyylpJW2+3l29/uR/5UszU89GOucGe2YGe2aGe1+MNbzYKxrerR9erRj+ja4zq+7runb3VO3u6ZugaN6iBgySCSA7eTNjskbbZM3WydutIyPNN8faQQ3fr1pfIRw90ea7g43jg02jA403BlqHBtqGBusvzPUcP9a89TNthk4BDWH6Vsd0zfbp262Tt1smbrRPHm9cfJ609SNponrjRPXmyZGmoncyDyvNU5cgxhI0AgJqCkbx+Gg1xpgC4nBcGKkYeJaA7EdaZgEd53w3L9WN36tnrqtI3ZRIydG6ieuUR2ZkmoCtuNkbjea4Hwmwd1ongB3vXnyRuvkDWqQOEojaTJJPRkwHL9ad2+wZrS3Yqyv4u5g9b2hmvtDtfeHa8BzbwiChLtPegar7pJB2DtcC/47A5Vj/VTXR5gTrr9ytK/8dk/Z6LMY8BB+ajJa+oHKO5DVYBVs7wxUQZBwZD5g3ls2caPtJWsxSmEEQRDkxVClsPDcvRdL4aXpRgVZ4a7ubyyFV+93sVw4451DLEG9vjIjznVMVM+Z3HXrak9mRlppVePDBWLZOypPhntaKBRKa/fw0iqtwxu73p+XnVVV3/54ZfN4b7iT7p6DzLU9xGrfwIPb3QrcJ/mUrObWNlYWHrS1No/PEEsBri3Pd7W1XhslVo8GFuemaiuLcwqK70wRSyXCed25da1n6DqcT0lJ+eDQUFNz28NF2sjT3RsDLR29yy81DoUgyOsCSmEEQRDkxZATJObu1XyvUpiV5ax3NiGFAV1ZFh41C/A0F6ZIiwjy8vNzcXLo2/qOzxFrOVdmJyrLySorKxqYOnRfI9Z+7q0rUJGXVlBUFBcTvxiYsMwghtdXxmX5mKT0vWhhKpQwh71H2IbuL0/e7NTVVKhoIb4XszR5y1xRLTChGPwLU7fcrfWlpOXkZCQ1DG0HxohVtIuSQ2SVNLSVRHmFxMJjrijIKJS3XiOy21gK9zQ1dglawNFkBPlRgVIYQRAEeTGvQAo/Ge9mvXDGLbka/ONXm9hPHjT1Snl4d1Ce77y4kmF2flHIReuDBw4HptWvr0xriXDLaNs2tjaXFJfdm3689njCWEVCxciprbMrKeIyOwdvUTPxfUGSxTutnEz77SOLaGEqrSXxe/fvb7o6eXewlv3ckfTSXohcuDMkdOi0xeVk8OdGXeTkl0gtqGmsyJPi57D2iofINC/DP/3lLQVNgxRKTmN9Ocfxgw4BaRC/PH1TgvOMoccV8CMI8iMCpTCCIAjyYl6BFF6b7Dt/9iifill0dISmrMCZc/xtI9N1lNBjx05XdRPfDgRNqyfNLqjpMDc5IsN+SsXSjz5b4mpT1snjx+OKO8F/f6SLg/m4uW8SuQuYGihnOf7J5fRaWphKZ3Xa/qNf1A3cuz9cz8t6KrOceM/v8d1h8RPn7QOy4Fja0pxyxu4PF9fWnzzyMJQ5w6V8Z2E987Lux18erRkkZ1A8ttMQ4VOygtO43Vly7sTZjMpv/LIggiA/LCiFEQRBkBfzCqTw06l+5tOHDrMI6uvramgbpBfUQ2TsRdMLLPJ3Z2kzcIPdDE6LKs/Oz6UG2p85dVrH3Lmln1i1rTnD+/13/80rJqeqqiovI/Hxl3vN/RJIE2B2pIb11OeuCZW0MJX28qR9h/Y2Dk/cG67nYz9NqSA+kvf43lXJUyxOwTmLy1P8Zz/Yf4JJTVVVTU3l/PH954W0rk8vp3nqsfLIElMlqOTHeh49xzNyf7Yqxes8l/i1abo4RxDkxwFKYQRBEOTFvIoJEhM9LMwntN0iu3r7b92hvbgW5qB34bzy+CPaDNzQiyYnhRRnlzfWlx6kRvmI8nNxi6l2XLvXnHb5i8/32Hj4hwQF+vv7h8cmdV29RZoAT2eH+M4f0XKOo4WpFMZd3HuYdXB8ZWyghoftdE71IESuTt2UPs3iHJyz8Hic58z7gjKqEeFhfn7+wSGhJXXty0/WEt302HkVHjybiDzaVcZy+nRCTtklKw0FEw98ZQ5BfnSgFEYQBEFezCt7bc4r+2sLnKUHOxw4dL7n9iw19NRCVZBfyWzu2aINA/V5Rz/d7xlVMFSfcOTY6erhh2T8cyway/MeZ5G+N0cbtV2eHTWR4xZRsV3e2LjdW818Zn9CUTvET1xtPv/pQafgnCdrsxJcJ409N839XY9z0WXjkZ9+tt7x+tKEnoyApJKOIDd3YOrWS78hCPI6g1IYQRAEeTGvQgrf6zx98rBL8tdm9F5rLTx7bJ+eg+/IzZvFaWGnDx8OTK2cnRpPT0ho7Bpsr85jOnzCN7F0ee6GEMtJWV2HnqHrwwNdxQXFI6P0WQwE9dkR+z/9RNXQvrKupa2pxsFI6Z//+JfnlULYNXd3kP/cAWlt+76BPl8Hrd/+7C+2fpkQH2ivffg0V2Zpw+3bN8oLC5rbByAy2k71zAXxKYZPfyT7W//77Te+OMTaPTZHi0IQ5McDSmEEQRDkxXR29chJv9QnNpammxRkv80nNp5M9PHzsvtkNdPCJE8WYgOcLjCf4xcQ4GC9oGPudn9+bfnBPWdjDW5eXm5OdklVwz6q6i1KCmY7z8TFKyAowKugYdwxNElmQPJ0aTbyst2ZE8eZL7DzcHOxsTDv+eJzIXn99sGbG2tL8b4Oxw4f5eEX1dNWZj3H4RGWAyZ3BlvUpAQhvaioMC+vaGxmFUSmXjLhF1WfYZgJcaM564O3/3RCUGeZFoEgyI8JlMIIgiDIi+ns7JGXEVqYqH+hFF6dbYGUXd9cCj9dnq2uKh8aIxYJZmTx0WRZfmZIcHBCatat+9SZEk9X+9sbYqPCI6LjOgdvkPOIn67M15blh4eFhEVEFlXUz85vfoNtZWGmqiQvMjwsMjqusraxMD2G/ewxNh7x/rHZpfnp/MykiKi4jp6+no62vqujpMnNwc6kuOjQ0NDk9JyRUUJb3x7srKptWWFYPHhhrJXp2H6jyym0MIIgPypQCiMIgiAvpn9gUEFW9N5IycbG8Ppq/3Ouj/RsbFx9dK9WRlJgaJj88MQr4lt91+JpXXG6sb5+6/B9WgQjL/3h5YacyKOHz+Q3jdDCCIL8qEApjCAIgryYyckpAz3tqCDbR/drwc3d28pRd6XEuGlpqkzPPKBZvt48mBp/tEh8vu5bE+ZiyC2ifufBEi2MIMiPCpTCCIIgyEuRl5cnIcprZ6Hm42nq7WHi7b7ZQby9pbqkGB8lK4tm89/P056W2rqmricM33lGEORHBEphBEEQ5KVYWlrKzMxyd3N1c3Xa3rmkpqU9fvyYZoMgCPJ6g1IYQRAE+QZMT8+Mjd0Zu3N3Czd2Z2pqmpYOQRDkxwBKYQRBEARBEGSXglIYQRAEQRAE2aWgFEYQBEEQBEF2KSiFEQRBEARBkF0KSmEEQRAEQRBkl4JSGEEQBEEQBNmloBRGEARBEARBdikohREEQRAEQZBdCkphBEEQBEEQZJeCUhhBEAT5HllfX6f5EARBXj9QCiMIgiAvxePHC5OTE1OTk1NTm90kg+eZm5h5MPPkyROaMYIgyGsJSmEEQRDkxczPP0pISPD28fX3D9jC+W12AX4BHu6ePT09NHsEQZDXEpTCCIIgyIsZHb3t5OxWUFJXWtFUUtYIrrS0obSksbSkoay4obyoobyQ6grqy/Pry/Pqa4tbXew9i4oKafYIgiCvJSiFEQRBkBdz4/p1L5+Aqfn1uaWNhwvrD8HzaH1ubv3Rw/VHs+vzD9bnZ9bnp9cXptYXJtcXxtefzq6nxGQVFxfR7H9s4AxnBNkloBRGEARBXszNGzcu+/iPTS9PzK6NzzyZmF6dmFqdmFyZnFidHF+ZvL8ydW95+u7y1J3laXCjy4/H1+Oj0opLimn2L8fs1P3hwf7unp6urq6BwZHF1TXajpfnu9OwK4sLt66PjE8/pIWfsfR47vrIyOz8Y1oYQZAfMyiFEQRBkBdDSuHRqeX7s2v3Z1bHZwgpPD65QlXDKxP3QQ0vgxoGKUyo4bHlhQmQwqnfUAo/CXTUZTnHxMvHz8XJqaBscnN6gbbnxazPzUxPPidb/y901+afP7xfWsfu3uwSLYpgJSfm0qEDB4PSy/CVQAT5LwClMIIgCPJiaFJ4evn+w7V7D1YJNfxsYHgCpDB1YJhQw9SBYXALk+vx0d9UCj9W5z1+8hx/cHhkSHBwXGLWzOMV2h4Gthv1TfX2dPFKpAWe8X9Zyq000fe3P/nJ79/ek984RIva2JifvKEndvYnP/mJomP4lsPCOLMCQX5coBRGEARBXgwphW9PL997uHb3werzapicJjH5bJrEwtR6QvQ3nCCx/khHhMXIPowWfMbK47mh/t7e/uHZedro7NO11ftjt3p7uoeu3VxafQoxU7cH1NiZz7Aptg8PT80urK6uTE7cf7xMVdJPV6fGxx8+WoQDPHww83Bufur+2PC16/PLxOyL+dmp/t7u/qGRBWqQkeIEnyP7j+7Zd9zyYjwtamOjvSJNlIP5+OlTCvahVCm8Pjs9Pkic3uCDR6Q2Xp+fe/jw0eOlhdnBvt7rt+8+QXGMIK8xKIURBEGQFwNS+JJPwOgMTQqDAynMqIYnaWqYmCYBanh+ej3hyjeXwqKsFq4xtCCV5bnxcG9HUSFBfgFhK7eAu9S5Cr3NxRrK8gJ8vDx8QkExuaBhMyOcvvjX397850cComLZ9X1jtwYtLfTru65B4icPx1yNdWOy6zc21pKCLhub2pnqK0srqLfcfPB48oaHnbGgoICAsNil0OS5JUJV0ymK8zrPLqmhqswroTGzTMatBTkbKqhoqymJyNoEg9Cevt3vYq4jIiTIzc1jZOs1Ng2n97Q0I97SxiMi7JKIAJygdFJ+I2mMIMhrCEphBEEQ5MUQo8K+/rcfLN+bW7szS0hhcmCYVMPPTxr+VlJ4XkeURc3Q497EzMTE+NTsI5CVeXE+vALikUlZaXHBAtwcvvGQ4WppeoiJnVt6FsXRVJ2ZTbDl6uSN7irxsyeOnRWKz8y8MfFouKv6wvmjGRXtkOvK5JDYmYNWvhkgZN31RH//v+8oaOgEh0dfvTsecclKWFo5NbswJsidk50rrbyDPBGSwtjL59jl42IjWM4zl/XchZiVB7fkhXh8wxNNNUSlrYOfbGz0NRWbGxjGJWdE+rmdOHw6LKMOkiX7W//l7+9pmlpFRUdK8bKc5ZUfuDNLzRJBkNcOlMIIgiDIiyGl8K3Z5buPCCl8hzowTJ8mMf7sFTr6pOH5mW8uhTce64qc27v/hI6egZa2zsXI9LnxG3qSPHJ6LrPLG2uLkxZK/DzihlPzT+Yf3Ls3+WB9Y6OzJvvEiYNxxYTkvaimJK3mQmZ0rauKm/tcdnUX+FenrsqznXYIooDWvmQk+P5+5pruWxD/YLSP9/xpa+9YULSPp4al2E4qmwWsUs1JQAqfZZVt6OyV52VyDMmGmP6qVDZ2gc6BIVM1YSnroKWNjcVHM6O3xpafrD+80y/JeljHKRySZQSb/fuL47n1g+Cvz4n88sv9CcWtRI4Igrx+oBRGEARBXgwxQcI34Obs8p35tTtzT8iBYUY1TFtQ4tmk4UcP1uNjvoUUZj52ksXWwcnG1i46q/RGbxP32UOcIvIXPT3c3d0keM/ziumMz61trM3npsV5eHiaGWh8vPfL6PwGMHZVkhNXciBXdQApzMNzLqeGlMLX5NnPOASBll131+PhVzIhl6W42l56cO+H0iq6kLebmyPn2WOKBt6PGVaFACl86oJ0z9hcmI0av4zR4sZGqKOWhIbd2sqcsYoASGGIgTxbq4u9L3q6OFic3POOln0QRKUGGDIJKdxdIuYITw42cR86eSkGPzWCIK8pKIURBEGQF0OVwv43ZpfH5tfG5lbvPNxaDdNfoXs0ux4fm/4tJkhoGF+afDD3YGZm6cnGzd6Gs8f3C8tpBPj7eHp6ujg7JedWLj5eSAtxYecWsHe9bGdh8On+PVGEFF53UZSTYJDCXNzn8uqIzz4/nb2pwn2OHBUGKSymbkFK4d7Ggv17P1TVN/f39fbw8HB2ds6raGN8xQ2k8IlzEgPjqz1lcSfPcOeXVSiLcbhfKQJ1baDIR06Q6ChL5ePk0DOz877oyHTwP5qkFPY3vCChep86wjxztU3wKJNnVD4RQBDk9QOlMIIgCPJiSCl8fXZ5dH5tdG4V1PDdh6v0ScP3nps0PDe7Hhf7bV6bM3GMpAVBR97o4j51zNg9enV9fXlpcWmJGId9eP+6KPcpHcdAUKJD7RUnzxyKyK2jSmEpSVUn0nCkq+o88/GMakIKL97p4T+23yk4h5TCIqpmj6hpbnVXHzl4MDClFOKXFonMV9e+togEIYWZxPrGny5ODPKynOXm5eXgEa7pvwdSWE+BV8Yu7OnGmreZGLOwysj06vLMdQ2hI2o2/mCYFmh8mE342iwhq2+2FZ04cCQko5qaJYIgrx0ohREEQZAXQ06QGAEpvLB2+9ETUMN3vq6GQQoTavjZpOG5uW8jhdX5z+pYBtJHZteXpuw1xc8LKrcP3ZyZmhjsaR+bmJ26d12A7ZiRW/jqk+WCON/3PvkwlFIDiQPNlM6yS3VdH11cXZ++3c/DdEjbwW/s7p2kAMe3fvsHpxBCCjtrsvIpGJJSePXBbXnBC5I6ttfHxqfG7/V0tN4Zf0DdQyMvyuPAcYH20ZWNjSVHHZGf/OQn/Co21KUkHmtIsYtZBq9tPPE0FGQW1bz1YGWko4z5y7eULX1hNyXc6o9v/sc9LPXuvTFfG409h8/X9NymZokgyGsHSmEEQRDkxTyTwiu3F9ZuPVq9/YgYGCYnDTO+Qnf/2aRhQgrHpRcXf7MJEqZy/JauUQyTFDZ6GnL4OZm5+MXU1VSE+Hmv5FSvPV2N9DBhusBpZGYuJ8R39MyFuEJitbL6vKgjB/YJiknnEPMiVkNcDQ8dOS4pK6+sKHP62BkvYmLDupeZmKyu7TyZ9cZGRWYIK/MZIXE5VWUFQQGhwvpe2g4qpUl+LFxyXaPEUHRJos8bv/uDazjx8tzGxrypuriqc9TTjY06SiTzmbPqusYG2ipH9n1h4UEMaWeFW/3j3fe4haXkZSVPnzhp4hS0wDAFGUGQ1wqUwgiCIMiLuXnj+iW/wJsLT+8sb9xZWr+zuH53cf3+4/VxcPOEm3i0PvlofWpufXpufWZ2/fHiRkJiVnExCNCXZn2lhJJUWtfJKIU31le7GssuubvYOzoHR8QO3rwHcQ/v34wJC/C47Eeh5FCysrqujkLk0vxUypVgO0eX1kFiCHZ24mZkwGVnN+/Kuvr8nNz6jqsQ2VCalZFfSdelT1fma4sprk4ODs6u0fGpoxNfW/Lsem9TXGLm+CNizu/ErcHI0LC+68SSahsbKyXZqYW1HXCeq48fFmXEu7t7pmRlUzLSq+o6YXdKgMlpHtFESt5FF4fQmLQ7U3NUKwRBXkdQCiMIgiAvBqSwy0Wv7vsPBifnB8bnBscfDo3PDY8/vDo+d+3+3LW7D0fA3Xl4/c7DG3ce3hx7eG9iOTwioajom62csLy0uLyyxQjqyuL8zOxDxg9gPH2yPPuQmOmwvra6+uTZHN+nq3OPFp4+k9KQZn6B+CTHk9WVlVUi25Xl5cWlzR9zXnj08AE1Kypf6fC11ZXFxUVabutPV1ZX6TkvLy5CkBbYWH84+5B6Busrq5DiaZyXzikB+XvLG6uL85s/YYcgyGsGSmEEQRDkxdwZG7N1dI6j5CXnFSflFiXnFKVQXWpOURq47KJ0cJSijGcuO7fM2cWzsrKCZr+LWEsLtuKU1iAXU0MQ5DUHpTCCIAjyYubm5jIyM9Pgj5LN6LK2d4mJSSMjIzT7XcR6f0tFfFruPM4PRpAfAyiFEQRBkBfz9OlTUMMz09MzMy/npqcfPHiw+tUsgl3E6srK48c4JowgPw5QCiMIgiAIgiC7FJTCCIIgCIIgyC4FpTCCIAiCIAiyS0EpjCAIgiAIguxSUAojCIIgCIIguxSUwgiCIAiCIMguBaUwgiAIgiAIsktBKYwgCIIgCILsUlAKIwiCIAiCILsUlMIIgiAIgiDILgWlMIIgCIIgCLJLQSmMIAiCIAiC7FJQCiMIgiAIgiC7FJTCCIIgCIIgyC4FpTCCIAiCIAiyS0EpjCAIgiAIguxSUAojCIIgCIIguxSUwgiCIAiCIMguBaUwgiAIgiAIsktBKYwgCIIgCILsUlAKIwiCIAiCILsUlMIIgiAIgiDILgWlMIIgCIIgCLJLQSmMIAiCIAiC7FJQCiMIgiAIgiC7FJTCCIIgCIIgyC4FpTCCIAiCIAiyS0EpjCAIgiAIguxSUAojCIIgCIIguxSUwgiCIAiCIMguBaUwgiAIgiAIsktBKYwgCIIgCILsUlAKIwiCIAiCILsUlMIIgiAIgiDILgWlMIIgCIIgCLJLQSmMIAiCIAiC7FJQCiMIgiAIgiC7FJTCCIIgCIIgyC4FpTCCIAiCIAiyS0EpjCAIgiAIguxSUAojCIIgCIIguxSUwgiCIAiCIMguBaUwgiAIgiAIsktBKYwgCIIgyA/P3bt3p6amaAEEeVWgFN693Lp1y9jY+Kc//elPfvKTI0eO8FFhZmb+7LPPIKaoqOjRo0dvvPGGpqYmzeDVsrKyAqf35z//GU5m//795eXltB0vR3FxMScn56FDh7i5uQ8ePCggIFBbW0vb9/3zAxZdYmKinp7eJ5988tFHHw0NDdFiqSQnJ587dw7KE350KNve3l7ajm1gZ2eHm4EW+L8xMjKirq7+7rvv7tu379SpU/CLQOHA/VZZWUlL8Z2ytLTk7e0NN7OysjIZ4+7u/rvf/e769etk8BXQ2toqKSkJpQ0wMTEdP378nXfegW1gYODa2hot0WvDqy+fnUlNTRUREdmzZw8tvD35+fmqqqq0wKvi21Xw769ZePDggZmZGSkit6vmcNdduXLl9OnTUA3hPoQ6yMXFZWVlxc/PD3svXrz49ttvg9Wbb77Jxsb25Zdf/vvf/5aXlx8YGCDNn2e7kg8LC4OfjxZ4aXJzc6FlgBPIysqiRX1H7HBvZ2RkiImJvcxthvx3g1J4twMNIrQ+N2/epIWpaGlpQXsEeuLMmTMeHh602FcLyAhoiMPDw/X19f+HygulGx1DQ0NIn5CQQAbX19cjIyN//vOf29nZkTHfNz9U0bW3t//lL395+vTp+Pg4CwtLc3Mzbccz+vv74eeGRwtaeEd0dHQEBQVpgRdx9+5dmu85QJ3/5je/4eHhoaeZnZ2Vk5P7Pro9kidPnvT19cE9AEchY2JjYw8fPnz//n0y+O3Y4Rq3BMQH3HW///3vySCcFTylwFW/eun2Qr6T8vkOGR0dhedYUGO08PaAbvv1r3/9iocSX76Cwz0AlZH0f0/Nwr1796Cyw5YW3qqaQ42D5zG4FZOSkqB9ICNLSkr++te/7t27lwza29uD1eXLl8kg1KC//e1vv/3tb69evUrGbGKHkndzc6M3vy9PWlra99EmPH9v0ysy3Gaffvrpy9xmyH83KIV3O2fPnoXWZ5MUHhoa6unpoQV+COAEQI7TAlQtBSdpbGxMC+9ISEgIJH5e9YI5xF+5coUW/m/E3Nx852YdfmgoBPjRaeHviMLCQgMDA1rg69TV1f3sZz87evTopqFQeD7h5ORMTk6mhb8H/vjHP9Kl8P+dsbExEBy0wEsD5wDQAhsbi4uLv/zlL3/xi1+AKqJFIdvAx8f3Qo0yMDAAz35wSzs5OdGiXjMsLCy+p+c9EqhWJ06c2PQvr+erOchWiMnLy6OFn9HU1ARakPR7e3tDGtiSQcDa2hpiYEsLM7BzyUPtZmZmbmtro4VfjsrKSsjwey0uYFNFhlJCKYygFN7tPC+Fu7u75+bmaAFqo0bzvUKgTWQcbFhdXYWTVFNTo4W3B1L+9a9//elPfzoxMUGLesatW7cgk3/961/0QZHvm1dZdHBRs7OzIiIicIHgAbY8+jeVwi9zCcPDw//7v/+rrq4OB11ZWaHFPuPkyZNwxIKCAlqYgfLy8ujoaFrgO4U8bdCgm6Twt/5F5ufnQc1/8cUXcI2PHz+mxb4EVCX8lRQG4fKrX/0KCuTRo0e0qNeJV3nH7gB5Gi8jhTU0NCgUyttvv/33v//9+Xvv++aFxZWamgq/dXx8PNw29Gbnuy3kwMBAqGK0wDM2VfPS0lIIHjlyhAxuQkZGhvQ8L4WdnZ0hxtDQkBZm4IUln5aW9tlnn32jxvZ7lcJksT9fkRml8Gty/yOvHpTCu53npbC0tDQEHz586OLiAk/2bGxstB3Uf5nx8PBoamru2bNHVFT04sWLQUFBt2/fBj+9CUtOTn7vvfcgCH7IJDg4GFrqsLAwCQkJUKiRkZEQDykhCI/m0Iy6urq+sAGamZmBDKOiomjh7ampqYGU9HGOTUCTB3s7OjpKSkr+9Kc/gR9Oj9xlbW39+9//vrGxEfzPn97zF+Lj40POBD127NiNGzfACh4hoOmHyC2L7vk88/LyIJ8///nP1dXVd+7cYWdnh9yEhITI3GJjY3/+8583NDSQ5oxUVVXx8/Nra2tzcHAwMTGRaeDJAWQf6ODf/va34AG2VGzPS2F4QoATg8cM+N337t1LHzUPDw+HkwGNSwYjIiLgcuCHht/3rbfe+tnPfgbaF+Khq4MzgTyhzOGgm0anxsfHYdfLDIKGhIQICgrq6OhAhw1lCAVCxi8uLkJPDOcGh/vyyy8tLCyWl5chHn4p6IwPHjwIevpvf/vbf/7zHzgT6JJNTU2FhYVhF3Twv/nNb+CUIDFIAWVlZUhDTk0GE8gQgp2dnQcOHIAzhF8WukbieNRxMsjBwcEBpBgXFxcZ7+fnBz8clAZkCCUDMXBPwtnCs8f7778P5Tk4OEi13swmKQz3MBzuxIkTtDBVHLu5uYmLix8+fPjDDz/Mzc0l4+EOgXOA/IF9+/bBhYNGgVt3ywvf7mRAisGVQv5wmfRZGc9HbiofYLu7Yrvb4Hnu378PtnDa0Fzs378fihoih4aG4NeEaghHhEKG+x/u2Pb2dtIEgEcjTk5OLS0t2Pv555/vLIUfPHgA5waXTyq2TY9V8DwPpero6Ai5wU9ANnFbRgJb1imS1tZWKSkpuAooWLCdnJx8voLDE7ilpSUUBVwvyCxybgA0jGSlPnfuHNw2UKTPNwtbHveFpcTIu+++C5dPCzxjUzWHGwaCWw7uMrJJCoNqJN8bgdaSjKGzc8mTwMMe3B4ZGRm0MANbVjFgkxTergrArpdvFoqLixnv7ecrMpQSFDW0xlB34OjQ/sBPDPF1dXVKSkpvvPEGFCacJ/wKkKy3txdqKPzEkBKOAuUAKZH/AlAK73agIYBaDQ/K4Dl9+jSpYskeAjpp6O3o7SmombfffptsPq5duwZNAzTr5H/cysrKGJswXV1dCIIHukN/f3/wQ98PTZKsrGxBQUFaWhq0g6T8DQwMhL3Qv1LttiUmJgY6g4WFBVp4e0BqQ4asrKy08NcB2Qp7yVc6yHaf/h9DaLJtbW3Bs+XpbXkh0By/+eabhw4domZAcOrUKbJZ31R0210yNPH/7//9P/K6RkdHoeegz+uANpc+WsMIaE0Qu8PDw2RQT0/vl7/8JX1O8AsH0jb1kdPT0//4xz/oQgd+QdhL7wvJJxzSD4bw8AC9C9wAcI36+vqwq6WlhdwLfjgT0s8IdO2wC24bWngb3N3dodMltTt0adAbwVWQJQliQl5enpqKOAeQL3CN4AdlAF3gH/7wB5A1oC9Bt0EkdI30QgPtBYcmpTAAz2wQJLtDeGy4cOECCHRzc/OJiQlyhqKnpyeZ8q9//Sv5YtOTJ0/gcCBfyHgoNPqsStgFOrKrqwv8UC/g5oS+FiLJvYxADvATx8fHw7OTgIAAPN7AocfGxmi7NzZABkH3TPrhfoCzIoUsREK2oDbADxcFZwiPYSMjI89f+HYnA+X561//GhQYxMMNDP06eLaMBBjLZ4e7YufbgBG44envXILcIf8lDUoUahmYgP6De3hgYABECWhfMhlUio8//pi8DUCHwTnsfDPDo7ivry94QLvARUGxkPEkcCD6nWNgYEBWmS0jd6hTINrg+YTURuQEXFL6b6rgAQEBsItMBmf1u9/9jlRmm7TdJqvtjrtzKTECtwoko1AotPAzNlVzaA8hCPcPGdwOskmEKgMprays4Eb6y1/+AjcGbTcDO5c8HZDpCgoKtAAD21WxTcW1XRWA4DdqFhjvbYCxIgMQhEuAAocnnOzsbEgJfoiHx0toTyAIOhsqLFQWKA2oWdARwJMPVDfoAV1dXclMkB87KIV3O9AQQG2H1gT8UMNv3LgBspgMAtBk0NtTeM6GlPRRK5CV9P+4bWrCyHeDSD+pSOjqCvj0009BUEKDC0D7BXt5eXlp+7YCOhVo3UBt08I7srMUhhOGvSkpKeAHIfvnP/+Zh4eH3AVnQs6p2O70nr8QwMPDAyLJARvowxjlIGPRbZdnVVUV+OlPAu+//z40tWTTDyIDumEynhEmJiYQ3LQAdXjmf/7nf+hT3yDzbySF7e3tQX8z/n/z8OHD9I6c8XcEIGe6IVws7AKFRwbBz3jtdKqrq2EXKCdaeCtA+oBYZByyys/PByvoIMnyYfzpTUxMIKa+vh78cLEgEcjiAjo6OmAXY6FBjwgFTvpJPUfvDiEegqQf+M1vfqP8bK0JIyOjoqIi8ECxgIhXVFQk4+Ha6T0o3EJwztTfkwBKBnIjx6s2AZ30r371K+gy4WSg79w0ag79K0QKCgqS+UDVg3y8vLxgFzyXHjx4kEwG5wPx9Ekmmy58u5MBRQU/LmgRkJWQjCyZLSMBxvLZ+a7Y4TZgBGoieSEAyGJ4xib90dHR9AMBHBwcH3zwAXjgcKCQ4KGIjAegjuxwM4OshGaBPs8EfibItrS0lAwCxsbGoF3gDgQ/SH8o6u0id6hTEO/g4EDGA/B4QB+dZazg8MgH2g7aT/CTshiebMG/qWEEGK12OO52pbSJ5ORkSMY4hk2yqZqTC0psKWoZIaUwHEtISAg8bGxs9HuMkReWPJ1Dhw7R72FGtqtim4pruyrwjZoFYFPdZ6zIAATfeecdWmBjAxorMTEx0m9nZweG9EFrKBnGG/Lvf/+7lJQULYD8yEEpvNuBhgBqO137AomJiWQPATA23NAHQEp42iaD0L1xcXGR/k1N2PNSmP4fNBCgEExLSyODL4OhoSE5reJlICdIfP7557Tw13nzzTdhL/1lDnNzcxAicO3QKUpLS0PMDqe36UJIZmZmQIWQIx9KSkr0AR6AXnQ7X/Jnn30GDxXgKSkpOX78OKQEz8LCwvnz58kEmwBdBW09LUAFegvo3Un/N5XC0NuBViP9JNBLQQJysY4dpPCm0gD/llIYChZ2/fznPyeF15Y0NjZCGsZnDEgMMdAfu7m5gQeORdtBXXEJYvz9/cG/6WLJxxLGOeJwaSANSf+m7hDiIUj6AcaUwPXr15OSkkAw/fOf/6THw7XTe1BTU9Pt7rFNQM4AeDIzM+GIH3/8MYhRchcAXTtEbvkOPjykgfokh0j7+vogWWtrK7lr04XvcDIg48AQ+mx43CL/KbFdJGP57HxX7HAbbAI0BBQj3M8CAgL0E94k8ujXQqpq8jGVZOebOSMjAxQPnAkJPG2COePQKYhRkDgQCTcSvX3bMnK7OgVCDdqH7a6OsW0Enjx5QqFQ4Acl12EgM9/UMAKMVjvU5e1KaRPkK8KMFYRkUzWXl5eHoJmZGRncDlIKwxYuHB5+wB8aGkrbx8ALS54OMzMzPNvTAl9nyyq2qbi2qwLfqFkANtV9OGd6RQYgyJge/PQfZZMU3pQzY0rkxw5K4d0ONARQ2+m9wiY2NffQZr3xxhvQHvX09HzwwQd0TbmpCdtBCoMOgCD0FmSQZG37ZVYDAgJeOJjBCDm2BB3Y8/ICdCoc+q233qIPGIyNjf3iF7+AHsLa2rqurg5idji97Xp9dXX13/zmN1CAm5pFetHtfMnkvAsoT1ZWVuinP/roIxERkeDg4O0mRv/617/etBQaHIU+A+H5bmATm/pIci4jozgjW39y+aT/uxQGPvnkE9i7wzqjTU1NkGDTAhQQIykp6e7uDp7s7Gxa7LM7jfxX76aLhQcb2HXt2jVa+NtKYUtLSykpKXK2A+RPj4drp/egcLbwCER20nS2vI0JIfxMVsJDHRwUHiDpdyD5D9mqqioySELmA5Xrz3/+s7a2Noh7OBzjbJlNF77zyeTn58NNBUcBK/pxn49kLJ+d74odbgNGQPqDnIJnRfBDGdJPeDuRRw77kfOvSDZd5ibOnDmzaXXFL7/8EnJgnLT94MEDDQ0NeBL77W9/S44Ebxm5XZ169OgRZGhhYUGL/TqMbePAwAAcnRwwJi/wZaTwDnV5u1LaREJCAiTbdP8Am6o5OXgMz0v0J58toUth8MNDLOjdX/7yl5v+jwG8TMmTwAmAUKYFGNiuim0qru2qwDdqFoBNdR/Oil6RAQgypgc/5ED6UQrvHlAK73agIYDa/pJSGHSGiorK5cuXQZ4yLmNJtkTp6elkUFNTE4Kk//nOEtr6f/3rX/SOFnpiXV3dLV+rio+PZ1yDE5pO8rW2nSH/QUkfvaYDknfTmQDi4uIgnTk4OGjh7U9vu16fnK63Z88exg4PYCy6HS4ZIqFLPnXqFLk+hpeXF3Q/R48eXVxcpKbdDAsLy89+9jP6SqUAPJPQO4ntukwAdP/CwgLZRwoLC5ORLi4uEGQcsVZWVn7nnXfILvM7kcIpKSmwF0oDnlJoUc+YnJwEAQRX+oc//IFxXPPOnTtgEhkZWV9fDx7GZfXgloDnHFLvbrrYiIgISExOYSRhFLibukOIhyDpB+gpySFqkHFk/D/+8Q96DnDt9B4UOl1IxjiSDd02/RVMRqhKmCaFV1dXyfU06LNBQFxCkLFDhX6Xvmigg4MD9Pqenp5QSowiZtOFb3cyd+/ezcnJgSBUHPI5Aa5uy0gIMpbPznfFDrcBHbjS3/3ud3CTk0HQPfQThsT0AwH0axkZGYF4AQEBMh7Y4WYG0ck4R5+EnIVPfzswJCSE9IBOhdpHvqy2ZeQOdQqu+s0336Q/V0Oh0Wd9MFZwyIf+mQZQ83AaLyOFdzjudqW0CXIuPmP+W1ZzOG1SrTIOutOBH5psixilMEDOU/r73/9Of4cVeJmSp7Nv3z5ubm5a4Bk7VLHni2vLKvCNmgVgU91nrMgABBnTgx9yIP0ohXcPKIV3O9AoQG2HLo0W/joffvjh6dOnSf/09DR0DHl5ecPDw6Ojo4yrQd2+fRsy4eLiunHjRnJyMvkvM0gPu8jOknFkl3zpGI4bExMDTTMrK+uWQ4bQfh0/fhy6BBLo76FVJd8N0tfX/9///d9b1Fd/ngdaTAMDg1/96lf0bCEGdNLPf/7z5wd4yCFJcjiBZLvTe/5C6LCzs0O3umlQkLHodr5k6ELg3KAMwT8zMwNnbmRkRO56npaWFtDKdJEBvQIUBf1DSqDpt5yYCz/W//zP/4DOIP/PmJiYSMY/fPgQGnTo20id+uDBA+j46S99w2MPJKYPJUJKei8OpwG76LOcf//7358/fx46VPp8VkacnJwg8ZkzZ8ifD1heXobeC8qNfIHMx8cHEtB7R5Bihw8fJk+Jn58fRBVZOPA7QtHRNTf0Q4wXC9cC3TaURnl5OZwz5AY6Y//+/eT/LsjxM9hFJt4khf/f//t/5AwZcpgWHpCgSP38/H7zm98wMTGRw108PDyQORQRSEnYwhMU5G9qagomrq6u586dI6fSMgKKEHpo+L3oZQiq4o033oBD0IXmhQsXIAg/DRQ73IdHjhwhe3RQACIiIq2trSBr4JmBnIdKsunCoave8mTAkJx4CkA5QwKonltGgp+xfHa+K3a4DehADhD/ySefDA0NwQ/x3nvvQQlDkcKpQl2GXXRdwsnJ+c9//pP0k6PR5GtJvb29IJJ+/etfFxcXg5IjE9AhJ8/QAs+AzKG0wYR8SoefmJw8CigpKZGzmLaM3KFOkf+0AYUKj1hw5lB09BnhjBUcbjOottXV1WALFQFMQEdCI0lOKgM9NzExQc7oZbTa4bg7lBIj0OaACf2jGNtVcwCy/eijj+B+BqVL/wcC3GmKiooXL14kg/CEBlaM6wRbWlpCDNyTdJOXKXkSuOfhEZf+5EBnhypWWFgIu+hSeIcq8PLNArCp7jNWZLCFmxn6NXIX8NZbb9FfICFfXiT/swHw8vJCRSD9ALT5O7/lgvyIQCm8ewEpSU6WhdoOzQHjszgwNTV16dIl2At9WHBwMDRD0BiRi8jQgR6O/u8zaO6hbQJJZG9vD37oM0DzQUcoJiYGKffs2UMfkIDmG1pY8ovKkOGm45JUVFRAJ0E9yFeAMib3amtrQyNLduHbAQ0fSGdoxKFdO3r0KDSdz/8bkQSSMa5NseXpbXkhdKAFZxy9fr7odr5k6PUZB1SUlZV3vrTGxkaQlXBdOjo6kpKSZC8C4gNkOvwEcAg7O7vu7m4yMR3ow6BIQd/AXugAaLHUDy9JSEgcO3YM+hJRUVHowsl46Izfpn6IFQQW9HCg0kCvg+SCywcTcnEJ+r9KoROFvXBp/f39pPkmoD+Do0BHAmr1wIEDIBegF2Ts2+AhAR4VZGVl1dTUtLS04HLIeFBjcJdCsWtqaoJahYIlTz48PBxOBs4BDk1qRwBO5tSpU1Dyf/zjH6EfBdkBRQSHhpsB4iExdF0gZUBhQOcHQUgDB4LbFfxwsfA7winBEx0IxC+pb2rCwwD4yWFaeAj87W9/C0KW/O8H5AM3JBwLIkFRQc9KPYWvgOOSghuAMydn4AClpaWQJygnuCXm5+fhboHChCAUIMhi+r+eoXx+8YtfkOYA/HagWkBebHnhW54MCAUQo3B7gziG+wRKeLvITeUDMdvdFTvfBoxAqcJFwS8OFxIbGwvnBj863Jak9IcfGvQNyGvybUJ4FgITUFRwSmAFFyslJQWaBvIH5UR/kCCBGxJyAEHJWKNBr8CvCVnBLigKOJCJicnnn38ONw/c8KCooPmCZFtGAlvWKRIQmnDTQrb79u0jf8TnKziUz1/+8hcIwq0Lj15QDd99912oC3DmzMzMkBJyBmW8yQqy2vK4w8PDO5TSJuBC6O9sANtVc2BxcRFU78GDB6F2fPrpp/C0CXcUvaGAywRtBweFJxBIRj4FQcNFvnIHRQrV4SVLnoyHcgCZSw6IMLJdFYOigF4DMgHdT77NuV0VgF0v3yw8f28zVmR4WiDLFi4fmgJyFvKf/vQnUMlgSPZ3FhYW0ACmpqaSD7Genp5wA8CjEZw2aG7GQQ3kxwtKYeRlGRsbg8Ya+r/29nYQE9BFwRM/PhYjyPcByBHop69evQryq7i4ODk5mY2Nja6nEYRkbm4Onm2gWaaFXxvg8Z6+QOG3A6sA8spAKYy8LPv3709KSqIFqKytre3wr3wEQb4d8Jz58ccf0wLPiI6Ops8wQRA6HR0dUlJS0BrTwq8BFRUVgoKCm4bzvxFYBZBXCUph5GX5/e9/f+DAgbKysvHx8dHR0fz8fHl5+e3+IY4gyLfm8uXLP/3pT52dnYeHh6empnp7ez09Pa1f9LUwZNdSWVlpYGDw/Gz1H4TS0lILCwtyisW3BqsA8ipBKYy8LHV1dUxMTL/61a+ghfrggw+MjY3pM+0QBPkOARlhamr6z3/+8yc/+cnvfvc7VlZWctkHBNmO8fHxHVbvfpXc3GY9om8EVgHkVYJSGEEQBEEQBNmloBRGEARBEARBdikohREEQRAEQZBdCkphBEEQBEEQZJeCUhhBEARBEATZpaAURhAEQRAEQXYpKIURBEEQBEGQXQpKYQRBEARBEGSXglIYQRAEQRAE2aWgFEYQBEEQBEF2KSiFEQRBEARBkF0KSmEEQRAEQRBkl4JSGEEQBEEQBNmloBRGEARBEARBdikohREEQRAEQZBdCkphBEEQBEEQZJeCUhhBEARBEATZpaAURhAEQRAEQXYpKIURBEEQBEGQXQpKYQRBEARBEGSXglIYQRAEQRAE2aWgFEYQBEEQBEF2KSiFEQRBEARBkF3Jxsb/B0IglNCp37/SAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋의 규모와 모델 학습 환경의 한계로 인해 충분한 학습을 하지 못하여 두 모델 다 만족할만한 결과를 얻지 못한 점이 아쉬웠으며, 시간 관계 상 학습을 진행함에 따른 모델의 learning curve를 그리지 못한 부분도 아쉬움이 남았다.\n",
    "<br><br>\n",
    "만약 시간이 넉넉하였다면, 개인적으로 Hourglass, Simplebaseline 모델 뿐만 아니라 다른 최신 모델들도 같은 학습을 진행하였을 때 결과가 어떻게 나올지 궁금하다. 특히 올해 7월에 올라온 논문인 'Joint Coordinate Regression and Association For Multi-Person Pose Estimation, A Pure Neural Network Approach'(https://arxiv.org/abs/2307.01004)에서 다루는 JCRA 알고리즘은 한 번 시험해볼만 한 듯 하다.\n",
    "<br><br>\n",
    "간략하게 논문의 내용을 설명하자면, JCRA는 1-stage end-to-end 방식으로, 히트맵 없이 직접 키포인트 좌표를 출력할 수 있으며, 트랜스포머 쿼리를 통해 해당 키포인트의 연결을 수행하며, 69.2 mAP와 함께 최신의 bottom-up 알고리즘 보다 78% 빠른 inference acceleration을 달성하였다. 다음은 JCRA 알고리즘의 개요를 나타내는 이미지다.<br>\n",
    "![제목 없음.png](<attachment:제목 없음.png>)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
